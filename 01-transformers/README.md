# 1Ô∏è‚É£ Transformers

## Overview

This section provides a deep dive into the Transformer architecture, the foundation of modern Large Language Models. Topics include attention mechanisms, positional encodings, and architectural variations.

## Key Concepts

- Self-attention and cross-attention mechanisms
- Multi-head attention
- Positional encodings (absolute and relative)
- Encoder and decoder architectures
- Layer normalization and residual connections
- Feed-forward networks
- Transformer variants (BERT, GPT, T5)

## üìÇ Directory Structure

### [Architecture](./architecture/)
Detailed exploration of transformer components and their mathematical foundations

### [Implementations](./implementations/)
Code implementations of transformers from scratch and using frameworks

### [Experiments](./experiments/)
Hands-on experiments exploring transformer behavior and capabilities

## Content to be Added

- [ ] Complete architecture documentation
- [ ] Implementation examples
- [ ] Training notebooks
- [ ] Visualization tools
- [ ] Performance comparisons

## Key Papers

- **Attention Is All You Need** - Vaswani et al., 2017 (Original transformer paper)
- **BERT** - Devlin et al., 2018 (Bidirectional encoder)
- **GPT-2/GPT-3** - Radford et al., 2019/Brown et al., 2020
- **T5** - Raffel et al., 2019 (Text-to-text framework)

## Further Reading

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)

---

[Back to Main](../README.md) | [Previous: Neural Networks](../00-neural-networks/README.md) | [Next: Fine-Tuning](../02-fine-tuning/README.md)