{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üî≤ The Convolution Operation - Heart of CNNs\n",
    "\n",
    "Welcome to the most important concept in CNNs! üéâ\n",
    "\n",
    "In the previous notebook, we learned **WHY** CNNs work (local connectivity, parameter sharing, translation invariance). Now we'll learn **HOW** they work by understanding the convolution operation!\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- **What is a filter/kernel** and how it detects patterns\n",
    "- **How convolution works** with step-by-step animations\n",
    "- **Implementing conv2d from scratch** in NumPy\n",
    "- **Different filter types**: edge detection, blur, sharpen\n",
    "- **Stride and padding** and how they affect output size\n",
    "- **Feature maps** and why we use multiple filters\n",
    "- **Visualizing** what different filters detect\n",
    "\n",
    "**Prerequisites:** Notebook 01 (What are CNNs)\n",
    "\n",
    "---\n",
    "\n",
    "## üé¨ The Movie Analogy\n",
    "\n",
    "Think of convolution like watching a movie through a small window:\n",
    "- **Filter**: The window (defines what you can see)\n",
    "- **Sliding**: Moving the window across the screen\n",
    "- **Output**: Your description of what you saw at each position\n",
    "\n",
    "Different window sizes and patterns reveal different aspects of the movie! üé•\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.patches import Rectangle\n",
    "from IPython.display import HTML\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üì¶ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-is-filter",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç What is a Filter (Kernel)?\n",
    "\n",
    "### üéØ The Core Idea\n",
    "\n",
    "A **filter** (also called **kernel**) is a small matrix of numbers that slides across an image to detect specific patterns!\n",
    "\n",
    "**Simple Example:**\n",
    "```python\n",
    "filter = [\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1]\n",
    "]\n",
    "```\n",
    "\n",
    "This 3√ó3 filter detects **vertical edges**!\n",
    "\n",
    "### ü§î Why These Numbers?\n",
    "\n",
    "- **Left column (1, 1, 1)**: Look for bright pixels on the left\n",
    "- **Middle column (0, 0, 0)**: Don't care about middle\n",
    "- **Right column (-1, -1, -1)**: Look for dark pixels on the right\n",
    "\n",
    "**Result**: Responds strongly where there's a bright‚Üídark transition (a vertical edge!) üéØ\n",
    "\n",
    "### üìê Filter Properties\n",
    "\n",
    "- **Size**: Usually 3√ó3, 5√ó5, or 7√ó7 (always odd numbers for symmetry)\n",
    "- **Values**: Can be any numbers (learned during training!)\n",
    "- **Purpose**: Each filter learns to detect a specific pattern\n",
    "\n",
    "Let's visualize some common filters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-filters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common filters\n",
    "filters = {\n",
    "    'Vertical Edge': np.array([\n",
    "        [1, 0, -1],\n",
    "        [1, 0, -1],\n",
    "        [1, 0, -1]\n",
    "    ]),\n",
    "    'Horizontal Edge': np.array([\n",
    "        [1, 1, 1],\n",
    "        [0, 0, 0],\n",
    "        [-1, -1, -1]\n",
    "    ]),\n",
    "    'Diagonal Edge': np.array([\n",
    "        [2, 1, 0],\n",
    "        [1, 0, -1],\n",
    "        [0, -1, -2]\n",
    "    ]),\n",
    "    'Sharpen': np.array([\n",
    "        [0, -1, 0],\n",
    "        [-1, 5, -1],\n",
    "        [0, -1, 0]\n",
    "    ]),\n",
    "    'Blur (Box)': np.ones((3, 3)) / 9,\n",
    "    'Identity': np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 0]\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Visualize all filters\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, filt) in enumerate(filters.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Display filter as heatmap\n",
    "    im = ax.imshow(filt, cmap='RdBu', vmin=-2, vmax=2, interpolation='nearest')\n",
    "    ax.set_title(f'{name}\\n({filt.shape[0]}√ó{filt.shape[1]})', \n",
    "                fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Add grid\n",
    "    for i in range(4):\n",
    "        ax.axhline(i - 0.5, color='black', linewidth=2)\n",
    "        ax.axvline(i - 0.5, color='black', linewidth=2)\n",
    "    \n",
    "    # Add values as text\n",
    "    for i in range(filt.shape[0]):\n",
    "        for j in range(filt.shape[1]):\n",
    "            text_color = 'white' if abs(filt[i, j]) > 0.5 else 'black'\n",
    "            ax.text(j, i, f'{filt[i, j]:.2f}', \n",
    "                   ha='center', va='center',\n",
    "                   color=text_color, fontweight='bold', fontsize=11)\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle('Common Filters and What They Detect', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ What Each Filter Detects:\")\n",
    "print(\"   ‚Ä¢ Vertical Edge: Bright‚ÜíDark transitions (left to right)\")\n",
    "print(\"   ‚Ä¢ Horizontal Edge: Bright‚ÜíDark transitions (top to bottom)\")\n",
    "print(\"   ‚Ä¢ Diagonal Edge: Diagonal boundaries\")\n",
    "print(\"   ‚Ä¢ Sharpen: Enhances edges and details\")\n",
    "print(\"   ‚Ä¢ Blur: Smooths by averaging neighbors\")\n",
    "print(\"   ‚Ä¢ Identity: Returns the original (no change)\")\n",
    "print(\"\\nüí° In CNNs, filters are LEARNED, not hand-designed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "how-convolution-works",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ How Does Convolution Work?\n",
    "\n",
    "### üìù The Algorithm (Step-by-Step)\n",
    "\n",
    "**Convolution** is simply:\n",
    "1. **Place** the filter on the top-left of the image\n",
    "2. **Multiply** each filter value with the corresponding image pixel\n",
    "3. **Sum** all those products to get ONE output number\n",
    "4. **Slide** the filter one position to the right\n",
    "5. **Repeat** steps 2-4 until you've covered the entire image\n",
    "\n",
    "### üßÆ The Math\n",
    "\n",
    "For a 3√ó3 filter at position (i, j):\n",
    "\n",
    "```\n",
    "Output[i,j] = Œ£ Œ£ Image[i+m, j+n] √ó Filter[m, n]\n",
    "              m n\n",
    "```\n",
    "\n",
    "**In plain English**: Multiply corresponding values and add them up!\n",
    "\n",
    "### üé¨ Let's See It in Action!\n",
    "\n",
    "I'll show you a simple 5√ó5 image convolved with a 3√ó3 filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-convolution-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple example\n",
    "simple_image = np.array([\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 1, 0, 0]\n",
    "])\n",
    "\n",
    "vertical_edge_filter = np.array([\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1]\n",
    "])\n",
    "\n",
    "# Manually compute one position (top-left 3x3)\n",
    "print(\"üîç Computing Convolution at Position (0, 0)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nImage patch (3√ó3):\")\n",
    "patch = simple_image[0:3, 0:3]\n",
    "print(patch)\n",
    "\n",
    "print(\"\\nFilter (3√ó3):\")\n",
    "print(vertical_edge_filter)\n",
    "\n",
    "print(\"\\nüìä Element-wise Multiplication:\")\n",
    "print(\"=\"*70)\n",
    "elementwise_product = patch * vertical_edge_filter\n",
    "print(elementwise_product)\n",
    "\n",
    "print(\"\\n‚ûï Sum of all elements:\")\n",
    "output_value = np.sum(elementwise_product)\n",
    "print(f\"   {output_value}\")\n",
    "\n",
    "print(\"\\nüéØ This is the output value at position (0, 0)!\")\n",
    "print(\"\\nNow we slide the filter and repeat...\")\n",
    "\n",
    "# Show the calculation in detail\n",
    "print(\"\\nüìù Detailed Calculation:\")\n",
    "print(\"=\"*70)\n",
    "total = 0\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        product = patch[i, j] * vertical_edge_filter[i, j]\n",
    "        total += product\n",
    "        print(f\"   [{i},{j}]: {patch[i,j]} √ó {vertical_edge_filter[i,j]:2} = {product:3}\")\n",
    "\n",
    "print(f\"\\n   Sum = {total}\")\n",
    "print(\"\\nüí° Notice: This filter is detecting the vertical edge at x=2 (bright‚Üídark)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-sliding",
   "metadata": {},
   "source": [
    "### üé® Visualizing the Sliding Window\n",
    "\n",
    "Let's create a visualization showing how the filter slides across the image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-convolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of convolution process\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# We'll show 6 different filter positions\n",
    "positions = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\n",
    "\n",
    "for idx, (pos_i, pos_j) in enumerate(positions):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(simple_image, cmap='gray', vmin=0, vmax=1, interpolation='nearest')\n",
    "    \n",
    "    # Highlight the current receptive field\n",
    "    rect = Rectangle((pos_j - 0.5, pos_i - 0.5), 3, 3,\n",
    "                     linewidth=4, edgecolor='red', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # Extract the patch and compute output\n",
    "    patch = simple_image[pos_i:pos_i+3, pos_j:pos_j+3]\n",
    "    output_val = np.sum(patch * vertical_edge_filter)\n",
    "    \n",
    "    ax.set_title(f'Position ({pos_i}, {pos_j})\\nOutput = {output_val:.0f}',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add grid\n",
    "    for i in range(6):\n",
    "        ax.axhline(i - 0.5, color='cyan', linewidth=1)\n",
    "        ax.axvline(i - 0.5, color='cyan', linewidth=1)\n",
    "    \n",
    "    # Show pixel values\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            ax.text(j, i, f'{simple_image[i, j]:.0f}',\n",
    "                   ha='center', va='center',\n",
    "                   color='yellow', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('Convolution: Sliding Window Process\\n(Filter slides ‚Üí computes output at each position)',\n",
    "            fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key Observations:\")\n",
    "print(\"   ‚Ä¢ Filter slides left-to-right, top-to-bottom\")\n",
    "print(\"   ‚Ä¢ Each position produces ONE output value\")\n",
    "print(\"   ‚Ä¢ Output is strongest at position (0,1) and (0,2) - where the edge is!\")\n",
    "print(\"   ‚Ä¢ This 5√ó5 image with 3√ó3 filter ‚Üí 3√ó3 output (5-3+1=3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implement-conv2d",
   "metadata": {},
   "source": [
    "---\n",
    "## üíª Implementing Conv2D from Scratch\n",
    "\n",
    "Let's implement convolution in pure NumPy to truly understand it!\n",
    "\n",
    "### üéØ Function Signature\n",
    "\n",
    "```python\n",
    "def conv2d(image, kernel, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Perform 2D convolution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : np.ndarray, shape (H, W)\n",
    "        Input image\n",
    "    kernel : np.ndarray, shape (K, K)\n",
    "        Convolution filter\n",
    "    stride : int\n",
    "        Step size for sliding window\n",
    "    padding : int\n",
    "        Border padding size\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : np.ndarray\n",
    "        Convolution output (feature map)\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Let's implement this step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conv2d-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(image, kernel, stride=1, padding=0, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform 2D convolution - the heart of CNNs!\n",
    "    \n",
    "    This is a simple but correct implementation using nested loops.\n",
    "    Real frameworks (PyTorch, TensorFlow) use highly optimized algorithms.\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    image_height, image_width = image.shape\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìä Input: {image.shape}\")\n",
    "        print(f\"üìä Kernel: {kernel.shape}\")\n",
    "        print(f\"üìä Stride: {stride}, Padding: {padding}\")\n",
    "    \n",
    "    # Step 1: Add padding if needed\n",
    "    if padding > 0:\n",
    "        image = np.pad(image, \n",
    "                      pad_width=padding,\n",
    "                      mode='constant',\n",
    "                      constant_values=0)\n",
    "        if verbose:\n",
    "            print(f\"üìä After padding: {image.shape}\")\n",
    "    \n",
    "    # Update dimensions after padding\n",
    "    padded_height, padded_width = image.shape\n",
    "    \n",
    "    # Step 2: Calculate output dimensions\n",
    "    # Formula: (W - K + 2P) / S + 1\n",
    "    output_height = (padded_height - kernel_height) // stride + 1\n",
    "    output_width = (padded_width - kernel_width) // stride + 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìä Output will be: ({output_height}, {output_width})\")\n",
    "    \n",
    "    # Step 3: Initialize output\n",
    "    output = np.zeros((output_height, output_width))\n",
    "    \n",
    "    # Step 4: Perform convolution\n",
    "    for i in range(output_height):\n",
    "        for j in range(output_width):\n",
    "            # Calculate the position in the padded image\n",
    "            h_start = i * stride\n",
    "            h_end = h_start + kernel_height\n",
    "            w_start = j * stride\n",
    "            w_end = w_start + kernel_width\n",
    "            \n",
    "            # Extract the receptive field (the patch we're looking at)\n",
    "            receptive_field = image[h_start:h_end, w_start:w_end]\n",
    "            \n",
    "            # Perform element-wise multiplication and sum\n",
    "            # This is the core of convolution!\n",
    "            output[i, j] = np.sum(receptive_field * kernel)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚úÖ Convolution complete!\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test our implementation!\n",
    "print(\"üß™ Testing our conv2d implementation...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_output = conv2d(simple_image, vertical_edge_filter, stride=1, padding=0, verbose=True)\n",
    "\n",
    "print(\"\\nüìä Output Feature Map:\")\n",
    "print(test_output)\n",
    "\n",
    "print(\"\\n‚úÖ Success! Our implementation works!\")\n",
    "print(\"\\nüí° Notice: Highest values (3.0) are at positions where the vertical edge is!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-different-filters",
   "metadata": {},
   "source": [
    "### üß™ Testing Different Filters\n",
    "\n",
    "Let's test our convolution implementation with different filters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-filters-on-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more interesting test image\n",
    "test_image = np.array([\n",
    "    [0, 0, 1, 1, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 1, 0],\n",
    "    [1, 0, 0, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 1, 1, 1, 0, 0]\n",
    "])\n",
    "\n",
    "# Test with multiple filters\n",
    "test_filters = {\n",
    "    'Vertical Edge': np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]),\n",
    "    'Horizontal Edge': np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]]),\n",
    "    'Blur': np.ones((3, 3)) / 9,\n",
    "    'Sharpen': np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "}\n",
    "\n",
    "# Apply each filter\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Show original image\n",
    "axes[0].imshow(test_image, cmap='gray', interpolation='nearest')\n",
    "axes[0].set_title('Original Image\\n(7√ó7)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, color='cyan', linewidth=1)\n",
    "axes[0].set_xticks(range(7))\n",
    "axes[0].set_yticks(range(7))\n",
    "\n",
    "# Apply each filter\n",
    "for idx, (name, filt) in enumerate(test_filters.items()):\n",
    "    ax = axes[idx + 1]\n",
    "    \n",
    "    # Apply convolution\n",
    "    output = conv2d(test_image, filt, stride=1, padding=0)\n",
    "    \n",
    "    # Display output\n",
    "    im = ax.imshow(output, cmap='RdBu', interpolation='nearest')\n",
    "    ax.set_title(f'{name}\\nOutput: {output.shape}', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, color='gray', linewidth=0.5)\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Show values\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            ax.text(j, i, f'{output[i,j]:.1f}',\n",
    "                   ha='center', va='center',\n",
    "                   color='black', fontsize=8)\n",
    "\n",
    "# Hide last subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.suptitle('Different Filters Detect Different Features!', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ What We Can See:\")\n",
    "print(\"   ‚Ä¢ Vertical Edge: Responds to left/right boundaries of the circle\")\n",
    "print(\"   ‚Ä¢ Horizontal Edge: Responds to top/bottom boundaries\")\n",
    "print(\"   ‚Ä¢ Blur: Smooths the image\")\n",
    "print(\"   ‚Ä¢ Sharpen: Enhances edges and details\")\n",
    "print(\"\\nüí° Each filter extracts DIFFERENT information from the same image!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stride-padding",
   "metadata": {},
   "source": [
    "---\n",
    "## üèÉ Stride: Controlling the Step Size\n",
    "\n",
    "### üéØ What is Stride?\n",
    "\n",
    "**Stride** is how many pixels we move the filter at each step!\n",
    "\n",
    "- **Stride = 1**: Move one pixel at a time (default)\n",
    "- **Stride = 2**: Skip every other pixel\n",
    "- **Stride = 3**: Skip two pixels, etc.\n",
    "\n",
    "### ü§î Why Use Stride > 1?\n",
    "\n",
    "‚úÖ **Reduces output size** (downsampling)\n",
    "‚úÖ **Faster computation** (fewer positions to compute)\n",
    "‚úÖ **Alternative to pooling** (can replace pooling layers)\n",
    "\n",
    "### üìê How Stride Affects Output Size\n",
    "\n",
    "```\n",
    "Output Size = (Input - Kernel) / Stride + 1\n",
    "```\n",
    "\n",
    "**Example**: 7√ó7 input, 3√ó3 kernel\n",
    "- Stride 1: (7-3)/1 + 1 = 5√ó5 output\n",
    "- Stride 2: (7-3)/2 + 1 = 3√ó3 output\n",
    "- Stride 3: (7-3)/3 + 1 = 2√ó2 output\n",
    "\n",
    "Let's visualize this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrate-stride",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 7x7 image\n",
    "stride_test_image = np.arange(49).reshape(7, 7)\n",
    "\n",
    "# Simple averaging filter\n",
    "avg_filter = np.ones((3, 3)) / 9\n",
    "\n",
    "# Test different strides\n",
    "strides = [1, 2, 3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
    "\n",
    "# Show original\n",
    "axes[0].imshow(stride_test_image, cmap='viridis', interpolation='nearest')\n",
    "axes[0].set_title('Original Image\\n(7√ó7)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, color='white', linewidth=1.5)\n",
    "\n",
    "# Add grid\n",
    "for i in range(8):\n",
    "    axes[0].axhline(i - 0.5, color='white', linewidth=1.5)\n",
    "    axes[0].axvline(i - 0.5, color='white', linewidth=1.5)\n",
    "\n",
    "# Test each stride\n",
    "for idx, stride in enumerate(strides):\n",
    "    ax = axes[idx + 1]\n",
    "    \n",
    "    # Apply convolution with this stride\n",
    "    output = conv2d(stride_test_image, avg_filter, stride=stride, padding=0)\n",
    "    \n",
    "    # Display\n",
    "    im = ax.imshow(output, cmap='viridis', interpolation='nearest')\n",
    "    ax.set_title(f'Stride = {stride}\\nOutput: {output.shape}',\n",
    "                fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Add grid\n",
    "    for i in range(output.shape[0] + 1):\n",
    "        ax.axhline(i - 0.5, color='white', linewidth=1.5)\n",
    "        ax.axvline(i - 0.5, color='white', linewidth=1.5)\n",
    "    \n",
    "    # Show values\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            ax.text(j, i, f'{output[i,j]:.1f}',\n",
    "                   ha='center', va='center',\n",
    "                   color='white', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle('Effect of Stride on Output Size', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Output Size Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Stride':<10} {'Formula':<25} {'Output Size':<15}\")\n",
    "print(\"=\"*50)\n",
    "for stride in strides:\n",
    "    size = (7 - 3) // stride + 1\n",
    "    formula = f\"(7-3)/{stride}+1 = {size}\"\n",
    "    print(f\"{stride:<10} {formula:<25} {size}√ó{size}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nüéØ Key Insight:\")\n",
    "print(\"   ‚Ä¢ Larger stride ‚Üí Smaller output\")\n",
    "print(\"   ‚Ä¢ Stride=2 is common in modern CNNs (alternative to pooling)\")\n",
    "print(\"   ‚Ä¢ Trade-off: Less computation but also less spatial information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "padding-section",
   "metadata": {},
   "source": [
    "---\n",
    "## üõ°Ô∏è Padding: Preserving Spatial Dimensions\n",
    "\n",
    "### üéØ The Problem\n",
    "\n",
    "Without padding:\n",
    "- 7√ó7 image + 3√ó3 filter ‚Üí 5√ó5 output (shrinks!)\n",
    "- Each convolution layer makes the image smaller\n",
    "- After many layers: 224√ó224 ‚Üí 222√ó220 ‚Üí 220√ó218 ‚Üí ... ‚Üí Too small!\n",
    "\n",
    "### üõ°Ô∏è The Solution: Padding\n",
    "\n",
    "**Padding** adds border pixels around the image!\n",
    "\n",
    "```\n",
    "Original:        With Padding=1:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  7√ó7‚îÇ          ‚îÇ0 0 0 0‚îÇ\n",
    "‚îÇ     ‚îÇ    ‚Üí     ‚îÇ0  7√ó7 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ0 0 0 0‚îÇ\n",
    "                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    9√ó9\n",
    "```\n",
    "\n",
    "### üìê Two Types of Padding\n",
    "\n",
    "1. **Valid Padding (no padding)**\n",
    "   - Output size = Input - Kernel + 1\n",
    "   - Image shrinks with each layer\n",
    "\n",
    "2. **Same Padding**\n",
    "   - Output size = Input size (when stride=1)\n",
    "   - Padding = (Kernel - 1) / 2\n",
    "   - Most common in modern CNNs!\n",
    "\n",
    "### üßÆ Calculating Required Padding\n",
    "\n",
    "For \"same\" padding with stride=1:\n",
    "```python\n",
    "padding = (kernel_size - 1) // 2\n",
    "```\n",
    "\n",
    "Examples:\n",
    "- 3√ó3 kernel ‚Üí padding = 1\n",
    "- 5√ó5 kernel ‚Üí padding = 2\n",
    "- 7√ó7 kernel ‚Üí padding = 3\n",
    "\n",
    "Let's see padding in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrate-padding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple image\n",
    "padding_test_image = np.ones((5, 5))\n",
    "padding_test_image[1:4, 1:4] = 0  # Create a dark square in the middle\n",
    "\n",
    "# Test different padding values\n",
    "padding_values = [0, 1, 2]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "for idx, pad in enumerate(padding_values):\n",
    "    # Show padded image\n",
    "    ax_img = axes[0, idx]\n",
    "    \n",
    "    if pad > 0:\n",
    "        padded_img = np.pad(padding_test_image, pad_width=pad, \n",
    "                           mode='constant', constant_values=0.5)\n",
    "    else:\n",
    "        padded_img = padding_test_image\n",
    "    \n",
    "    ax_img.imshow(padded_img, cmap='gray', interpolation='nearest', vmin=0, vmax=1)\n",
    "    ax_img.set_title(f'Padding = {pad}\\nSize: {padded_img.shape}',\n",
    "                    fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add grid\n",
    "    for i in range(padded_img.shape[0] + 1):\n",
    "        ax_img.axhline(i - 0.5, color='red', linewidth=1.5)\n",
    "        ax_img.axvline(i - 0.5, color='red', linewidth=1.5)\n",
    "    \n",
    "    # Highlight the padding region\n",
    "    if pad > 0:\n",
    "        rect = Rectangle((pad - 0.5, pad - 0.5), 5, 5,\n",
    "                        linewidth=3, edgecolor='yellow', facecolor='none')\n",
    "        ax_img.add_patch(rect)\n",
    "        ax_img.text(padded_img.shape[1]/2, -1, \n",
    "                   'Yellow box = original image',\n",
    "                   ha='center', fontsize=10, color='yellow', fontweight='bold')\n",
    "    \n",
    "    # Apply convolution with this padding\n",
    "    ax_output = axes[1, idx]\n",
    "    output = conv2d(padding_test_image, avg_filter, stride=1, padding=pad)\n",
    "    \n",
    "    im = ax_output.imshow(output, cmap='viridis', interpolation='nearest')\n",
    "    ax_output.set_title(f'After Convolution\\nOutput: {output.shape}',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add grid\n",
    "    for i in range(output.shape[0] + 1):\n",
    "        ax_output.axhline(i - 0.5, color='white', linewidth=1)\n",
    "        ax_output.axvline(i - 0.5, color='white', linewidth=1)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax_output, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle('Effect of Padding on Output Size\\n(3√ó3 filter, stride=1)',\n",
    "            fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Output Size Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Padding':<15} {'Input Size':<15} {'Output Size':<15} {'Change':<15}\")\n",
    "print(\"=\"*60)\n",
    "for pad in padding_values:\n",
    "    output = conv2d(padding_test_image, avg_filter, stride=1, padding=pad)\n",
    "    change = \"Shrinks\" if pad == 0 else (\"Same\" if pad == 1 else \"Grows\")\n",
    "    print(f\"{pad:<15} {'5√ó5':<15} {f'{output.shape[0]}√ó{output.shape[1]}':<15} {change:<15}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüéØ Key Points:\")\n",
    "print(\"   ‚Ä¢ Padding=0: Output shrinks (valid padding)\")\n",
    "print(\"   ‚Ä¢ Padding=1: Output stays same size with 3√ó3 kernel (same padding)\")\n",
    "print(\"   ‚Ä¢ Padding=2: Output grows (usually not desired)\")\n",
    "print(\"\\nüí° Most CNNs use 'same' padding to maintain spatial dimensions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output-size-formula",
   "metadata": {},
   "source": [
    "### üßÆ The Complete Output Size Formula\n",
    "\n",
    "Combining everything we've learned:\n",
    "\n",
    "```\n",
    "Output Size = ‚åä(Input + 2√óPadding - Kernel) / Stride‚åã + 1\n",
    "```\n",
    "\n",
    "Where ‚åä ‚åã means floor division (round down).\n",
    "\n",
    "Let's create a calculator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "output-size-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_output_size(input_size, kernel_size, stride, padding):\n",
    "    \"\"\"\n",
    "    Calculate output size after convolution.\n",
    "    \n",
    "    Formula: ‚åä(Input + 2√óPadding - Kernel) / Stride‚åã + 1\n",
    "    \"\"\"\n",
    "    output_size = (input_size + 2 * padding - kernel_size) // stride + 1\n",
    "    return output_size\n",
    "\n",
    "# Test various configurations\n",
    "print(\"üßÆ Convolution Output Size Calculator\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Input':<10} {'Kernel':<10} {'Stride':<10} {'Padding':<10} {'Output':<10} {'Description':<25}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_configs = [\n",
    "    (32, 3, 1, 0, \"No padding (shrinks)\"),\n",
    "    (32, 3, 1, 1, \"Same padding (preserves size)\"),\n",
    "    (32, 3, 2, 1, \"Stride=2 (downsampling)\"),\n",
    "    (224, 7, 2, 3, \"ImageNet first layer\"),\n",
    "    (28, 5, 1, 0, \"MNIST with 5√ó5 filter\"),\n",
    "    (64, 3, 1, 1, \"Typical CNN layer\"),\n",
    "    (56, 3, 2, 1, \"Downsampling layer\"),\n",
    "]\n",
    "\n",
    "for input_size, kernel, stride, padding, description in test_configs:\n",
    "    output = calculate_output_size(input_size, kernel, stride, padding)\n",
    "    print(f\"{input_size:<10} {kernel:<10} {stride:<10} {padding:<10} {output:<10} {description:<25}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüí° Design Tips:\")\n",
    "print(\"   ‚Ä¢ Use padding=1 with 3√ó3 kernels to maintain size\")\n",
    "print(\"   ‚Ä¢ Use stride=2 for downsampling (alternative to pooling)\")\n",
    "print(\"   ‚Ä¢ First layer often uses larger kernel (7√ó7) and stride=2\")\n",
    "print(\"   ‚Ä¢ Later layers typically use 3√ó3 kernels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-maps",
   "metadata": {},
   "source": [
    "---\n",
    "## üó∫Ô∏è Feature Maps: Multiple Filters\n",
    "\n",
    "### üéØ The Big Idea\n",
    "\n",
    "In real CNNs, we don't use just ONE filter - we use MANY!\n",
    "\n",
    "**Why?**\n",
    "- One filter = detects one pattern (e.g., vertical edges)\n",
    "- Multiple filters = detect multiple patterns!\n",
    "  - Filter 1: Vertical edges\n",
    "  - Filter 2: Horizontal edges\n",
    "  - Filter 3: Diagonal edges\n",
    "  - Filter 4: Corners\n",
    "  - Filter 5: Textures\n",
    "  - ... and so on!\n",
    "\n",
    "### üìä Feature Map Dimensions\n",
    "\n",
    "```\n",
    "Input: (H, W, C_in)\n",
    "  ‚Ä¢ H = height\n",
    "  ‚Ä¢ W = width\n",
    "  ‚Ä¢ C_in = input channels\n",
    "\n",
    "Filters: N filters of size (K, K, C_in)\n",
    "  ‚Ä¢ N = number of filters\n",
    "  ‚Ä¢ K = kernel size\n",
    "  ‚Ä¢ C_in = must match input channels\n",
    "\n",
    "Output: (H_out, W_out, N)\n",
    "  ‚Ä¢ H_out, W_out = calculated using formula\n",
    "  ‚Ä¢ N = number of feature maps (one per filter)\n",
    "```\n",
    "\n",
    "### üé® Visualizing Multiple Feature Maps\n",
    "\n",
    "Let's create a real example with multiple filters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-filters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a richer test image (10√ó10 with various features)\n",
    "rich_image = np.zeros((10, 10))\n",
    "\n",
    "# Add vertical edge\n",
    "rich_image[:, 3] = 1\n",
    "\n",
    "# Add horizontal edge\n",
    "rich_image[6, :] = 1\n",
    "\n",
    "# Add a bright square\n",
    "rich_image[1:3, 7:9] = 1\n",
    "\n",
    "# Define multiple filters\n",
    "filters_dict = {\n",
    "    'Vertical\\nEdge': np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]),\n",
    "    'Horizontal\\nEdge': np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]]),\n",
    "    'Diagonal\\nEdge': np.array([[2, 1, 0], [1, 0, -1], [0, -1, -2]]),\n",
    "    'Corner\\nDetector': np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]),\n",
    "}\n",
    "\n",
    "# Apply each filter\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Show original\n",
    "axes[0].imshow(rich_image, cmap='gray', interpolation='nearest')\n",
    "axes[0].set_title('Original Image\\n(10√ó10)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, color='cyan', linewidth=0.5)\n",
    "\n",
    "# Apply each filter and show feature map\n",
    "for idx, (name, filt) in enumerate(filters_dict.items()):\n",
    "    ax = axes[idx + 1]\n",
    "    \n",
    "    # Apply convolution\n",
    "    feature_map = conv2d(rich_image, filt, stride=1, padding=0)\n",
    "    \n",
    "    # Display feature map\n",
    "    im = ax.imshow(feature_map, cmap='RdBu', interpolation='nearest')\n",
    "    ax.set_title(f'{name}\\nFeature Map: {feature_map.shape}',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, color='gray', linewidth=0.5)\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "# Show the filters\n",
    "axes[5].axis('off')\n",
    "axes[5].text(0.5, 0.5,\n",
    "            'üéØ Each filter detects\\n'\n",
    "            'different patterns!\\n\\n'\n",
    "            '‚Ä¢ Vertical filter ‚Üí strong\\n'\n",
    "            '  response at vertical edges\\n\\n'\n",
    "            '‚Ä¢ Horizontal filter ‚Üí strong\\n'\n",
    "            '  response at horizontal edges\\n\\n'\n",
    "            '‚Ä¢ Each feature map shows\\n'\n",
    "            '  WHERE that pattern exists\\n\\n'\n",
    "            'üß† CNN learns these filters\\n'\n",
    "            'automatically during training!',\n",
    "            ha='center', va='center', fontsize=11,\n",
    "            bbox=dict(boxstyle='round,pad=1', facecolor='lightyellow',\n",
    "                     edgecolor='black', linewidth=2))\n",
    "\n",
    "plt.suptitle('Multiple Filters ‚Üí Multiple Feature Maps\\n'\n",
    "            '(Each filter specializes in detecting different patterns)',\n",
    "            fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Understanding Feature Maps:\")\n",
    "print(\"   ‚Ä¢ Original image: 10√ó10√ó1 (one channel)\")\n",
    "print(\"   ‚Ä¢ We applied 4 different 3√ó3 filters\")\n",
    "print(\"   ‚Ä¢ Result: 4 feature maps, each 8√ó8\")\n",
    "print(\"   ‚Ä¢ Output shape: 8√ó8√ó4 (height √ó width √ó channels)\")\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   Each filter acts as a 'feature detector' for a specific pattern!\")\n",
    "print(\"   More filters ‚Üí more features can be detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-cnn-filters",
   "metadata": {},
   "source": [
    "### üß† What Do Real CNN Filters Learn?\n",
    "\n",
    "In a trained CNN:\n",
    "\n",
    "**First Layer Filters**:\n",
    "- Simple patterns: edges, colors, simple textures\n",
    "- Look like edge detectors, color blobs\n",
    "\n",
    "**Middle Layer Filters**:\n",
    "- More complex: corners, curves, patterns\n",
    "- Combinations of first layer features\n",
    "\n",
    "**Deep Layer Filters**:\n",
    "- Very complex: object parts (wheels, faces, eyes)\n",
    "- Semantic features\n",
    "\n",
    "**Final Layers**:\n",
    "- Whole objects and scenes\n",
    "\n",
    "This creates a **hierarchical feature representation**! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hierarchical-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hierarchical feature learning\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Define layers\n",
    "layers = [\n",
    "    {'name': 'Input\\nImage', 'x': 1, 'y': 3, 'height': 4, 'color': '#FFE4E1'},\n",
    "    {'name': 'Layer 1\\nEdges\\nColors', 'x': 3, 'y': 2.5, 'height': 5, 'color': '#87CEEB'},\n",
    "    {'name': 'Layer 2\\nTextures\\nPatterns', 'x': 5.5, 'y': 2, 'height': 6, 'color': '#90EE90'},\n",
    "    {'name': 'Layer 3\\nShapes\\nParts', 'x': 8, 'y': 2.5, 'height': 5, 'color': '#FFB6C1'},\n",
    "    {'name': 'Layer 4\\nObjects', 'x': 10.5, 'y': 3, 'height': 4, 'color': '#DDA0DD'}\n",
    "]\n",
    "\n",
    "# Draw layers\n",
    "for i, layer in enumerate(layers):\n",
    "    # Draw box\n",
    "    from matplotlib.patches import FancyBboxPatch\n",
    "    box = FancyBboxPatch((layer['x'], layer['y']), 1.2, layer['height'],\n",
    "                         boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor=layer['color'],\n",
    "                         edgecolor='black', linewidth=3)\n",
    "    ax.add_patch(box)\n",
    "    \n",
    "    # Add text\n",
    "    ax.text(layer['x'] + 0.6, layer['y'] + layer['height']/2,\n",
    "           layer['name'], ha='center', va='center',\n",
    "           fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Add arrows\n",
    "    if i < len(layers) - 1:\n",
    "        next_layer = layers[i + 1]\n",
    "        arrow = mpatches.FancyArrowPatch(\n",
    "            (layer['x'] + 1.2, layer['y'] + layer['height']/2),\n",
    "            (next_layer['x'], next_layer['y'] + next_layer['height']/2),\n",
    "            arrowstyle='->', mutation_scale=30,\n",
    "            linewidth=3, color='black', alpha=0.6\n",
    "        )\n",
    "        ax.add_patch(arrow)\n",
    "\n",
    "# Add title and description\n",
    "ax.text(6, 9, 'Hierarchical Feature Learning in CNNs',\n",
    "       ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "ax.text(6, 0.5,\n",
    "       'Each layer builds on the previous layer, detecting increasingly complex patterns!',\n",
    "       ha='center', fontsize=12, style='italic',\n",
    "       bbox=dict(boxstyle='round,pad=0.8', facecolor='lightyellow',\n",
    "                edgecolor='black', linewidth=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüß† How CNNs Build Understanding:\")\n",
    "print(\"\\n   Layer 1 (Early):\")\n",
    "print(\"     ‚Ä¢ Learns: Simple edges, colors, basic textures\")\n",
    "print(\"     ‚Ä¢ Example: Horizontal line, blue blob, rough texture\")\n",
    "print(\"\\n   Layer 2 (Middle):\")\n",
    "print(\"     ‚Ä¢ Learns: Combinations of edges ‚Üí shapes\")\n",
    "print(\"     ‚Ä¢ Example: Corner, circle, grid pattern\")\n",
    "print(\"\\n   Layer 3 (Middle-Deep):\")\n",
    "print(\"     ‚Ä¢ Learns: Object parts\")\n",
    "print(\"     ‚Ä¢ Example: Eye, wheel, door, window\")\n",
    "print(\"\\n   Layer 4 (Deep):\")\n",
    "print(\"     ‚Ä¢ Learns: Complete objects\")\n",
    "print(\"     ‚Ä¢ Example: Cat face, car, house\")\n",
    "print(\"\\nüí° This is why deep networks work so well!\")\n",
    "print(\"   They automatically learn the right features at each level!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rgb-convolution",
   "metadata": {},
   "source": [
    "---\n",
    "## üåà Convolution with RGB Images\n",
    "\n",
    "### üéØ The Challenge\n",
    "\n",
    "So far we've worked with grayscale images (2D). But most real images are **RGB** (3D)!\n",
    "\n",
    "```\n",
    "Grayscale: (H, W)     - 2D\n",
    "RGB:       (H, W, 3)  - 3D (Red, Green, Blue channels)\n",
    "```\n",
    "\n",
    "### üîç How Does Convolution Work with RGB?\n",
    "\n",
    "**Key Insight**: The filter must have the SAME depth as the input!\n",
    "\n",
    "```\n",
    "Input: (H, W, 3)     - RGB image\n",
    "Filter: (K, K, 3)    - 3 channels to match!\n",
    "Output: (H', W', 1)  - Single feature map per filter\n",
    "```\n",
    "\n",
    "**The Process**:\n",
    "1. Filter has 3 layers (one for R, one for G, one for B)\n",
    "2. Convolve each channel separately\n",
    "3. **Sum** the results from all 3 channels\n",
    "4. Result: One output value\n",
    "\n",
    "### üé® Mathematical Formula\n",
    "\n",
    "```python\n",
    "output[i,j] = sum over all channels (\n",
    "    image[i:i+k, j:j+k, channel] * filter[:,:,channel]\n",
    ")\n",
    "```\n",
    "\n",
    "Let's implement this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conv2d-rgb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_rgb(image, kernel, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Perform 2D convolution on RGB images.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : np.ndarray, shape (H, W, 3)\n",
    "        RGB input image\n",
    "    kernel : np.ndarray, shape (K, K, 3)\n",
    "        3D convolution filter (must have 3 channels)\n",
    "    stride : int\n",
    "        Step size\n",
    "    padding : int\n",
    "        Border padding\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : np.ndarray, shape (H', W')\n",
    "        Feature map (2D output)\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    height, width, channels = image.shape\n",
    "    kernel_height, kernel_width, kernel_channels = kernel.shape\n",
    "    \n",
    "    assert channels == kernel_channels, \"Image and kernel must have same number of channels!\"\n",
    "    \n",
    "    # Add padding if needed (pad all channels)\n",
    "    if padding > 0:\n",
    "        image = np.pad(image,\n",
    "                      pad_width=((padding, padding), (padding, padding), (0, 0)),\n",
    "                      mode='constant',\n",
    "                      constant_values=0)\n",
    "    \n",
    "    # Update dimensions\n",
    "    padded_height, padded_width, _ = image.shape\n",
    "    \n",
    "    # Calculate output size\n",
    "    output_height = (padded_height - kernel_height) // stride + 1\n",
    "    output_width = (padded_width - kernel_width) // stride + 1\n",
    "    \n",
    "    # Initialize output\n",
    "    output = np.zeros((output_height, output_width))\n",
    "    \n",
    "    # Perform convolution\n",
    "    for i in range(output_height):\n",
    "        for j in range(output_width):\n",
    "            h_start = i * stride\n",
    "            h_end = h_start + kernel_height\n",
    "            w_start = j * stride\n",
    "            w_end = w_start + kernel_width\n",
    "            \n",
    "            # Extract 3D receptive field\n",
    "            receptive_field = image[h_start:h_end, w_start:w_end, :]\n",
    "            \n",
    "            # Convolve: multiply element-wise and sum EVERYTHING\n",
    "            # This sums across height, width, AND channels!\n",
    "            output[i, j] = np.sum(receptive_field * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test with a simple RGB image\n",
    "rgb_image = np.random.rand(8, 8, 3)  # Random RGB image\n",
    "\n",
    "# Create an RGB filter (3 channels)\n",
    "rgb_filter = np.random.randn(3, 3, 3) * 0.1\n",
    "\n",
    "print(\"üß™ Testing RGB Convolution...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input image shape: {rgb_image.shape}\")\n",
    "print(f\"Filter shape: {rgb_filter.shape}\")\n",
    "\n",
    "output = conv2d_rgb(rgb_image, rgb_filter, stride=1, padding=0)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"\\n‚úÖ Success! RGB convolution works!\")\n",
    "\n",
    "print(\"\\nüîç What Happened:\")\n",
    "print(\"   ‚Ä¢ 8√ó8√ó3 image (RGB)\")\n",
    "print(\"   ‚Ä¢ 3√ó3√ó3 filter (matches 3 channels)\")\n",
    "print(\"   ‚Ä¢ Output: 6√ó6 feature map (single channel)\")\n",
    "print(\"\\nüí° One filter ‚Üí One feature map (regardless of input channels!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-rgb",
   "metadata": {},
   "source": [
    "### üé® Visualizing RGB Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-rgb-conv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clearer example\n",
    "# Make an RGB image with distinct channel patterns\n",
    "demo_rgb = np.zeros((6, 6, 3))\n",
    "demo_rgb[:, :3, 0] = 1.0  # Left half is RED\n",
    "demo_rgb[:, 3:, 1] = 1.0  # Right half is GREEN\n",
    "\n",
    "# Create a filter that responds to red (looks at R channel)\n",
    "red_detector = np.zeros((3, 3, 3))\n",
    "red_detector[:, :, 0] = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]])  # Vertical edge on R channel\n",
    "\n",
    "# Apply convolution\n",
    "red_response = conv2d_rgb(demo_rgb, red_detector, stride=1, padding=0)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Show RGB image\n",
    "axes[0, 0].imshow(demo_rgb)\n",
    "axes[0, 0].set_title('RGB Image\\n(Red left, Green right)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, color='white', linewidth=1)\n",
    "\n",
    "# Show individual channels\n",
    "for idx, (channel_name, channel_idx, color) in enumerate([('R', 0, 'Reds'),\n",
    "                                                           ('G', 1, 'Greens'),\n",
    "                                                           ('B', 2, 'Blues')]):\n",
    "    ax = axes[0, idx] if idx == 0 else axes[1, idx - 1]\n",
    "    if idx > 0:\n",
    "        ax.imshow(demo_rgb[:, :, channel_idx], cmap=color, vmin=0, vmax=1)\n",
    "        ax.set_title(f'{channel_name} Channel', fontsize=12, fontweight='bold')\n",
    "        ax.grid(True, color='gray', linewidth=0.5)\n",
    "\n",
    "# Show filter\n",
    "axes[1, 1].imshow(red_detector[:, :, 0], cmap='RdBu', vmin=-1, vmax=1)\n",
    "axes[1, 1].set_title('Filter\\n(R channel only)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, color='black', linewidth=1)\n",
    "\n",
    "# Show output\n",
    "im = axes[1, 2].imshow(red_response, cmap='hot', interpolation='nearest')\n",
    "axes[1, 2].set_title(f'Output Feature Map\\n{red_response.shape}', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "axes[1, 2].grid(True, color='gray', linewidth=0.5)\n",
    "plt.colorbar(im, ax=axes[1, 2])\n",
    "\n",
    "plt.suptitle('RGB Convolution: How Filters See Color Channels',\n",
    "            fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Understanding RGB Convolution:\")\n",
    "print(\"   ‚Ä¢ Filter has 3 layers (one per color channel)\")\n",
    "print(\"   ‚Ä¢ Each layer convolves with its corresponding channel\")\n",
    "print(\"   ‚Ä¢ Results are SUMMED to produce one output value\")\n",
    "print(\"   ‚Ä¢ Output is strongest where the filter pattern matches the image\")\n",
    "print(\"\\nüí° This is how CNNs process color images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Summary: The Convolution Operation\n",
    "\n",
    "Congratulations! You now understand the heart of CNNs! üéâ\n",
    "\n",
    "### ‚úÖ What We Learned\n",
    "\n",
    "1. **Filters/Kernels**:\n",
    "   - Small matrices that detect patterns\n",
    "   - Different filters detect different features\n",
    "   - Usually 3√ó3, 5√ó5, or 7√ó7\n",
    "\n",
    "2. **The Convolution Operation**:\n",
    "   - Slide filter across image\n",
    "   - Multiply element-wise and sum\n",
    "   - Produces feature map showing where pattern exists\n",
    "\n",
    "3. **Stride**:\n",
    "   - Controls step size (usually 1 or 2)\n",
    "   - Larger stride ‚Üí smaller output\n",
    "   - Used for downsampling\n",
    "\n",
    "4. **Padding**:\n",
    "   - Adds border pixels\n",
    "   - Preserves spatial dimensions\n",
    "   - \"Same\" padding keeps size constant\n",
    "\n",
    "5. **Feature Maps**:\n",
    "   - Output of convolution\n",
    "   - Multiple filters ‚Üí multiple feature maps\n",
    "   - Each detects different pattern\n",
    "\n",
    "6. **RGB Convolution**:\n",
    "   - Filter depth must match input depth\n",
    "   - Results summed across all channels\n",
    "   - One filter ‚Üí one feature map\n",
    "\n",
    "### üßÆ Key Formulas\n",
    "\n",
    "**Output Size**:\n",
    "```\n",
    "Output = ‚åä(Input + 2√óPadding - Kernel) / Stride‚åã + 1\n",
    "```\n",
    "\n",
    "**Same Padding** (stride=1):\n",
    "```\n",
    "Padding = (Kernel - 1) / 2\n",
    "```\n",
    "\n",
    "**Convolution Operation**:\n",
    "```\n",
    "Output[i,j] = Œ£ Œ£ Image[i+m, j+n] √ó Filter[m,n]\n",
    "```\n",
    "\n",
    "### üí° Key Insights\n",
    "\n",
    "- **Convolution = Pattern Matching**: The filter looks for its pattern everywhere in the image\n",
    "- **Local Operation**: Each output depends only on a local patch\n",
    "- **Parameter Sharing**: Same filter weights used everywhere\n",
    "- **Hierarchical**: Early layers detect simple features, deep layers detect complex objects\n",
    "\n",
    "### üéì What's Next?\n",
    "\n",
    "Now that you understand convolution, you're ready for:\n",
    "\n",
    "**Next Notebook: Pooling Layers**\n",
    "- Downsampling feature maps\n",
    "- Max pooling vs average pooling\n",
    "- Why pooling helps CNNs\n",
    "\n",
    "**Then**: Building a complete CNN!\n",
    "\n",
    "Let's continue the journey! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practice",
   "metadata": {},
   "source": [
    "---\n",
    "## üéÆ Practice Exercises\n",
    "\n",
    "Test your understanding with these exercises:\n",
    "\n",
    "### Exercise 1: Design Your Own Filter\n",
    "Create a 3√ó3 filter that detects diagonal edges (top-left to bottom-right).\n",
    "Test it on an image with diagonal patterns.\n",
    "\n",
    "### Exercise 2: Calculate Output Sizes\n",
    "Given:\n",
    "- Input: 64√ó64√ó3\n",
    "- 32 filters, each 5√ó5\n",
    "- Stride = 2\n",
    "- Padding = 2\n",
    "\n",
    "What is the output shape?\n",
    "\n",
    "### Exercise 3: Implement Multi-Channel Convolution\n",
    "Modify our `conv2d_rgb` function to:\n",
    "- Accept multiple filters\n",
    "- Return a 3D output (H √ó W √ó num_filters)\n",
    "\n",
    "### Exercise 4: Visualize Filter Learning\n",
    "Create a simple image and several random filters.\n",
    "Visualize which filters respond most strongly to different parts of the image.\n",
    "\n",
    "### Exercise 5: Compare Stride vs Pooling\n",
    "- Apply conv with stride=2\n",
    "- Apply conv with stride=1 + max pooling\n",
    "- Compare the outputs - what's different?\n",
    "\n",
    "**Try these exercises in the exercises.ipynb notebook!**\n",
    "\n",
    "---\n",
    "\n",
    "*Great work! You now understand the core operation that powers all CNNs!* üí™\n",
    "\n",
    "*Ready to learn about pooling? Let's go!* ‚Üí **[Next: Notebook 03 - Pooling Layers](03_pooling_layers.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
