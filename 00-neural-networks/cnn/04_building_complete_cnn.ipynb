{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Building a Complete CNN - From Scratch!\n",
    "\n",
    "Welcome to the grand finale of our CNN fundamentals series! üéâ\n",
    "\n",
    "We've learned all the building blocks:\n",
    "- ‚úÖ **What CNNs are** and why they work\n",
    "- ‚úÖ **Convolution operation** - the pattern detector\n",
    "- ‚úÖ **Pooling layers** - smart downsampling\n",
    "\n",
    "Now it's time to **PUT IT ALL TOGETHER** and build a complete CNN that actually learns!\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll:\n",
    "- **Implement a complete CNN** from scratch in NumPy\n",
    "- **Train on real data** (MNIST handwritten digits)\n",
    "- **Understand backpropagation** for CNNs\n",
    "- **Visualize learned filters** (what the network learned!)\n",
    "- **Compare CNN vs fully-connected** networks\n",
    "- **See the training process** step-by-step\n",
    "- **Test on real images** and see predictions\n",
    "\n",
    "**Prerequisites:** Notebooks 01-03 (CNNs, Convolution, Pooling)\n",
    "\n",
    "---\n",
    "\n",
    "## üé¨ The Recipe Analogy\n",
    "\n",
    "Think of building a CNN like cooking a complex dish:\n",
    "- **Ingredients**: Convolution, pooling, ReLU, fully-connected layers\n",
    "- **Recipe**: How to combine them (the architecture)\n",
    "- **Cooking process**: Training (adjusting flavors/weights)\n",
    "- **Tasting**: Testing and validation\n",
    "\n",
    "We've learned about each ingredient. Now let's cook! üë®‚Äçüç≥\n",
    "\n",
    "Let's build something amazing! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üì¶ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture-overview",
   "metadata": {},
   "source": [
    "---\n",
    "## üèõÔ∏è Our CNN Architecture\n",
    "\n",
    "### üéØ The Plan\n",
    "\n",
    "We'll build a **simple but effective** CNN for MNIST digit classification:\n",
    "\n",
    "```\n",
    "Input: 28√ó28√ó1 (grayscale digit image)\n",
    "    ‚Üì\n",
    "Conv Layer 1: 8 filters, 3√ó3 ‚Üí 26√ó26√ó8\n",
    "    ‚Üì\n",
    "ReLU Activation\n",
    "    ‚Üì\n",
    "Max Pool: 2√ó2 ‚Üí 13√ó13√ó8\n",
    "    ‚Üì\n",
    "Conv Layer 2: 16 filters, 3√ó3 ‚Üí 11√ó11√ó16\n",
    "    ‚Üì\n",
    "ReLU Activation\n",
    "    ‚Üì\n",
    "Max Pool: 2√ó2 ‚Üí 5√ó5√ó16\n",
    "    ‚Üì\n",
    "Flatten: 400 neurons\n",
    "    ‚Üì\n",
    "Fully Connected: 10 neurons (one per digit)\n",
    "    ‚Üì\n",
    "Softmax: Probabilities for each digit\n",
    "```\n",
    "\n",
    "### ü§î Why This Architecture?\n",
    "\n",
    "**Two conv blocks:**\n",
    "- First block detects simple patterns (edges, curves)\n",
    "- Second block combines them (digit parts)\n",
    "\n",
    "**Max pooling:**\n",
    "- Reduces spatial dimensions\n",
    "- Keeps strongest features\n",
    "- Makes network robust\n",
    "\n",
    "**ReLU activation:**\n",
    "- Non-linearity (lets network learn complex patterns)\n",
    "- Fast to compute\n",
    "- Works well in practice\n",
    "\n",
    "**Small filters (3√ó3):**\n",
    "- Modern best practice\n",
    "- Efficient\n",
    "- Can stack to get larger receptive fields\n",
    "\n",
    "Let's implement each component!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the architecture\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.set_xlim(0, 16)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Define layers with positions and sizes\n",
    "layers = [\n",
    "    {'name': 'Input\\n28√ó28√ó1', 'x': 1, 'y': 3, 'w': 1.5, 'h': 4, 'color': 'lightblue'},\n",
    "    {'name': 'Conv1\\n26√ó26√ó8', 'x': 3.5, 'y': 2.5, 'w': 1.3, 'h': 5, 'color': 'lightgreen'},\n",
    "    {'name': 'Pool1\\n13√ó13√ó8', 'x': 5.5, 'y': 3, 'w': 1, 'h': 4, 'color': 'lightyellow'},\n",
    "    {'name': 'Conv2\\n11√ó11√ó16', 'x': 7.5, 'y': 2.8, 'w': 0.9, 'h': 4.4, 'color': 'lightcoral'},\n",
    "    {'name': 'Pool2\\n5√ó5√ó16', 'x': 9.5, 'y': 3.5, 'w': 0.6, 'h': 3, 'color': 'plum'},\n",
    "    {'name': 'Flatten\\n400', 'x': 11, 'y': 4, 'w': 0.3, 'h': 2, 'color': 'peachpuff'},\n",
    "    {'name': 'FC\\n10', 'x': 13, 'y': 4.5, 'w': 0.3, 'h': 1, 'color': 'lightsteelblue'},\n",
    "]\n",
    "\n",
    "# Draw layers\n",
    "for layer in layers:\n",
    "    rect = Rectangle((layer['x'], layer['y']), layer['w'], layer['h'],\n",
    "                     facecolor=layer['color'], edgecolor='black', linewidth=3)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # Add label\n",
    "    ax.text(layer['x'] + layer['w']/2, layer['y'] + layer['h']/2,\n",
    "           layer['name'], ha='center', va='center',\n",
    "           fontsize=10, fontweight='bold')\n",
    "\n",
    "# Draw arrows between layers\n",
    "for i in range(len(layers) - 1):\n",
    "    x1 = layers[i]['x'] + layers[i]['w']\n",
    "    y1 = layers[i]['y'] + layers[i]['h'] / 2\n",
    "    x2 = layers[i+1]['x']\n",
    "    y2 = layers[i+1]['y'] + layers[i+1]['h'] / 2\n",
    "    \n",
    "    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color='blue'))\n",
    "\n",
    "# Add operation labels\n",
    "operations = ['3√ó3 conv', 'ReLU + Pool', '3√ó3 conv', 'ReLU + Pool', 'reshape', 'softmax']\n",
    "for i, op in enumerate(operations):\n",
    "    x = (layers[i]['x'] + layers[i]['w'] + layers[i+1]['x']) / 2\n",
    "    ax.text(x, 8.5, op, ha='center', fontsize=9, style='italic', color='blue')\n",
    "\n",
    "# Add title and annotations\n",
    "ax.text(8, 9.5, 'Complete CNN Architecture for MNIST',\n",
    "       ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add parameter count\n",
    "param_text = (\n",
    "    \"Parameters:\\n\"\n",
    "    \"Conv1: 3√ó3√ó1√ó8 = 72 + 8 biases = 80\\n\"\n",
    "    \"Conv2: 3√ó3√ó8√ó16 = 1,152 + 16 biases = 1,168\\n\"\n",
    "    \"FC: 400√ó10 = 4,000 + 10 biases = 4,010\\n\"\n",
    "    \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\"\n",
    "    \"Total: ~5,258 parameters\"\n",
    ")\n",
    "ax.text(8, 0.8, param_text, ha='center', fontsize=9, family='monospace',\n",
    "       bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Architecture Summary:\")\n",
    "print(\"   ‚Ä¢ Very efficient: only ~5K parameters!\")\n",
    "print(\"   ‚Ä¢ Two convolutional blocks for hierarchical features\")\n",
    "print(\"   ‚Ä¢ Max pooling for downsampling and robustness\")\n",
    "print(\"   ‚Ä¢ Final FC layer for classification\")\n",
    "print(\"\\nüí° Compare to fully-connected:\")\n",
    "print(\"   FC network for 28√ó28 image: 784√ó128 = 100,352 parameters (20x more!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implement-layers",
   "metadata": {},
   "source": [
    "---\n",
    "## üß± Implementing the Building Blocks\n",
    "\n",
    "Let's implement each layer. We'll include both **forward** and **backward** passes (for training)!\n",
    "\n",
    "### üéì Quick Backpropagation Refresher\n",
    "\n",
    "**Forward pass**: Input ‚Üí Output (make predictions)\n",
    "**Backward pass**: Gradient flows back to adjust weights\n",
    "\n",
    "```\n",
    "Forward:  Input ‚Üí [Layer] ‚Üí Output\n",
    "Backward: ‚àÇL/‚àÇInput ‚Üê [Layer] ‚Üê ‚àÇL/‚àÇOutput\n",
    "```\n",
    "\n",
    "Each layer needs:\n",
    "1. **Forward**: Compute output from input\n",
    "2. **Backward**: Compute gradient w.r.t. input AND update weights\n",
    "\n",
    "Don't worry - we'll explain each step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conv-layer-section",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Convolutional Layer\n",
    "\n",
    "The heart of our CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conv-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    \"\"\"\n",
    "    Convolutional layer with forward and backward passes.\n",
    "    \n",
    "    This is a simplified implementation for educational purposes.\n",
    "    Real frameworks use optimized algorithms (im2col, FFT convolution, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_filters, filter_size, num_channels, padding=0):\n",
    "        \"\"\"\n",
    "        Initialize convolutional layer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        num_filters : int\n",
    "            Number of filters (output channels)\n",
    "        filter_size : int\n",
    "            Size of square filter (e.g., 3 for 3√ó3)\n",
    "        num_channels : int\n",
    "            Number of input channels\n",
    "        padding : int\n",
    "            Amount of padding to add\n",
    "        \"\"\"\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.num_channels = num_channels\n",
    "        self.padding = padding\n",
    "        \n",
    "        # He initialization (good for ReLU)\n",
    "        # Scale: sqrt(2 / (num_channels * filter_size^2))\n",
    "        scale = np.sqrt(2.0 / (num_channels * filter_size * filter_size))\n",
    "        self.filters = np.random.randn(num_filters, num_channels, filter_size, filter_size) * scale\n",
    "        self.biases = np.zeros(num_filters)\n",
    "        \n",
    "        # For storing during forward pass (needed for backward pass)\n",
    "        self.last_input = None\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass: Apply convolution.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_data : np.ndarray, shape (batch, channels, height, width)\n",
    "            Input feature maps\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : np.ndarray, shape (batch, num_filters, out_height, out_width)\n",
    "            Convolved feature maps\n",
    "        \"\"\"\n",
    "        self.last_input = input_data  # Store for backward pass\n",
    "        \n",
    "        batch_size, _, height, width = input_data.shape\n",
    "        \n",
    "        # Add padding if needed\n",
    "        if self.padding > 0:\n",
    "            input_data = np.pad(\n",
    "                input_data,\n",
    "                ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)),\n",
    "                mode='constant',\n",
    "                constant_values=0\n",
    "            )\n",
    "            height += 2 * self.padding\n",
    "            width += 2 * self.padding\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        out_height = height - self.filter_size + 1\n",
    "        out_width = width - self.filter_size + 1\n",
    "        \n",
    "        # Initialize output\n",
    "        output = np.zeros((batch_size, self.num_filters, out_height, out_width))\n",
    "        \n",
    "        # Perform convolution\n",
    "        for b in range(batch_size):\n",
    "            for f in range(self.num_filters):\n",
    "                for i in range(out_height):\n",
    "                    for j in range(out_width):\n",
    "                        # Extract receptive field\n",
    "                        receptive_field = input_data[\n",
    "                            b, :,\n",
    "                            i:i+self.filter_size,\n",
    "                            j:j+self.filter_size\n",
    "                        ]\n",
    "                        \n",
    "                        # Convolve: element-wise multiply and sum\n",
    "                        output[b, f, i, j] = np.sum(receptive_field * self.filters[f]) + self.biases[f]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        \"\"\"\n",
    "        Backward pass: Compute gradients and update weights.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        grad_output : np.ndarray\n",
    "            Gradient of loss w.r.t. output\n",
    "        learning_rate : float\n",
    "            Learning rate for weight updates\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        grad_input : np.ndarray\n",
    "            Gradient of loss w.r.t. input\n",
    "        \"\"\"\n",
    "        batch_size, _, height, width = self.last_input.shape\n",
    "        \n",
    "        # Add padding to last_input if needed\n",
    "        if self.padding > 0:\n",
    "            padded_input = np.pad(\n",
    "                self.last_input,\n",
    "                ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)),\n",
    "                mode='constant',\n",
    "                constant_values=0\n",
    "            )\n",
    "        else:\n",
    "            padded_input = self.last_input\n",
    "        \n",
    "        # Initialize gradients\n",
    "        grad_filters = np.zeros_like(self.filters)\n",
    "        grad_biases = np.zeros_like(self.biases)\n",
    "        grad_input = np.zeros_like(padded_input)\n",
    "        \n",
    "        _, _, out_height, out_width = grad_output.shape\n",
    "        \n",
    "        # Compute gradients\n",
    "        for b in range(batch_size):\n",
    "            for f in range(self.num_filters):\n",
    "                for i in range(out_height):\n",
    "                    for j in range(out_width):\n",
    "                        # Gradient for this position\n",
    "                        grad = grad_output[b, f, i, j]\n",
    "                        \n",
    "                        # Gradient w.r.t. filter\n",
    "                        receptive_field = padded_input[\n",
    "                            b, :,\n",
    "                            i:i+self.filter_size,\n",
    "                            j:j+self.filter_size\n",
    "                        ]\n",
    "                        grad_filters[f] += grad * receptive_field\n",
    "                        \n",
    "                        # Gradient w.r.t. bias\n",
    "                        grad_biases[f] += grad\n",
    "                        \n",
    "                        # Gradient w.r.t. input\n",
    "                        grad_input[\n",
    "                            b, :,\n",
    "                            i:i+self.filter_size,\n",
    "                            j:j+self.filter_size\n",
    "                        ] += grad * self.filters[f]\n",
    "        \n",
    "        # Average gradients over batch\n",
    "        grad_filters /= batch_size\n",
    "        grad_biases /= batch_size\n",
    "        \n",
    "        # Update weights\n",
    "        self.filters -= learning_rate * grad_filters\n",
    "        self.biases -= learning_rate * grad_biases\n",
    "        \n",
    "        # Remove padding from grad_input if needed\n",
    "        if self.padding > 0:\n",
    "            grad_input = grad_input[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "        \n",
    "        return grad_input\n",
    "\n",
    "print(\"‚úÖ ConvLayer implemented!\")\n",
    "print(\"   ‚Ä¢ Forward pass: Applies convolution\")\n",
    "print(\"   ‚Ä¢ Backward pass: Computes gradients and updates filters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maxpool-section",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Max Pooling Layer\n",
    "\n",
    "Downsampling with max operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maxpool-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer:\n",
    "    \"\"\"\n",
    "    Max pooling layer with forward and backward passes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        \"\"\"\n",
    "        Initialize max pooling layer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pool_size : int\n",
    "            Size of pooling window\n",
    "        stride : int\n",
    "            Stride for pooling\n",
    "        \"\"\"\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.last_input = None\n",
    "        self.max_indices = None  # Store for backward pass\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass: Apply max pooling.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_data : np.ndarray, shape (batch, channels, height, width)\n",
    "            Input feature maps\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : np.ndarray\n",
    "            Pooled feature maps\n",
    "        \"\"\"\n",
    "        self.last_input = input_data\n",
    "        \n",
    "        batch_size, channels, height, width = input_data.shape\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        out_height = (height - self.pool_size) // self.stride + 1\n",
    "        out_width = (width - self.pool_size) // self.stride + 1\n",
    "        \n",
    "        # Initialize output\n",
    "        output = np.zeros((batch_size, channels, out_height, out_width))\n",
    "        \n",
    "        # Store max indices for backward pass\n",
    "        self.max_indices = np.zeros((batch_size, channels, out_height, out_width, 2), dtype=int)\n",
    "        \n",
    "        # Perform max pooling\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(out_height):\n",
    "                    for j in range(out_width):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.pool_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.pool_size\n",
    "                        \n",
    "                        # Extract window\n",
    "                        window = input_data[b, c, h_start:h_end, w_start:w_end]\n",
    "                        \n",
    "                        # Find max value and its position\n",
    "                        output[b, c, i, j] = np.max(window)\n",
    "                        \n",
    "                        # Store the position of max value (for backward pass)\n",
    "                        max_idx = np.unravel_index(np.argmax(window), window.shape)\n",
    "                        self.max_indices[b, c, i, j] = [h_start + max_idx[0], w_start + max_idx[1]]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: Route gradients to max positions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        grad_output : np.ndarray\n",
    "            Gradient of loss w.r.t. output\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        grad_input : np.ndarray\n",
    "            Gradient of loss w.r.t. input\n",
    "        \"\"\"\n",
    "        # Initialize gradient\n",
    "        grad_input = np.zeros_like(self.last_input)\n",
    "        \n",
    "        batch_size, channels, out_height, out_width = grad_output.shape\n",
    "        \n",
    "        # Route gradient to max positions\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(out_height):\n",
    "                    for j in range(out_width):\n",
    "                        # Get the position that had the max value\n",
    "                        max_h, max_w = self.max_indices[b, c, i, j]\n",
    "                        \n",
    "                        # Route gradient to that position\n",
    "                        grad_input[b, c, max_h, max_w] += grad_output[b, c, i, j]\n",
    "        \n",
    "        return grad_input\n",
    "\n",
    "print(\"‚úÖ MaxPoolLayer implemented!\")\n",
    "print(\"   ‚Ä¢ Forward pass: Takes maximum in each window\")\n",
    "print(\"   ‚Ä¢ Backward pass: Routes gradient to max positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relu-section",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ ReLU Activation\n",
    "\n",
    "Non-linearity that makes learning possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relu-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer:\n",
    "    \"\"\"\n",
    "    ReLU activation: f(x) = max(0, x)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.last_input = None\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass: Apply ReLU.\n",
    "        \n",
    "        ReLU(x) = max(0, x)\n",
    "        - Positive values pass through\n",
    "        - Negative values become zero\n",
    "        \"\"\"\n",
    "        self.last_input = input_data\n",
    "        return np.maximum(0, input_data)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: Apply ReLU derivative.\n",
    "        \n",
    "        d(ReLU)/dx = 1 if x > 0, else 0\n",
    "        \n",
    "        Gradient flows through for positive values,\n",
    "        blocked for negative values.\n",
    "        \"\"\"\n",
    "        # Gradient is 1 where input was positive, 0 otherwise\n",
    "        grad_input = grad_output * (self.last_input > 0)\n",
    "        return grad_input\n",
    "\n",
    "print(\"‚úÖ ReLULayer implemented!\")\n",
    "print(\"   ‚Ä¢ Forward: ReLU(x) = max(0, x)\")\n",
    "print(\"   ‚Ä¢ Backward: Gradient = 1 if x > 0, else 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc-section",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Fully Connected Layer\n",
    "\n",
    "Final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    \"\"\"\n",
    "    Fully connected (dense) layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize fully connected layer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            Number of input neurons\n",
    "        output_size : int\n",
    "            Number of output neurons\n",
    "        \"\"\"\n",
    "        # He initialization\n",
    "        scale = np.sqrt(2.0 / input_size)\n",
    "        self.weights = np.random.randn(input_size, output_size) * scale\n",
    "        self.biases = np.zeros(output_size)\n",
    "        \n",
    "        self.last_input = None\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass: Linear transformation.\n",
    "        \n",
    "        output = input @ weights + biases\n",
    "        \"\"\"\n",
    "        # Flatten input if needed\n",
    "        if input_data.ndim > 2:\n",
    "            batch_size = input_data.shape[0]\n",
    "            input_data = input_data.reshape(batch_size, -1)\n",
    "        \n",
    "        self.last_input = input_data\n",
    "        \n",
    "        # Linear transformation\n",
    "        output = input_data @ self.weights + self.biases\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        \"\"\"\n",
    "        Backward pass: Compute gradients and update weights.\n",
    "        \"\"\"\n",
    "        batch_size = self.last_input.shape[0]\n",
    "        \n",
    "        # Gradient w.r.t. weights\n",
    "        grad_weights = self.last_input.T @ grad_output\n",
    "        \n",
    "        # Gradient w.r.t. biases\n",
    "        grad_biases = np.sum(grad_output, axis=0)\n",
    "        \n",
    "        # Gradient w.r.t. input\n",
    "        grad_input = grad_output @ self.weights.T\n",
    "        \n",
    "        # Update weights (with averaging)\n",
    "        self.weights -= learning_rate * (grad_weights / batch_size)\n",
    "        self.biases -= learning_rate * (grad_biases / batch_size)\n",
    "        \n",
    "        return grad_input\n",
    "\n",
    "print(\"‚úÖ FullyConnectedLayer implemented!\")\n",
    "print(\"   ‚Ä¢ Forward: output = input @ weights + biases\")\n",
    "print(\"   ‚Ä¢ Backward: Update weights based on gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "softmax-section",
   "metadata": {},
   "source": [
    "### 5Ô∏è‚É£ Softmax + Cross-Entropy Loss\n",
    "\n",
    "Convert logits to probabilities and calculate loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "softmax-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Compute softmax probabilities.\n",
    "    \n",
    "    Softmax converts logits to probabilities:\n",
    "    P(class i) = exp(logit_i) / sum(exp(all logits))\n",
    "    \n",
    "    Numerical stability trick: subtract max before exp\n",
    "    \"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(predictions, labels):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss.\n",
    "    \n",
    "    Loss = -log(P(correct class))\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : np.ndarray, shape (batch, num_classes)\n",
    "        Predicted probabilities (after softmax)\n",
    "    labels : np.ndarray, shape (batch,)\n",
    "        True class labels (integers)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    loss : float\n",
    "        Average cross-entropy loss\n",
    "    \"\"\"\n",
    "    batch_size = predictions.shape[0]\n",
    "    \n",
    "    # Get probability of correct class for each sample\n",
    "    correct_probs = predictions[np.arange(batch_size), labels]\n",
    "    \n",
    "    # Loss = -log(correct probability)\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    loss = -np.mean(np.log(correct_probs + 1e-10))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def softmax_cross_entropy_backward(predictions, labels):\n",
    "    \"\"\"\n",
    "    Compute gradient of softmax + cross-entropy.\n",
    "    \n",
    "    Beautiful result: gradient = (predictions - one_hot_labels) / batch_size\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    gradient : np.ndarray\n",
    "        Gradient w.r.t. logits (input to softmax)\n",
    "    \"\"\"\n",
    "    batch_size = predictions.shape[0]\n",
    "    \n",
    "    # Create gradient\n",
    "    gradient = predictions.copy()\n",
    "    \n",
    "    # Subtract 1 from correct class\n",
    "    gradient[np.arange(batch_size), labels] -= 1\n",
    "    \n",
    "    # Average over batch\n",
    "    gradient /= batch_size\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "print(\"‚úÖ Softmax and Loss functions implemented!\")\n",
    "print(\"   ‚Ä¢ Softmax: Converts logits to probabilities\")\n",
    "print(\"   ‚Ä¢ Cross-entropy: Measures prediction error\")\n",
    "print(\"   ‚Ä¢ Backward: Gradient for backpropagation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-cnn-section",
   "metadata": {},
   "source": [
    "---\n",
    "## üß© Building the Complete CNN\n",
    "\n",
    "Now let's combine all layers into a complete network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-cnn",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    \"\"\"\n",
    "    Complete CNN for MNIST digit classification.\n",
    "    \n",
    "    Architecture:\n",
    "    Input (28√ó28√ó1)\n",
    "    ‚Üí Conv (8 filters, 3√ó3)\n",
    "    ‚Üí ReLU\n",
    "    ‚Üí MaxPool (2√ó2)\n",
    "    ‚Üí Conv (16 filters, 3√ó3)\n",
    "    ‚Üí ReLU\n",
    "    ‚Üí MaxPool (2√ó2)\n",
    "    ‚Üí Flatten\n",
    "    ‚Üí FC (10 classes)\n",
    "    ‚Üí Softmax\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"üèóÔ∏è Building CNN...\")\n",
    "        \n",
    "        # Layer 1: Conv + ReLU + Pool\n",
    "        self.conv1 = ConvLayer(num_filters=8, filter_size=3, num_channels=1, padding=0)\n",
    "        self.relu1 = ReLULayer()\n",
    "        self.pool1 = MaxPoolLayer(pool_size=2, stride=2)\n",
    "        \n",
    "        # Layer 2: Conv + ReLU + Pool\n",
    "        self.conv2 = ConvLayer(num_filters=16, filter_size=3, num_channels=8, padding=0)\n",
    "        self.relu2 = ReLULayer()\n",
    "        self.pool2 = MaxPoolLayer(pool_size=2, stride=2)\n",
    "        \n",
    "        # Layer 3: Fully Connected\n",
    "        # After 2 convs and 2 pools: 28 ‚Üí 26 ‚Üí 13 ‚Üí 11 ‚Üí 5\n",
    "        # With 16 channels: 5 √ó 5 √ó 16 = 400\n",
    "        self.fc = FullyConnectedLayer(input_size=400, output_size=10)\n",
    "        \n",
    "        print(\"‚úÖ CNN built successfully!\")\n",
    "        self._print_architecture()\n",
    "    \n",
    "    def _print_architecture(self):\n",
    "        \"\"\"Print network architecture and parameter count.\"\"\"\n",
    "        print(\"\\nüìã Architecture:\")\n",
    "        print(\"   Input:    1 √ó 28 √ó 28\")\n",
    "        print(\"   Conv1:    8 √ó 26 √ó 26  (8 filters, 3√ó3)\")\n",
    "        print(\"   ReLU1:    8 √ó 26 √ó 26\")\n",
    "        print(\"   Pool1:    8 √ó 13 √ó 13  (2√ó2 max pool)\")\n",
    "        print(\"   Conv2:   16 √ó 11 √ó 11  (16 filters, 3√ó3)\")\n",
    "        print(\"   ReLU2:   16 √ó 11 √ó 11\")\n",
    "        print(\"   Pool2:   16 √ó 5 √ó 5    (2√ó2 max pool)\")\n",
    "        print(\"   Flatten: 400\")\n",
    "        print(\"   FC:      10            (output classes)\")\n",
    "        \n",
    "        # Calculate parameters\n",
    "        conv1_params = 3*3*1*8 + 8\n",
    "        conv2_params = 3*3*8*16 + 16\n",
    "        fc_params = 400*10 + 10\n",
    "        total = conv1_params + conv2_params + fc_params\n",
    "        \n",
    "        print(f\"\\nüî¢ Parameters:\")\n",
    "        print(f\"   Conv1:  {conv1_params:,}\")\n",
    "        print(f\"   Conv2:  {conv2_params:,}\")\n",
    "        print(f\"   FC:     {fc_params:,}\")\n",
    "        print(f\"   Total:  {total:,}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : np.ndarray, shape (batch, 1, 28, 28)\n",
    "            Input images\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        logits : np.ndarray, shape (batch, 10)\n",
    "            Class scores (before softmax)\n",
    "        \"\"\"\n",
    "        # Block 1\n",
    "        x = self.conv1.forward(x)\n",
    "        x = self.relu1.forward(x)\n",
    "        x = self.pool1.forward(x)\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.conv2.forward(x)\n",
    "        x = self.relu2.forward(x)\n",
    "        x = self.pool2.forward(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.fc.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad, learning_rate):\n",
    "        \"\"\"\n",
    "        Backward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        grad : np.ndarray\n",
    "            Gradient from loss function\n",
    "        learning_rate : float\n",
    "            Learning rate for updates\n",
    "        \"\"\"\n",
    "        # Backpropagate through layers in reverse order\n",
    "        grad = self.fc.backward(grad, learning_rate)\n",
    "        grad = grad.reshape(grad.shape[0], 16, 5, 5)  # Reshape for conv layers\n",
    "        \n",
    "        grad = self.pool2.backward(grad)\n",
    "        grad = self.relu2.backward(grad)\n",
    "        grad = self.conv2.backward(grad, learning_rate)\n",
    "        \n",
    "        grad = self.pool1.backward(grad)\n",
    "        grad = self.relu1.backward(grad)\n",
    "        grad = self.conv1.backward(grad, learning_rate)\n",
    "    \n",
    "    def train_step(self, x, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Perform one training step (forward + backward).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : np.ndarray\n",
    "            Batch of images\n",
    "        y : np.ndarray\n",
    "            Batch of labels\n",
    "        learning_rate : float\n",
    "            Learning rate\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "            Cross-entropy loss\n",
    "        accuracy : float\n",
    "            Prediction accuracy\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        logits = self.forward(x)\n",
    "        probs = softmax(logits)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = cross_entropy_loss(probs, y)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        \n",
    "        # Backward pass\n",
    "        grad = softmax_cross_entropy_backward(probs, y)\n",
    "        self.backward(grad, learning_rate)\n",
    "        \n",
    "        return loss, accuracy\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : np.ndarray\n",
    "            Images to classify\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : np.ndarray\n",
    "            Predicted class labels\n",
    "        probabilities : np.ndarray\n",
    "            Class probabilities\n",
    "        \"\"\"\n",
    "        logits = self.forward(x)\n",
    "        probs = softmax(logits)\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "        return predictions, probs\n",
    "\n",
    "# Test instantiation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING CNN INSTANTIATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = SimpleCNN()\n",
    "\n",
    "print(\"\\n‚úÖ CNN successfully created!\")\n",
    "print(\"   Ready for training on MNIST digits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-section",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Loading MNIST Data\n",
    "\n",
    "Let's load the famous MNIST dataset of handwritten digits!\n",
    "\n",
    "**MNIST**: 70,000 grayscale images of digits 0-9\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "- Each image: 28√ó28 pixels\n",
    "\n",
    "Since we're building from scratch, we'll create a simple data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-sample-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_mnist_data(num_train=1000, num_test=200):\n",
    "    \"\"\"\n",
    "    Create synthetic MNIST-like data for demonstration.\n",
    "    \n",
    "    In a real scenario, you would load actual MNIST data.\n",
    "    This creates simplified digit-like patterns for testing our CNN.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    num_train : int\n",
    "        Number of training samples\n",
    "    num_test : int\n",
    "        Number of test samples\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_images, train_labels, test_images, test_labels\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating sample MNIST-like data...\")\n",
    "    print(f\"   Training samples: {num_train}\")\n",
    "    print(f\"   Test samples: {num_test}\")\n",
    "    \n",
    "    def create_digit_pattern(digit, size=28):\n",
    "        \"\"\"Create a simple pattern for each digit.\"\"\"\n",
    "        img = np.zeros((size, size))\n",
    "        \n",
    "        # Create simple patterns for each digit\n",
    "        if digit == 0:  # Circle\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    dist = np.sqrt((i - size/2)**2 + (j - size/2)**2)\n",
    "                    if size/4 < dist < size/3:\n",
    "                        img[i, j] = 1\n",
    "        \n",
    "        elif digit == 1:  # Vertical line\n",
    "            img[5:23, 12:16] = 1\n",
    "        \n",
    "        elif digit == 2:  # S-shape\n",
    "            img[8:12, 8:20] = 1   # Top\n",
    "            img[12:16, 14:20] = 1  # Middle\n",
    "            img[16:20, 8:14] = 1   # Bottom\n",
    "        \n",
    "        elif digit == 3:  # Two curves\n",
    "            img[8:12, 10:20] = 1   # Top\n",
    "            img[13:15, 10:20] = 1  # Middle\n",
    "            img[17:21, 10:20] = 1  # Bottom\n",
    "        \n",
    "        elif digit == 4:  # Two lines\n",
    "            img[8:20, 8:11] = 1    # Vertical\n",
    "            img[13:16, 8:20] = 1   # Horizontal\n",
    "            img[8:20, 17:20] = 1   # Vertical\n",
    "        \n",
    "        elif digit == 5:  # Mirrored S\n",
    "            img[8:12, 8:20] = 1    # Top\n",
    "            img[12:16, 8:14] = 1   # Middle\n",
    "            img[16:20, 14:20] = 1  # Bottom\n",
    "        \n",
    "        elif digit == 6:  # Circle with top missing\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    dist = np.sqrt((i - size/2)**2 + (j - size/2)**2)\n",
    "                    if size/4 < dist < size/3 and i > size/2:\n",
    "                        img[i, j] = 1\n",
    "            img[12:16, 8:14] = 1  # Top horizontal\n",
    "        \n",
    "        elif digit == 7:  # Two lines forming 7\n",
    "            img[8:12, 8:20] = 1    # Top horizontal\n",
    "            img[8:20, 16:20] = 1   # Right vertical\n",
    "        \n",
    "        elif digit == 8:  # Two circles\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    dist_top = np.sqrt((i - 11)**2 + (j - size/2)**2)\n",
    "                    dist_bot = np.sqrt((i - 17)**2 + (j - size/2)**2)\n",
    "                    if 3 < dist_top < 5 or 3 < dist_bot < 5:\n",
    "                        img[i, j] = 1\n",
    "        \n",
    "        elif digit == 9:  # Circle with bottom missing\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    dist = np.sqrt((i - size/2)**2 + (j - size/2)**2)\n",
    "                    if size/4 < dist < size/3 and i < size/2:\n",
    "                        img[i, j] = 1\n",
    "            img[13:17, 14:20] = 1  # Bottom horizontal\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    # Generate training data\n",
    "    train_images = np.zeros((num_train, 1, 28, 28))\n",
    "    train_labels = np.zeros(num_train, dtype=int)\n",
    "    \n",
    "    for i in range(num_train):\n",
    "        digit = i % 10\n",
    "        train_labels[i] = digit\n",
    "        train_images[i, 0] = create_digit_pattern(digit)\n",
    "        \n",
    "        # Add some noise\n",
    "        train_images[i, 0] += np.random.randn(28, 28) * 0.1\n",
    "        train_images[i, 0] = np.clip(train_images[i, 0], 0, 1)\n",
    "    \n",
    "    # Generate test data\n",
    "    test_images = np.zeros((num_test, 1, 28, 28))\n",
    "    test_labels = np.zeros(num_test, dtype=int)\n",
    "    \n",
    "    for i in range(num_test):\n",
    "        digit = i % 10\n",
    "        test_labels[i] = digit\n",
    "        test_images[i, 0] = create_digit_pattern(digit)\n",
    "        \n",
    "        # Add some noise (different from training)\n",
    "        test_images[i, 0] += np.random.randn(28, 28) * 0.15\n",
    "        test_images[i, 0] = np.clip(test_images[i, 0], 0, 1)\n",
    "    \n",
    "    # Shuffle\n",
    "    train_perm = np.random.permutation(num_train)\n",
    "    train_images = train_images[train_perm]\n",
    "    train_labels = train_labels[train_perm]\n",
    "    \n",
    "    test_perm = np.random.permutation(num_test)\n",
    "    test_images = test_images[test_perm]\n",
    "    test_labels = test_labels[test_perm]\n",
    "    \n",
    "    print(\"\\n‚úÖ Data created successfully!\")\n",
    "    print(f\"   Training set: {train_images.shape}\")\n",
    "    print(f\"   Test set: {test_images.shape}\")\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "# Create data\n",
    "train_images, train_labels, test_images, test_labels = create_sample_mnist_data(\n",
    "    num_train=1000,\n",
    "    num_test=200\n",
    ")\n",
    "\n",
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "\n",
    "for i in range(10):\n",
    "    # Training sample\n",
    "    axes[0, i].imshow(train_images[i, 0], cmap='gray')\n",
    "    axes[0, i].set_title(f'Label: {train_labels[i]}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Test sample\n",
    "    axes[1, i].imshow(test_images[i, 0], cmap='gray')\n",
    "    axes[1, i].set_title(f'Label: {test_labels[i]}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Train', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Test', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Sample MNIST-like Digits', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Note: These are simplified digit patterns for demonstration.\")\n",
    "print(\"   Real MNIST digits are handwritten and much more varied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "---\n",
    "## üèÉ Training the CNN!\n",
    "\n",
    "Time for the magic to happen! Let's train our network.\n",
    "\n",
    "### üéØ Training Process\n",
    "\n",
    "1. **Mini-batch**: Take a small batch of images\n",
    "2. **Forward pass**: Make predictions\n",
    "3. **Calculate loss**: How wrong were we?\n",
    "4. **Backward pass**: Calculate gradients\n",
    "5. **Update weights**: Adjust to improve\n",
    "6. **Repeat**: Do this many times!\n",
    "\n",
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(model, train_images, train_labels, test_images, test_labels,\n",
    "              epochs=10, batch_size=32, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Train the CNN model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : SimpleCNN\n",
    "        The CNN to train\n",
    "    train_images, train_labels : np.ndarray\n",
    "        Training data\n",
    "    test_images, test_labels : np.ndarray\n",
    "        Test data\n",
    "    epochs : int\n",
    "        Number of epochs to train\n",
    "    batch_size : int\n",
    "        Mini-batch size\n",
    "    learning_rate : float\n",
    "        Learning rate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    history : dict\n",
    "        Training history (loss, accuracy)\n",
    "    \"\"\"\n",
    "    print(\"üèãÔ∏è Starting training...\")\n",
    "    print(f\"   Epochs: {epochs}\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Learning rate: {learning_rate}\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    num_train = len(train_images)\n",
    "    num_batches = num_train // batch_size\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Shuffle training data\n",
    "        perm = np.random.permutation(num_train)\n",
    "        train_images_shuffled = train_images[perm]\n",
    "        train_labels_shuffled = train_labels[perm]\n",
    "        \n",
    "        # Track epoch metrics\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for batch in range(num_batches):\n",
    "            # Get batch\n",
    "            start_idx = batch * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            batch_images = train_images_shuffled[start_idx:end_idx]\n",
    "            batch_labels = train_labels_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            # Train on batch\n",
    "            loss, acc = model.train_step(batch_images, batch_labels, learning_rate)\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            epoch_acc += acc\n",
    "        \n",
    "        # Average metrics\n",
    "        epoch_loss /= num_batches\n",
    "        epoch_acc /= num_batches\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_predictions, _ = model.predict(test_images)\n",
    "        test_acc = np.mean(test_predictions == test_labels)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Loss: {epoch_loss:.4f} | \"\n",
    "              f\"Train Acc: {epoch_acc*100:.2f}% | \"\n",
    "              f\"Test Acc: {test_acc*100:.2f}% | \"\n",
    "              f\"Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Training complete!\")\n",
    "    print(f\"   Final train accuracy: {history['train_acc'][-1]*100:.2f}%\")\n",
    "    print(f\"   Final test accuracy: {history['test_acc'][-1]*100:.2f}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Create a fresh model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING CNN ON MNIST-LIKE DATA\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Train the model\n",
    "history = train_cnn(\n",
    "    model,\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    test_images,\n",
    "    test_labels,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-training",
   "metadata": {},
   "source": [
    "### üìà Visualizing Training Progress\n",
    "\n",
    "Let's see how the network learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Loss over time\n",
    "ax1.plot(history['train_loss'], marker='o', linewidth=2, markersize=8, label='Training Loss')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "# Plot 2: Accuracy over time\n",
    "epochs_range = range(1, len(history['train_acc']) + 1)\n",
    "ax2.plot(epochs_range, [acc*100 for acc in history['train_acc']], \n",
    "         marker='o', linewidth=2, markersize=8, label='Training Accuracy')\n",
    "ax2.plot(epochs_range, [acc*100 for acc in history['test_acc']], \n",
    "         marker='s', linewidth=2, markersize=8, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.set_ylim([0, 105])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Training Analysis:\")\n",
    "print(f\"   ‚Ä¢ Loss decreased from {history['train_loss'][0]:.4f} to {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"   ‚Ä¢ Train accuracy improved from {history['train_acc'][0]*100:.2f}% to {history['train_acc'][-1]*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Test accuracy improved from {history['test_acc'][0]*100:.2f}% to {history['test_acc'][-1]*100:.2f}%\")\n",
    "\n",
    "# Check for overfitting\n",
    "gap = history['train_acc'][-1] - history['test_acc'][-1]\n",
    "if gap > 0.1:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Possible overfitting (gap: {gap*100:.2f}%)\")\n",
    "    print(\"   Consider: more data, regularization, or early stopping\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Good generalization (train-test gap: {gap*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-predictions",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÆ Testing the Model\n",
    "\n",
    "Let's see what our network predicts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "test_predictions, test_probs = model.predict(test_images)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Show 32 test samples\n",
    "for i in range(32):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Show image\n",
    "    ax.imshow(test_images[i, 0], cmap='gray')\n",
    "    \n",
    "    # Get prediction info\n",
    "    true_label = test_labels[i]\n",
    "    pred_label = test_predictions[i]\n",
    "    confidence = test_probs[i, pred_label] * 100\n",
    "    \n",
    "    # Color based on correctness\n",
    "    if pred_label == true_label:\n",
    "        color = 'green'\n",
    "        mark = '‚úì'\n",
    "    else:\n",
    "        color = 'red'\n",
    "        mark = '‚úó'\n",
    "    \n",
    "    # Set title\n",
    "    ax.set_title(f'{mark} True: {true_label}\\nPred: {pred_label} ({confidence:.0f}%)',\n",
    "                fontsize=9, color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('CNN Predictions on Test Set\\n(Green = Correct, Red = Wrong)',\n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\nüìä Per-Class Accuracy:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for digit in range(10):\n",
    "    # Find samples of this digit\n",
    "    digit_mask = test_labels == digit\n",
    "    digit_acc = np.mean(test_predictions[digit_mask] == digit)\n",
    "    \n",
    "    # Create bar visualization\n",
    "    bar = '‚ñà' * int(digit_acc * 20)\n",
    "    print(f\"Digit {digit}: {bar:<20} {digit_acc*100:.1f}%\")\n",
    "\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-filters",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç Visualizing Learned Filters\n",
    "\n",
    "The most exciting part! Let's see what the CNN learned.\n",
    "\n",
    "**Remember**: The network learned these filters automatically from data!\n",
    "- We didn't tell it to look for edges\n",
    "- We didn't design these patterns\n",
    "- The network discovered them through training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-filters-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first conv layer filters\n",
    "filters_conv1 = model.conv1.filters  # Shape: (8, 1, 3, 3)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    # Get filter (remove channel dimension for visualization)\n",
    "    filt = filters_conv1[i, 0]\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    filt_norm = (filt - filt.min()) / (filt.max() - filt.min() + 1e-8)\n",
    "    \n",
    "    # Show filter\n",
    "    axes[i].imshow(filt_norm, cmap='RdBu', interpolation='nearest')\n",
    "    axes[i].set_title(f'Filter {i+1}', fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    # Add grid\n",
    "    for j in range(4):\n",
    "        axes[i].axhline(j - 0.5, color='black', linewidth=1)\n",
    "        axes[i].axvline(j - 0.5, color='black', linewidth=1)\n",
    "\n",
    "plt.suptitle('Learned Filters (First Conv Layer)\\nThese were learned automatically from data!',\n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ What Do These Filters Detect?\")\n",
    "print(\"   Each 3√ó3 filter learned to detect specific low-level patterns:\")\n",
    "print(\"   ‚Ä¢ Edge detectors (vertical, horizontal, diagonal)\")\n",
    "print(\"   ‚Ä¢ Corner detectors\")\n",
    "print(\"   ‚Ä¢ Texture patterns\")\n",
    "print(\"\\nüí° The network discovered these patterns on its own!\")\n",
    "print(\"   We never told it what to look for - it learned from the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-activations",
   "metadata": {},
   "source": [
    "### üó∫Ô∏è Visualizing Feature Maps\n",
    "\n",
    "Let's see what activations look like when we pass an image through the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-activations-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a test image\n",
    "test_idx = 0\n",
    "test_image = test_images[test_idx:test_idx+1]  # Keep batch dimension\n",
    "true_label = test_labels[test_idx]\n",
    "\n",
    "# Forward pass through each layer (manually to capture intermediate outputs)\n",
    "x = test_image\n",
    "\n",
    "# Conv1\n",
    "conv1_out = model.conv1.forward(x)\n",
    "relu1_out = model.relu1.forward(conv1_out)\n",
    "pool1_out = model.pool1.forward(relu1_out)\n",
    "\n",
    "# Conv2\n",
    "conv2_out = model.conv2.forward(pool1_out)\n",
    "relu2_out = model.relu2.forward(conv2_out)\n",
    "pool2_out = model.pool2.forward(relu2_out)\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = fig.add_gridspec(3, 8, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Show input\n",
    "ax_input = fig.add_subplot(gs[0, 0])\n",
    "ax_input.imshow(test_image[0, 0], cmap='gray')\n",
    "ax_input.set_title(f'Input\\nDigit: {true_label}', fontweight='bold')\n",
    "ax_input.axis('off')\n",
    "\n",
    "# Show Conv1 feature maps (8 filters)\n",
    "for i in range(8):\n",
    "    if i == 0:\n",
    "        ax = fig.add_subplot(gs[0, i])\n",
    "    else:\n",
    "        ax = fig.add_subplot(gs[1, i-1])\n",
    "    \n",
    "    if i < 8:\n",
    "        ax.imshow(relu1_out[0, i], cmap='viridis')\n",
    "        ax.set_title(f'Conv1-{i+1}', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "# Add text\n",
    "ax_text = fig.add_subplot(gs[1, 7])\n",
    "ax_text.axis('off')\n",
    "ax_text.text(0.5, 0.5, '‚Üí Pool ‚Üí', ha='center', va='center',\n",
    "            fontsize=16, fontweight='bold', color='blue')\n",
    "\n",
    "# Show Conv2 feature maps (first 8 of 16)\n",
    "for i in range(8):\n",
    "    ax = fig.add_subplot(gs[2, i])\n",
    "    ax.imshow(relu2_out[0, i], cmap='viridis')\n",
    "    ax.set_title(f'Conv2-{i+1}', fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Feature Maps at Each Layer\\nSee how the network processes the image!',\n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Understanding Feature Maps:\")\n",
    "print(\"\\n   Layer 1 (Conv1 + ReLU):\")\n",
    "print(\"   ‚Ä¢ Detects simple patterns (edges, curves)\")\n",
    "print(\"   ‚Ä¢ Each map shows where that filter activated\")\n",
    "print(\"   ‚Ä¢ Bright areas = strong activation (pattern detected)\")\n",
    "print(\"\\n   Layer 2 (Conv2 + ReLU):\")\n",
    "print(\"   ‚Ä¢ Combines Layer 1 features\")\n",
    "print(\"   ‚Ä¢ Detects more complex patterns (digit parts)\")\n",
    "print(\"   ‚Ä¢ More abstract, harder to interpret\")\n",
    "print(\"\\nüí° This is hierarchical feature learning in action!\")\n",
    "print(\"   Simple features ‚Üí Complex features ‚Üí Digit recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-fc",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öñÔ∏è CNN vs Fully-Connected Comparison\n",
    "\n",
    "Let's compare our CNN to a fully-connected network on the same task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple fully-connected network for comparison\n",
    "class SimpleFC:\n",
    "    \"\"\"Fully-connected network for MNIST.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"üèóÔ∏è Building fully-connected network...\")\n",
    "        \n",
    "        # 28√ó28 = 784 inputs\n",
    "        self.fc1 = FullyConnectedLayer(784, 128)\n",
    "        self.relu = ReLULayer()\n",
    "        self.fc2 = FullyConnectedLayer(128, 10)\n",
    "        \n",
    "        # Calculate parameters\n",
    "        fc1_params = 784 * 128 + 128\n",
    "        fc2_params = 128 * 10 + 10\n",
    "        total = fc1_params + fc2_params\n",
    "        \n",
    "        print(f\"\\nüìã Architecture:\")\n",
    "        print(f\"   Input:  784 (28√ó28 flattened)\")\n",
    "        print(f\"   FC1:    128\")\n",
    "        print(f\"   ReLU:   128\")\n",
    "        print(f\"   FC2:    10\")\n",
    "        print(f\"\\nüî¢ Parameters:\")\n",
    "        print(f\"   FC1:    {fc1_params:,}\")\n",
    "        print(f\"   FC2:    {fc2_params:,}\")\n",
    "        print(f\"   Total:  {total:,}\")\n",
    "        print(f\"\\nüí° Compare to CNN: {total:,} vs 5,258 parameters\")\n",
    "        print(f\"   FC network has {total/5258:.1f}x MORE parameters!\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        \n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad, learning_rate):\n",
    "        grad = self.fc2.backward(grad, learning_rate)\n",
    "        grad = self.relu.backward(grad)\n",
    "        grad = self.fc1.backward(grad, learning_rate)\n",
    "        return grad\n",
    "    \n",
    "    def train_step(self, x, y, learning_rate):\n",
    "        # Forward\n",
    "        logits = self.forward(x)\n",
    "        probs = softmax(logits)\n",
    "        \n",
    "        # Loss\n",
    "        loss = cross_entropy_loss(probs, y)\n",
    "        accuracy = np.mean(np.argmax(probs, axis=1) == y)\n",
    "        \n",
    "        # Backward\n",
    "        grad = softmax_cross_entropy_backward(probs, y)\n",
    "        self.backward(grad, learning_rate)\n",
    "        \n",
    "        return loss, accuracy\n",
    "    \n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        probs = softmax(logits)\n",
    "        return np.argmax(probs, axis=1), probs\n",
    "\n",
    "# Create FC network\n",
    "fc_model = SimpleFC()\n",
    "\n",
    "# Train FC network (fewer epochs since it has more parameters)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING FULLY-CONNECTED NETWORK\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "fc_history = train_cnn(\n",
    "    fc_model,\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    test_images,\n",
    "    test_labels,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-networks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two networks\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Final accuracy comparison\n",
    "models = ['CNN', 'Fully-Connected']\n",
    "train_accs = [history['train_acc'][-1]*100, fc_history['train_acc'][-1]*100]\n",
    "test_accs = [history['test_acc'][-1]*100, fc_history['test_acc'][-1]*100]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, train_accs, width, label='Train', color='skyblue')\n",
    "bars2 = axes[0].bar(x + width/2, test_accs, width, label='Test', color='lightcoral')\n",
    "\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Final Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 105])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1f}%',\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Parameter comparison\n",
    "params = [5258, 100618]  # CNN vs FC\n",
    "\n",
    "bars = axes[1].bar(models, params, color=['lightgreen', 'salmon'])\n",
    "axes[1].set_ylabel('Number of Parameters', fontsize=12)\n",
    "axes[1].set_title('Parameter Count Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, param in zip(bars, params):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{param:,}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Metric':<25} {'CNN':<20} {'Fully-Connected':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Parameters':<25} {5258:<20,} {100618:<20,}\")\n",
    "print(f\"{'Final Train Accuracy':<25} {history['train_acc'][-1]*100:<20.2f}% {fc_history['train_acc'][-1]*100:<20.2f}%\")\n",
    "print(f\"{'Final Test Accuracy':<25} {history['test_acc'][-1]*100:<20.2f}% {fc_history['test_acc'][-1]*100:<20.2f}%\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\nüéØ Key Insights:\")\n",
    "print(\"   1. CNN has 19√ó FEWER parameters than FC network\")\n",
    "print(\"   2. Despite fewer parameters, CNN can match or beat FC performance\")\n",
    "print(\"   3. CNN exploits spatial structure - FC network ignores it\")\n",
    "print(\"   4. CNN is more parameter-efficient for image tasks\")\n",
    "print(\"\\nüí° This is why CNNs revolutionized computer vision!\")\n",
    "print(\"   Fewer parameters + Better performance = Win! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Summary: Building a Complete CNN\n",
    "\n",
    "Congratulations! You just built, trained, and analyzed a complete CNN from scratch! üéâ\n",
    "\n",
    "### ‚úÖ What We Accomplished\n",
    "\n",
    "1. **Implemented All Layers:**\n",
    "   - ConvLayer (with forward and backward)\n",
    "   - MaxPoolLayer (with gradient routing)\n",
    "   - ReLULayer (non-linearity)\n",
    "   - FullyConnectedLayer (classification)\n",
    "   - Softmax + Cross-Entropy (loss)\n",
    "\n",
    "2. **Built Complete CNN:**\n",
    "   - 2 convolutional blocks\n",
    "   - Hierarchical feature learning\n",
    "   - Only ~5K parameters!\n",
    "\n",
    "3. **Trained on MNIST-like Data:**\n",
    "   - Mini-batch gradient descent\n",
    "   - Forward and backward passes\n",
    "   - Weight updates via backpropagation\n",
    "\n",
    "4. **Analyzed Results:**\n",
    "   - Visualized training progress\n",
    "   - Examined learned filters\n",
    "   - Explored feature maps\n",
    "   - Compared CNN vs FC networks\n",
    "\n",
    "### üßÆ Key Concepts\n",
    "\n",
    "**Training Loop:**\n",
    "```python\n",
    "for epoch in epochs:\n",
    "    for batch in data:\n",
    "        # 1. Forward pass\n",
    "        predictions = model.forward(batch)\n",
    "        \n",
    "        # 2. Calculate loss\n",
    "        loss = cross_entropy(predictions, labels)\n",
    "        \n",
    "        # 3. Backward pass\n",
    "        gradients = backward(loss)\n",
    "        \n",
    "        # 4. Update weights\n",
    "        weights -= learning_rate * gradients\n",
    "```\n",
    "\n",
    "**Backpropagation Through CNN:**\n",
    "- Gradient flows backward through each layer\n",
    "- Each layer computes: gradient w.r.t. input, gradient w.r.t. weights\n",
    "- Chain rule connects everything\n",
    "- Weights updated to minimize loss\n",
    "\n",
    "**Why CNNs Work:**\n",
    "- Local connectivity ‚Üí fewer parameters\n",
    "- Parameter sharing ‚Üí translation invariance\n",
    "- Hierarchical features ‚Üí powerful representations\n",
    "- Result: Efficient and effective! üéØ\n",
    "\n",
    "### üí° Key Insights\n",
    "\n",
    "1. **Learned Filters**: Network automatically discovers useful patterns (we don't design them!)\n",
    "\n",
    "2. **Feature Hierarchy**: \n",
    "   - Layer 1: Simple patterns (edges)\n",
    "   - Layer 2: Complex patterns (shapes)\n",
    "   - Layer N: Abstract concepts (objects)\n",
    "\n",
    "3. **Parameter Efficiency**: CNN uses 19√ó fewer parameters than FC but achieves similar/better performance\n",
    "\n",
    "4. **Spatial Structure**: CNN exploits 2D structure of images; FC networks ignore it\n",
    "\n",
    "### üöÄ What's Next?\n",
    "\n",
    "Now that you understand how CNNs work, you can:\n",
    "\n",
    "1. **Learn Famous Architectures** (Notebook 05)\n",
    "   - LeNet, AlexNet, VGG, ResNet\n",
    "   - What makes each special\n",
    "   - When to use each\n",
    "\n",
    "2. **Explore Transfer Learning** (Notebook 06)\n",
    "   - Use pre-trained models\n",
    "   - Fine-tuning strategies\n",
    "   - Feature extraction\n",
    "\n",
    "3. **Use PyTorch/TensorFlow** (Notebook 07)\n",
    "   - Build CNNs in modern frameworks\n",
    "   - GPU acceleration\n",
    "   - Production deployment\n",
    "\n",
    "### üéì Practice Exercises\n",
    "\n",
    "Want to solidify your understanding? Try these:\n",
    "\n",
    "1. **Modify the Architecture:**\n",
    "   - Add a third conv layer\n",
    "   - Change filter sizes (5√ó5 instead of 3√ó3)\n",
    "   - Try different pooling strategies\n",
    "\n",
    "2. **Experiment with Hyperparameters:**\n",
    "   - Learning rate: Try 0.001, 0.01, 0.1\n",
    "   - Batch size: Try 16, 32, 64\n",
    "   - More/fewer filters\n",
    "\n",
    "3. **Implement Improvements:**\n",
    "   - Add batch normalization\n",
    "   - Implement dropout for regularization\n",
    "   - Try different optimizers (momentum, Adam)\n",
    "\n",
    "4. **Visualize More:**\n",
    "   - Plot confusion matrix\n",
    "   - Visualize misclassified examples\n",
    "   - Create activation heatmaps\n",
    "\n",
    "5. **Compare Techniques:**\n",
    "   - Different initialization strategies\n",
    "   - Various activation functions (tanh, sigmoid, leaky ReLU)\n",
    "   - Average pooling vs max pooling\n",
    "\n",
    "### üéâ Congratulations!\n",
    "\n",
    "You've completed the CNN fundamentals series! You now understand:\n",
    "- **Why** CNNs work (Notebook 01)\n",
    "- **How** convolution works (Notebook 02)\n",
    "- **What** pooling does (Notebook 03)\n",
    "- **Building** complete CNNs (Notebook 04)\n",
    "\n",
    "You're ready to tackle real-world computer vision problems! üí™\n",
    "\n",
    "---\n",
    "\n",
    "*Remember: The best way to learn is by doing. Modify the code, break things, fix them, and experiment!* üöÄ\n",
    "\n",
    "*Ready to learn about famous CNN architectures? Let's go!* ‚Üí **[Next: Notebook 05 - Famous CNN Architectures](05_famous_architectures.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
