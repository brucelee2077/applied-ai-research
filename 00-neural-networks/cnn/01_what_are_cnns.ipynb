{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ce8c48",
   "metadata": {},
   "source": [
    "# üñºÔ∏è What Are CNNs? Introduction to Convolutional Neural Networks\n",
    "\n",
    "Welcome to the world of **Convolutional Neural Networks (CNNs)**! üéâ\n",
    "\n",
    "In the fundamentals series, we learned about regular neural networks (also called fully-connected or dense networks). Now we're going to learn about a specialized type of neural network that's absolutely **amazing** at working with images!\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- **Why regular neural networks fail** for image tasks (the parameter explosion problem)\n",
    "- **What makes CNNs special** and different\n",
    "- **Three key principles**: Local connectivity, parameter sharing, translation invariance\n",
    "- **Real-world applications** of CNNs\n",
    "- **Visual comparison** between fully-connected and convolutional layers\n",
    "\n",
    "**Prerequisites:** Understanding of basic neural networks (neurons, layers, activation functions) from the fundamentals series.\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a43d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch, ConnectionPatch\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üì¶ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584ef955",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§î The Problem: Why Regular Neural Networks Fail for Images\n",
    "\n",
    "### üí• The Parameter Explosion Problem\n",
    "\n",
    "Let's start by understanding why we can't just use regular neural networks for images.\n",
    "\n",
    "**Imagine you want to classify images of cats and dogs.**\n",
    "\n",
    "A tiny image might be:\n",
    "- **28 √ó 28 pixels** (like MNIST digits)\n",
    "- **Grayscale** (1 color channel)\n",
    "- **Total inputs**: 28 √ó 28 √ó 1 = **784 pixels**\n",
    "\n",
    "That's manageable! But real images are much bigger:\n",
    "- **224 √ó 224 pixels** (typical for computer vision)\n",
    "- **RGB color** (3 channels: Red, Green, Blue)\n",
    "- **Total inputs**: 224 √ó 224 √ó 3 = **150,528 pixels**\n",
    "\n",
    "Now let's count the parameters in a regular neural network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5fd8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate parameters for different image sizes with fully-connected networks\n",
    "\n",
    "def calculate_fc_parameters(image_height, image_width, channels, hidden_size):\n",
    "    \"\"\"\n",
    "    Calculate the number of parameters in a fully-connected layer.\n",
    "    \n",
    "    Args:\n",
    "        image_height: Height of the image in pixels\n",
    "        image_width: Width of the image in pixels\n",
    "        channels: Number of color channels (1 for grayscale, 3 for RGB)\n",
    "        hidden_size: Number of neurons in the hidden layer\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with parameter counts\n",
    "    \"\"\"\n",
    "    input_size = image_height * image_width * channels\n",
    "    \n",
    "    # Parameters = weights + biases\n",
    "    # weights = input_size √ó hidden_size\n",
    "    # biases = hidden_size\n",
    "    weights = input_size * hidden_size\n",
    "    biases = hidden_size\n",
    "    total = weights + biases\n",
    "    \n",
    "    return {\n",
    "        'input_size': input_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'weights': weights,\n",
    "        'biases': biases,\n",
    "        'total': total\n",
    "    }\n",
    "\n",
    "# Test with different image sizes\n",
    "test_cases = [\n",
    "    (\"MNIST (tiny)\", 28, 28, 1, 128),\n",
    "    (\"Small RGB\", 64, 64, 3, 128),\n",
    "    (\"Medium RGB\", 128, 128, 3, 256),\n",
    "    (\"ImageNet (typical)\", 224, 224, 3, 512),\n",
    "    (\"HD Image\", 512, 512, 3, 1024)\n",
    "]\n",
    "\n",
    "print(\"üî• PARAMETER EXPLOSION IN FULLY-CONNECTED NETWORKS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Image Type':<20} {'Input Size':<15} {'Hidden':<10} {'Parameters':<20}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for name, h, w, c, hidden in test_cases:\n",
    "    params = calculate_fc_parameters(h, w, c, hidden)\n",
    "    results.append((name, params))\n",
    "    \n",
    "    # Format large numbers with commas\n",
    "    input_str = f\"{params['input_size']:,}\"\n",
    "    total_str = f\"{params['total']:,}\"\n",
    "    \n",
    "    print(f\"{name:<20} {input_str:<15} {hidden:<10} {total_str:<20}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚ùå PROBLEMS WITH THIS APPROACH:\")\n",
    "print(\"   1. MASSIVE number of parameters (millions for a single layer!)\")\n",
    "print(\"   2. Takes FOREVER to train (too many weights to learn)\")\n",
    "print(\"   3. Easy to OVERFIT (network memorizes instead of generalizing)\")\n",
    "print(\"   4. Requires TONS of memory (cannot fit in GPU)\")\n",
    "print(\"   5. Ignores IMAGE STRUCTURE (treats pixels as independent)\")\n",
    "print(\"\\nüí° We need a better approach... Enter CNNs! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_1",
   "metadata": {},
   "source": [
    "### üìä Visualizing the Parameter Explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Parameter counts (log scale for visibility)\n",
    "names = [r[0] for r in results]\n",
    "param_counts = [r[1]['total'] for r in results]\n",
    "colors = ['lightgreen', 'yellow', 'orange', 'red', 'darkred']\n",
    "\n",
    "bars = ax1.barh(names, param_counts, color=colors, edgecolor='black', linewidth=2)\n",
    "ax1.set_xlabel('Number of Parameters', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Parameter Count in Fully-Connected Networks\\n(just ONE hidden layer!)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_xscale('log')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, param_counts):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width, bar.get_y() + bar.get_height()/2, \n",
    "            f'{count:,}', ha='left', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Plot 2: Memory requirements (assuming 32-bit floats)\n",
    "memory_mb = [r[1]['total'] * 4 / (1024**2) for r in results]  # 4 bytes per parameter\n",
    "\n",
    "bars2 = ax2.barh(names, memory_mb, color=colors, edgecolor='black', linewidth=2)\n",
    "ax2.set_xlabel('Memory (MB)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Memory Requirements\\n(for weights only, one layer!)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mem in zip(bars2, memory_mb):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width, bar.get_y() + bar.get_height()/2, \n",
    "            f'{mem:.1f} MB', ha='left', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí≠ Think about it:\")\n",
    "print(\"   ‚Ä¢ HD images need over 800 MILLION parameters for ONE layer!\")\n",
    "print(\"   ‚Ä¢ That's over 3 GB of memory just for the weights!\")\n",
    "print(\"   ‚Ä¢ And we haven't even added more layers yet!\")\n",
    "print(\"   ‚Ä¢ Training would take forever and probably wouldn't work well...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_3",
   "metadata": {},
   "source": [
    "### üß© The Core Problem: Ignoring Image Structure\n",
    "\n",
    "**The fundamental issue:** Fully-connected networks treat images as flat vectors!\n",
    "\n",
    "```python\n",
    "# What fully-connected networks see:\n",
    "[pixel1, pixel2, pixel3, ..., pixel150528]\n",
    "# Just a HUGE list of numbers with no structure!\n",
    "\n",
    "# What images actually are:\n",
    "# 2D spatial structure with local patterns!\n",
    "```\n",
    "\n",
    "**Images have special properties:**\n",
    "1. **Local connectivity**: Nearby pixels are related (edges, textures)\n",
    "2. **Spatial structure**: Position matters (eyes are above mouth)\n",
    "3. **Translation invariance**: A cat is a cat whether it's on the left or right\n",
    "4. **Hierarchical patterns**: Pixels ‚Üí edges ‚Üí shapes ‚Üí objects\n",
    "\n",
    "**Fully-connected networks ignore ALL of this!** üò±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual showing how FC networks \"see\" images\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Create a simple example \"image\"\n",
    "example_image = np.array([\n",
    "    [0, 0, 1, 1, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 1, 0],\n",
    "    [1, 0, 0, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 1, 1, 1, 0, 0],\n",
    "])\n",
    "\n",
    "# Plot 1: Original 2D structure\n",
    "axes[0].imshow(example_image, cmap='gray', interpolation='nearest')\n",
    "axes[0].set_title('Original Image\\n(2D structure)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, color='cyan', linewidth=1.5)\n",
    "axes[0].set_xticks(range(7))\n",
    "axes[0].set_yticks(range(7))\n",
    "axes[0].text(3, -1.5, 'Spatial relationships preserved', \n",
    "            ha='center', fontsize=11, style='italic',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "# Plot 2: Flattened to vector (what FC networks see)\n",
    "flat_image = example_image.flatten()\n",
    "axes[1].bar(range(len(flat_image)), flat_image, color='gray', edgecolor='black')\n",
    "axes[1].set_title('Fully-Connected View\\n(flattened to 1D vector)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Pixel Index', fontsize=11)\n",
    "axes[1].set_ylabel('Pixel Value', fontsize=11)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].text(24, -0.3, 'All spatial structure LOST!', \n",
    "            ha='center', fontsize=11, style='italic',\n",
    "            bbox=dict(boxstyle='round', facecolor='red', alpha=0.5))\n",
    "\n",
    "# Plot 3: Show the connection problem\n",
    "axes[2].axis('off')\n",
    "axes[2].set_xlim(0, 10)\n",
    "axes[2].set_ylim(0, 10)\n",
    "axes[2].set_title('Connection Explosion\\n(every pixel ‚Üí every neuron)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "\n",
    "# Draw input nodes\n",
    "for i in range(7):\n",
    "    y = 1 + i * 1.2\n",
    "    circle = plt.Circle((2, y), 0.3, color='lightblue', ec='black', linewidth=1.5, zorder=5)\n",
    "    axes[2].add_patch(circle)\n",
    "    if i == 0:\n",
    "        axes[2].text(0.5, y, f'{49} input\\npixels', ha='right', va='center', fontsize=9)\n",
    "\n",
    "# Draw output nodes\n",
    "for i in range(5):\n",
    "    y = 2 + i * 1.5\n",
    "    circle = plt.Circle((8, y), 0.3, color='lightcoral', ec='black', linewidth=1.5, zorder=5)\n",
    "    axes[2].add_patch(circle)\n",
    "    if i == 0:\n",
    "        axes[2].text(9.5, y, f'100\\nneurons', ha='left', va='center', fontsize=9)\n",
    "\n",
    "# Draw some connections (not all, would be too messy)\n",
    "np.random.seed(42)\n",
    "for _ in range(50):\n",
    "    i = np.random.randint(0, 7)\n",
    "    j = np.random.randint(0, 5)\n",
    "    y1 = 1 + i * 1.2\n",
    "    y2 = 2 + j * 1.5\n",
    "    axes[2].plot([2.3, 7.7], [y1, y2], 'gray', linewidth=0.3, alpha=0.3, zorder=1)\n",
    "\n",
    "axes[2].text(5, 0.5, f'49 √ó 100 = 4,900 connections!\\n(and that\\'s just a 7√ó7 image)', \n",
    "            ha='center', fontsize=10, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Key Observations:\")\n",
    "print(\"   1. Fully-connected networks destroy spatial relationships\")\n",
    "print(\"   2. Every pixel connects to every neuron (too many connections!)\")\n",
    "print(\"   3. Nearby pixels (which usually relate to each other) are treated independently\")\n",
    "print(\"   4. The network has to re-learn the same pattern everywhere in the image\")\n",
    "print(\"\\nüí° We need a better way that respects image structure...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_5",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚ú® The Solution: Convolutional Neural Networks!\n",
    "\n",
    "**CNNs solve these problems with three brilliant ideas:**\n",
    "\n",
    "### 1Ô∏è‚É£ Local Connectivity\n",
    "### 2Ô∏è‚É£ Parameter Sharing  \n",
    "### 3Ô∏è‚É£ Translation Invariance\n",
    "\n",
    "Let's understand each one!\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Principle #1: Local Connectivity\n",
    "\n",
    "### üéØ The Big Idea\n",
    "\n",
    "**Instead of connecting every pixel to every neuron, connect each neuron to only a SMALL LOCAL REGION of pixels!**\n",
    "\n",
    "### üèòÔ∏è The Neighborhood Analogy\n",
    "\n",
    "Think about how you understand your city:\n",
    "- You don't need to know about EVERY street simultaneously\n",
    "- You understand your **neighborhood** first\n",
    "- Then how neighborhoods connect\n",
    "- Then how districts form\n",
    "\n",
    "**CNNs work the same way with images!**\n",
    "\n",
    "### üìê How It Works\n",
    "\n",
    "- Use a small **filter** (also called kernel), like 3√ó3 or 5√ó5\n",
    "- Each neuron only looks at its local 3√ó3 region\n",
    "- Move this filter across the image\n",
    "- Dramatically reduces parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize local connectivity\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Create example image\n",
    "img_size = 8\n",
    "image = np.random.rand(img_size, img_size)\n",
    "\n",
    "# Plot 1: Show the image with receptive field\n",
    "ax1.imshow(image, cmap='viridis', interpolation='nearest')\n",
    "ax1.set_title('Input Image (8√ó8)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Highlight a 3x3 receptive field\n",
    "rect = Rectangle((1.5, 1.5), 3, 3, linewidth=4, edgecolor='red', facecolor='none')\n",
    "ax1.add_patch(rect)\n",
    "ax1.text(3, 0.5, 'Receptive Field\\n(3√ó3 region)', ha='center', fontsize=10, \n",
    "         fontweight='bold', color='red',\n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
    "\n",
    "# Add grid\n",
    "for i in range(img_size + 1):\n",
    "    ax1.axhline(i - 0.5, color='white', linewidth=0.5)\n",
    "    ax1.axvline(i - 0.5, color='white', linewidth=0.5)\n",
    "\n",
    "# Plot 2: Show fully-connected (bad)\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Fully-Connected\\n‚ùå Every pixel connects', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Draw pixels (left side)\n",
    "for i in range(6):\n",
    "    for j in range(3):\n",
    "        y = 1.5 + i * 1.2\n",
    "        x = 1.5 + j * 0.6\n",
    "        circle = plt.Circle((x, y), 0.15, color='lightblue', ec='black', zorder=5)\n",
    "        ax2.add_patch(circle)\n",
    "\n",
    "# Draw neuron (right side)\n",
    "neuron = plt.Circle((8, 5), 0.4, color='lightcoral', ec='black', linewidth=2, zorder=5)\n",
    "ax2.add_patch(neuron)\n",
    "\n",
    "# Draw many connections\n",
    "for i in range(6):\n",
    "    for j in range(3):\n",
    "        y1 = 1.5 + i * 1.2\n",
    "        x1 = 1.5 + j * 0.6\n",
    "        ax2.plot([x1 + 0.15, 7.6], [y1, 5], 'gray', linewidth=0.5, alpha=0.3, zorder=1)\n",
    "\n",
    "ax2.text(5, 0.5, '64 pixels √ó 1 neuron\\n= 64 connections', ha='center', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='red', alpha=0.5))\n",
    "\n",
    "# Plot 3: Show local connectivity (good)\n",
    "ax3.set_xlim(0, 10)\n",
    "ax3.set_ylim(0, 10)\n",
    "ax3.axis('off')\n",
    "ax3.set_title('Local Connectivity (CNN)\\n‚úÖ Only local connections', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Draw 3x3 patch of pixels (left side)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        y = 3 + i * 1.0\n",
    "        x = 1.5 + j * 1.0\n",
    "        circle = plt.Circle((x, y), 0.2, color='lightblue', ec='black', linewidth=1.5, zorder=5)\n",
    "        ax3.add_patch(circle)\n",
    "\n",
    "# Draw neuron (right side)\n",
    "neuron2 = plt.Circle((8, 4.5), 0.4, color='lightcoral', ec='black', linewidth=2, zorder=5)\n",
    "ax3.add_patch(neuron2)\n",
    "\n",
    "# Draw only local connections\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        y1 = 3 + i * 1.0\n",
    "        x1 = 1.5 + j * 1.0\n",
    "        ax3.plot([x1 + 0.2, 7.6], [y1, 4.5], 'green', linewidth=1.5, alpha=0.6, zorder=1)\n",
    "\n",
    "ax3.text(5, 0.5, 'Only 3√ó3 = 9 connections!\\n‚ú® Much more efficient', ha='center', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "# Add arrows and labels\n",
    "ax2.text(1.8, 8.5, '64 pixels', ha='center', fontsize=9, fontweight='bold')\n",
    "ax3.text(2.5, 6.5, '3√ó3 patch', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Local Connectivity Benefits:\")\n",
    "print(\"   1. Respects spatial structure (nearby pixels are related)\")\n",
    "print(\"   2. Dramatically fewer parameters (9 vs 64 in this tiny example!)\")\n",
    "print(\"   3. Each neuron becomes a 'feature detector' for its local region\")\n",
    "print(\"   4. Similar to how our eyes work (receptive fields in visual cortex)\")\n",
    "print(\"\\nüìä Parameter Comparison for 224√ó224 RGB image:\")\n",
    "print(f\"   Fully-connected to 512 neurons: {224*224*3*512:,} parameters\")\n",
    "print(f\"   CNN with 3√ó3 filters, 512 neurons: {3*3*3*512:,} parameters\")\n",
    "print(f\"   Reduction: {(224*224*3*512) / (3*3*3*512):.0f}x fewer parameters! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_7",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Principle #2: Parameter Sharing\n",
    "\n",
    "### üéØ The Big Idea\n",
    "\n",
    "**Use the SAME filter (same weights) across the entire image!**\n",
    "\n",
    "### üîç The Pattern Recognition Analogy\n",
    "\n",
    "Imagine you're learning to spot stop signs:\n",
    "- Once you learn what a stop sign looks like, you can recognize it **anywhere** in your field of vision\n",
    "- You don't need to learn \"stop sign on left\" separately from \"stop sign on right\"\n",
    "- **The same pattern recognition applies everywhere!**\n",
    "\n",
    "### üìê How It Works\n",
    "\n",
    "- One 3√ó3 filter has just **9 weights**\n",
    "- Apply this SAME filter to every position in the image\n",
    "- If the filter detects \"vertical edge\", it detects it everywhere\n",
    "- The network learns: \"What patterns exist?\" not \"Where are they?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate parameter sharing\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Create a simple image with a vertical edge\n",
    "simple_image = np.zeros((8, 8))\n",
    "simple_image[:, 0:3] = 0.3\n",
    "simple_image[:, 4:7] = 0.9\n",
    "\n",
    "# Create a vertical edge detector filter\n",
    "edge_filter = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1]\n",
    "])\n",
    "\n",
    "# Plot 1: Original image\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.imshow(simple_image, cmap='gray', interpolation='nearest')\n",
    "ax1.set_title('Input Image\\n(has vertical edges)', fontsize=12, fontweight='bold')\n",
    "for i in range(9):\n",
    "    ax1.axhline(i - 0.5, color='cyan', linewidth=0.5)\n",
    "    ax1.axvline(i - 0.5, color='cyan', linewidth=0.5)\n",
    "\n",
    "# Plot 2: The filter (shared everywhere)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "im = ax2.imshow(edge_filter, cmap='RdBu', interpolation='nearest', vmin=-1, vmax=1)\n",
    "ax2.set_title('Shared Filter (3√ó3)\\n\"Vertical Edge Detector\"', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax2)\n",
    "\n",
    "# Add values to filter\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax2.text(j, i, f'{edge_filter[i, j]:.0f}', ha='center', va='center',\n",
    "                color='black', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax2.text(1, -1, 'üìå Same 9 weights used EVERYWHERE', ha='center', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# Plot 3: Show filter applied at different positions\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.imshow(simple_image, cmap='gray', interpolation='nearest', alpha=0.3)\n",
    "ax3.set_title('Filter Sliding Across Image\\n(same weights everywhere!)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "\n",
    "# Show filter at 3 different positions\n",
    "positions = [(0, 0, 'red'), (2, 2, 'green'), (4, 4, 'blue')]\n",
    "for pos_y, pos_x, color in positions:\n",
    "    rect = Rectangle((pos_x - 0.5, pos_y - 0.5), 3, 3, \n",
    "                     linewidth=3, edgecolor=color, facecolor='none')\n",
    "    ax3.add_patch(rect)\n",
    "\n",
    "# Legend for positions\n",
    "legend_elements = [mpatches.Patch(facecolor='none', edgecolor=c, linewidth=2, label=f'Position {i+1}')\n",
    "                  for i, (_, _, c) in enumerate(positions)]\n",
    "ax3.legend(handles=legend_elements, loc='upper right', fontsize=9)\n",
    "\n",
    "# Plot 4-6: Compute activations at each position\n",
    "for idx, (pos_y, pos_x, color) in enumerate(positions):\n",
    "    ax = fig.add_subplot(gs[1, idx])\n",
    "    \n",
    "    # Extract patch\n",
    "    patch = simple_image[pos_y:pos_y+3, pos_x:pos_x+3]\n",
    "    \n",
    "    # Compute convolution (element-wise multiply and sum)\n",
    "    activation = np.sum(patch * edge_filter)\n",
    "    \n",
    "    # Display\n",
    "    ax.imshow(patch, cmap='gray', interpolation='nearest')\n",
    "    ax.set_title(f'Position {idx+1}\\nActivation = {activation:.2f}', \n",
    "                fontsize=11, fontweight='bold', color=color)\n",
    "    \n",
    "    # Add border\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(color)\n",
    "        spine.set_linewidth(3)\n",
    "    \n",
    "    # Show computation\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            ax.text(j, i, f'{patch[i, j]:.1f}', ha='center', va='center',\n",
    "                   color='yellow', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Plot 7: Output feature map\n",
    "ax7 = fig.add_subplot(gs[2, :])\n",
    "\n",
    "# Compute full feature map (convolution output)\n",
    "output_size = 6  # 8 - 3 + 1\n",
    "feature_map = np.zeros((output_size, output_size))\n",
    "\n",
    "for i in range(output_size):\n",
    "    for j in range(output_size):\n",
    "        patch = simple_image[i:i+3, j:j+3]\n",
    "        feature_map[i, j] = np.sum(patch * edge_filter)\n",
    "\n",
    "im = ax7.imshow(feature_map, cmap='RdBu', interpolation='nearest')\n",
    "ax7.set_title('Output Feature Map (6√ó6)\\nHighlights where vertical edges are!', \n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax7, label='Activation strength')\n",
    "\n",
    "# Add grid\n",
    "for i in range(output_size + 1):\n",
    "    ax7.axhline(i - 0.5, color='black', linewidth=0.5)\n",
    "    ax7.axvline(i - 0.5, color='black', linewidth=0.5)\n",
    "\n",
    "ax7.text(3, -1.2, 'üéØ Same filter applied to every 3√ó3 region = Parameter Sharing!', \n",
    "        ha='center', fontsize=11, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "plt.suptitle('Parameter Sharing: One Filter, Applied Everywhere', \n",
    "            fontsize=15, fontweight='bold', y=0.995)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Parameter Sharing Benefits:\")\n",
    "print(\"   1. Learn pattern ONCE, detect it EVERYWHERE\")\n",
    "print(\"   2. Dramatically fewer parameters (9 weights for entire image!)\")\n",
    "print(\"   3. Filter learns to detect specific features (edges, textures, patterns)\")\n",
    "print(\"   4. Makes the network translation invariant (more on this next!)\")\n",
    "print(\"\\nüìä Example: 224√ó224 image with 64 filters:\")\n",
    "print(f\"   Without sharing: {224*224*64:,} different weight sets needed\")\n",
    "print(f\"   With sharing: only 64 filters √ó 9 weights = {64*9:,} weights\")\n",
    "print(f\"   Reduction: {(224*224*64)/(64*9):.0f}x fewer parameters! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_9",
   "metadata": {},
   "source": [
    "---\n",
    "## üåç Principle #3: Translation Invariance\n",
    "\n",
    "### üéØ The Big Idea\n",
    "\n",
    "**The network recognizes objects regardless of WHERE they appear in the image!**\n",
    "\n",
    "### üê± The Cat Analogy\n",
    "\n",
    "Imagine showing someone photos of cats:\n",
    "- Cat in the center ‚Üí \"That's a cat!\"\n",
    "- Cat on the left ‚Üí \"That's a cat!\"\n",
    "- Cat on the right ‚Üí \"That's a cat!\"\n",
    "- Cat upside down ‚Üí \"That's still a cat (being silly)!\"\n",
    "\n",
    "**You don't need to re-learn what a cat is for each position!**\n",
    "\n",
    "### üìê How It Works\n",
    "\n",
    "Because we use parameter sharing:\n",
    "- The same filter slides across the entire image\n",
    "- It responds to its pattern wherever it appears\n",
    "- A \"cat ear detector\" finds ears anywhere\n",
    "- Higher layers combine these detections to recognize \"cat\" anywhere\n",
    "\n",
    "**This is a NATURAL consequence of parameter sharing!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate translation invariance\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Create a simple \"object\" - a small bright square\n",
    "def create_image_with_object(position):\n",
    "    \"\"\"Create 10x10 image with 2x2 bright square at given position\"\"\"\n",
    "    img = np.zeros((10, 10))\n",
    "    y, x = position\n",
    "    img[y:y+2, x:x+2] = 1.0\n",
    "    return img\n",
    "\n",
    "# Create a simple object detector filter\n",
    "detector_filter = np.array([\n",
    "    [1, 1],\n",
    "    [1, 1]\n",
    "]) / 4  # Average filter\n",
    "\n",
    "# Test at 4 different positions\n",
    "positions = [(1, 1), (1, 6), (6, 1), (6, 6)]\n",
    "position_names = ['Top-Left', 'Top-Right', 'Bottom-Left', 'Bottom-Right']\n",
    "\n",
    "for idx, (pos, name) in enumerate(zip(positions, position_names)):\n",
    "    # Create image with object at this position\n",
    "    img = create_image_with_object(pos)\n",
    "    \n",
    "    # Plot input image\n",
    "    ax_input = axes[0, idx]\n",
    "    ax_input.imshow(img, cmap='gray', interpolation='nearest')\n",
    "    ax_input.set_title(f'Object at {name}\\nPosition: {pos}', \n",
    "                       fontsize=11, fontweight='bold')\n",
    "    ax_input.set_xticks([])\n",
    "    ax_input.set_yticks([])\n",
    "    \n",
    "    # Add grid\n",
    "    for i in range(11):\n",
    "        ax_input.axhline(i - 0.5, color='cyan', linewidth=0.5)\n",
    "        ax_input.axvline(i - 0.5, color='cyan', linewidth=0.5)\n",
    "    \n",
    "    # Compute feature map (apply filter)\n",
    "    output_size = 9  # 10 - 2 + 1\n",
    "    feature_map = np.zeros((output_size, output_size))\n",
    "    \n",
    "    for i in range(output_size):\n",
    "        for j in range(output_size):\n",
    "            patch = img[i:i+2, j:j+2]\n",
    "            feature_map[i, j] = np.sum(patch * detector_filter)\n",
    "    \n",
    "    # Plot output feature map\n",
    "    ax_output = axes[1, idx]\n",
    "    im = ax_output.imshow(feature_map, cmap='hot', interpolation='nearest', vmin=0, vmax=1)\n",
    "    ax_output.set_title(f'Feature Map\\nMax activation: {feature_map.max():.2f}', \n",
    "                        fontsize=11, fontweight='bold')\n",
    "    ax_output.set_xticks([])\n",
    "    ax_output.set_yticks([])\n",
    "    \n",
    "    # Mark the maximum activation\n",
    "    max_pos = np.unravel_index(feature_map.argmax(), feature_map.shape)\n",
    "    ax_output.plot(max_pos[1], max_pos[0], 'g*', markersize=20, \n",
    "                  markeredgecolor='lime', markeredgewidth=2)\n",
    "    \n",
    "    # Add grid\n",
    "    for i in range(output_size + 1):\n",
    "        ax_output.axhline(i - 0.5, color='gray', linewidth=0.3)\n",
    "        ax_output.axvline(i - 0.5, color='gray', linewidth=0.3)\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(im, ax=axes[1, :], orientation='horizontal', pad=0.1, \n",
    "            label='Activation Strength', fraction=0.05)\n",
    "\n",
    "# Add overall title and explanation\n",
    "fig.suptitle('Translation Invariance: Same Filter Detects Object Anywhere!', \n",
    "            fontsize=15, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Translation Invariance in Action:\")\n",
    "print(\"   ‚úÖ Object detected in all 4 positions\")\n",
    "print(\"   ‚úÖ Same filter (same weights) used everywhere\")\n",
    "print(\"   ‚úÖ Maximum activation occurs at object location (green star)\")\n",
    "print(\"   ‚úÖ No need to retrain for different positions!\")\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   Because we use the SAME filter everywhere (parameter sharing),\")\n",
    "print(\"   the network automatically becomes translation invariant!\")\n",
    "print(\"   This is why CNNs are so good at computer vision tasks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_11",
   "metadata": {},
   "source": [
    "### üîÑ But Wait... What About Other Transformations?\n",
    "\n",
    "**Translation Invariance:** ‚úÖ Built into CNNs\n",
    "- Object moves left/right/up/down ‚Üí CNN still detects it\n",
    "\n",
    "**Other transformations require help:**\n",
    "- **Rotation:** ‚ùì Not naturally invariant\n",
    "  - Solution: Data augmentation (train on rotated images)\n",
    "- **Scale:** ‚ùì Not naturally invariant\n",
    "  - Solution: Multi-scale training, image pyramids\n",
    "- **Perspective/Deformation:** ‚ùì Not naturally invariant\n",
    "  - Solution: More data, deeper networks\n",
    "\n",
    "**This is actually a feature, not a bug!**\n",
    "- A cat lying down is different from a standing cat\n",
    "- A car viewed from the side vs from above is different\n",
    "- We WANT the network to learn these as different features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what transformations CNNs handle\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Create a simple arrow shape\n",
    "arrow = np.zeros((12, 12))\n",
    "arrow[5:7, 2:10] = 1  # Horizontal line\n",
    "arrow[3:5, 8:10] = 1  # Top part of arrowhead\n",
    "arrow[7:9, 8:10] = 1  # Bottom part of arrowhead\n",
    "\n",
    "# Transformation 1: Translation (‚úÖ CNN handles well)\n",
    "translated = np.zeros((12, 12))\n",
    "translated[8:10, 5:13] = arrow[5:7, 2:10][:, :8]\n",
    "translated[6:8, 11:13] = arrow[3:5, 8:10]\n",
    "translated[10:12, 11:13] = arrow[7:9, 8:10]\n",
    "\n",
    "axes[0, 0].imshow(arrow, cmap='gray', interpolation='nearest')\n",
    "axes[0, 0].set_title('Original Arrow', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(translated, cmap='gray', interpolation='nearest')\n",
    "axes[0, 1].set_title('Translated Arrow\\n‚úÖ CNN handles this!', \n",
    "                     fontsize=12, fontweight='bold', color='green')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Show why: Same filter responds\n",
    "axes[0, 2].text(0.5, 0.5, \n",
    "                '‚úÖ Why it works:\\n\\n'\n",
    "                'Same filter slides\\n'\n",
    "                'across entire image\\n'\n",
    "                '‚Üì\\n'\n",
    "                'Detects arrow\\n'\n",
    "                'wherever it is!\\n\\n'\n",
    "                'Built-in translation\\n'\n",
    "                'invariance',\n",
    "                ha='center', va='center', fontsize=11,\n",
    "                bbox=dict(boxstyle='round,pad=1', facecolor='lightgreen', alpha=0.8))\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Transformation 2: Rotation (‚ùì CNN struggles)\n",
    "from scipy import ndimage\n",
    "rotated = ndimage.rotate(arrow, 45, reshape=False, order=0)\n",
    "\n",
    "axes[1, 0].imshow(arrow, cmap='gray', interpolation='nearest')\n",
    "axes[1, 0].set_title('Original Arrow', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(rotated, cmap='gray', interpolation='nearest')\n",
    "axes[1, 1].set_title('Rotated Arrow\\n‚ùì CNN needs help', \n",
    "                     fontsize=12, fontweight='bold', color='orange')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Show why it's challenging\n",
    "axes[1, 2].text(0.5, 0.5,\n",
    "                '‚ùì Why it\\'s harder:\\n\\n'\n",
    "                'Filter designed for\\n'\n",
    "                'horizontal arrow\\n'\n",
    "                '‚Üì\\n'\n",
    "                'Doesn\\'t match\\n'\n",
    "                'rotated arrow\\n\\n'\n",
    "                'üí° Solution:\\n'\n",
    "                'Data augmentation\\n'\n",
    "                '(train on rotations)',\n",
    "                ha='center', va='center', fontsize=11,\n",
    "                bbox=dict(boxstyle='round,pad=1', facecolor='lightyellow', alpha=0.8))\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('What Transformations Do CNNs Handle?', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Summary of CNN Invariances:\")\n",
    "print(\"\\n‚úÖ Built-in:\")\n",
    "print(\"   ‚Ä¢ Translation (shifting left/right/up/down)\")\n",
    "print(\"   ‚Ä¢ Small deformations (pooling helps)\")\n",
    "print(\"\\n‚ùì Requires help (data augmentation, special architectures):\")\n",
    "print(\"   ‚Ä¢ Rotation\")\n",
    "print(\"   ‚Ä¢ Scaling\")\n",
    "print(\"   ‚Ä¢ Perspective changes\")\n",
    "print(\"   ‚Ä¢ Extreme deformations\")\n",
    "print(\"\\nüí° This is actually good! We want to learn meaningful differences:\")\n",
    "print(\"   ‚Ä¢ Upright vs upside-down text\")\n",
    "print(\"   ‚Ä¢ Front view vs side view of cars\")\n",
    "print(\"   ‚Ä¢ Standing vs sitting person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_13",
   "metadata": {},
   "source": [
    "---\n",
    "## üåü Real-World CNN Applications\n",
    "\n",
    "CNNs have revolutionized computer vision! Here are some amazing applications:\n",
    "\n",
    "### üñºÔ∏è Image Classification\n",
    "**What it does:** Assign a label to an entire image\n",
    "- \"This image contains a dog\"\n",
    "- Medical diagnosis: \"This X-ray shows pneumonia\"\n",
    "- Quality control: \"This product is defective\"\n",
    "\n",
    "**Famous examples:**\n",
    "- ImageNet classification (ResNet, VGG, Inception)\n",
    "- Google Photos automatic categorization\n",
    "- Plant disease detection apps\n",
    "\n",
    "### üì¶ Object Detection\n",
    "**What it does:** Find and locate multiple objects in an image\n",
    "- Self-driving cars: Detect pedestrians, cars, traffic signs\n",
    "- Surveillance: Count people, detect suspicious behavior\n",
    "- Retail: Track inventory, prevent theft\n",
    "\n",
    "**Famous examples:**\n",
    "- YOLO (You Only Look Once)\n",
    "- Faster R-CNN\n",
    "- Tesla Autopilot vision system\n",
    "\n",
    "### üë§ Face Recognition\n",
    "**What it does:** Identify specific people from their faces\n",
    "- Smartphone unlock (Face ID)\n",
    "- Airport security\n",
    "- Facebook photo tagging\n",
    "\n",
    "**Famous examples:**\n",
    "- Apple Face ID\n",
    "- Facebook DeepFace\n",
    "- Amazon Rekognition\n",
    "\n",
    "### ü©∫ Medical Imaging\n",
    "**What it does:** Analyze medical images for diagnosis\n",
    "- Detect tumors in MRI/CT scans\n",
    "- Identify diabetic retinopathy from eye scans\n",
    "- Analyze skin lesions for melanoma\n",
    "\n",
    "**Impact:**\n",
    "- Often matches or exceeds human expert performance\n",
    "- Faster diagnosis\n",
    "- More accessible healthcare\n",
    "\n",
    "### üé® Image Generation & Editing\n",
    "**What it does:** Create or modify images\n",
    "- Style transfer (make photos look like paintings)\n",
    "- Super-resolution (enhance image quality)\n",
    "- Image inpainting (fill in missing parts)\n",
    "\n",
    "**Famous examples:**\n",
    "- DALL-E, Stable Diffusion (text-to-image)\n",
    "- DeepDream (neural art)\n",
    "- Topaz Gigapixel (image upscaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual summary of CNN applications\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Create simple visualizations for each application\n",
    "applications = [\n",
    "    {\n",
    "        'title': 'üñºÔ∏è Image Classification',\n",
    "        'description': 'Entire image ‚Üí Single label\\n\\nExamples:\\n‚Ä¢ Cat vs Dog\\n‚Ä¢ Disease detection\\n‚Ä¢ Quality control',\n",
    "        'color': 'lightblue',\n",
    "        'pos': (0, 0)\n",
    "    },\n",
    "    {\n",
    "        'title': 'üì¶ Object Detection',\n",
    "        'description': 'Multiple objects ‚Üí Boxes + labels\\n\\nExamples:\\n‚Ä¢ Self-driving cars\\n‚Ä¢ Surveillance\\n‚Ä¢ Retail analytics',\n",
    "        'color': 'lightgreen',\n",
    "        'pos': (0, 1)\n",
    "    },\n",
    "    {\n",
    "        'title': 'üéØ Semantic Segmentation',\n",
    "        'description': 'Classify every pixel\\n\\nExamples:\\n‚Ä¢ Medical imaging\\n‚Ä¢ Satellite imagery\\n‚Ä¢ Autonomous navigation',\n",
    "        'color': 'lightyellow',\n",
    "        'pos': (0, 2)\n",
    "    },\n",
    "    {\n",
    "        'title': 'üë§ Face Recognition',\n",
    "        'description': 'Identify people from faces\\n\\nExamples:\\n‚Ä¢ Phone unlock\\n‚Ä¢ Security systems\\n‚Ä¢ Photo organization',\n",
    "        'color': 'lightcoral',\n",
    "        'pos': (1, 0)\n",
    "    },\n",
    "    {\n",
    "        'title': 'ü©∫ Medical Diagnosis',\n",
    "        'description': 'Analyze medical images\\n\\nExamples:\\n‚Ä¢ Tumor detection\\n‚Ä¢ Retinopathy screening\\n‚Ä¢ Bone fracture detection',\n",
    "        'color': 'plum',\n",
    "        'pos': (1, 1)\n",
    "    },\n",
    "    {\n",
    "        'title': 'üé® Image Generation',\n",
    "        'description': 'Create/modify images\\n\\nExamples:\\n‚Ä¢ Style transfer\\n‚Ä¢ Super-resolution\\n‚Ä¢ Text-to-image',\n",
    "        'color': 'peachpuff',\n",
    "        'pos': (1, 2)\n",
    "    },\n",
    "    {\n",
    "        'title': 'üìπ Video Analysis',\n",
    "        'description': 'Understand video content\\n\\nExamples:\\n‚Ä¢ Action recognition\\n‚Ä¢ Video surveillance\\n‚Ä¢ Sports analytics',\n",
    "        'color': 'lightsteelblue',\n",
    "        'pos': (2, 0)\n",
    "    },\n",
    "    {\n",
    "        'title': 'ü§ñ Robotics Vision',\n",
    "        'description': 'Help robots see the world\\n\\nExamples:\\n‚Ä¢ Object grasping\\n‚Ä¢ Navigation\\n‚Ä¢ Quality inspection',\n",
    "        'color': 'khaki',\n",
    "        'pos': (2, 1)\n",
    "    },\n",
    "    {\n",
    "        'title': 'üåç Satellite Analysis',\n",
    "        'description': 'Analyze Earth from space\\n\\nExamples:\\n‚Ä¢ Crop monitoring\\n‚Ä¢ Disaster response\\n‚Ä¢ Urban planning',\n",
    "        'color': 'palegreen',\n",
    "        'pos': (2, 2)\n",
    "    }\n",
    "]\n",
    "\n",
    "for app in applications:\n",
    "    row, col = app['pos']\n",
    "    ax = fig.add_subplot(gs[row, col])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create colored box\n",
    "    ax.add_patch(Rectangle((0, 0), 1, 1, facecolor=app['color'], \n",
    "                           edgecolor='black', linewidth=3))\n",
    "    \n",
    "    # Add title and description\n",
    "    ax.text(0.5, 0.85, app['title'], ha='center', va='top',\n",
    "           fontsize=13, fontweight='bold')\n",
    "    ax.text(0.5, 0.4, app['description'], ha='center', va='center',\n",
    "           fontsize=9, linespacing=1.5)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.suptitle('üåü Real-World CNN Applications üåü\\n\" CNNs are used in nearly every computer vision application!\"', \n",
    "            fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüöÄ The CNN Revolution:\")\n",
    "print(\"   CNNs achieved what was thought impossible:\")\n",
    "print(\"   ‚Ä¢ 2012: AlexNet wins ImageNet (error drops from 26% to 15%)\")\n",
    "print(\"   ‚Ä¢ 2015: ResNet surpasses human performance on ImageNet\")\n",
    "print(\"   ‚Ä¢ 2016: AlphaGo defeats world Go champion (uses CNNs)\")\n",
    "print(\"   ‚Ä¢ 2020+: CNNs power most computer vision in production\")\n",
    "print(\"\\nüí° Why CNNs won:\")\n",
    "print(\"   1. Fewer parameters (efficient)\")\n",
    "print(\"   2. Translation invariance (robust)\")\n",
    "print(\"   3. Hierarchical features (powerful)\")\n",
    "print(\"   4. End-to-end learning (automatic feature engineering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_15",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Architecture Comparison: Fully-Connected vs CNN\n",
    "\n",
    "Let's put everything together and compare the two approaches side-by-side!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_cell_16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison visualization\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(4, 2, hspace=0.5, wspace=0.3)\n",
    "\n",
    "# Title\n",
    "fig.suptitle('Fully-Connected vs Convolutional Neural Networks\\nComprehensive Comparison', \n",
    "            fontsize=16, fontweight='bold')\n",
    "\n",
    "# ===== FULLY-CONNECTED SIDE (LEFT) =====\n",
    "\n",
    "# FC: Architecture diagram\n",
    "ax_fc_arch = fig.add_subplot(gs[0, 0])\n",
    "ax_fc_arch.set_xlim(0, 10)\n",
    "ax_fc_arch.set_ylim(0, 10)\n",
    "ax_fc_arch.axis('off')\n",
    "ax_fc_arch.set_title('Fully-Connected Architecture', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Draw FC network\n",
    "# Input layer (many nodes)\n",
    "for i in range(10):\n",
    "    y = 1 + i * 0.8\n",
    "    circle = plt.Circle((2, y), 0.2, color='lightblue', ec='black', zorder=5)\n",
    "    ax_fc_arch.add_patch(circle)\n",
    "\n",
    "# Hidden layer\n",
    "for i in range(6):\n",
    "    y = 2.5 + i * 1.1\n",
    "    circle = plt.Circle((5, y), 0.25, color='lightgreen', ec='black', zorder=5)\n",
    "    ax_fc_arch.add_patch(circle)\n",
    "\n",
    "# Output layer\n",
    "for i in range(3):\n",
    "    y = 4 + i * 1.5\n",
    "    circle = plt.Circle((8, y), 0.25, color='lightcoral', ec='black', zorder=5)\n",
    "    ax_fc_arch.add_patch(circle)\n",
    "\n",
    "# Draw connections (sample)\n",
    "for i in range(10):\n",
    "    for j in range(6):\n",
    "        if np.random.random() < 0.3:  # Show only 30% of connections\n",
    "            y1 = 1 + i * 0.8\n",
    "            y2 = 2.5 + j * 1.1\n",
    "            ax_fc_arch.plot([2.2, 4.75], [y1, y2], 'gray', linewidth=0.3, alpha=0.3, zorder=1)\n",
    "\n",
    "# Labels\n",
    "ax_fc_arch.text(2, 0.2, 'Flatten image\\nto vector\\n(e.g., 784 pixels)', \n",
    "               ha='center', fontsize=9, style='italic')\n",
    "ax_fc_arch.text(5, 0.8, 'Hidden\\nLayer', ha='center', fontsize=9, style='italic')\n",
    "ax_fc_arch.text(8, 2, 'Output', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# FC: Problems\n",
    "ax_fc_problems = fig.add_subplot(gs[1, 0])\n",
    "ax_fc_problems.axis('off')\n",
    "problems_text = (\n",
    "    '‚ùå Problems with Fully-Connected:\\n\\n'\n",
    "    '1. Parameter Explosion\\n'\n",
    "    '   ‚Ä¢ 784 inputs √ó 128 hidden = 100,352 params\\n'\n",
    "    '   ‚Ä¢ Grows quadratically with image size\\n\\n'\n",
    "    '2. Ignores Spatial Structure\\n'\n",
    "    '   ‚Ä¢ Treats image as flat vector\\n'\n",
    "    '   ‚Ä¢ Loses 2D relationships\\n\\n'\n",
    "    '3. No Translation Invariance\\n'\n",
    "    '   ‚Ä¢ Must learn patterns at every position\\n'\n",
    "    '   ‚Ä¢ Cat on left ‚â† cat on right\\n\\n'\n",
    "    '4. Memory Intensive\\n'\n",
    "    '   ‚Ä¢ Cannot scale to large images\\n'\n",
    "    '   ‚Ä¢ HD images = billions of parameters'\n",
    ")\n",
    "ax_fc_problems.text(0.1, 0.95, problems_text, ha='left', va='top', \n",
    "                   fontsize=9, family='monospace',\n",
    "                   bbox=dict(boxstyle='round,pad=1', facecolor='mistyrose', alpha=0.8))\n",
    "\n",
    "# FC: Parameter calculation\n",
    "ax_fc_params = fig.add_subplot(gs[2, 0])\n",
    "ax_fc_params.axis('off')\n",
    "ax_fc_params.set_title('Parameter Calculation (28√ó28 image)', fontsize=11, fontweight='bold')\n",
    "\n",
    "params_text = (\n",
    "    'Layer 1: Input ‚Üí Hidden\\n'\n",
    "    '  784 √ó 128 weights = 100,352\\n'\n",
    "    '  + 128 biases\\n'\n",
    "    '  = 100,480 parameters\\n\\n'\n",
    "    'Layer 2: Hidden ‚Üí Output\\n'\n",
    "    '  128 √ó 10 weights = 1,280\\n'\n",
    "    '  + 10 biases\\n'\n",
    "    '  = 1,290 parameters\\n\\n'\n",
    "    '‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n'\n",
    "    'TOTAL: 101,770 parameters\\n'\n",
    "    '‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\\n'\n",
    "    'For 224√ó224 RGB:\\n'\n",
    "    '150,528 √ó 512 = 77,070,336 params!'\n",
    ")\n",
    "ax_fc_params.text(0.5, 0.5, params_text, ha='center', va='center',\n",
    "                 fontsize=9, family='monospace',\n",
    "                 bbox=dict(boxstyle='round,pad=1', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "# ===== CNN SIDE (RIGHT) =====\n",
    "\n",
    "# CNN: Architecture diagram\n",
    "ax_cnn_arch = fig.add_subplot(gs[0, 1])\n",
    "ax_cnn_arch.set_xlim(0, 12)\n",
    "ax_cnn_arch.set_ylim(0, 10)\n",
    "ax_cnn_arch.axis('off')\n",
    "ax_cnn_arch.set_title('Convolutional Architecture', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Draw CNN layers as feature maps\n",
    "# Input image\n",
    "input_rect = Rectangle((1, 3), 2, 4, facecolor='lightblue', edgecolor='black', linewidth=2)\n",
    "ax_cnn_arch.add_patch(input_rect)\n",
    "ax_cnn_arch.text(2, 7.5, '28√ó28\\nInput', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Conv layer 1\n",
    "for i in range(3):\n",
    "    rect = Rectangle((4 + i*0.1, 2.5 + i*0.1), 1.5, 3.5, \n",
    "                     facecolor='lightgreen', edgecolor='black', linewidth=1.5, alpha=0.7)\n",
    "    ax_cnn_arch.add_patch(rect)\n",
    "ax_cnn_arch.text(5, 6.8, '24√ó24\\n16 filters', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Pool layer 1\n",
    "for i in range(3):\n",
    "    rect = Rectangle((6.5 + i*0.1, 3 + i*0.1), 1, 2.5, \n",
    "                     facecolor='khaki', edgecolor='black', linewidth=1.5, alpha=0.7)\n",
    "    ax_cnn_arch.add_patch(rect)\n",
    "ax_cnn_arch.text(7.5, 6.2, '12√ó12\\nPool', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Conv layer 2\n",
    "for i in range(4):\n",
    "    rect = Rectangle((8.5 + i*0.08, 3.2 + i*0.08), 0.8, 2, \n",
    "                     facecolor='lightcoral', edgecolor='black', linewidth=1.5, alpha=0.7)\n",
    "    ax_cnn_arch.add_patch(rect)\n",
    "ax_cnn_arch.text(9.5, 5.8, '8√ó8\\n32 filters', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Arrows\n",
    "arrow_props = dict(arrowstyle='->', lw=2, color='blue')\n",
    "ax_cnn_arch.annotate('', xy=(4, 5), xytext=(3, 5), arrowprops=arrow_props)\n",
    "ax_cnn_arch.annotate('', xy=(6.5, 4.5), xytext=(5.7, 4.5), arrowprops=arrow_props)\n",
    "ax_cnn_arch.annotate('', xy=(8.5, 4.5), xytext=(7.5, 4.5), arrowprops=arrow_props)\n",
    "\n",
    "# Labels\n",
    "ax_cnn_arch.text(3.5, 3.5, 'Conv', ha='center', fontsize=8, style='italic')\n",
    "ax_cnn_arch.text(6, 3.5, 'Pool', ha='center', fontsize=8, style='italic')\n",
    "ax_cnn_arch.text(8, 3.5, 'Conv', ha='center', fontsize=8, style='italic')\n",
    "\n",
    "# CNN: Advantages\n",
    "ax_cnn_advantages = fig.add_subplot(gs[1, 1])\n",
    "ax_cnn_advantages.axis('off')\n",
    "advantages_text = (\n",
    "    '‚úÖ Advantages of CNNs:\\n\\n'\n",
    "    '1. Local Connectivity\\n'\n",
    "    '   ‚Ä¢ Small filters (3√ó3, 5√ó5)\\n'\n",
    "    '   ‚Ä¢ Respects spatial structure\\n\\n'\n",
    "    '2. Parameter Sharing\\n'\n",
    "    '   ‚Ä¢ Same filter across entire image\\n'\n",
    "    '   ‚Ä¢ Dramatically fewer parameters\\n\\n'\n",
    "    '3. Translation Invariance\\n'\n",
    "    '   ‚Ä¢ Detects patterns anywhere\\n'\n",
    "    '   ‚Ä¢ Robust to position changes\\n\\n'\n",
    "    '4. Hierarchical Features\\n'\n",
    "    '   ‚Ä¢ Layer 1: edges, textures\\n'\n",
    "    '   ‚Ä¢ Layer 2: shapes, patterns\\n'\n",
    "    '   ‚Ä¢ Layer 3: objects, concepts'\n",
    ")\n",
    "ax_cnn_advantages.text(0.1, 0.95, advantages_text, ha='left', va='top',\n",
    "                      fontsize=9, family='monospace',\n",
    "                      bbox=dict(boxstyle='round,pad=1', facecolor='honeydew', alpha=0.8))\n",
    "\n",
    "# CNN: Parameter calculation\n",
    "ax_cnn_params = fig.add_subplot(gs[2, 1])\n",
    "ax_cnn_params.axis('off')\n",
    "ax_cnn_params.set_title('Parameter Calculation (28√ó28 image)', fontsize=11, fontweight='bold')\n",
    "\n",
    "cnn_params_text = (\n",
    "    'Conv Layer 1: 16 filters, 3√ó3\\n'\n",
    "    '  3√ó3√ó1√ó16 weights = 144\\n'\n",
    "    '  + 16 biases\\n'\n",
    "    '  = 160 parameters\\n\\n'\n",
    "    'Conv Layer 2: 32 filters, 3√ó3\\n'\n",
    "    '  3√ó3√ó16√ó32 weights = 4,608\\n'\n",
    "    '  + 32 biases\\n'\n",
    "    '  = 4,640 parameters\\n\\n'\n",
    "    'FC Layer: 32√ó8√ó8 ‚Üí 10\\n'\n",
    "    '  2,048√ó10 = 20,480\\n\\n'\n",
    "    '‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n'\n",
    "    'TOTAL: ~25,280 parameters\\n'\n",
    "    '‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\\n'\n",
    "    '4x fewer than FC! üéâ'\n",
    ")\n",
    "ax_cnn_params.text(0.5, 0.5, cnn_params_text, ha='center', va='center',\n",
    "                  fontsize=9, family='monospace',\n",
    "                  bbox=dict(boxstyle='round,pad=1', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "# Bottom: Summary comparison table\n",
    "ax_summary = fig.add_subplot(gs[3, :])\n",
    "ax_summary.axis('off')\n",
    "\n",
    "# Create comparison table\n",
    "table_data = [\n",
    "    ['Aspect', 'Fully-Connected', 'Convolutional'],\n",
    "    ['Parameters (28√ó28)', '~100K', '~25K (4x less)'],\n",
    "    ['Spatial Structure', '‚ùå Destroyed', '‚úÖ Preserved'],\n",
    "    ['Translation Invariance', '‚ùå No', '‚úÖ Yes'],\n",
    "    ['Scalability', '‚ùå Poor', '‚úÖ Excellent'],\n",
    "    ['Training Speed', '‚ùå Slow', '‚úÖ Fast'],\n",
    "    ['Memory Usage', '‚ùå High', '‚úÖ Low'],\n",
    "    ['Best For', 'Tabular data', 'Images, spatial data']\n",
    "]\n",
    "\n",
    "table = ax_summary.table(cellText=table_data, cellLoc='center', loc='center',\n",
    "                        colWidths=[0.2, 0.4, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Color the header row\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor('lightgray')\n",
    "    table[(0, i)].set_text_props(weight='bold')\n",
    "\n",
    "# Color the columns\n",
    "for i in range(1, len(table_data)):\n",
    "    table[(i, 1)].set_facecolor('mistyrose')\n",
    "    table[(i, 2)].set_facecolor('honeydew')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ KEY TAKEAWAY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nCNNs solve the fundamental problems of applying neural networks to images:\")\n",
    "print(\"\\n1. Reduce parameters (local connectivity + sharing)\")\n",
    "print(\"2. Respect spatial structure (2D convolutions)\")\n",
    "print(\"3. Translation invariance (same filter everywhere)\")\n",
    "print(\"4. Hierarchical features (layers build on each other)\")\n",
    "print(\"\\nResult: State-of-the-art performance on virtually all computer vision tasks!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_17",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Summary: The Three Pillars of CNNs\n",
    "\n",
    "### 1Ô∏è‚É£ Local Connectivity\n",
    "- Each neuron only looks at a small region (receptive field)\n",
    "- Respects spatial structure of images\n",
    "- Dramatically reduces connections\n",
    "\n",
    "### 2Ô∏è‚É£ Parameter Sharing\n",
    "- Same filter weights used across entire image\n",
    "- Learn patterns once, detect everywhere\n",
    "- Massive parameter reduction\n",
    "\n",
    "### 3Ô∏è‚É£ Translation Invariance\n",
    "- Recognizes patterns regardless of position\n",
    "- Natural consequence of parameter sharing\n",
    "- Makes CNNs robust to spatial variations\n",
    "\n",
    "### üîç Why This Matters\n",
    "\n",
    "**Traditional Neural Networks:**\n",
    "- Treat images as flat vectors\n",
    "- Millions of parameters\n",
    "- Don't scale to real images\n",
    "- Ignore spatial structure\n",
    "\n",
    "**Convolutional Neural Networks:**\n",
    "- Preserve 2D structure\n",
    "- Orders of magnitude fewer parameters\n",
    "- Scale to HD images and beyond\n",
    "- Learn hierarchical features\n",
    "\n",
    "**Result:** CNNs are the foundation of modern computer vision! üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "## üéì What's Next?\n",
    "\n",
    "Now that you understand WHY CNNs work, let's learn HOW they work!\n",
    "\n",
    "In the next notebooks, we'll dive into:\n",
    "\n",
    "1. **Notebook 2: Convolution Operation** üî≤\n",
    "   - What exactly IS a convolution?\n",
    "   - Implement conv2d from scratch\n",
    "   - Filters, stride, padding\n",
    "   - Visualize different edge detectors\n",
    "\n",
    "2. **Notebook 3: Pooling Layers** üéØ\n",
    "   - Downsampling and dimensionality reduction\n",
    "   - Max vs average pooling\n",
    "   - Why pooling helps\n",
    "\n",
    "3. **Notebook 4: Building a Complete CNN** üèóÔ∏è\n",
    "   - Put it all together\n",
    "   - Train on MNIST/Fashion-MNIST\n",
    "   - Visualize learned filters\n",
    "   - Compare to fully-connected network\n",
    "\n",
    "Ready to understand how convolution actually works? Let's go! üöÄ\n",
    "\n",
    "**[‚Üí Continue to Notebook 2: Convolution Operation](02_convolution_operation.ipynb)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_cell_18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéÆ Optional: Interactive Exploration\n",
    "\n",
    "Want to play around and build intuition? Try these exercises:\n",
    "\n",
    "### Exercise 1: Parameter Counting\n",
    "Calculate parameters for your own network configurations:\n",
    "- What if we used 5√ó5 filters instead of 3√ó3?\n",
    "- How many parameters for a 512√ó512 RGB image?\n",
    "- Compare FC vs CNN for different image sizes\n",
    "\n",
    "### Exercise 2: Receptive Field\n",
    "- What size image region does one output neuron \"see\"?\n",
    "- How does this change with filter size?\n",
    "- What about with multiple layers?\n",
    "\n",
    "### Exercise 3: Translation Test\n",
    "- Create a simple pattern (like our arrow)\n",
    "- Move it to different positions\n",
    "- Verify that the same filter responds at all positions\n",
    "\n",
    "**Try modifying the code cells above to explore these questions!**\n",
    "\n",
    "---\n",
    "\n",
    "*Congratulations! You now understand the fundamental principles that make CNNs work!* üéâ\n",
    "\n",
    "*Next up: Let's implement the convolution operation from scratch!* üí™"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
