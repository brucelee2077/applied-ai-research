{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Neural Network Layers\n",
    "\n",
    "## From Single Neurons to Powerful Teams\n",
    "\n",
    "Welcome back! In our previous notebooks, we learned about:\n",
    "- What a neural network is (Notebook 1)\n",
    "- How a single neuron works (Notebook 2)\n",
    "- Different activation functions (Notebook 3)\n",
    "\n",
    "Now it's time to scale up! üöÄ\n",
    "\n",
    "### üí° Key Question: Why Do We Need Multiple Neurons?\n",
    "\n",
    "Think about it this way:\n",
    "- **One neuron** is like one person trying to solve a complex problem\n",
    "- **Multiple neurons (a layer)** is like a team of experts, each focusing on different aspects\n",
    "\n",
    "### üè≠ The Assembly Line Analogy\n",
    "\n",
    "Imagine a car manufacturing plant:\n",
    "- **Single neuron**: One person building an entire car (slow, limited)\n",
    "- **Layer of neurons**: An assembly line where multiple workers process the car simultaneously\n",
    "  - Worker 1: Checks the engine\n",
    "  - Worker 2: Inspects the wheels  \n",
    "  - Worker 3: Tests the electronics\n",
    "  - Worker 4: Examines the body\n",
    "\n",
    "Each neuron in a layer looks at the same input data but learns to detect **different features**!\n",
    "\n",
    "Let's build this step by step! üîß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools - these are libraries that help us work with numbers and create visualizations\n",
    "import numpy as np  # NumPy: for mathematical operations and arrays\n",
    "import matplotlib.pyplot as plt  # Matplotlib: for creating graphs and visualizations\n",
    "\n",
    "# This makes our plots appear directly in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed so we get the same random numbers every time (makes our experiments reproducible)\n",
    "np.random.seed(42)  # 42 is just a popular choice (from \"Hitchhiker's Guide to the Galaxy\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üì¶ NumPy version:\", np.__version__)  # Show which version of NumPy we're using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: From One Neuron to Many\n",
    "\n",
    "### üß† Recap: Single Neuron\n",
    "\n",
    "Remember, a single neuron does this:\n",
    "1. Takes inputs: `x1, x2, x3, ...`\n",
    "2. Multiplies each by a weight: `w1*x1, w2*x2, w3*x3, ...`\n",
    "3. Adds them up: `sum = w1*x1 + w2*x2 + w3*x3 + ...`\n",
    "4. Adds a bias: `sum + bias`\n",
    "5. Applies activation function: `activation(sum + bias)`\n",
    "\n",
    "Let's code a single neuron as a refresher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple ReLU activation function (from Notebook 3)\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation function: Returns max(0, x)\n",
    "    - If x is positive, return x\n",
    "    - If x is negative or zero, return 0\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)  # Element-wise maximum between 0 and x\n",
    "\n",
    "# Single neuron function\n",
    "def single_neuron(inputs, weights, bias):\n",
    "    \"\"\"\n",
    "    Compute the output of a single neuron.\n",
    "    \n",
    "    Parameters:\n",
    "    - inputs: array of input values [x1, x2, x3, ...]\n",
    "    - weights: array of weights [w1, w2, w3, ...]\n",
    "    - bias: single number (the bias term)\n",
    "    \n",
    "    Returns:\n",
    "    - output: the neuron's output after activation\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate weighted sum (multiply inputs by weights and sum)\n",
    "    weighted_sum = np.dot(inputs, weights)  # Same as: inputs[0]*weights[0] + inputs[1]*weights[1] + ...\n",
    "    \n",
    "    # Step 2: Add the bias\n",
    "    z = weighted_sum + bias\n",
    "    \n",
    "    # Step 3: Apply activation function (ReLU)\n",
    "    output = relu(z)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example: Let's test our single neuron\n",
    "example_inputs = np.array([1.0, 2.0, 3.0])  # Three input values\n",
    "example_weights = np.array([0.5, -0.3, 0.8])  # Three weights (one for each input)\n",
    "example_bias = 0.1  # One bias value\n",
    "\n",
    "# Run the neuron\n",
    "neuron_output = single_neuron(example_inputs, example_weights, example_bias)\n",
    "\n",
    "print(\"üî¢ Input values:\", example_inputs)\n",
    "print(\"‚öñÔ∏è  Weights:\", example_weights)\n",
    "print(\"‚ûï Bias:\", example_bias)\n",
    "print(\"\\nüìä Calculation:\")\n",
    "print(f\"   Weighted sum: {example_inputs[0]}*{example_weights[0]} + {example_inputs[1]}*{example_weights[1]} + {example_inputs[2]}*{example_weights[2]} = {np.dot(example_inputs, example_weights):.2f}\")\n",
    "print(f\"   Add bias: {np.dot(example_inputs, example_weights):.2f} + {example_bias} = {np.dot(example_inputs, example_weights) + example_bias:.2f}\")\n",
    "print(f\"   Apply ReLU: max(0, {np.dot(example_inputs, example_weights) + example_bias:.2f}) = {neuron_output:.2f}\")\n",
    "print(f\"\\n‚ú® Final output: {neuron_output:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating a Layer of Neurons\n",
    "\n",
    "### üë• The Committee Analogy\n",
    "\n",
    "Now imagine we have **4 neurons** all looking at the **same 3 inputs**:\n",
    "- **Neuron 1**: Might learn to detect \"vertical edges\"\n",
    "- **Neuron 2**: Might learn to detect \"horizontal edges\"\n",
    "- **Neuron 3**: Might learn to detect \"curves\"\n",
    "- **Neuron 4**: Might learn to detect \"brightness\"\n",
    "\n",
    "Each neuron has its **own weights and bias** - they're all specialists!\n",
    "\n",
    "### üìä Layer Structure\n",
    "\n",
    "If we have:\n",
    "- **3 inputs** (x1, x2, x3)\n",
    "- **4 neurons** in our layer\n",
    "\n",
    "Then we need:\n",
    "- **12 weights** (3 weights per neuron √ó 4 neurons)\n",
    "- **4 biases** (1 bias per neuron)\n",
    "\n",
    "Let's implement this the **simple way first** (with a loop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using a loop (easy to understand, but slower)\n",
    "def layer_with_loop(inputs, weights, biases):\n",
    "    \"\"\"\n",
    "    Compute the output of a layer using a loop.\n",
    "    \n",
    "    Parameters:\n",
    "    - inputs: array of shape (num_inputs,) - e.g., [x1, x2, x3]\n",
    "    - weights: 2D array of shape (num_inputs, num_neurons) - weights for all neurons\n",
    "    - biases: array of shape (num_neurons,) - one bias per neuron\n",
    "    \n",
    "    Returns:\n",
    "    - outputs: array of shape (num_neurons,) - outputs from all neurons\n",
    "    \"\"\"\n",
    "    # Get the number of neurons in this layer\n",
    "    num_neurons = weights.shape[1]  # weights.shape[1] tells us how many columns (neurons) we have\n",
    "    \n",
    "    # Create an empty list to store outputs from each neuron\n",
    "    outputs = []  # We'll append each neuron's output to this list\n",
    "    \n",
    "    # Loop through each neuron\n",
    "    for i in range(num_neurons):  # i goes from 0 to num_neurons-1\n",
    "        # Get the weights for this specific neuron (column i from weights matrix)\n",
    "        neuron_weights = weights[:, i]  # The : means \"all rows\", i means \"column i\"\n",
    "        \n",
    "        # Get the bias for this specific neuron\n",
    "        neuron_bias = biases[i]  # Get the i-th bias value\n",
    "        \n",
    "        # Calculate this neuron's output using our single_neuron function\n",
    "        neuron_output = single_neuron(inputs, neuron_weights, neuron_bias)\n",
    "        \n",
    "        # Add this neuron's output to our list\n",
    "        outputs.append(neuron_output)\n",
    "        \n",
    "        # Print what this neuron is doing (for learning purposes)\n",
    "        print(f\"Neuron {i+1}: weights={neuron_weights}, bias={neuron_bias:.2f} ‚Üí output={neuron_output:.2f}\")\n",
    "    \n",
    "    # Convert the list to a NumPy array and return\n",
    "    return np.array(outputs)\n",
    "\n",
    "# Create example data for a layer\n",
    "layer_inputs = np.array([1.0, 2.0, 3.0])  # 3 input values\n",
    "\n",
    "# Create weights for 4 neurons (each neuron needs 3 weights for 3 inputs)\n",
    "# Shape: (3 inputs, 4 neurons)\n",
    "layer_weights = np.array([\n",
    "    [0.2, -0.3, 0.5, 0.1],  # Weights from input 1 to each of the 4 neurons\n",
    "    [0.8, 0.4, -0.2, 0.6],  # Weights from input 2 to each of the 4 neurons\n",
    "    [-0.5, 0.7, 0.3, -0.1]  # Weights from input 3 to each of the 4 neurons\n",
    "])\n",
    "\n",
    "# Create biases (one per neuron)\n",
    "layer_biases = np.array([0.1, -0.2, 0.3, 0.15])  # 4 biases for 4 neurons\n",
    "\n",
    "print(\"üîÑ Computing layer output using a loop:\\n\")\n",
    "print(\"üì• Inputs:\", layer_inputs)\n",
    "print(\"‚öñÔ∏è  Weights shape:\", layer_weights.shape, \"(3 inputs √ó 4 neurons)\")\n",
    "print(\"‚ûï Biases:\", layer_biases)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Compute the layer output\n",
    "layer_outputs_loop = layer_with_loop(layer_inputs, layer_weights, layer_biases)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚ú® Final layer outputs:\", layer_outputs_loop)\n",
    "print(\"üìä We went from 3 inputs ‚Üí 4 outputs (one from each neuron)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Insight: What Just Happened?\n",
    "\n",
    "Each neuron in the layer:\n",
    "1. Looked at the **same 3 inputs**: [1.0, 2.0, 3.0]\n",
    "2. Used **different weights** (its own set of 3 weights)\n",
    "3. Had **its own bias**\n",
    "4. Produced **its own output**\n",
    "\n",
    "So we transformed:\n",
    "- **3 input values** ‚Üí **4 output values**\n",
    "\n",
    "This is what a **layer** does! It's like having multiple specialists all analyzing the same data simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Matrix Multiplication - The Fast Way! üöÄ\n",
    "\n",
    "### üì¶ The Batch Processing Analogy\n",
    "\n",
    "Using a loop is like:\n",
    "- Processing emails one by one (slow!)\n",
    "\n",
    "Using matrix multiplication is like:\n",
    "- Batch processing all emails at once (fast!)\n",
    "\n",
    "### üéØ How Matrix Multiplication Works for Neural Networks\n",
    "\n",
    "Instead of looping through each neuron, we can do **all calculations at once** using matrix multiplication!\n",
    "\n",
    "Here's the magic formula:\n",
    "```\n",
    "outputs = activation(inputs @ weights + biases)\n",
    "```\n",
    "\n",
    "Where `@` is matrix multiplication (also written as `np.dot()`).\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using matrix multiplication (fast and efficient!)\n",
    "def layer_with_matrix(inputs, weights, biases):\n",
    "    \"\"\"\n",
    "    Compute the output of a layer using matrix multiplication.\n",
    "    \n",
    "    This does the SAME thing as layer_with_loop, but much faster!\n",
    "    \n",
    "    Parameters:\n",
    "    - inputs: array of shape (num_inputs,)\n",
    "    - weights: 2D array of shape (num_inputs, num_neurons)\n",
    "    - biases: array of shape (num_neurons,)\n",
    "    \n",
    "    Returns:\n",
    "    - outputs: array of shape (num_neurons,)\n",
    "    \"\"\"\n",
    "    # Step 1: Matrix multiplication - this computes weighted sums for ALL neurons at once!\n",
    "    # inputs @ weights means: multiply inputs by weights using matrix multiplication\n",
    "    weighted_sums = np.dot(inputs, weights)  # Same as inputs @ weights\n",
    "    \n",
    "    # Step 2: Add biases - NumPy automatically adds the bias to each weighted sum\n",
    "    z = weighted_sums + biases  # This adds each bias to the corresponding weighted sum\n",
    "    \n",
    "    # Step 3: Apply activation function to ALL outputs at once\n",
    "    outputs = relu(z)  # ReLU works element-wise on the entire array\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Use the SAME inputs, weights, and biases as before\n",
    "print(\"‚ö° Computing layer output using matrix multiplication:\\n\")\n",
    "print(\"üì• Inputs:\", layer_inputs)\n",
    "print(\"‚öñÔ∏è  Weights shape:\", layer_weights.shape)\n",
    "print(\"‚ûï Biases:\", layer_biases)\n",
    "\n",
    "# Compute the layer output using the fast method\n",
    "layer_outputs_matrix = layer_with_matrix(layer_inputs, layer_weights, layer_biases)\n",
    "\n",
    "print(\"\\n‚ú® Final layer outputs:\", layer_outputs_matrix)\n",
    "print(\"\\nüîç Verification: Are both methods the same?\")\n",
    "print(\"   Loop method outputs:  \", layer_outputs_loop)\n",
    "print(\"   Matrix method outputs:\", layer_outputs_matrix)\n",
    "print(\"   Are they equal?\", np.allclose(layer_outputs_loop, layer_outputs_matrix))  # allclose checks if arrays are nearly equal\n",
    "print(\"\\nüéâ Success! Both methods give the same result, but matrix is MUCH faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßÆ Understanding Matrix Multiplication Visually\n",
    "\n",
    "Let's break down what happened with **actual numbers**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's manually show what matrix multiplication does\n",
    "print(\"üìä DETAILED CALCULATION BREAKDOWN:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nInputs:\", layer_inputs)\n",
    "print(\"\\nWeights matrix (each column = one neuron's weights):\")\n",
    "print(layer_weights)\n",
    "print(\"\\nBiases (one per neuron):\", layer_biases)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEURON-BY-NEURON CALCULATION:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show calculation for each neuron\n",
    "for i in range(4):  # We have 4 neurons\n",
    "    print(f\"\\nNeuron {i+1}:\")\n",
    "    \n",
    "    # Get weights for this neuron (column i)\n",
    "    weights_i = layer_weights[:, i]\n",
    "    \n",
    "    # Calculate weighted sum step by step\n",
    "    print(f\"  Weights: {weights_i}\")\n",
    "    print(f\"  Calculation: ({layer_inputs[0]}√ó{weights_i[0]}) + ({layer_inputs[1]}√ó{weights_i[1]}) + ({layer_inputs[2]}√ó{weights_i[2]})\")\n",
    "    \n",
    "    weighted_sum = np.dot(layer_inputs, weights_i)\n",
    "    print(f\"  Weighted sum: {weighted_sum:.2f}\")\n",
    "    \n",
    "    with_bias = weighted_sum + layer_biases[i]\n",
    "    print(f\"  Add bias ({layer_biases[i]}): {with_bias:.2f}\")\n",
    "    \n",
    "    after_relu = relu(with_bias)\n",
    "    print(f\"  After ReLU: {after_relu:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"\\n‚úÖ FINAL OUTPUT: {layer_outputs_matrix}\")\n",
    "print(\"\\nThis is what the layer computed in ONE STEP using matrix multiplication!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìè Understanding Weight Matrix Shape\n",
    "\n",
    "The shape of the weight matrix is **SUPER IMPORTANT**!\n",
    "\n",
    "**Rule of thumb:**\n",
    "```\n",
    "Weights shape: (number of inputs, number of neurons in layer)\n",
    "```\n",
    "\n",
    "In our example:\n",
    "- **3 inputs** ‚Üí number of rows\n",
    "- **4 neurons** ‚Üí number of columns\n",
    "- **Weights shape**: (3, 4)\n",
    "\n",
    "Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weight matrix structure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Show the weight matrix as a heatmap\n",
    "im = axes[0].imshow(layer_weights, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[0].set_xlabel('Neurons (outputs)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Inputs', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Weight Matrix Visualization\\n(darker = negative, lighter = positive)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(range(4))\n",
    "axes[0].set_xticklabels(['Neuron 1', 'Neuron 2', 'Neuron 3', 'Neuron 4'])\n",
    "axes[0].set_yticks(range(3))\n",
    "axes[0].set_yticklabels(['Input 1', 'Input 2', 'Input 3'])\n",
    "\n",
    "# Add weight values as text\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        text = axes[0].text(j, i, f'{layer_weights[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=axes[0], label='Weight value')\n",
    "\n",
    "# Right plot: Show the connections as a network diagram\n",
    "axes[1].set_xlim(-0.5, 3.5)\n",
    "axes[1].set_ylim(-0.5, 4.5)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Layer Connection Diagram\\n(3 inputs ‚Üí 4 neurons)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Draw input nodes (left side)\n",
    "input_positions = [3.5, 2.5, 1.5]  # y-positions for 3 inputs\n",
    "for i, y in enumerate(input_positions):\n",
    "    circle = plt.Circle((0.5, y), 0.3, color='lightblue', ec='black', linewidth=2, zorder=5)\n",
    "    axes[1].add_patch(circle)\n",
    "    axes[1].text(0.5, y, f'x{i+1}', ha='center', va='center', fontsize=11, fontweight='bold', zorder=6)\n",
    "    axes[1].text(-0.3, y, f'Input {i+1}', ha='right', va='center', fontsize=9)\n",
    "\n",
    "# Draw output nodes (right side)\n",
    "output_positions = [4, 3, 2, 1]  # y-positions for 4 neurons\n",
    "for i, y in enumerate(output_positions):\n",
    "    circle = plt.Circle((2.5, y), 0.3, color='lightcoral', ec='black', linewidth=2, zorder=5)\n",
    "    axes[1].add_patch(circle)\n",
    "    axes[1].text(2.5, y, f'n{i+1}', ha='center', va='center', fontsize=11, fontweight='bold', zorder=6)\n",
    "    axes[1].text(3.3, y, f'Neuron {i+1}', ha='left', va='center', fontsize=9)\n",
    "\n",
    "# Draw connections (lines from each input to each neuron)\n",
    "for i, input_y in enumerate(input_positions):\n",
    "    for j, output_y in enumerate(output_positions):\n",
    "        # Line thickness based on weight magnitude\n",
    "        weight = layer_weights[i, j]\n",
    "        linewidth = abs(weight) * 3  # Thicker lines for larger weights\n",
    "        color = 'red' if weight < 0 else 'green'\n",
    "        alpha = min(abs(weight), 0.7)  # Transparency based on weight\n",
    "        axes[1].plot([0.8, 2.2], [input_y, output_y], color=color, linewidth=linewidth, alpha=alpha, zorder=1)\n",
    "\n",
    "# Add legend\n",
    "axes[1].plot([], [], color='green', linewidth=3, label='Positive weight')\n",
    "axes[1].plot([], [], color='red', linewidth=3, label='Negative weight')\n",
    "axes[1].legend(loc='upper left', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('layer_structure.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ Each INPUT connects to ALL NEURONS (fully connected)\")\n",
    "print(\"   ‚Ä¢ Green lines = positive weights (excitatory)\")\n",
    "print(\"   ‚Ä¢ Red lines = negative weights (inhibitory)\")\n",
    "print(\"   ‚Ä¢ Thicker lines = larger weight magnitudes (stronger connections)\")\n",
    "print(f\"   ‚Ä¢ Total connections: 3 inputs √ó 4 neurons = {3*4} weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Hidden Layers - The Secret Feature Detectors üïµÔ∏è\n",
    "\n",
    "### üîç What Are Hidden Layers?\n",
    "\n",
    "In a neural network:\n",
    "- **Input layer**: The raw data (what we feed in)\n",
    "- **Hidden layer(s)**: Intermediate processing (the \"black box\" where magic happens)\n",
    "- **Output layer**: The final answer (predictions)\n",
    "\n",
    "### üé® The Feature Detector Analogy\n",
    "\n",
    "Think of hidden layers like Instagram filters:\n",
    "- **Layer 1**: Detects basic features (edges, colors, textures)\n",
    "- **Layer 2**: Combines basic features into patterns (shapes, faces)\n",
    "- **Layer 3**: Combines patterns into complex concepts (objects, scenes)\n",
    "\n",
    "Each layer learns to detect increasingly **abstract features**!\n",
    "\n",
    "Let's visualize how a layer transforms data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple example showing how a layer transforms data\n",
    "# We'll use 2D inputs so we can visualize them\n",
    "\n",
    "# Generate some random 2D data points\n",
    "num_points = 100  # Number of data points\n",
    "input_data = np.random.randn(num_points, 2)  # 100 points with 2 features each\n",
    "\n",
    "# Create a layer with 2 inputs and 3 neurons\n",
    "layer_weights_2d = np.array([\n",
    "    [1.5, -0.8, 0.3],   # Weights from input 1 to each neuron\n",
    "    [-0.5, 1.2, 0.9]    # Weights from input 2 to each neuron\n",
    "])\n",
    "\n",
    "layer_biases_2d = np.array([0.5, -0.3, 0.2])  # Biases for 3 neurons\n",
    "\n",
    "# Transform all data points through the layer\n",
    "def transform_batch(inputs, weights, biases):\n",
    "    \"\"\"\n",
    "    Transform multiple data points through a layer.\n",
    "    \n",
    "    Parameters:\n",
    "    - inputs: shape (num_samples, num_inputs)\n",
    "    - weights: shape (num_inputs, num_neurons)\n",
    "    - biases: shape (num_neurons,)\n",
    "    \n",
    "    Returns:\n",
    "    - outputs: shape (num_samples, num_neurons)\n",
    "    \"\"\"\n",
    "    # Matrix multiplication handles all samples at once!\n",
    "    z = np.dot(inputs, weights) + biases  # Compute for ALL samples simultaneously\n",
    "    outputs = relu(z)  # Apply activation to all outputs\n",
    "    return outputs\n",
    "\n",
    "# Transform the data\n",
    "output_data = transform_batch(input_data, layer_weights_2d, layer_biases_2d)\n",
    "\n",
    "print(\"üìä Data Transformation:\")\n",
    "print(f\"   Input shape: {input_data.shape} (100 samples, 2 features each)\")\n",
    "print(f\"   Output shape: {output_data.shape} (100 samples, 3 features each)\")\n",
    "print(\"\\n   ‚ú® The layer transformed 2D data into 3D data!\")\n",
    "\n",
    "# Visualize the transformation\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Original input data (2D)\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.scatter(input_data[:, 0], input_data[:, 1], alpha=0.6, s=50, c='blue', edgecolors='black')\n",
    "ax1.set_xlabel('Input Feature 1', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Input Feature 2', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Original Input Data (2D)\\n100 points with 2 features', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax1.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Plot 2: Output data projected to 2D (neurons 1 and 2)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.scatter(output_data[:, 0], output_data[:, 1], alpha=0.6, s=50, c='red', edgecolors='black')\n",
    "ax2.set_xlabel('Neuron 1 Output', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Neuron 2 Output', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Transformed Data (neurons 1 & 2)\\nAfter passing through layer', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax2.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Plot 3: 3D visualization of all outputs\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "ax3.scatter(output_data[:, 0], output_data[:, 1], output_data[:, 2], \n",
    "           alpha=0.6, s=50, c='green', edgecolors='black')\n",
    "ax3.set_xlabel('Neuron 1', fontsize=10, fontweight='bold')\n",
    "ax3.set_ylabel('Neuron 2', fontsize=10, fontweight='bold')\n",
    "ax3.set_zlabel('Neuron 3', fontsize=10, fontweight='bold')\n",
    "ax3.set_title('Full Output (3D)\\nAll 3 neurons', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('layer_transformation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° What Just Happened?\")\n",
    "print(\"   ‚Ä¢ The layer acted as a FEATURE EXTRACTOR\")\n",
    "print(\"   ‚Ä¢ It transformed the data into a new representation\")\n",
    "print(\"   ‚Ä¢ Each neuron detected different patterns in the data\")\n",
    "print(\"   ‚Ä¢ The network can now see the data in 3D instead of 2D!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Interactive Experimentation üß™\n",
    "\n",
    "### Try It Yourself!\n",
    "\n",
    "Let's play with different layer configurations and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive exploration: Build your own layer!\n",
    "\n",
    "def create_and_test_layer(num_inputs, num_neurons, use_random_weights=True):\n",
    "    \"\"\"\n",
    "    Create a layer and test it with random input.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_inputs: how many input features\n",
    "    - num_neurons: how many neurons in the layer\n",
    "    - use_random_weights: if True, use random weights; if False, use small positive weights\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üèóÔ∏è  BUILDING A LAYER: {num_inputs} inputs ‚Üí {num_neurons} neurons\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create random input data\n",
    "    test_input = np.random.randn(num_inputs)  # Random values around 0\n",
    "    \n",
    "    # Create weights\n",
    "    if use_random_weights:\n",
    "        # Random weights between -1 and 1\n",
    "        weights = np.random.randn(num_inputs, num_neurons) * 0.5\n",
    "    else:\n",
    "        # Small positive weights\n",
    "        weights = np.random.rand(num_inputs, num_neurons) * 0.3\n",
    "    \n",
    "    # Create biases\n",
    "    biases = np.random.randn(num_neurons) * 0.1  # Small random biases\n",
    "    \n",
    "    # Compute output\n",
    "    output = layer_with_matrix(test_input, weights, biases)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüì• Input: {test_input}\")\n",
    "    print(f\"‚öñÔ∏è  Weights shape: {weights.shape}\")\n",
    "    print(f\"‚ûï Biases: {biases}\")\n",
    "    print(f\"\\nüì§ Output: {output}\")\n",
    "    print(f\"\\nüìä Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Total parameters: {weights.size + biases.size} ({weights.size} weights + {biases.size} biases)\")\n",
    "    print(f\"   ‚Ä¢ Active neurons (output > 0): {np.sum(output > 0)} out of {num_neurons}\")\n",
    "    print(f\"   ‚Ä¢ Output mean: {np.mean(output):.4f}\")\n",
    "    print(f\"   ‚Ä¢ Output std: {np.std(output):.4f}\")\n",
    "    \n",
    "    return test_input, weights, biases, output\n",
    "\n",
    "# Experiment 1: Small layer\n",
    "print(\"\\nüî¨ EXPERIMENT 1: Small Layer\")\n",
    "create_and_test_layer(num_inputs=2, num_neurons=3)\n",
    "\n",
    "# Experiment 2: Larger layer\n",
    "print(\"\\n\\nüî¨ EXPERIMENT 2: Larger Layer\")\n",
    "create_and_test_layer(num_inputs=5, num_neurons=10)\n",
    "\n",
    "# Experiment 3: Wide layer (many neurons)\n",
    "print(\"\\n\\nüî¨ EXPERIMENT 3: Wide Layer\")\n",
    "create_and_test_layer(num_inputs=3, num_neurons=20)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üí° Observations:\")\n",
    "print(\"   ‚Ä¢ More neurons = more outputs (more feature detectors)\")\n",
    "print(\"   ‚Ä¢ More inputs = more parameters (more to learn)\")\n",
    "print(\"   ‚Ä¢ Parameters = (num_inputs √ó num_neurons) + num_neurons\")\n",
    "print(\"   ‚Ä¢ ReLU makes some outputs zero (inactive neurons)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Common Mistakes ‚ö†Ô∏è\n",
    "\n",
    "Let's learn from common errors so you can avoid them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Mistake #1: Dimension Mismatch\n",
    "print(\"‚ö†Ô∏è  COMMON MISTAKE #1: Wrong Weight Dimensions\\n\")\n",
    "\n",
    "# Correct way\n",
    "correct_inputs = np.array([1.0, 2.0, 3.0])  # 3 inputs\n",
    "correct_weights = np.array([[0.5, 0.2],     # 3 rows (one per input)\n",
    "                           [0.3, 0.4],\n",
    "                           [0.1, 0.6]])     # 2 columns (one per neuron)\n",
    "correct_biases = np.array([0.1, 0.2])       # 2 biases (one per neuron)\n",
    "\n",
    "print(\"‚úÖ CORRECT:\")\n",
    "print(f\"   Inputs shape: {correct_inputs.shape} (3 inputs)\")\n",
    "print(f\"   Weights shape: {correct_weights.shape} (3 inputs √ó 2 neurons)\")\n",
    "print(f\"   Biases shape: {correct_biases.shape} (2 neurons)\")\n",
    "result = layer_with_matrix(correct_inputs, correct_weights, correct_biases)\n",
    "print(f\"   Output: {result} ‚úì\")\n",
    "\n",
    "print(\"\\n‚ùå WRONG:\")\n",
    "# Wrong way - transposed weights\n",
    "wrong_weights = np.array([[0.5, 0.3, 0.1],  # 2 rows (should be 3!)\n",
    "                         [0.2, 0.4, 0.6]])  # Neurons as rows instead of columns\n",
    "\n",
    "print(f\"   Inputs shape: {correct_inputs.shape} (3 inputs)\")\n",
    "print(f\"   Weights shape: {wrong_weights.shape} (2√ó3 - WRONG!)\")\n",
    "\n",
    "try:\n",
    "    layer_with_matrix(correct_inputs, wrong_weights, correct_biases)\n",
    "except ValueError as e:\n",
    "    print(f\"   ERROR: {e}\")\n",
    "    print(\"   ‚ùó The shapes don't match for matrix multiplication!\")\n",
    "\n",
    "print(\"\\nüí° Remember: Weights shape must be (num_inputs, num_neurons)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Common Mistake #2: Wrong number of biases\n",
    "print(\"\\n‚ö†Ô∏è  COMMON MISTAKE #2: Wrong Number of Biases\\n\")\n",
    "\n",
    "print(\"‚úÖ CORRECT: One bias per neuron\")\n",
    "print(f\"   Neurons: 2\")\n",
    "print(f\"   Biases: {correct_biases} (length 2) ‚úì\")\n",
    "\n",
    "print(\"\\n‚ùå WRONG: Biases don't match neurons\")\n",
    "wrong_biases = np.array([0.1, 0.2, 0.3])  # 3 biases for 2 neurons!\n",
    "print(f\"   Neurons: 2\")\n",
    "print(f\"   Biases: {wrong_biases} (length 3 - WRONG!)\")\n",
    "\n",
    "try:\n",
    "    layer_with_matrix(correct_inputs, correct_weights, wrong_biases)\n",
    "except ValueError as e:\n",
    "    print(f\"   ERROR: {e}\")\n",
    "    print(\"   ‚ùó Number of biases must equal number of neurons!\")\n",
    "\n",
    "print(\"\\nüí° Remember: You need exactly ONE bias per neuron\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Common Mistake #3: Forgetting activation function\n",
    "print(\"\\n‚ö†Ô∏è  COMMON MISTAKE #3: Forgetting Activation Function\\n\")\n",
    "\n",
    "test_inputs = np.array([1.0, -2.0])\n",
    "test_weights = np.array([[0.5], [-0.5]])\n",
    "test_bias = np.array([0.0])\n",
    "\n",
    "# Without activation (LINEAR)\n",
    "linear_output = np.dot(test_inputs, test_weights) + test_bias\n",
    "print(\"‚ùå WITHOUT activation (linear):\")\n",
    "print(f\"   Input: {test_inputs}\")\n",
    "print(f\"   Output: {linear_output[0]:.2f}\")\n",
    "print(\"   Problem: Can be any value (including negative)\")\n",
    "\n",
    "# With activation (NON-LINEAR)\n",
    "nonlinear_output = relu(linear_output)\n",
    "print(\"\\n‚úÖ WITH activation (ReLU):\")\n",
    "print(f\"   Input: {test_inputs}\")\n",
    "print(f\"   Output: {nonlinear_output[0]:.2f}\")\n",
    "print(\"   Benefit: Non-linearity allows learning complex patterns!\")\n",
    "\n",
    "print(\"\\nüí° Remember: ALWAYS use an activation function!\")\n",
    "print(\"   Without it, your network is just linear algebra (boring and limited)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Visualizing Layer Architecture üìê\n",
    "\n",
    "Let's create a comprehensive visualization of how layers connect in a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of a multi-layer architecture\n",
    "def visualize_network_architecture(layer_sizes):\n",
    "    \"\"\"\n",
    "    Visualize a neural network architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    - layer_sizes: list of integers representing neurons in each layer\n",
    "                  e.g., [3, 4, 2] means 3 inputs, 4 hidden neurons, 2 outputs\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.set_xlim(-1, len(layer_sizes))\n",
    "    ax.set_ylim(-1, max(layer_sizes) + 1)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Layer names\n",
    "    layer_names = ['Input Layer'] + [f'Hidden Layer {i}' for i in range(1, len(layer_sizes)-1)] + ['Output Layer']\n",
    "    \n",
    "    ax.set_title('Neural Network Architecture\\n' + ' ‚Üí '.join([f'{size} neurons' for size in layer_sizes]), \n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Draw each layer\n",
    "    neuron_positions = []  # Store positions for drawing connections\n",
    "    \n",
    "    for layer_idx, num_neurons in enumerate(layer_sizes):\n",
    "        layer_x = layer_idx  # x-position of this layer\n",
    "        \n",
    "        # Calculate y-positions to center the neurons vertically\n",
    "        start_y = (max(layer_sizes) - num_neurons) / 2\n",
    "        \n",
    "        layer_neuron_positions = []\n",
    "        \n",
    "        # Draw neurons in this layer\n",
    "        for neuron_idx in range(num_neurons):\n",
    "            neuron_y = start_y + neuron_idx\n",
    "            layer_neuron_positions.append((layer_x, neuron_y))\n",
    "            \n",
    "            # Choose color based on layer type\n",
    "            if layer_idx == 0:\n",
    "                color = 'lightblue'  # Input layer\n",
    "            elif layer_idx == len(layer_sizes) - 1:\n",
    "                color = 'lightcoral'  # Output layer\n",
    "            else:\n",
    "                color = 'lightgreen'  # Hidden layers\n",
    "            \n",
    "            # Draw neuron circle\n",
    "            circle = plt.Circle((layer_x, neuron_y), 0.2, color=color, ec='black', linewidth=2, zorder=5)\n",
    "            ax.add_patch(circle)\n",
    "        \n",
    "        neuron_positions.append(layer_neuron_positions)\n",
    "        \n",
    "        # Add layer label\n",
    "        ax.text(layer_x, max(layer_sizes) + 0.5, layer_names[layer_idx], \n",
    "               ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Draw connections between layers\n",
    "    for layer_idx in range(len(layer_sizes) - 1):\n",
    "        current_layer = neuron_positions[layer_idx]\n",
    "        next_layer = neuron_positions[layer_idx + 1]\n",
    "        \n",
    "        # Connect each neuron in current layer to each neuron in next layer\n",
    "        for x1, y1 in current_layer:\n",
    "            for x2, y2 in next_layer:\n",
    "                ax.plot([x1 + 0.2, x2 - 0.2], [y1, y2], 'gray', linewidth=0.5, alpha=0.3, zorder=1)\n",
    "    \n",
    "    # Add connection count annotations\n",
    "    for layer_idx in range(len(layer_sizes) - 1):\n",
    "        num_connections = layer_sizes[layer_idx] * layer_sizes[layer_idx + 1]\n",
    "        mid_x = layer_idx + 0.5\n",
    "        mid_y = max(layer_sizes) + 0.5\n",
    "        \n",
    "        ax.text(mid_x, -0.7, f'{num_connections} weights', \n",
    "               ha='center', va='top', fontsize=9, style='italic',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('network_architecture.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    total_params = 0\n",
    "    print(\"\\nüìä NETWORK STATISTICS:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        num_weights = layer_sizes[i] * layer_sizes[i+1]\n",
    "        num_biases = layer_sizes[i+1]\n",
    "        layer_params = num_weights + num_biases\n",
    "        total_params += layer_params\n",
    "        \n",
    "        print(f\"\\nLayer {i} ‚Üí Layer {i+1}:\")\n",
    "        print(f\"  ‚Ä¢ Neurons: {layer_sizes[i]} ‚Üí {layer_sizes[i+1]}\")\n",
    "        print(f\"  ‚Ä¢ Weights: {num_weights} ({layer_sizes[i]} √ó {layer_sizes[i+1]})\")\n",
    "        print(f\"  ‚Ä¢ Biases: {num_biases}\")\n",
    "        print(f\"  ‚Ä¢ Total parameters: {layer_params}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üéØ TOTAL PARAMETERS IN NETWORK: {total_params}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Example: Visualize a 3-layer network\n",
    "print(\"üé® Visualizing a 3-layer network: [3, 4, 2]\")\n",
    "visualize_network_architecture([3, 4, 2])\n",
    "\n",
    "# Example: Visualize a deeper network\n",
    "print(\"\\n\\nüé® Visualizing a deeper network: [5, 8, 6, 3]\")\n",
    "visualize_network_architecture([5, 8, 6, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways üìö\n",
    "\n",
    "### üéâ What We Learned Today:\n",
    "\n",
    "1. **Layers are Teams**: Multiple neurons working in parallel, each detecting different features\n",
    "\n",
    "2. **Two Ways to Compute**:\n",
    "   - Loop method: Easy to understand, but slow\n",
    "   - Matrix multiplication: Fast and efficient!\n",
    "\n",
    "3. **Weight Matrix Shape**: Always `(num_inputs, num_neurons)`\n",
    "\n",
    "4. **Hidden Layers**: Act as feature detectors, transforming data into new representations\n",
    "\n",
    "5. **Important Formula**:\n",
    "   ```\n",
    "   Output = Activation(Inputs @ Weights + Biases)\n",
    "   ```\n",
    "\n",
    "6. **Common Mistakes**:\n",
    "   - Wrong weight dimensions\n",
    "   - Wrong number of biases\n",
    "   - Forgetting activation functions\n",
    "\n",
    "### üîÆ What's Next?\n",
    "\n",
    "In **Notebook 5: Forward Propagation**, we'll learn:\n",
    "- How to chain multiple layers together\n",
    "- How data flows through an entire network\n",
    "- Building a complete neural network from scratch!\n",
    "- Making predictions with our network\n",
    "\n",
    "### üí™ Practice Challenge:\n",
    "\n",
    "Before moving on, try modifying the code above to:\n",
    "1. Create a layer with 5 inputs and 7 neurons\n",
    "2. Test it with your own input data\n",
    "3. Visualize the weight matrix\n",
    "4. Count the total number of parameters\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: Each layer is like adding a team of specialists to your network. The more layers you have, the more complex patterns your network can learn! üöÄ\n",
    "\n",
    "Ready to move forward? Let's learn how to connect these layers together in the next notebook! üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}