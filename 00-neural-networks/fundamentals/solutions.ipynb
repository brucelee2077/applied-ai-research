{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Fundamentals - Exercise Solutions\n",
    "\n",
    "This notebook provides complete solutions to all exercises in `exercises.ipynb`. Each solution includes:\n",
    "- Working code with detailed comments\n",
    "- Explanation of the approach\n",
    "- Common mistakes to avoid\n",
    "- Extensions and variations to explore\n",
    "\n",
    "**Note:** Try to solve the exercises yourself before looking at these solutions. Learning happens through struggle!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from viz_utils import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution 1: Single Neuron Variations\n",
    "\n",
    "### Solution 1.1: Linear Neuron\n",
    "\n",
    "**Approach:** Compute weighted sum of inputs plus bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_neuron(inputs, weights, bias):\n",
    "    \"\"\"\n",
    "    Implement a linear neuron.\n",
    "    \n",
    "    This is the simplest type of neuron - just a weighted sum.\n",
    "    No activation function is applied.\n",
    "    \"\"\"\n",
    "    # Compute dot product of inputs and weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Add bias term\n",
    "    output = weighted_sum + bias\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test implementation\n",
    "test_inputs = np.array([1.0, 2.0, 3.0])\n",
    "test_weights = np.array([0.5, -0.3, 0.8])\n",
    "test_bias = 0.1\n",
    "\n",
    "result = linear_neuron(test_inputs, test_weights, test_bias)\n",
    "expected = np.dot(test_inputs, test_weights) + test_bias\n",
    "\n",
    "print(f\"Linear neuron output: {result}\")\n",
    "print(f\"Expected output: {expected}\")\n",
    "print(f\"Match: {np.isclose(result, expected)}\")\n",
    "\n",
    "# Detailed calculation:\n",
    "print(\"\\n=== Detailed Calculation ===\")\n",
    "print(f\"Weighted sum: (1.0 × 0.5) + (2.0 × -0.3) + (3.0 × 0.8) = {0.5 - 0.6 + 2.4}\")\n",
    "print(f\"Plus bias: {0.5 - 0.6 + 2.4} + 0.1 = {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "- Linear neurons can only learn linear relationships\n",
    "- They're the building block for more complex neurons\n",
    "- The bias allows shifting the output up or down\n",
    "\n",
    "**Common Mistakes:**\n",
    "- Forgetting to add the bias term\n",
    "- Using wrong array dimensions for dot product\n",
    "\n",
    "**Extensions:**\n",
    "- Try vectorizing to handle multiple samples at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.2: Sigmoid Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_neuron(inputs, weights, bias):\n",
    "    \"\"\"\n",
    "    Implement a sigmoid-activated neuron.\n",
    "    \n",
    "    Steps:\n",
    "    1. Compute linear combination (same as linear neuron)\n",
    "    2. Apply sigmoid activation to squash output to [0, 1]\n",
    "    \"\"\"\n",
    "    # Step 1: Linear transformation\n",
    "    z = np.dot(inputs, weights) + bias\n",
    "    \n",
    "    # Step 2: Apply sigmoid activation\n",
    "    output = sigmoid(z)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test with 5 inputs\n",
    "test_inputs = np.random.randn(5)\n",
    "test_weights = np.random.randn(5) * 0.1\n",
    "test_bias = 0.0\n",
    "\n",
    "output = sigmoid_neuron(test_inputs, test_weights, test_bias)\n",
    "\n",
    "print(f\"Input values: {test_inputs}\")\n",
    "print(f\"Weights: {test_weights}\")\n",
    "print(f\"Bias: {test_bias}\")\n",
    "print(f\"\\nSigmoid neuron output: {output:.4f}\")\n",
    "print(f\"Output in [0, 1]: {0 <= output <= 1}\")\n",
    "\n",
    "# Demonstrate sigmoid properties\n",
    "print(\"\\n=== Sigmoid Properties ===\")\n",
    "print(f\"Sigmoid(0) = {sigmoid(0):.4f} (always 0.5)\")\n",
    "print(f\"Sigmoid(large positive) = {sigmoid(10):.6f} (approaches 1)\")\n",
    "print(f\"Sigmoid(large negative) = {sigmoid(-10):.6f} (approaches 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Sigmoid?**\n",
    "- Squashes output to [0, 1] range (good for probabilities)\n",
    "- Smooth, differentiable everywhere\n",
    "- Historically popular, though less common now due to vanishing gradients\n",
    "\n",
    "**Common Mistakes:**\n",
    "- Numerical overflow with `exp()` for very negative values (use `np.clip()` if needed)\n",
    "- Confusing sigmoid with softmax (softmax normalizes across multiple outputs)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: ELU Activation Function\n",
    "\n",
    "### Solution 2.1: Implement ELU and Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(x, alpha=1.0):\n",
    "    \"\"\"\n",
    "    ELU activation function.\n",
    "    \n",
    "    ELU smoothly handles negative values unlike ReLU which zeros them out.\n",
    "    This helps with the \"dying ReLU\" problem.\n",
    "    \"\"\"\n",
    "    # Use np.where for element-wise conditional\n",
    "    # If x > 0: return x\n",
    "    # If x <= 0: return alpha * (exp(x) - 1)\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def elu_derivative(x, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Derivative of ELU.\n",
    "    \n",
    "    For x > 0: derivative is 1\n",
    "    For x <= 0: derivative is alpha * exp(x) = ELU(x) + alpha\n",
    "    \"\"\"\n",
    "    return np.where(x > 0, 1, alpha * np.exp(x))\n",
    "\n",
    "# Test and visualize\n",
    "x = np.linspace(-3, 3, 1000)\n",
    "y = elu(x)\n",
    "dy = elu_derivative(x)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot ELU\n",
    "ax1.plot(x, y, 'b-', linewidth=2.5, label='ELU', zorder=3)\n",
    "ax1.plot(x, np.maximum(0, x), 'r--', linewidth=2, label='ReLU (comparison)', alpha=0.7)\n",
    "ax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax1.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax1.axhline(y=-1, color='g', linestyle=':', alpha=0.5, label='Asymptote at -α')\n",
    "ax1.set_title('ELU Activation Function', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Input', fontsize=12)\n",
    "ax1.set_ylabel('Output', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.set_ylim([-2, 3])\n",
    "\n",
    "# Plot derivative\n",
    "ax2.plot(x, dy, 'r-', linewidth=2.5, label='ELU Derivative')\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axhline(y=1, color='b', linestyle=':', alpha=0.5, label='Gradient = 1 for x > 0')\n",
    "ax2.set_title('ELU Gradient', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Input', fontsize=12)\n",
    "ax2.set_ylabel('Gradient', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.set_ylim([-0.1, 1.5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== ELU Properties ===\")\n",
    "print(f\"ELU(0) = {elu(0):.4f} (smooth at origin)\")\n",
    "print(f\"ELU(-1) = {elu(-1):.4f} (negative values allowed)\")\n",
    "print(f\"ELU(-10) ≈ {elu(-10):.4f} (approaches -α for large negatives)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2.2: ELU vs ReLU Discussion\n",
    "\n",
    "**Advantages of ELU over ReLU:**\n",
    "\n",
    "1. **No \"Dying ReLU\" Problem:**\n",
    "   - ReLU neurons can become permanently inactive (output always 0) if they enter a state where all inputs produce negative pre-activations\n",
    "   - ELU allows small negative values, keeping gradients flowing\n",
    "\n",
    "2. **Smooth at Zero:**\n",
    "   - ELU is differentiable everywhere, including at x=0\n",
    "   - ReLU has a sharp corner at x=0 (though this rarely causes issues in practice)\n",
    "\n",
    "3. **Self-Normalizing:**\n",
    "   - ELU's negative saturation helps push mean activations closer to zero\n",
    "   - This can improve training stability\n",
    "\n",
    "4. **Better Gradient Flow:**\n",
    "   - For negative inputs, ELU still has non-zero gradients\n",
    "   - ReLU has zero gradient for all negative inputs\n",
    "\n",
    "**When to Use ELU:**\n",
    "- Deeper networks where dying ReLU is a concern\n",
    "- When you want slightly better performance (at cost of computation)\n",
    "- When training stability is important\n",
    "\n",
    "**When to Use ReLU:**\n",
    "- Default choice for most applications (simpler, faster)\n",
    "- When computational efficiency is critical\n",
    "- When dying ReLU isn't a problem (can be mitigated with proper initialization)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3: Dense Layer and 3-Layer Network\n",
    "\n",
    "### Solution 3.1: Dense Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    \"\"\"\n",
    "    A fully connected (dense) neural network layer.\n",
    "    \n",
    "    This implementation shows how layers work internally before using\n",
    "    frameworks like PyTorch or TensorFlow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize layer with random weights and zero biases.\n",
    "        \n",
    "        Weight initialization is crucial:\n",
    "        - Too large: exploding gradients\n",
    "        - Too small: vanishing gradients\n",
    "        - We use small random values (0.01 scale)\n",
    "        \"\"\"\n",
    "        # Initialize weights from normal distribution, scaled down\n",
    "        # Shape: (n_inputs, n_neurons) for matrix multiplication\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons) * 0.01\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        # Shape: (1, n_neurons) for broadcasting\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass: linear transformation + activation.\n",
    "        \n",
    "        Matrix shapes:\n",
    "        - inputs: (batch_size, n_inputs)\n",
    "        - weights: (n_inputs, n_neurons)\n",
    "        - output: (batch_size, n_neurons)\n",
    "        \"\"\"\n",
    "        # Linear transformation: Z = XW + b\n",
    "        # @ operator is matrix multiplication (same as np.dot)\n",
    "        z = inputs @ self.weights + self.biases\n",
    "        \n",
    "        # Apply activation function\n",
    "        if self.activation == 'relu':\n",
    "            output = np.maximum(0, z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            output = 1 / (1 + np.exp(-z))\n",
    "        elif self.activation == 'linear':\n",
    "            output = z  # No activation\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {self.activation}\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Comprehensive testing\n",
    "print(\"=== Testing Dense Layer ===\")\n",
    "\n",
    "# Test 1: Basic functionality\n",
    "layer = DenseLayer(n_inputs=10, n_neurons=5, activation='relu')\n",
    "test_input = np.random.randn(3, 10)  # 3 samples, 10 features\n",
    "output = layer.forward(test_input)\n",
    "\n",
    "print(f\"\\nTest 1: Basic functionality\")\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Weight shape: {layer.weights.shape}\")\n",
    "print(f\"  Bias shape: {layer.biases.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Expected: (3, 5) ✓\" if output.shape == (3, 5) else \"  ✗ FAILED\")\n",
    "\n",
    "# Test 2: ReLU activation (no negative values)\n",
    "print(f\"\\nTest 2: ReLU activation\")\n",
    "print(f\"  All outputs >= 0: {np.all(output >= 0)}\")\n",
    "print(f\"  Some outputs > 0: {np.any(output > 0)}\")\n",
    "\n",
    "# Test 3: Different activation functions\n",
    "layer_sigmoid = DenseLayer(10, 5, activation='sigmoid')\n",
    "output_sigmoid = layer_sigmoid.forward(test_input)\n",
    "print(f\"\\nTest 3: Sigmoid activation\")\n",
    "print(f\"  All outputs in [0,1]: {np.all((output_sigmoid >= 0) & (output_sigmoid <= 1))}\")\n",
    "\n",
    "# Test 4: Batch processing\n",
    "large_batch = np.random.randn(100, 10)\n",
    "large_output = layer.forward(large_batch)\n",
    "print(f\"\\nTest 4: Batch processing\")\n",
    "print(f\"  Processed 100 samples: {large_output.shape == (100, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Concepts:**\n",
    "\n",
    "1. **Weight Initialization:** Small random values prevent gradient problems\n",
    "2. **Matrix Multiplication:** Shape compatibility is crucial\n",
    "3. **Broadcasting:** Biases automatically expand to match batch size\n",
    "4. **Activation Functions:** Applied element-wise to introduce non-linearity\n",
    "\n",
    "**Common Mistakes:**\n",
    "- Wrong weight matrix shape (should be `(n_inputs, n_neurons)`)\n",
    "- Forgetting to initialize biases\n",
    "- Using `*` instead of `@` for matrix multiplication\n",
    "- Not handling batch dimension properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3.2: Three-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNetwork:\n",
    "    \"\"\"\n",
    "    A complete 3-layer neural network for MNIST classification.\n",
    "    \n",
    "    Architecture:\n",
    "    Input (784) -> Hidden1 (128, ReLU) -> Hidden2 (64, ReLU) -> Output (10, Linear)\n",
    "    \n",
    "    Note: We don't apply softmax in the network itself. It's typically\n",
    "    combined with the loss function for numerical stability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize all three layers.\"\"\"\n",
    "        # Layer 1: 784 -> 128 with ReLU\n",
    "        self.layer1 = DenseLayer(n_inputs=784, n_neurons=128, activation='relu')\n",
    "        \n",
    "        # Layer 2: 128 -> 64 with ReLU\n",
    "        self.layer2 = DenseLayer(n_inputs=128, n_neurons=64, activation='relu')\n",
    "        \n",
    "        # Layer 3: 64 -> 10 with linear activation\n",
    "        # (softmax will be applied during loss computation)\n",
    "        self.layer3 = DenseLayer(n_inputs=64, n_neurons=10, activation='linear')\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through all layers.\n",
    "        \n",
    "        The data flows:\n",
    "        X -> layer1 -> h1 -> layer2 -> h2 -> layer3 -> output\n",
    "        \"\"\"\n",
    "        # Pass through first hidden layer\n",
    "        h1 = self.layer1.forward(X)\n",
    "        \n",
    "        # Pass through second hidden layer\n",
    "        h2 = self.layer2.forward(h1)\n",
    "        \n",
    "        # Pass through output layer\n",
    "        output = self.layer3.forward(h2)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions (with softmax for probabilities).\n",
    "        \"\"\"\n",
    "        # Get raw scores (logits)\n",
    "        logits = self.forward(X)\n",
    "        \n",
    "        # Apply softmax for probabilities\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "# Test the network\n",
    "print(\"=== Testing Three-Layer Network ===\")\n",
    "\n",
    "network = ThreeLayerNetwork()\n",
    "\n",
    "# Test with random MNIST-like data\n",
    "test_batch = np.random.randn(5, 784)\n",
    "raw_output = network.forward(test_batch)\n",
    "predictions = network.predict(test_batch)\n",
    "\n",
    "print(f\"\\nInput shape: {test_batch.shape}\")\n",
    "print(f\"Raw output shape: {raw_output.shape}\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "print(f\"\\nFirst sample raw scores (logits):\")\n",
    "print(raw_output[0])\n",
    "\n",
    "print(f\"\\nFirst sample probabilities (after softmax):\")\n",
    "print(predictions[0])\n",
    "print(f\"Sum of probabilities: {predictions[0].sum():.6f} (should be 1.0)\")\n",
    "\n",
    "print(f\"\\nPredicted class: {np.argmax(predictions[0])}\")\n",
    "\n",
    "# Visualize network architecture\n",
    "print(\"\\n=== Network Architecture ===\")\n",
    "print(\"Layer 1: 784 inputs  -> 128 neurons (ReLU)\")\n",
    "print(\"Layer 2: 128 inputs  -> 64 neurons  (ReLU)\")\n",
    "print(\"Layer 3: 64 inputs   -> 10 neurons  (Linear)\")\n",
    "print(f\"\\nTotal parameters: {784*128 + 128 + 128*64 + 64 + 64*10 + 10:,}\")\n",
    "\n",
    "# Visualize with plotting function\n",
    "fig = plot_network_architecture([784, 128, 64, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Design Decisions:**\n",
    "\n",
    "1. **Layer Sizes:** Gradually decreasing (128 -> 64 -> 10) creates a funnel\n",
    "2. **Activations:** ReLU for hidden layers (fast, effective), linear for output\n",
    "3. **Softmax Placement:** Applied during prediction, not in forward pass\n",
    "\n",
    "**Alternative Approaches:**\n",
    "- Could use dropout for regularization\n",
    "- Could add batch normalization between layers\n",
    "- Could use different activation functions (ELU, Leaky ReLU)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4: Manual Forward Propagation\n",
    "\n",
    "### Solution 4.1: Hand Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual Calculation:**\n",
    "\n",
    "Given:\n",
    "- Input: x = [2.0, -1.0]\n",
    "- Weights: W = [[0.5, -0.3], [0.2, 0.8]]\n",
    "- Biases: b = [0.1, -0.1]\n",
    "- Activation: ReLU\n",
    "\n",
    "**Step-by-Step:**\n",
    "\n",
    "```\n",
    "Neuron 1 (first column):\n",
    "  z₁ = (0.5)(2.0) + (-0.3)(-1.0) + 0.1\n",
    "     = 1.0 + 0.3 + 0.1\n",
    "     = 1.4\n",
    "  a₁ = ReLU(1.4) = max(0, 1.4) = 1.4\n",
    "\n",
    "Neuron 2 (second column):\n",
    "  z₂ = (0.2)(2.0) + (0.8)(-1.0) + (-0.1)\n",
    "     = 0.4 + (-0.8) + (-0.1)\n",
    "     = -0.5\n",
    "  a₂ = ReLU(-0.5) = max(0, -0.5) = 0.0\n",
    "\n",
    "Output: [1.4, 0.0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the calculation\n",
    "print(\"=== Verifying Manual Calculation ===\")\n",
    "\n",
    "x = np.array([2.0, -1.0])\n",
    "W = np.array([[0.5, -0.3],   # First row: weights for neuron 1 from both inputs\n",
    "              [0.2, 0.8]])    # Second row: weights for neuron 2 from both inputs\n",
    "b = np.array([0.1, -0.1])\n",
    "\n",
    "# Method 1: Using @ operator (matrix multiplication)\n",
    "z = x @ W + b\n",
    "output = np.maximum(0, z)  # ReLU\n",
    "\n",
    "print(f\"\\nMethod 1 (Matrix multiplication):\")\n",
    "print(f\"Pre-activation (z): {z}\")\n",
    "print(f\"After ReLU: {output}\")\n",
    "\n",
    "# Method 2: Manual calculation (showing each neuron)\n",
    "print(f\"\\nMethod 2 (Step-by-step):\")\n",
    "z1 = x[0] * W[0, 0] + x[1] * W[1, 0] + b[0]\n",
    "z2 = x[0] * W[0, 1] + x[1] * W[1, 1] + b[1]\n",
    "print(f\"Neuron 1: z₁ = {z1:.1f}, a₁ = {max(0, z1):.1f}\")\n",
    "print(f\"Neuron 2: z₂ = {z2:.1f}, a₂ = {max(0, z2):.1f}\")\n",
    "\n",
    "# Detailed breakdown\n",
    "print(f\"\\n=== Detailed Breakdown ===\")\n",
    "print(f\"Neuron 1 calculation:\")\n",
    "print(f\"  (2.0 × 0.5) = {2.0 * 0.5}\")\n",
    "print(f\"  (-1.0 × -0.3) = {-1.0 * -0.3}\")\n",
    "print(f\"  bias = {b[0]}\")\n",
    "print(f\"  Sum = {z1}\")\n",
    "print(f\"  ReLU({z1}) = {max(0, z1)}\")\n",
    "\n",
    "print(f\"\\nNeuron 2 calculation:\")\n",
    "print(f\"  (2.0 × 0.2) = {2.0 * 0.2}\")\n",
    "print(f\"  (-1.0 × 0.8) = {-1.0 * 0.8}\")\n",
    "print(f\"  bias = {b[1]}\")\n",
    "print(f\"  Sum = {z2}\")\n",
    "print(f\"  ReLU({z2}) = {max(0, z2)} (negative value zeroed out!)\")\n",
    "\n",
    "print(f\"\\n✓ Manual calculation verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights:**\n",
    "\n",
    "1. **Matrix Multiplication:** Each output neuron computes a weighted sum from ALL inputs\n",
    "2. **ReLU Effect:** Neuron 2's negative pre-activation becomes 0\n",
    "3. **Bias Impact:** Shifts the activation threshold\n",
    "\n",
    "**Common Mistakes:**\n",
    "- Confusing row/column indexing in weight matrix\n",
    "- Forgetting to apply activation function\n",
    "- Sign errors in arithmetic\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 5: Mean Absolute Error Loss\n",
    "\n",
    "### Solution 5.1: Implement MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Absolute Error (L1 Loss).\n",
    "    \n",
    "    More robust to outliers than MSE because it doesn't square errors.\n",
    "    \"\"\"\n",
    "    # Compute absolute differences\n",
    "    abs_errors = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # Return mean\n",
    "    return np.mean(abs_errors)\n",
    "\n",
    "def mae_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Gradient of MAE with respect to predictions.\n",
    "    \n",
    "    The derivative is:\n",
    "    - +1 if y_pred > y_true (prediction too high)\n",
    "    - -1 if y_pred < y_true (prediction too low)\n",
    "    -  0 if y_pred == y_true (perfect prediction)\n",
    "    \n",
    "    Note: The gradient is constant (doesn't depend on magnitude of error)\n",
    "    This is different from MSE where larger errors have larger gradients.\n",
    "    \"\"\"\n",
    "    # Use np.sign to get -1, 0, or +1\n",
    "    return np.sign(y_pred - y_true) / len(y_true)\n",
    "\n",
    "# Test implementation\n",
    "print(\"=== Testing MAE Implementation ===\")\n",
    "\n",
    "y_true = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "y_pred = np.array([1.2, 1.8, 3.1, 4.5])\n",
    "\n",
    "mae = mae_loss(y_true, y_pred)\n",
    "expected = np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "print(f\"\\nTrue values: {y_true}\")\n",
    "print(f\"Predictions: {y_pred}\")\n",
    "print(f\"Absolute errors: {np.abs(y_true - y_pred)}\")\n",
    "print(f\"\\nMAE Loss: {mae:.4f}\")\n",
    "print(f\"Expected: {expected:.4f}\")\n",
    "print(f\"Match: {np.isclose(mae, expected)}\")\n",
    "\n",
    "# Test gradient\n",
    "grad = mae_derivative(y_true, y_pred)\n",
    "print(f\"\\nGradient: {grad}\")\n",
    "print(f\"Interpretation:\")\n",
    "for i, (yt, yp, g) in enumerate(zip(y_true, y_pred, grad)):\n",
    "    direction = \"decrease\" if g < 0 else \"increase\" if g > 0 else \"perfect\"\n",
    "    print(f\"  Sample {i}: true={yt}, pred={yp:.1f}, gradient={g:.3f} -> {direction} prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5.2: Compare MAE and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"Mean Squared Error for comparison.\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Compare with outlier\n",
    "print(\"=== Comparing MAE vs MSE ===\")\n",
    "\n",
    "# Case 1: Normal errors\n",
    "y_true_normal = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_pred_normal = np.array([1.1, 2.1, 2.9, 4.1, 4.9])\n",
    "\n",
    "# Case 2: One large outlier\n",
    "y_pred_outlier = np.array([1.1, 2.1, 2.9, 4.1, 15.0])  # Last prediction is way off!\n",
    "\n",
    "print(\"\\nCase 1: Small, consistent errors\")\n",
    "print(f\"MAE: {mae_loss(y_true_normal, y_pred_normal):.4f}\")\n",
    "print(f\"MSE: {mse_loss(y_true_normal, y_pred_normal):.4f}\")\n",
    "\n",
    "print(\"\\nCase 2: One large outlier error\")\n",
    "print(f\"MAE: {mae_loss(y_true_normal, y_pred_outlier):.4f}\")\n",
    "print(f\"MSE: {mse_loss(y_true_normal, y_pred_outlier):.4f}\")\n",
    "print(\"\\nNotice: MSE increases dramatically due to outlier (10.0)² = 100\")\n",
    "print(\"        MAE increases linearly with outlier magnitude\")\n",
    "\n",
    "# Visualize\n",
    "errors = np.linspace(-5, 5, 100)\n",
    "mae_curve = np.abs(errors)\n",
    "mse_curve = errors ** 2\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(errors, mae_curve, 'b-', linewidth=2, label='MAE (L1)')\n",
    "plt.plot(errors, mse_curve, 'r-', linewidth=2, label='MSE (L2)')\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('Error (y_true - y_pred)', fontsize=12)\n",
    "plt.ylabel('Loss Contribution', fontsize=12)\n",
    "plt.title('MAE vs MSE: Response to Errors', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 10])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observation: MSE penalizes large errors much more heavily (quadratically)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to Use MAE:**\n",
    "\n",
    "1. **Outliers Present:** MAE is less sensitive to outliers\n",
    "2. **Uniform Error Penalty:** All errors treated equally regardless of magnitude\n",
    "3. **Interpretability:** MAE is in same units as target variable\n",
    "\n",
    "**When to Use MSE:**\n",
    "\n",
    "1. **Penalize Large Errors:** Want to heavily discourage large mistakes\n",
    "2. **Smooth Gradients:** MSE has smoother gradients near optimum\n",
    "3. **Standard Practice:** MSE is more common in many domains\n",
    "\n",
    "**Gradient Behavior:**\n",
    "- MAE: Constant gradient (doesn't depend on error size)\n",
    "- MSE: Gradient proportional to error (larger errors → larger gradients)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 6: Manual Gradient Computation\n",
    "\n",
    "### Solution 6.1: Backpropagation by Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual Calculation:**\n",
    "\n",
    "Given:\n",
    "- x = 3.0\n",
    "- w = 0.5\n",
    "- b = 0.2\n",
    "- y = 2.0\n",
    "\n",
    "**Forward Pass:**\n",
    "```\n",
    "ŷ = wx + b = 0.5(3.0) + 0.2 = 1.5 + 0.2 = 1.7\n",
    "L = (y - ŷ)² = (2.0 - 1.7)² = (0.3)² = 0.09\n",
    "```\n",
    "\n",
    "**Backward Pass (Chain Rule):**\n",
    "\n",
    "1. **∂L/∂ŷ:** Derivative of loss with respect to prediction\n",
    "   ```\n",
    "   L = (y - ŷ)²\n",
    "   ∂L/∂ŷ = 2(ŷ - y) = 2(1.7 - 2.0) = 2(-0.3) = -0.6\n",
    "   ```\n",
    "\n",
    "2. **∂L/∂w:** How does loss change with weight?\n",
    "   ```\n",
    "   Using chain rule: ∂L/∂w = ∂L/∂ŷ × ∂ŷ/∂w\n",
    "   \n",
    "   ∂ŷ/∂w = ∂(wx + b)/∂w = x = 3.0\n",
    "   \n",
    "   ∂L/∂w = (-0.6)(3.0) = -1.8\n",
    "   ```\n",
    "\n",
    "3. **∂L/∂b:** How does loss change with bias?\n",
    "   ```\n",
    "   ∂L/∂b = ∂L/∂ŷ × ∂ŷ/∂b\n",
    "   \n",
    "   ∂ŷ/∂b = ∂(wx + b)/∂b = 1.0\n",
    "   \n",
    "   ∂L/∂b = (-0.6)(1.0) = -0.6\n",
    "   ```\n",
    "\n",
    "**Results:**\n",
    "- Loss: 0.09\n",
    "- ∂L/∂w = -1.8 (increasing w will decrease loss)\n",
    "- ∂L/∂b = -0.6 (increasing b will decrease loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with code\n",
    "print(\"=== Verifying Manual Gradient Computation ===\")\n",
    "\n",
    "x = 3.0\n",
    "w = 0.5\n",
    "b = 0.2\n",
    "y_true = 2.0\n",
    "\n",
    "# Forward pass\n",
    "y_pred = w * x + b\n",
    "loss = (y_true - y_pred) ** 2\n",
    "\n",
    "print(f\"\\n=== Forward Pass ===\")\n",
    "print(f\"Prediction: ŷ = {w} × {x} + {b} = {y_pred}\")\n",
    "print(f\"Loss: L = (y - ŷ)² = ({y_true} - {y_pred})² = {loss}\")\n",
    "\n",
    "# Backward pass\n",
    "dL_dyhat = 2 * (y_pred - y_true)\n",
    "dL_dw = dL_dyhat * x  # Chain rule: ∂L/∂w = ∂L/∂ŷ × ∂ŷ/∂w\n",
    "dL_db = dL_dyhat * 1.0  # Chain rule: ∂L/∂b = ∂L/∂ŷ × ∂ŷ/∂b\n",
    "\n",
    "print(f\"\\n=== Backward Pass ===\")\n",
    "print(f\"∂L/∂ŷ = 2(ŷ - y) = 2({y_pred} - {y_true}) = {dL_dyhat}\")\n",
    "print(f\"∂L/∂w = ∂L/∂ŷ × ∂ŷ/∂w = {dL_dyhat} × {x} = {dL_dw}\")\n",
    "print(f\"∂L/∂b = ∂L/∂ŷ × ∂ŷ/∂b = {dL_dyhat} × 1.0 = {dL_db}\")\n",
    "\n",
    "# Verify using numerical gradients (finite differences)\n",
    "print(f\"\\n=== Numerical Verification (Finite Differences) ===\")\n",
    "epsilon = 1e-5\n",
    "\n",
    "# Numerical gradient for w\n",
    "loss_plus_w = (y_true - ((w + epsilon) * x + b)) ** 2\n",
    "loss_minus_w = (y_true - ((w - epsilon) * x + b)) ** 2\n",
    "numerical_dL_dw = (loss_plus_w - loss_minus_w) / (2 * epsilon)\n",
    "\n",
    "# Numerical gradient for b\n",
    "loss_plus_b = (y_true - (w * x + (b + epsilon))) ** 2\n",
    "loss_minus_b = (y_true - (w * x + (b - epsilon))) ** 2\n",
    "numerical_dL_db = (loss_plus_b - loss_minus_b) / (2 * epsilon)\n",
    "\n",
    "print(f\"Analytical ∂L/∂w: {dL_dw:.6f}\")\n",
    "print(f\"Numerical ∂L/∂w:  {numerical_dL_dw:.6f}\")\n",
    "print(f\"Match: {np.isclose(dL_dw, numerical_dL_dw)}\")\n",
    "\n",
    "print(f\"\\nAnalytical ∂L/∂b: {dL_db:.6f}\")\n",
    "print(f\"Numerical ∂L/∂b:  {numerical_dL_db:.6f}\")\n",
    "print(f\"Match: {np.isclose(dL_db, numerical_dL_db)}\")\n",
    "\n",
    "# Show gradient descent update\n",
    "print(f\"\\n=== Gradient Descent Update ===\")\n",
    "learning_rate = 0.1\n",
    "w_new = w - learning_rate * dL_dw\n",
    "b_new = b - learning_rate * dL_db\n",
    "\n",
    "print(f\"Old parameters: w={w}, b={b}\")\n",
    "print(f\"Gradients: ∂L/∂w={dL_dw}, ∂L/∂b={dL_db}\")\n",
    "print(f\"New parameters: w={w_new}, b={b_new}\")\n",
    "\n",
    "# Verify new loss is lower\n",
    "y_pred_new = w_new * x + b_new\n",
    "loss_new = (y_true - y_pred_new) ** 2\n",
    "print(f\"\\nOld loss: {loss:.6f}\")\n",
    "print(f\"New loss: {loss_new:.6f}\")\n",
    "print(f\"Loss decreased: {loss_new < loss} ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Chain Rule:**\n",
    "\n",
    "The chain rule is fundamental to backpropagation:\n",
    "\n",
    "```\n",
    "If y = f(g(x)), then dy/dx = (df/dg) × (dg/dx)\n",
    "```\n",
    "\n",
    "In our case:\n",
    "- Loss depends on prediction: L(ŷ)\n",
    "- Prediction depends on weight: ŷ(w)\n",
    "- So: ∂L/∂w = (∂L/∂ŷ) × (∂ŷ/∂w)\n",
    "\n",
    "**Gradient Sign Interpretation:**\n",
    "- Negative gradient: Increasing parameter decreases loss\n",
    "- Positive gradient: Decreasing parameter decreases loss\n",
    "- Zero gradient: At local optimum (or saddle point)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 7: Debug Broken Training Loop\n",
    "\n",
    "### Solution 7.1: Fixed Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_training_loop(X, y, epochs=10, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Corrected training loop with all bugs fixed.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_classes = y.shape[1]\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    W = np.random.randn(n_features, n_classes) * 0.01\n",
    "    b = np.zeros((1, n_classes))\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        # BUG #1 FIX: Should be PLUS bias, not minus\n",
    "        z = X @ W + b  # ✓ FIXED\n",
    "        \n",
    "        # Softmax activation\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        predictions = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.mean(np.sum(y * np.log(predictions + 1e-8), axis=1))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        dz = predictions - y\n",
    "        \n",
    "        # BUG #2 FIX: Divide by batch size to get average gradient\n",
    "        dW = (X.T @ dz) / n_samples  # ✓ FIXED\n",
    "        db = np.sum(dz, axis=0, keepdims=True) / n_samples\n",
    "        \n",
    "        # Update parameters\n",
    "        # BUG #3 FIX: Gradient descent moves AGAINST gradient (minus)\n",
    "        W = W - learning_rate * dW  # ✓ Already correct\n",
    "        b = b - learning_rate * db  # ✓ FIXED (was plus)\n",
    "        \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return W, b, losses\n",
    "\n",
    "print(\"=== Testing Fixed Training Loop ===\")\n",
    "print(\"\\n📋 Bug Fixes Applied:\")\n",
    "print(\"  Bug #1: Changed 'X @ W - b' to 'X @ W + b'\")\n",
    "print(\"  Bug #2: Added '/ n_samples' to dW calculation\")\n",
    "print(\"  Bug #3: Changed 'b + learning_rate * db' to 'b - learning_rate * db'\")\n",
    "\n",
    "# Test with small dataset\n",
    "np.random.seed(42)\n",
    "X_small = np.random.randn(100, 10)\n",
    "y_small = np.eye(3)[np.random.randint(0, 3, 100)]  # 3 classes\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "W, b, losses = fixed_training_loop(X_small, y_small, epochs=20, learning_rate=0.1)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Plot losses - should decrease!\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "ax1.plot(losses, 'b-o', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss (Should Decrease!) ✓', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "ax1.annotate(f'Start: {losses[0]:.3f}', \n",
    "            xy=(0, losses[0]), xytext=(5, losses[0] + 0.1),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'),\n",
    "            fontsize=10, color='red')\n",
    "ax1.annotate(f'End: {losses[-1]:.3f}', \n",
    "            xy=(len(losses)-1, losses[-1]), xytext=(len(losses)-5, losses[-1] - 0.1),\n",
    "            arrowprops=dict(arrowstyle='->', color='green'),\n",
    "            fontsize=10, color='green')\n",
    "\n",
    "# Loss improvement\n",
    "improvement = ((losses[0] - losses[-1]) / losses[0]) * 100\n",
    "ax2.bar(['Initial Loss', 'Final Loss'], [losses[0], losses[-1]], \n",
    "       color=['red', 'green'], alpha=0.7)\n",
    "ax2.set_ylabel('Loss', fontsize=12)\n",
    "ax2.set_title(f'Loss Improvement: {improvement:.1f}%', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Training successful! Loss decreased from {losses[0]:.4f} to {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detailed Bug Explanations:**\n",
    "\n",
    "**Bug #1: Bias Sign**\n",
    "- **Wrong:** `z = X @ W - b`\n",
    "- **Correct:** `z = X @ W + b`\n",
    "- **Why:** Bias should shift activations up, not down\n",
    "- **Effect:** Model couldn't learn proper decision boundaries\n",
    "\n",
    "**Bug #2: Missing Gradient Averaging**\n",
    "- **Wrong:** `dW = X.T @ dz`\n",
    "- **Correct:** `dW = (X.T @ dz) / n_samples`\n",
    "- **Why:** Gradients should be averaged over batch\n",
    "- **Effect:** Learning rate was effectively `n_samples` times too large\n",
    "\n",
    "**Bug #3: Wrong Gradient Direction**\n",
    "- **Wrong:** `b = b + learning_rate * db`\n",
    "- **Correct:** `b = b - learning_rate * db`\n",
    "- **Why:** Gradient descent moves OPPOSITE to gradient direction\n",
    "- **Effect:** Parameters moved in wrong direction (increasing loss!)\n",
    "\n",
    "**Debugging Tips:**\n",
    "1. Plot loss curves - they should decrease\n",
    "2. Check gradient signs - ensure moving toward lower loss\n",
    "3. Verify matrix dimensions match expectations\n",
    "4. Test on simple toy problems first\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed all the core solutions. The remaining exercises (8-10) are more open-ended and designed for experimentation. Here are some starting points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 8: Hyperparameter Tuning (Starter Code)\n",
    "\n",
    "This is an open-ended exercise. Here's a framework to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example hyperparameter tuning workflow\n",
    "print(\"=== Hyperparameter Tuning Framework ===\")\n",
    "print(\"\\n💡 Tips for Tuning:\")\n",
    "print(\"1. Start with learning rate (most impactful)\")\n",
    "print(\"2. Then tune architecture (layer sizes)\")\n",
    "print(\"3. Finally tune batch size and epochs\")\n",
    "print(\"4. Use validation set to prevent overfitting\")\n",
    "print(\"5. Track all experiments in a table/dict\")\n",
    "\n",
    "print(\"\\n📊 Typical Learning Rate Ranges:\")\n",
    "print(\"  - Too small: 0.0001 (very slow learning)\")\n",
    "print(\"  - Good range: 0.001 - 0.1\")\n",
    "print(\"  - Too large: 1.0+ (unstable training)\")\n",
    "\n",
    "print(\"\\n🏗️ Architecture Guidelines:\")\n",
    "print(\"  - Start simple: [784, 128, 10]\")\n",
    "print(\"  - Add depth: [784, 256, 128, 10]\")\n",
    "print(\"  - Wider layers: [784, 512, 10]\")",
    "print(\"  - Balance: More parameters ≠ always better\")",
    "",
    "# Your experimentation code here...",
    "print(\"\\n🔬 Ready to experiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 9: PyTorch Implementation (Starter)\n",
    "\n",
    "Here's a complete PyTorch implementation to compare with your NumPy version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MNISTNet(nn.Module):\n",
    "    \"\"\"PyTorch implementation of MNIST classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass with ReLU activation\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # No activation (logits)\n",
    "        return x\n",
    "\n",
    "print(\"✓ PyTorch model defined!\")\n",
    "print(\"\\nKey Differences from NumPy:\")\n",
    "print(\"1. Automatic differentiation (no manual backprop)\")\n",
    "print(\"2. GPU acceleration available\")\n",
    "print(\"3. Built-in optimizers\")\n",
    "print(\"4. Less code, more abstraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "Through these exercises, you've:\n",
    "\n",
    "1. ✅ **Understood Neurons:** Built single neurons from scratch\n",
    "2. ✅ **Activation Functions:** Implemented and compared different activations\n",
    "3. ✅ **Network Architecture:** Composed layers into networks\n",
    "4. ✅ **Forward Propagation:** Traced information flow\n",
    "5. ✅ **Loss Functions:** Implemented MAE and compared with MSE\n",
    "6. ✅ **Backpropagation:** Computed gradients manually\n",
    "7. ✅ **Debugging:** Fixed broken training code\n",
    "8. ✅ **Experimentation:** Tuned hyperparameters (open-ended)\n",
    "9. ✅ **Framework Translation:** Connected NumPy to PyTorch\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Conceptual Understanding:**\n",
    "- Neural networks are just compositions of simple functions\n",
    "- Training is optimization: find parameters that minimize loss\n",
    "- Backpropagation is just the chain rule applied systematically\n",
    "\n",
    "**Practical Skills:**\n",
    "- Matrix operations are the core of neural networks\n",
    "- Activation functions introduce non-linearity (crucial!)\n",
    "- Hyperparameters significantly impact performance\n",
    "- Debugging requires understanding fundamentals\n",
    "\n",
    "**Why Build from Scratch?**\n",
    "- Frameworks like PyTorch are \"magic\" until you know what's inside\n",
    "- Debugging is easier when you understand the internals\n",
    "- Helps you make better architectural choices\n",
    "- Enables you to implement custom components\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Continue Learning:**\n",
    "1. 📚 Study advanced architectures (CNNs, RNNs, Transformers)\n",
    "2. 🔬 Read research papers and implement them\n",
    "3. 🏗️ Build projects that solve real problems\n",
    "4. 🤝 Contribute to open-source ML projects\n",
    "\n",
    "**Practice Suggestions:**\n",
    "- Implement more activation functions (Swish, GELU)\n",
    "- Add regularization (L2, dropout)\n",
    "- Implement advanced optimizers (Adam, RMSprop)\n",
    "- Try different datasets (Fashion-MNIST, CIFAR-10)\n",
    "\n",
    "**Resources:**\n",
    "- Deep Learning Book (Goodfellow et al.)\n",
    "- Fast.ai courses\n",
    "- PyTorch tutorials\n",
    "- Papers with Code\n",
    "\n",
    "### Remember\n",
    "\n",
    "> \"The best way to learn is by doing. The second best way is by teaching.\"\n",
    "\n",
    "Share what you've learned with others. Explaining concepts solidifies your understanding.\n",
    "\n",
    "**Keep experimenting, stay curious, and happy learning! 🚀**\n",
    "\n",
    "---\n",
    "\n",
    "*If you found these exercises helpful, consider creating your own variations or contributing improvements to help other learners!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}