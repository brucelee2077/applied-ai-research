{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”µ Single Neuron - The Building Block\n",
    "\n",
    "Welcome back! In the previous notebook, we learned what neural networks are at a high level. Now we're going to get our hands dirty and **build a neuron from scratch**!\n",
    "\n",
    "## ðŸŽ¯ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "- Understand the exact math behind a single neuron\n",
    "- Implement a neuron using Python and NumPy\n",
    "- Visualize what a neuron does\n",
    "- Understand the role of weights and bias\n",
    "- See how changing weights affects predictions\n",
    "\n",
    "**Prerequisites:** Basic Python (loops, functions) and the concepts from Notebook 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¬ Analogy: A Neuron as a Decision Maker\n",
    "\n",
    "Before we dive into code, let's build intuition with a real-world example.\n",
    "\n",
    "### Scenario: Should You Buy This Coffee? â˜•\n",
    "\n",
    "Imagine you're deciding whether to buy a cup of coffee. You consider:\n",
    "\n",
    "1. **Price** ($5.00)\n",
    "2. **Size** (16 oz)\n",
    "3. **Quality rating** (4.5 stars)\n",
    "\n",
    "But these factors don't all matter equally to you:\n",
    "- Price is **very important** (weight = 0.5)\n",
    "- Size is **somewhat important** (weight = 0.3)\n",
    "- Quality is **moderately important** (weight = 0.4)\n",
    "\n",
    "You also have a **baseline preference** - you generally like coffee, so you're already leaning towards buying it (+2 points).\n",
    "\n",
    "### The Calculation:\n",
    "\n",
    "```\n",
    "Decision Score = (Price Ã— Price_Weight) + (Size Ã— Size_Weight) + (Quality Ã— Quality_Weight) + Baseline\n",
    "\n",
    "Decision Score = (5.0 Ã— 0.5) + (16 Ã— 0.3) + (4.5 Ã— 0.4) + 2\n",
    "               = 2.5 + 4.8 + 1.8 + 2\n",
    "               = 11.1\n",
    "```\n",
    "\n",
    "If the score is above some threshold (let's say 10), you buy it!\n",
    "\n",
    "**This is EXACTLY how a neuron works!** ðŸŽ‰\n",
    "\n",
    "- **Inputs** = Price, Size, Quality\n",
    "- **Weights** = How much you care about each factor\n",
    "- **Bias** = Your baseline preference\n",
    "- **Output** = Your decision score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ§® The Math Behind a Neuron\n",
    "\n",
    "Now let's formalize this with math (don't worry, it's just multiplication and addition!).\n",
    "\n",
    "### The Neuron Formula\n",
    "\n",
    "A neuron computes:\n",
    "\n",
    "$$\\text{output} = (x_1 \\times w_1) + (x_2 \\times w_2) + (x_3 \\times w_3) + ... + b$$\n",
    "\n",
    "Or more compactly:\n",
    "\n",
    "$$\\text{output} = \\sum_{i=1}^{n} (x_i \\times w_i) + b$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ = inputs (the data)\n",
    "- $w_i$ = weights (importance of each input)\n",
    "- $b$ = bias (baseline adjustment)\n",
    "- $\\sum$ = sum symbol (add them all up)\n",
    "\n",
    "### Breaking It Down\n",
    "\n",
    "1. **Multiply each input by its weight**: This scales each input by its importance\n",
    "2. **Add them all together**: This combines all the weighted inputs\n",
    "3. **Add the bias**: This shifts the result up or down\n",
    "\n",
    "**That's it!** A neuron is just a weighted sum plus a bias. No magic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ’» Let's Code: The Simplest Neuron\n",
    "\n",
    "We'll build a neuron step by step, starting with the simplest version and gradually making it more sophisticated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import NumPy - we'll use it for efficient numerical operations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "# This ensures we get the same random numbers each time we run the code\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1: Single Input, Single Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_neuron(input_value, weight, bias):\n",
    "    \"\"\"\n",
    "    The simplest possible neuron: one input, one weight.\n",
    "    \n",
    "    Think of it like this:\n",
    "    - input_value: How many hours you studied\n",
    "    - weight: How effective each hour of studying is\n",
    "    - bias: Your baseline knowledge (before studying)\n",
    "    \n",
    "    Args:\n",
    "        input_value (float): The input value\n",
    "        weight (float): How important this input is\n",
    "        bias (float): Baseline adjustment\n",
    "    \n",
    "    Returns:\n",
    "        float: The neuron's output\n",
    "    \"\"\"\n",
    "    # Multiply input by weight, then add bias\n",
    "    output = (input_value * weight) + bias\n",
    "    return output\n",
    "\n",
    "# Let's test it!\n",
    "hours_studied = 5.0  # Input: studied for 5 hours\n",
    "study_effectiveness = 2.0  # Weight: each hour is worth 2 points\n",
    "baseline_knowledge = 10.0  # Bias: started with 10 points of knowledge\n",
    "\n",
    "test_score = simple_neuron(hours_studied, study_effectiveness, baseline_knowledge)\n",
    "\n",
    "print(f\"ðŸ“š Study Example:\")\n",
    "print(f\"   Hours studied: {hours_studied}\")\n",
    "print(f\"   Study effectiveness (weight): {study_effectiveness}\")\n",
    "print(f\"   Baseline knowledge (bias): {baseline_knowledge}\")\n",
    "print(f\"   Predicted test score: {test_score}\")\n",
    "print(f\"\\n   Calculation: ({hours_studied} Ã— {study_effectiveness}) + {baseline_knowledge} = {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2: Multiple Inputs (Using a Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_with_loop(inputs, weights, bias):\n",
    "    \"\"\"\n",
    "    A neuron with multiple inputs, implemented using a loop.\n",
    "    \n",
    "    This is more realistic - most neurons have multiple inputs!\n",
    "    \n",
    "    Args:\n",
    "        inputs (list): List of input values [x1, x2, x3, ...]\n",
    "        weights (list): List of weights [w1, w2, w3, ...]\n",
    "        bias (float): Bias value\n",
    "    \n",
    "    Returns:\n",
    "        float: The neuron's output\n",
    "    \"\"\"\n",
    "    # Start with the bias\n",
    "    output = bias\n",
    "    \n",
    "    # Loop through each input and its corresponding weight\n",
    "    for input_value, weight in zip(inputs, weights):\n",
    "        # Multiply input by weight and add to running total\n",
    "        output += input_value * weight\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example: Predicting if a student will pass based on multiple factors\n",
    "student_data = [\n",
    "    5.0,   # Hours studied per day\n",
    "    85.0,  # Previous test score\n",
    "    7.0    # Hours of sleep\n",
    "]\n",
    "\n",
    "# These weights represent how important each factor is\n",
    "importance_weights = [\n",
    "    0.3,   # Study hours are somewhat important\n",
    "    0.7,   # Previous scores are very important\n",
    "    0.2    # Sleep is less important (but still matters!)\n",
    "]\n",
    "\n",
    "baseline_score = -50.0  # Bias: starting baseline\n",
    "\n",
    "predicted_score = neuron_with_loop(student_data, importance_weights, baseline_score)\n",
    "\n",
    "print(f\"ðŸŽ“ Student Pass Prediction:\")\n",
    "print(f\"   Study hours/day: {student_data[0]}\")\n",
    "print(f\"   Previous test score: {student_data[1]}\")\n",
    "print(f\"   Sleep hours: {student_data[2]}\")\n",
    "print(f\"\\n   Predicted score: {predicted_score:.2f}\")\n",
    "print(f\"   Will pass? {'Yes! âœ“' if predicted_score > 0 else 'No âœ—'}\")\n",
    "\n",
    "# Let's break down the calculation step by step\n",
    "print(f\"\\nðŸ“Š Step-by-step calculation:\")\n",
    "print(f\"   Start with bias: {baseline_score}\")\n",
    "for i, (inp, w) in enumerate(zip(student_data, importance_weights)):\n",
    "    contribution = inp * w\n",
    "    print(f\"   + ({inp} Ã— {w}) = {contribution:.2f}\")\n",
    "print(f\"   = {predicted_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 3: Vectorized with NumPy (The Professional Way!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_vectorized(inputs, weights, bias):\n",
    "    \"\"\"\n",
    "    A neuron using NumPy's dot product - much faster!\n",
    "    \n",
    "    The dot product does all the multiplications and additions in one go.\n",
    "    This is the same as our loop version, but optimized.\n",
    "    \n",
    "    Args:\n",
    "        inputs (numpy array): Input values\n",
    "        weights (numpy array): Weight values\n",
    "        bias (float): Bias value\n",
    "    \n",
    "    Returns:\n",
    "        float: The neuron's output\n",
    "    \"\"\"\n",
    "    # np.dot() computes: (x1*w1) + (x2*w2) + (x3*w3) + ...\n",
    "    # Then we add the bias\n",
    "    output = np.dot(inputs, weights) + bias\n",
    "    return output\n",
    "\n",
    "# Convert our previous example to NumPy arrays\n",
    "inputs_array = np.array([5.0, 85.0, 7.0])\n",
    "weights_array = np.array([0.3, 0.7, 0.2])\n",
    "\n",
    "# This should give us the same result as before\n",
    "predicted_score_vectorized = neuron_vectorized(inputs_array, weights_array, -50.0)\n",
    "\n",
    "print(f\"âš¡ Vectorized Neuron Output: {predicted_score_vectorized:.2f}\")\n",
    "print(f\"   (Same as loop version: {abs(predicted_score_vectorized - predicted_score) < 0.001})\")\n",
    "\n",
    "# Why use NumPy? Let's see the speed difference!\n",
    "import time\n",
    "\n",
    "# Create large random arrays for testing\n",
    "large_inputs = np.random.randn(10000)\n",
    "large_weights = np.random.randn(10000)\n",
    "\n",
    "# Test loop version\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = neuron_with_loop(large_inputs.tolist(), large_weights.tolist(), 0.0)\n",
    "loop_time = time.time() - start\n",
    "\n",
    "# Test vectorized version\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = neuron_vectorized(large_inputs, large_weights, 0.0)\n",
    "vectorized_time = time.time() - start\n",
    "\n",
    "print(f\"\\nâ±ï¸  Speed Comparison:\")\n",
    "print(f\"   Loop version: {loop_time:.4f} seconds\")\n",
    "print(f\"   Vectorized version: {vectorized_time:.4f} seconds\")\n",
    "print(f\"   Speedup: {loop_time/vectorized_time:.1f}x faster! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¨ Visualizing What a Neuron Does\n",
    "\n",
    "Let's visualize how a neuron creates a **decision boundary** - a line that separates different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 2D example\n",
    "# Let's say we're predicting if someone will like a movie based on:\n",
    "# - x-axis: Action level (0-10)\n",
    "# - y-axis: Comedy level (0-10)\n",
    "\n",
    "# Our neuron's weights and bias\n",
    "weight_action = 0.5   # Likes action somewhat\n",
    "weight_comedy = -0.3  # Doesn't like comedy much (negative weight!)\n",
    "bias = 2.0           # Slightly positive baseline\n",
    "\n",
    "# Create a grid of points\n",
    "action_levels = np.linspace(0, 10, 100)\n",
    "comedy_levels = np.linspace(0, 10, 100)\n",
    "\n",
    "# Create meshgrid for visualization\n",
    "X, Y = np.meshgrid(action_levels, comedy_levels)\n",
    "\n",
    "# Calculate neuron output for each point\n",
    "# Z = (action * weight_action) + (comedy * weight_comedy) + bias\n",
    "Z = (X * weight_action) + (Y * weight_comedy) + bias\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Left plot: Continuous output (heatmap)\n",
    "plt.subplot(1, 2, 1)\n",
    "contour = plt.contourf(X, Y, Z, levels=20, cmap='RdYlGn', alpha=0.7)\n",
    "plt.colorbar(contour, label='Neuron Output')\n",
    "plt.contour(X, Y, Z, levels=[0], colors='black', linewidths=3)  # Decision boundary\n",
    "plt.xlabel('Action Level', fontsize=12)\n",
    "plt.ylabel('Comedy Level', fontsize=12)\n",
    "plt.title('Neuron Output (Continuous)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: Binary decision (will like / won't like)\n",
    "plt.subplot(1, 2, 2)\n",
    "# Create binary mask (1 if output > 0, else 0)\n",
    "Z_binary = (Z > 0).astype(int)\n",
    "plt.contourf(X, Y, Z_binary, levels=1, colors=['lightcoral', 'lightgreen'], alpha=0.7)\n",
    "plt.contour(X, Y, Z, levels=[0], colors='black', linewidths=3, linestyles='--')\n",
    "plt.xlabel('Action Level', fontsize=12)\n",
    "plt.ylabel('Comedy Level', fontsize=12)\n",
    "plt.title('Binary Decision (Like / Don\\'t Like)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='lightgreen', label='Will Like (output > 0)'),\n",
    "    Patch(facecolor='lightcoral', label='Won\\'t Like (output < 0)'),\n",
    "    Patch(facecolor='none', edgecolor='black', linewidth=2, label='Decision Boundary')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸŽ¬ Movie Preference Neuron:\")\n",
    "print(f\"   Weight for Action: {weight_action} (positive = likes action)\")\n",
    "print(f\"   Weight for Comedy: {weight_comedy} (negative = dislikes comedy)\")\n",
    "print(f\"   Bias: {bias}\")\n",
    "print(f\"\\n   The black line is the decision boundary!\")\n",
    "print(f\"   Green area = will like the movie (neuron output > 0)\")\n",
    "print(f\"   Red area = won't like the movie (neuron output < 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Understanding the Decision Boundary\n",
    "\n",
    "The black line is where the neuron's output equals exactly 0. This is called the **decision boundary**.\n",
    "\n",
    "The equation of this line is:\n",
    "```\n",
    "0 = (action Ã— 0.5) + (comedy Ã— -0.3) + 2.0\n",
    "```\n",
    "\n",
    "Rearranging:\n",
    "```\n",
    "comedy = (2.0 + 0.5 Ã— action) / 0.3\n",
    "```\n",
    "\n",
    "**Key insight:** A single neuron creates a **linear** (straight line) decision boundary! It can only separate data that's linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âš–ï¸ The Role of Bias\n",
    "\n",
    "You might be wondering: **\"Why do we need bias? Can't we just use weights?\"**\n",
    "\n",
    "Great question! Let's see what happens without bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Same grid as before\n",
    "X, Y = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 10, 100))\n",
    "\n",
    "# LEFT: Without bias (bias = 0)\n",
    "Z_no_bias = (X * 0.5) + (Y * -0.3) + 0  # bias = 0\n",
    "Z_no_bias_binary = (Z_no_bias > 0).astype(int)\n",
    "ax1.contourf(X, Y, Z_no_bias_binary, levels=1, colors=['lightcoral', 'lightgreen'], alpha=0.7)\n",
    "ax1.contour(X, Y, Z_no_bias, levels=[0], colors='black', linewidths=3)\n",
    "ax1.set_xlabel('Action Level', fontsize=12)\n",
    "ax1.set_ylabel('Comedy Level', fontsize=12)\n",
    "ax1.set_title('WITHOUT Bias (bias = 0)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.text(5, 9, 'Line passes through origin!', \n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "         ha='center', fontsize=10)\n",
    "\n",
    "# RIGHT: With bias (bias = 2)\n",
    "Z_with_bias = (X * 0.5) + (Y * -0.3) + 2  # bias = 2\n",
    "Z_with_bias_binary = (Z_with_bias > 0).astype(int)\n",
    "ax2.contourf(X, Y, Z_with_bias_binary, levels=1, colors=['lightcoral', 'lightgreen'], alpha=0.7)\n",
    "ax2.contour(X, Y, Z_with_bias, levels=[0], colors='black', linewidths=3)\n",
    "ax2.set_xlabel('Action Level', fontsize=12)\n",
    "ax2.set_ylabel('Comedy Level', fontsize=12)\n",
    "ax2.set_title('WITH Bias (bias = 2)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.text(5, 9, 'Line shifted up!', \n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7),\n",
    "         ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ The Role of Bias:\")\n",
    "print(\"\\n   WITHOUT bias:\")\n",
    "print(\"   â€¢ The decision boundary MUST pass through the origin (0, 0)\")\n",
    "print(\"   â€¢ Very limiting! Real-world data rarely works this way\")\n",
    "print(\"\\n   WITH bias:\")\n",
    "print(\"   â€¢ The decision boundary can be anywhere\")\n",
    "print(\"   â€¢ Much more flexible!\")\n",
    "print(\"\\n   ðŸ’¡ Bias shifts the decision boundary up/down or left/right\")\n",
    "print(\"   ðŸ’¡ Think of bias as your 'default opinion' before seeing any data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ® Interactive: Play with Weights and Bias\n",
    "\n",
    "Let's create an interactive example where you can adjust weights and see the effect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_neuron(w1, w2, bias):\n",
    "    \"\"\"\n",
    "    Visualize how weights and bias affect the decision boundary.\n",
    "    \n",
    "    Args:\n",
    "        w1: Weight for first input (x-axis)\n",
    "        w2: Weight for second input (y-axis)\n",
    "        bias: Bias value\n",
    "    \"\"\"\n",
    "    # Create grid\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    y = np.linspace(-5, 5, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Calculate neuron output\n",
    "    Z = w1 * X + w2 * Y + bias\n",
    "    Z_binary = (Z > 0).astype(int)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(X, Y, Z_binary, levels=1, colors=['#ffcccc', '#ccffcc'], alpha=0.6)\n",
    "    plt.contour(X, Y, Z, levels=[0], colors='blue', linewidths=3, linestyles='-')\n",
    "    \n",
    "    # Add some sample points\n",
    "    np.random.seed(42)\n",
    "    # Class 1 points (should be in green region)\n",
    "    class1_x = np.random.randn(20) + 2\n",
    "    class1_y = np.random.randn(20) + 2\n",
    "    # Class 2 points (should be in red region)\n",
    "    class2_x = np.random.randn(20) - 2\n",
    "    class2_y = np.random.randn(20) - 2\n",
    "    \n",
    "    plt.scatter(class1_x, class1_y, c='green', marker='o', s=100, edgecolors='black', label='Class 1', alpha=0.7)\n",
    "    plt.scatter(class2_x, class2_y, c='red', marker='s', s=100, edgecolors='black', label='Class 2', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Input 1', fontsize=12)\n",
    "    plt.ylabel('Input 2', fontsize=12)\n",
    "    plt.title(f'Neuron: output = {w1}Ã—xâ‚ + {w2}Ã—xâ‚‚ + {bias}', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(-5, 5)\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    class1_predictions = w1 * class1_x + w2 * class1_y + bias\n",
    "    class2_predictions = w1 * class2_x + w2 * class2_y + bias\n",
    "    \n",
    "    class1_correct = np.sum(class1_predictions > 0)\n",
    "    class2_correct = np.sum(class2_predictions < 0)\n",
    "    total_correct = class1_correct + class2_correct\n",
    "    accuracy = total_correct / 40 * 100\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Classification Accuracy: {accuracy:.1f}%\")\n",
    "    print(f\"   Class 1 correct: {class1_correct}/20\")\n",
    "    print(f\"   Class 2 correct: {class2_correct}/20\")\n",
    "\n",
    "# Try different weights and biases!\n",
    "print(\"ðŸŽ® TRY IT YOURSELF!\")\n",
    "print(\"   Modify the values below to see how the decision boundary changes:\\n\")\n",
    "\n",
    "# Experiment with these values:\n",
    "weight1 = 1.0\n",
    "weight2 = 1.0\n",
    "bias_value = 0.0\n",
    "\n",
    "visualize_neuron(weight1, weight2, bias_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§ª Experiments to Try:\n",
    "\n",
    "Modify the weights and bias in the cell above and re-run it. Here are some things to try:\n",
    "\n",
    "1. **Rotate the line**: Change `weight2` from 1.0 to -1.0\n",
    "   - Notice how the line rotates!\n",
    "\n",
    "2. **Shift the line**: Keep weights the same, but change `bias_value` to 2.0\n",
    "   - The line shifts without rotating!\n",
    "\n",
    "3. **Vertical line**: Set `weight1 = 1.0`, `weight2 = 0.0`, `bias_value = 0.0`\n",
    "   - Creates a vertical decision boundary!\n",
    "\n",
    "4. **Horizontal line**: Set `weight1 = 0.0`, `weight2 = 1.0`, `bias_value = 0.0`\n",
    "   - Creates a horizontal decision boundary!\n",
    "\n",
    "5. **Challenge**: Can you find weights and bias that get 100% accuracy?\n",
    "   - Hint: The green points are around (2, 2) and red points are around (-2, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸš¨ Common Pitfalls and Misconceptions\n",
    "\n",
    "Let's address some common questions and mistakes:\n",
    "\n",
    "### â“ \"Why do we need bias? Can't we just add another input that's always 1?\"\n",
    "\n",
    "**Great observation!** You absolutely can! In fact, that's mathematically equivalent:\n",
    "\n",
    "```python\n",
    "# These two are the same:\n",
    "\n",
    "# Version 1: Explicit bias\n",
    "output = w1*x1 + w2*x2 + b\n",
    "\n",
    "# Version 2: Bias as a weight with input = 1\n",
    "output = w1*x1 + w2*x2 + w_bias*1\n",
    "```\n",
    "\n",
    "We typically keep bias separate just for clarity in code.\n",
    "\n",
    "### â“ \"Can one neuron solve any problem?\"\n",
    "\n",
    "**No!** A single neuron can only create a **linear decision boundary** (a straight line in 2D, a plane in 3D, etc.).\n",
    "\n",
    "**The XOR Problem** - Classic example where one neuron fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR (exclusive OR) problem\n",
    "# Output is 1 if inputs are different, 0 if same\n",
    "# (0,0) -> 0\n",
    "# (0,1) -> 1\n",
    "# (1,0) -> 1\n",
    "# (1,1) -> 0\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot XOR points\n",
    "plt.scatter([0, 1], [0, 1], c='red', s=200, marker='o', edgecolors='black', linewidths=2, label='Output = 0')\n",
    "plt.scatter([0, 1], [1, 0], c='green', s=200, marker='o', edgecolors='black', linewidths=2, label='Output = 1')\n",
    "\n",
    "plt.xlabel('Input 1', fontsize=12)\n",
    "plt.ylabel('Input 2', fontsize=12)\n",
    "plt.title('XOR Problem - Not Linearly Separable!', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "\n",
    "# Try to draw a straight line that separates red from green\n",
    "# It's impossible!\n",
    "plt.text(0.5, -0.3, \"Can you draw ONE straight line to separate red from green?\\nImpossible!\", \n",
    "         ha='center', fontsize=11, style='italic',\n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâŒ A single neuron CANNOT solve XOR!\")\n",
    "print(\"   No straight line can separate the red points from green points.\")\n",
    "print(\"\\nâœ… Solution: Use MULTIPLE neurons (next notebook!)\")\n",
    "print(\"   With 2+ neurons, we can create non-linear decision boundaries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â“ \"What if all weights are the same?\"\n",
    "\n",
    "**Then all inputs contribute equally!** This might be okay for some problems, but usually different inputs have different importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: All weights equal vs different weights\n",
    "inputs = np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "# Case 1: All weights equal\n",
    "weights_equal = np.array([1.0, 1.0, 1.0])\n",
    "output_equal = neuron_vectorized(inputs, weights_equal, 0.0)\n",
    "\n",
    "# Case 2: Different weights\n",
    "weights_different = np.array([0.1, 0.5, 2.0])  # Last input is much more important\n",
    "output_different = neuron_vectorized(inputs, weights_different, 0.0)\n",
    "\n",
    "print(\"Input values:\", inputs)\n",
    "print(\"\\nCase 1 - Equal weights [1.0, 1.0, 1.0]:\")\n",
    "print(f\"  Output: {output_equal}\")\n",
    "print(f\"  All inputs contribute equally: (1Ã—1) + (2Ã—1) + (3Ã—1) = {output_equal}\")\n",
    "\n",
    "print(\"\\nCase 2 - Different weights [0.1, 0.5, 2.0]:\")\n",
    "print(f\"  Output: {output_different}\")\n",
    "print(f\"  Last input dominates: (1Ã—0.1) + (2Ã—0.5) + (3Ã—2.0) = {output_different}\")\n",
    "print(f\"\\n  Notice how the third input (value=3.0, weight=2.0) contributes {3.0*2.0}/{output_different}\")\n",
    "print(f\"  = {3.0*2.0/output_different*100:.1f}% of the total output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â“ \"Why do we need activation functions?\"\n",
    "\n",
    "**Preview for next notebook!** Right now, our neuron just outputs a number. But:\n",
    "\n",
    "1. Sometimes we want a **probability** (between 0 and 1)\n",
    "2. Sometimes we want to **add non-linearity** (curves instead of straight lines)\n",
    "3. Sometimes we want to **threshold** the output (only activate if strong enough)\n",
    "\n",
    "That's where **activation functions** come in! We'll explore them in detail in Notebook 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "Congratulations! You've learned how a single neuron works. Let's recap:\n",
    "\n",
    "### 1. **The Neuron Formula**\n",
    "```\n",
    "output = (xâ‚ Ã— wâ‚) + (xâ‚‚ Ã— wâ‚‚) + ... + (xâ‚™ Ã— wâ‚™) + bias\n",
    "```\n",
    "- Multiply each input by its weight\n",
    "- Add them all together\n",
    "- Add the bias\n",
    "\n",
    "### 2. **Components**\n",
    "- **Inputs (x)**: The data we're processing\n",
    "- **Weights (w)**: How important each input is (learned during training)\n",
    "- **Bias (b)**: Baseline adjustment (also learned)\n",
    "- **Output**: The neuron's prediction\n",
    "\n",
    "### 3. **What Weights Do**\n",
    "- **Positive weight**: Input contributes positively to output\n",
    "- **Negative weight**: Input contributes negatively\n",
    "- **Large absolute value**: Input is very important\n",
    "- **Small absolute value**: Input matters less\n",
    "\n",
    "### 4. **What Bias Does**\n",
    "- Shifts the decision boundary\n",
    "- Allows flexibility in where the boundary is placed\n",
    "- Think of it as a \"default opinion\" before seeing data\n",
    "\n",
    "### 5. **Limitations**\n",
    "- Single neuron = linear decision boundary only\n",
    "- Can't solve problems like XOR\n",
    "- Need multiple neurons for complex patterns\n",
    "\n",
    "### 6. **Implementation**\n",
    "- Can use loops (clear but slow)\n",
    "- Should use NumPy dot product (fast and efficient)\n",
    "- Same math, different implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ§ª Practice Exercises\n",
    "\n",
    "Try these exercises to solidify your understanding:\n",
    "\n",
    "### Exercise 1: Coffee Decision Neuron\n",
    "Create a neuron that decides if you should buy a coffee based on:\n",
    "- Price (dollars)\n",
    "- Size (ounces)\n",
    "- Quality rating (1-5 stars)\n",
    "\n",
    "Choose your own weights and bias!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: Use the neuron_vectorized function we created\n",
    "\n",
    "# Example coffee\n",
    "coffee = np.array([5.0, 16.0, 4.5])  # [$5, 16oz, 4.5 stars]\n",
    "\n",
    "# Your weights (choose values that make sense to you!)\n",
    "weights = np.array([?, ?, ?])  # Replace ? with your choices\n",
    "bias = ?  # Replace ? with your choice\n",
    "\n",
    "# Calculate\n",
    "decision_score = neuron_vectorized(coffee, weights, bias)\n",
    "\n",
    "print(f\"Decision score: {decision_score}\")\n",
    "print(f\"Buy coffee? {'Yes!' if decision_score > 0 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Find the Weights\n",
    "Given these data points, can you find weights and bias that correctly classify them?\n",
    "- Point A: (1, 1) â†’ Class 0 (red)\n",
    "- Point B: (2, 3) â†’ Class 1 (green)\n",
    "- Point C: (3, 2) â†’ Class 1 (green)\n",
    "- Point D: (1, 2) â†’ Class 0 (red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Try different weight combinations and check if they work\n",
    "\n",
    "points = np.array([\n",
    "    [1, 1],  # Class 0 (should output < 0)\n",
    "    [2, 3],  # Class 1 (should output > 0)\n",
    "    [3, 2],  # Class 1 (should output > 0)\n",
    "    [1, 2]   # Class 0 (should output < 0)\n",
    "])\n",
    "\n",
    "# Try to find these:\n",
    "w1 = ?  # weight for x-coordinate\n",
    "w2 = ?  # weight for y-coordinate\n",
    "b = ?   # bias\n",
    "\n",
    "# Check your answer\n",
    "for i, point in enumerate(points):\n",
    "    output = neuron_vectorized(point, np.array([w1, w2]), b)\n",
    "    expected_class = 1 if i in [1, 2] else 0\n",
    "    predicted_class = 1 if output > 0 else 0\n",
    "    print(f\"Point {i}: {point}, Output: {output:.2f}, Expected: {expected_class}, Predicted: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸš€ What's Next?\n",
    "\n",
    "Excellent work! You now understand how a single neuron works and can implement it from scratch! ðŸŽ‰\n",
    "\n",
    "But we're missing a crucial piece: **activation functions**!\n",
    "\n",
    "Right now, our neuron just outputs a number (could be any value). In the next notebook, we'll learn:\n",
    "\n",
    "- **Why we need activation functions** (hint: non-linearity!)\n",
    "- **Common activation functions**: ReLU, Sigmoid, Tanh\n",
    "- **How to implement them** from scratch\n",
    "- **When to use which** activation function\n",
    "- **How they enable complex patterns** (solving XOR!)\n",
    "\n",
    "**Ready to add some non-linearity?** â†’ [Continue to Notebook 3: Activation Functions](03_activation_functions.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Great job on completing Notebook 2! You're building a strong foundation! ðŸ’ª*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}