
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Forward Propagation\n",
    "\n",
    "## The Journey of Data Through a Neural Network üöÄ\n",
    "\n",
    "Welcome back! So far we've learned:\n",
    "- What neural networks are (Notebook 1)\n",
    "- How single neurons work (Notebook 2)\n",
    "- Different activation functions (Notebook 3)\n",
    "- How to create layers of neurons (Notebook 4)\n",
    "\n",
    "Now it's time to put it all together and watch data **flow through an entire network**!\n",
    "\n",
    "### üè≠ The Assembly Line Analogy\n",
    "\n",
    "Think of forward propagation like a car assembly line:\n",
    "- **Input**: Raw materials (your data)\n",
    "- **Layer 1**: First assembly station (extracts basic features)\n",
    "- **Layer 2**: Second assembly station (combines features)\n",
    "- **Output**: Finished car (final prediction)\n",
    "\n",
    "Each station transforms the product step by step, just like each layer transforms the data!\n",
    "\n",
    "### üåä The Information Pipeline\n",
    "\n",
    "Forward propagation is literally data **flowing forward** through the network:\n",
    "```\n",
    "Input ‚Üí Layer 1 ‚Üí Activation ‚Üí Layer 2 ‚Üí Activation ‚Üí Output\n",
    "```\n",
    "\n",
    "Let's build this step by step! üîß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our essential tools\n",
    "import numpy as np  # For numerical computations and arrays\n",
    "import matplotlib.pyplot as plt  # For creating visualizations\n",
    "from matplotlib.patches import Circle  # For drawing neurons in diagrams\n",
    "from matplotlib.colors import ListedColormap  # For custom color maps\n",
    "\n",
    "# Make plots appear in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility (same results every time)\n",
    "np.random.seed(42)  # 42 is a classic choice from \"Hitchhiker's Guide to the Galaxy\"\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üì¶ NumPy version:\", np.__version__)  # Display NumPy version\n",
    "print(\"üì¶ Matplotlib version:\", plt.matplotlib.__version__)  # Display Matplotlib version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Forward Propagation\n",
    "\n",
    "### üí° What Is Forward Propagation?\n",
    "\n",
    "**Forward propagation** is the process of passing input data through all layers of the network to produce an output.\n",
    "\n",
    "Think of it like a **recipe**:\n",
    "1. Start with raw ingredients (input data)\n",
    "2. Mix and process at station 1 (layer 1 + activation)\n",
    "3. Mix and process at station 2 (layer 2 + activation)\n",
    "4. Get the final dish (output/prediction)\n",
    "\n",
    "### üìä The Step-by-Step Process\n",
    "\n",
    "For a 2-layer network:\n",
    "\n",
    "```\n",
    "1. Input: x = [x1, x2, ...]              (raw data)\n",
    "\n",
    "2. Layer 1 (Hidden Layer):\n",
    "   z1 = x @ W1 + b1                      (weighted sum)\n",
    "   a1 = activation(z1)                   (apply activation)\n",
    "\n",
    "3. Layer 2 (Output Layer):\n",
    "   z2 = a1 @ W2 + b2                     (weighted sum using layer 1's output)\n",
    "   a2 = activation(z2)                   (final output)\n",
    "\n",
    "4. Output: prediction = a2               (the network's answer)\n",
    "```\n",
    "\n",
    "Let's code this step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define our activation functions (from Notebook 3)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function: Squashes values to range (0, 1)\n",
    "    Formula: 1 / (1 + e^(-x))\n",
    "    \n",
    "    Use case: Great for binary classification (yes/no, true/false)\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))  # e^(-x) is calculated, then divided into 1\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU (Rectified Linear Unit): Returns max(0, x)\n",
    "    - If x > 0, returns x\n",
    "    - If x ‚â§ 0, returns 0\n",
    "    \n",
    "    Use case: Most common activation for hidden layers\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)  # Element-wise maximum between 0 and x\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Tanh (Hyperbolic Tangent): Squashes values to range (-1, 1)\n",
    "    \n",
    "    Use case: When you need outputs centered around zero\n",
    "    \"\"\"\n",
    "    return np.tanh(x)  # NumPy has built-in tanh function\n",
    "\n",
    "print(\"‚úÖ Activation functions defined!\")\n",
    "print(\"   ‚Ä¢ sigmoid(x): outputs between 0 and 1\")\n",
    "print(\"   ‚Ä¢ relu(x): outputs 0 or positive values\")\n",
    "print(\"   ‚Ä¢ tanh(x): outputs between -1 and 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a Simple 2-Layer Network\n",
    "\n",
    "### üèóÔ∏è Network Architecture\n",
    "\n",
    "Let's build a network to solve a simple problem:\n",
    "- **Input**: 2 features (x1, x2) - like coordinates on a graph\n",
    "- **Hidden Layer**: 4 neurons - learn patterns in the data\n",
    "- **Output Layer**: 1 neuron - final prediction (0 or 1)\n",
    "\n",
    "Architecture: **[2 inputs] ‚Üí [4 hidden neurons] ‚Üí [1 output]**\n",
    "\n",
    "### üéØ Our Task: The XOR Problem\n",
    "\n",
    "We'll solve the classic XOR (exclusive OR) problem:\n",
    "- If inputs are different ‚Üí output 1\n",
    "- If inputs are same ‚Üí output 0\n",
    "\n",
    "| x1 | x2 | Output |\n",
    "|----|-------|--------|\n",
    "| 0  | 0     | 0      |\n",
    "| 0  | 1     | 1      |\n",
    "| 1  | 0     | 1      |\n",
    "| 1  | 1     | 0      |\n",
    "\n",
    "This is impossible for a single neuron, but easy for a 2-layer network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the network parameters (weights and biases)\n",
    "\n",
    "def initialize_network(input_size, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for a 2-layer neural network.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_size: number of input features (2 for XOR)\n",
    "    - hidden_size: number of neurons in hidden layer (4)\n",
    "    - output_size: number of output neurons (1 for binary classification)\n",
    "    \n",
    "    Returns:\n",
    "    - parameters: dictionary containing W1, b1, W2, b2\n",
    "    \"\"\"\n",
    "    \n",
    "    # Layer 1 weights: connect inputs to hidden layer\n",
    "    # Shape: (input_size, hidden_size) = (2, 4)\n",
    "    # Each column = weights for one hidden neuron\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.5  # Random values * 0.5 to keep them small\n",
    "    \n",
    "    # Layer 1 biases: one per hidden neuron\n",
    "    # Shape: (hidden_size,) = (4,)\n",
    "    b1 = np.zeros((hidden_size,))  # Start with zeros\n",
    "    \n",
    "    # Layer 2 weights: connect hidden layer to output\n",
    "    # Shape: (hidden_size, output_size) = (4, 1)\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.5  # Random values * 0.5\n",
    "    \n",
    "    # Layer 2 biases: one per output neuron\n",
    "    # Shape: (output_size,) = (1,)\n",
    "    b2 = np.zeros((output_size,))  # Start with zeros\n",
    "    \n",
    "    # Store all parameters in a dictionary for easy access\n",
    "    parameters = {\n",
    "        'W1': W1,  # Weights for layer 1\n",
    "        'b1': b1,  # Biases for layer 1\n",
    "        'W2': W2,  # Weights for layer 2\n",
    "        'b2': b2   # Biases for layer 2\n",
    "    }\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Initialize our network\n",
    "# Architecture: 2 inputs ‚Üí 4 hidden neurons ‚Üí 1 output\n",
    "params = initialize_network(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "print(\"üèóÔ∏è  NETWORK INITIALIZED!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nLayer 1 (Input ‚Üí Hidden):\")\n",
    "print(f\"  ‚Ä¢ W1 shape: {params['W1'].shape} (2 inputs √ó 4 neurons)\")\n",
    "print(f\"  ‚Ä¢ b1 shape: {params['b1'].shape} (4 biases)\")\n",
    "print(f\"  ‚Ä¢ Total parameters: {params['W1'].size + params['b1'].size} (8 weights + 4 biases)\")\n",
    "\n",
    "print(\"\\nLayer 2 (Hidden ‚Üí Output):\")\n",
    "print(f\"  ‚Ä¢ W2 shape: {params['W2'].shape} (4 neurons √ó 1 output)\")\n",
    "print(f\"  ‚Ä¢ b2 shape: {params['b2'].shape} (1 bias)\")\n",
    "print(f\"  ‚Ä¢ Total parameters: {params['W2'].size + params['b2'].size} (4 weights + 1 bias)\")\n",
    "\n",
    "total_params = params['W1'].size + params['b1'].size + params['W2'].size + params['b2'].size\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üéØ TOTAL NETWORK PARAMETERS: {total_params}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implementing Forward Propagation\n",
    "\n",
    "### üåä The Data Flow\n",
    "\n",
    "Now let's implement the forward propagation function that pushes data through our network:\n",
    "\n",
    "```\n",
    "Input (x) \n",
    "    ‚Üì\n",
    "Layer 1: z1 = x @ W1 + b1      (compute weighted sum)\n",
    "    ‚Üì\n",
    "Activation: a1 = tanh(z1)       (apply non-linearity)\n",
    "    ‚Üì\n",
    "Layer 2: z2 = a1 @ W2 + b2     (compute weighted sum)\n",
    "    ‚Üì\n",
    "Activation: a2 = sigmoid(z2)    (get probability 0-1)\n",
    "    ‚Üì\n",
    "Output (prediction)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement forward propagation with EXTENSIVE comments on EVERY line\n",
    "\n",
    "def forward_propagation(X, parameters, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform forward propagation through the network.\n",
    "    \n",
    "    This function takes input data and passes it through all layers\n",
    "    to produce a final output/prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: input data, shape (num_samples, num_features)\n",
    "         For XOR: (4 samples, 2 features)\n",
    "    - parameters: dictionary with W1, b1, W2, b2\n",
    "    - verbose: if True, print detailed step-by-step info\n",
    "    \n",
    "    Returns:\n",
    "    - output: final predictions from the network\n",
    "    - cache: dictionary storing intermediate values (needed for learning later)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract parameters from dictionary (for easier reading)\n",
    "    W1 = parameters['W1']  # Weights for layer 1 (input ‚Üí hidden)\n",
    "    b1 = parameters['b1']  # Biases for layer 1\n",
    "    W2 = parameters['W2']  # Weights for layer 2 (hidden ‚Üí output)\n",
    "    b2 = parameters['b2']  # Biases for layer 2\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"üåä FORWARD PROPAGATION - Step by Step\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nüì• Input shape: {X.shape}\")\n",
    "        print(f\"   Data:\\n{X}\")\n",
    "    \n",
    "    # ========== LAYER 1: INPUT ‚Üí HIDDEN ==========\n",
    "    \n",
    "    # Step 1a: Compute weighted sum for layer 1\n",
    "    # This is matrix multiplication: each row of X multiplied by W1\n",
    "    # Result shape: (num_samples, hidden_size)\n",
    "    z1 = np.dot(X, W1) + b1  # @ is same as np.dot()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LAYER 1 (Hidden Layer)\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nStep 1a: Compute weighted sum\")\n",
    "        print(f\"   z1 = X @ W1 + b1\")\n",
    "        print(f\"   z1 shape: {z1.shape}\")\n",
    "        print(f\"   z1 (before activation):\\n{z1}\")\n",
    "    \n",
    "    # Step 1b: Apply activation function (tanh) to layer 1\n",
    "    # This adds non-linearity, allowing the network to learn complex patterns\n",
    "    # Output range: (-1, 1)\n",
    "    a1 = tanh(z1)  # Activation for hidden layer\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 1b: Apply tanh activation\")\n",
    "        print(f\"   a1 = tanh(z1)\")\n",
    "        print(f\"   a1 shape: {a1.shape}\")\n",
    "        print(f\"   a1 (hidden layer outputs):\\n{a1}\")\n",
    "        print(f\"   Range: [{a1.min():.3f}, {a1.max():.3f}]\")\n",
    "    \n",
    "    # ========== LAYER 2: HIDDEN ‚Üí OUTPUT ==========\n",
    "    \n",
    "    # Step 2a: Compute weighted sum for layer 2\n",
    "    # Now we use the outputs from layer 1 (a1) as inputs!\n",
    "    # Result shape: (num_samples, output_size)\n",
    "    z2 = np.dot(a1, W2) + b2  # Matrix multiplication with layer 1 outputs\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LAYER 2 (Output Layer)\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nStep 2a: Compute weighted sum\")\n",
    "        print(f\"   z2 = a1 @ W2 + b2\")\n",
    "        print(f\"   z2 shape: {z2.shape}\")\n",
    "        print(f\"   z2 (before activation):\\n{z2}\")\n",
    "    \n",
    "    # Step 2b: Apply activation function (sigmoid) to get final output\n",
    "    # Sigmoid squashes values to (0, 1) - perfect for binary classification!\n",
    "    # Values close to 1 mean \"yes\", close to 0 mean \"no\"\n",
    "    a2 = sigmoid(z2)  # Final output (probabilities)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 2b: Apply sigmoid activation\")\n",
    "        print(f\"   a2 = sigmoid(z2)\")\n",
    "        print(f\"   a2 shape: {a2.shape}\")\n",
    "        print(f\"   a2 (final predictions):\\n{a2}\")\n",
    "        print(f\"   Range: [{a2.min():.3f}, {a2.max():.3f}]\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ Forward propagation complete!\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    # Store intermediate values (we'll need these for backpropagation later)\n",
    "    # This is called a \"cache\" - like saving your work along the way\n",
    "    cache = {\n",
    "        'X': X,      # Input data\n",
    "        'z1': z1,    # Layer 1 weighted sum\n",
    "        'a1': a1,    # Layer 1 activation\n",
    "        'z2': z2,    # Layer 2 weighted sum\n",
    "        'a2': a2     # Layer 2 activation (final output)\n",
    "    }\n",
    "    \n",
    "    # Return the final output and the cache\n",
    "    return a2, cache\n",
    "\n",
    "print(\"‚úÖ Forward propagation function defined!\")\n",
    "print(\"   This function pushes data through all layers to get predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Testing Forward Propagation\n",
    "\n",
    "Let's test our network with the XOR data and watch the data flow through!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XOR dataset\n",
    "# This is the classic problem that proves we need non-linear networks!\n",
    "\n",
    "# Input data: all possible combinations of 0 and 1\n",
    "X_xor = np.array([\n",
    "    [0, 0],  # Sample 1: both inputs are 0\n",
    "    [0, 1],  # Sample 2: first is 0, second is 1\n",
    "    [1, 0],  # Sample 3: first is 1, second is 0\n",
    "    [1, 1]   # Sample 4: both inputs are 1\n",
    "])\n",
    "\n",
    "# Target outputs: XOR logic (1 if inputs are different, 0 if same)\n",
    "y_xor = np.array([\n",
    "    [0],  # 0 XOR 0 = 0 (same)\n",
    "    [1],  # 0 XOR 1 = 1 (different)\n",
    "    [1],  # 1 XOR 0 = 1 (different)\n",
    "    [0]   # 1 XOR 1 = 0 (same)\n",
    "])\n",
    "\n",
    "print(\"üìä XOR DATASET:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nInputs (X):\")\n",
    "print(\"    x1  x2\")\n",
    "for i, row in enumerate(X_xor):\n",
    "    print(f\"  [{row[0]}   {row[1]}]  ‚Üí  Target: {y_xor[i][0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nüéØ GOAL: Network should predict 1 when inputs differ,\")\n",
    "print(\"         and 0 when inputs are the same.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward propagation with verbose output to see every step!\n",
    "\n",
    "print(\"\\n\" + \"üß™ TESTING FORWARD PROPAGATION\" + \"\\n\")\n",
    "\n",
    "# Run forward propagation (with verbose=True to see all details)\n",
    "predictions, cache = forward_propagation(X_xor, params, verbose=True)\n",
    "\n",
    "# Show the results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nInput ‚Üí Prediction ‚Üí Target:\")\n",
    "for i in range(len(X_xor)):\n",
    "    pred_value = predictions[i][0]  # Get the prediction\n",
    "    target_value = y_xor[i][0]      # Get the true target\n",
    "    \n",
    "    # Convert prediction to binary (0 or 1) using threshold of 0.5\n",
    "    pred_binary = 1 if pred_value > 0.5 else 0\n",
    "    \n",
    "    # Check if correct\n",
    "    is_correct = \"‚úì\" if pred_binary == target_value else \"‚úó\"\n",
    "    \n",
    "    print(f\"  {X_xor[i]} ‚Üí {pred_value:.4f} (binary: {pred_binary}) ‚Üí Target: {target_value} {is_correct}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° Note: The predictions are RANDOM right now because we haven't\")\n",
    "print(\"   trained the network yet! Training will adjust the weights to\")\n",
    "print(\"   make correct predictions. We'll learn about that later!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualizing the Network Architecture\n",
    "\n",
    "Let's create a beautiful diagram showing how our network is structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our 2-layer network architecture\n",
    "\n",
    "def visualize_network():\n",
    "    \"\"\"\n",
    "    Draw a diagram of our neural network architecture.\n",
    "    Shows: 2 inputs ‚Üí 4 hidden neurons ‚Üí 1 output\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 5)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Neural Network Architecture for XOR Problem\\n2 Inputs ‚Üí 4 Hidden Neurons ‚Üí 1 Output', \n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Define positions for each layer\n",
    "    input_x = 0.5    # x-position for input layer\n",
    "    hidden_x = 1.5   # x-position for hidden layer\n",
    "    output_x = 2.5   # x-position for output layer\n",
    "    \n",
    "    # Input layer (2 neurons)\n",
    "    input_positions = [(input_x, 2.5), (input_x, 1.5)]  # y-positions for 2 inputs\n",
    "    \n",
    "    # Hidden layer (4 neurons)\n",
    "    hidden_positions = [(hidden_x, 3.5), (hidden_x, 2.5), (hidden_x, 1.5), (hidden_x, 0.5)]\n",
    "    \n",
    "    # Output layer (1 neuron)\n",
    "    output_positions = [(output_x, 2)]  # Center it\n",
    "    \n",
    "    # Draw input neurons\n",
    "    for i, (x, y) in enumerate(input_positions):\n",
    "        circle = Circle((x, y), 0.2, color='lightblue', ec='black', linewidth=2, zorder=5)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y, f'x{i+1}', ha='center', va='center', fontsize=12, fontweight='bold', zorder=6)\n",
    "        ax.text(x-0.5, y, f'Input {i+1}', ha='right', va='center', fontsize=10)\n",
    "    \n",
    "    # Draw hidden neurons\n",
    "    for i, (x, y) in enumerate(hidden_positions):\n",
    "        circle = Circle((x, y), 0.2, color='lightgreen', ec='black', linewidth=2, zorder=5)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y, f'h{i+1}', ha='center', va='center', fontsize=12, fontweight='bold', zorder=6)\n",
    "    \n",
    "    # Draw output neuron\n",
    "    for i, (x, y) in enumerate(output_positions):\n",
    "        circle = Circle((x, y), 0.2, color='lightcoral', ec='black', linewidth=2, zorder=5)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y, 'out', ha='center', va='center', fontsize=12, fontweight='bold', zorder=6)\n",
    "        ax.text(x+0.5, y, 'Output', ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    # Draw connections: Input ‚Üí Hidden\n",
    "    for x1, y1 in input_positions:\n",
    "        for x2, y2 in hidden_positions:\n",
    "            ax.plot([x1+0.2, x2-0.2], [y1, y2], 'gray', linewidth=1, alpha=0.5, zorder=1)\n",
    "    \n",
    "    # Draw connections: Hidden ‚Üí Output\n",
    "    for x1, y1 in hidden_positions:\n",
    "        for x2, y2 in output_positions:\n",
    "            ax.plot([x1+0.2, x2-0.2], [y1, y2], 'gray', linewidth=1, alpha=0.5, zorder=1)\n",
    "    \n",
    "    # Add layer labels\n",
    "    ax.text(input_x, 4.2, 'Input Layer\\n(2 neurons)', ha='center', fontsize=11, \n",
    "           fontweight='bold', bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.3))\n",
    "    ax.text(hidden_x, 4.2, 'Hidden Layer\\n(4 neurons)', ha='center', fontsize=11,\n",
    "           fontweight='bold', bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.3))\n",
    "    ax.text(output_x, 4.2, 'Output Layer\\n(1 neuron)', ha='center', fontsize=11,\n",
    "           fontweight='bold', bbox=dict(boxstyle='round,pad=0.5', facecolor='lightcoral', alpha=0.3))\n",
    "    \n",
    "    # Add activation function labels\n",
    "    ax.text(1.0, -0.3, 'tanh\\nactivation', ha='center', fontsize=9, style='italic',\n",
    "           bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3))\n",
    "    ax.text(2.0, -0.3, 'sigmoid\\nactivation', ha='center', fontsize=9, style='italic',\n",
    "           bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3))\n",
    "    \n",
    "    # Add parameter counts\n",
    "    ax.text(1.0, 4.7, '2√ó4 = 8 weights\\n+ 4 biases', ha='center', fontsize=8, style='italic')\n",
    "    ax.text(2.0, 4.7, '4√ó1 = 4 weights\\n+ 1 bias', ha='center', fontsize=8, style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('forward_prop_architecture.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create the visualization\n",
    "visualize_network()\n",
    "\n",
    "print(\"\\nüí° Key Points:\")\n",
    "print(\"   ‚Ä¢ Data flows LEFT to RIGHT (forward)\")\n",
    "print(\"   ‚Ä¢ Each layer transforms the data\")\n",
    "print(\"   ‚Ä¢ Activation functions add non-linearity\")\n",
    "print(\"   ‚Ä¢ Total: 17 learnable parameters (13 weights + 4 biases)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualizing Data Flow and Activations\n",
    "\n",
    "Let's see what happens to our data at each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how data transforms at each layer\n",
    "\n",
    "def visualize_activations(X, cache):\n",
    "    \"\"\"\n",
    "    Visualize the activation values at each layer.\n",
    "    Shows how the network transforms the input data.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Plot 1: Input data\n",
    "    im1 = axes[0].imshow(X.T, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "    axes[0].set_title('Input Data (X)\\n2 features √ó 4 samples', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Samples', fontsize=10)\n",
    "    axes[0].set_ylabel('Features', fontsize=10)\n",
    "    axes[0].set_xticks(range(4))\n",
    "    axes[0].set_xticklabels(['[0,0]', '[0,1]', '[1,0]', '[1,1]'])\n",
    "    axes[0].set_yticks(range(2))\n",
    "    axes[0].set_yticklabels(['x1', 'x2'])\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Add values as text\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            axes[0].text(j, i, f'{X[j, i]:.1f}', ha='center', va='center', \n",
    "                        color='white' if abs(X[j, i]) > 0.5 else 'black', fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Hidden layer activations\n",
    "    a1 = cache['a1']\n",
    "    im2 = axes[1].imshow(a1.T, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "    axes[1].set_title('Hidden Layer (a1)\\n4 neurons √ó 4 samples', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Samples', fontsize=10)\n",
    "    axes[1].set_ylabel('Hidden Neurons', fontsize=10)\n",
    "    axes[1].set_xticks(range(4))\n",
    "    axes[1].set_xticklabels(['[0,0]', '[0,1]', '[1,0]', '[1,1]'])\n",
    "    axes[1].set_yticks(range(4))\n",
    "    axes[1].set_yticklabels(['h1', 'h2', 'h3', 'h4'])\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    # Add values as text\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axes[1].text(j, i, f'{a1[j, i]:.2f}', ha='center', va='center',\n",
    "                        color='white' if abs(a1[j, i]) > 0.5 else 'black', fontsize=9)\n",
    "    \n",
    "    # Plot 3: Output layer\n",
    "    a2 = cache['a2']\n",
    "    im3 = axes[2].imshow(a2.T, cmap='RdBu', aspect='auto', vmin=0, vmax=1)\n",
    "    axes[2].set_title('Output (a2)\\n1 neuron √ó 4 samples', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_xlabel('Samples', fontsize=10)\n",
    "    axes[2].set_ylabel('Output', fontsize=10)\n",
    "    axes[2].set_xticks(range(4))\n",
    "    axes[2].set_xticklabels(['[0,0]', '[0,1]', '[1,0]', '[1,1]'])\n",
    "    axes[2].set_yticks([0])\n",
    "    axes[2].set_yticklabels(['out'])\n",
    "    plt.colorbar(im3, ax=axes[2])\n",
    "    \n",
    "    # Add values as text\n",
    "    for j in range(4):\n",
    "        axes[2].text(j, 0, f'{a2[j, 0]:.3f}', ha='center', va='center',\n",
    "                    color='white' if a2[j, 0] > 0.5 else 'black', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('activation_values.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° What This Shows:\")\n",
    "    print(\"   ‚Ä¢ INPUT: Raw data (just 0s and 1s)\")\n",
    "    print(\"   ‚Ä¢ HIDDEN: Network detects patterns (values between -1 and 1)\")\n",
    "    print(\"   ‚Ä¢ OUTPUT: Final predictions (probabilities between 0 and 1)\")\n",
    "    print(\"\\n   Each column represents one input sample flowing through the network!\")\n",
    "\n",
    "# Visualize the activations\n",
    "visualize_activations(X_xor, cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Simulating a Trained Network (Manually Set Weights)\n",
    "\n",
    "To really understand forward propagation, let's manually create a network that **actually solves XOR**!\n",
    "\n",
    "We'll set the weights ourselves to show that with the right values, the network CAN solve XOR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a network that actually solves XOR by setting weights manually\n",
    "# (This is what training does automatically!)\n",
    "\n",
    "# These weights are carefully chosen to solve XOR\n",
    "trained_params = {\n",
    "    # Layer 1: Create feature detectors\n",
    "    'W1': np.array([\n",
    "        [3.0, -3.0, 3.0, -3.0],   # Weights from x1\n",
    "        [3.0, 3.0, -3.0, -3.0]    # Weights from x2\n",
    "    ]),\n",
    "    'b1': np.array([-1.5, -4.5, -4.5, -1.5]),  # Biases for hidden layer\n",
    "    \n",
    "    # Layer 2: Combine features to detect XOR\n",
    "    'W2': np.array([\n",
    "        [5.0],    # Weight from h1\n",
    "        [5.0],    # Weight from h2\n",
    "        [5.0],    # Weight from h3\n",
    "        [5.0]     # Weight from h4\n",
    "    ]),\n",
    "    'b2': np.array([-2.5])  # Bias for output\n",
    "}\n",
    "\n",
    "print(\"üéØ TESTING A 'TRAINED' NETWORK (weights manually set)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run forward propagation with our trained network\n",
    "trained_predictions, trained_cache = forward_propagation(X_xor, trained_params, verbose=False)\n",
    "\n",
    "print(\"\\nüìä RESULTS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nInput ‚Üí Hidden Activations ‚Üí Output ‚Üí Target\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for i in range(len(X_xor)):\n",
    "    # Get values\n",
    "    input_val = X_xor[i]\n",
    "    hidden_val = trained_cache['a1'][i]\n",
    "    output_val = trained_predictions[i][0]\n",
    "    target_val = y_xor[i][0]\n",
    "    \n",
    "    # Convert to binary\n",
    "    output_binary = 1 if output_val > 0.5 else 0\n",
    "    is_correct = \"‚úì CORRECT\" if output_binary == target_val else \"‚úó WRONG\"\n",
    "    \n",
    "    print(f\"{input_val} ‚Üí [{hidden_val[0]:5.2f}, {hidden_val[1]:5.2f}, {hidden_val[2]:5.2f}, {hidden_val[3]:5.2f}] ‚Üí {output_val:.4f} ({output_binary}) ‚Üí {target_val} {is_correct}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions_binary = (trained_predictions > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions_binary == y_xor) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"üéâ ACCURACY: {accuracy:.1f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   With the RIGHT weights, this simple 2-layer network PERFECTLY solves XOR!\")\n",
    "print(\"   Training is the process of finding these optimal weights automatically.\")\n",
    "print(\"   (We'll learn about training in the next notebooks!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Visualizing Decision Boundary\n",
    "\n",
    "Let's visualize what our trained network has learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of the decision boundary\n",
    "\n",
    "def plot_decision_boundary(params, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary learned by the network.\n",
    "    \n",
    "    The decision boundary shows where the network switches from\n",
    "    predicting 0 to predicting 1.\n",
    "    \"\"\"\n",
    "    # Create a mesh grid to evaluate the network at many points\n",
    "    h = 0.01  # Step size in the mesh\n",
    "    x_min, x_max = -0.5, 1.5  # x-axis range\n",
    "    y_min, y_max = -0.5, 1.5  # y-axis range\n",
    "    \n",
    "    # Create the grid\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Flatten the grid and stack to create input points\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]  # Combine x and y coordinates\n",
    "    \n",
    "    # Get predictions for all grid points\n",
    "    predictions, _ = forward_propagation(grid_points, params, verbose=False)\n",
    "    \n",
    "    # Reshape predictions back to grid shape\n",
    "    Z = predictions.reshape(xx.shape)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot the decision boundary (contour at 0.5)\n",
    "    plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.3, colors=['blue', 'red'])\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    # Plot the training points\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, \n",
    "                         cmap=ListedColormap(['blue', 'red']),\n",
    "                         s=200, edgecolors='black', linewidths=2,\n",
    "                         marker='o', zorder=5)\n",
    "    \n",
    "    # Add labels to each point\n",
    "    for i, (x1, x2) in enumerate(X):\n",
    "        plt.annotate(f'({int(x1)},{int(x2)})', \n",
    "                    xy=(x1, x2), xytext=(x1+0.08, x2+0.08),\n",
    "                    fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.xlabel('x1 (Input 1)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('x2 (Input 2)', fontsize=12, fontweight='bold')\n",
    "    plt.title(title + '\\nBlue region = predict 0, Red region = predict 1', \n",
    "             fontsize=13, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(['Decision boundary (0.5)', 'Class 0', 'Class 1'], \n",
    "              loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('decision_boundary.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary for untrained network\n",
    "print(\"üìä UNTRAINED NETWORK (random weights):\")\n",
    "plot_decision_boundary(params, X_xor, y_xor.ravel(), \n",
    "                      \"Untrained Network Decision Boundary\")\n",
    "\n",
    "print(\"\\nüìä TRAINED NETWORK (optimized weights):\")\n",
    "plot_decision_boundary(trained_params, X_xor, y_xor.ravel(), \n",
    "                      \"Trained Network Decision Boundary\")\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"   ‚Ä¢ UNTRAINED: Decision boundary is essentially random\")\n",
    "print(\"   ‚Ä¢ TRAINED: Decision boundary perfectly separates the XOR pattern!\")\n",
    "print(\"   ‚Ä¢ The black line shows where the network outputs exactly 0.5\")\n",
    "print(\"   ‚Ä¢ Points on one side ‚Üí predict 0, points on other side ‚Üí predict 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Try It Yourself! üß™\n",
    "\n",
    "### Interactive Experimentation\n",
    "\n",
    "Now it's your turn to experiment with the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Try different network sizes\n",
    "\n",
    "def create_and_test_network(input_size, hidden_size, output_size, test_input):\n",
    "    \"\"\"\n",
    "    Create a network with custom architecture and test it.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üèóÔ∏è  NETWORK: {input_size} inputs ‚Üí {hidden_size} hidden ‚Üí {output_size} output\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize network\n",
    "    params = initialize_network(input_size, hidden_size, output_size)\n",
    "    \n",
    "    # Calculate total parameters\n",
    "    total = (input_size * hidden_size + hidden_size + \n",
    "             hidden_size * output_size + output_size)\n",
    "    print(f\"\\nüìä Total parameters: {total}\")\n",
    "    \n",
    "    # Test with input\n",
    "    output, cache = forward_propagation(test_input, params, verbose=False)\n",
    "    \n",
    "    print(f\"\\nüì• Test input: {test_input}\")\n",
    "    print(f\"üì§ Output: {output}\")\n",
    "    print(f\"üìä Hidden layer activations: {cache['a1']}\")\n",
    "    \n",
    "    return params\n",
    "\n",
    "print(\"üß™ EXPERIMENT: Try different architectures!\\n\")\n",
    "\n",
    "# Small network\n",
    "test_data = np.array([[0.5, 0.3]])  # Single test sample\n",
    "create_and_test_network(2, 3, 1, test_data)\n",
    "\n",
    "print(\"\\n\" * 2)\n",
    "\n",
    "# Larger network\n",
    "create_and_test_network(2, 10, 1, test_data)\n",
    "\n",
    "print(\"\\n\" * 2)\n",
    "\n",
    "# Very deep hidden layer\n",
    "create_and_test_network(2, 20, 1, test_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üí° What did you notice?\")\n",
    "print(\"   ‚Ä¢ More hidden neurons = more parameters = more capacity to learn\")\n",
    "print(\"   ‚Ä¢ But more isn't always better! (overfitting risk)\")\n",
    "print(\"   ‚Ä¢ The architecture should match the problem complexity\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Test different activation functions\n",
    "\n",
    "def test_activations(x_value):\n",
    "    \"\"\"\n",
    "    Compare different activation functions on the same input.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüî¨ Testing activation functions with input: {x_value}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test each activation\n",
    "    sigmoid_out = sigmoid(x_value)\n",
    "    relu_out = relu(x_value)\n",
    "    tanh_out = tanh(x_value)\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Sigmoid({x_value:5.2f}) = {sigmoid_out:6.4f}  (range: 0 to 1)\")\n",
    "    print(f\"  ‚Ä¢ ReLU({x_value:5.2f})    = {relu_out:6.4f}  (range: 0 to ‚àû)\")\n",
    "    print(f\"  ‚Ä¢ Tanh({x_value:5.2f})    = {tanh_out:6.4f}  (range: -1 to 1)\")\n",
    "\n",
    "print(\"üß™ EXPERIMENT: How do activations respond to different inputs?\\n\")\n",
    "\n",
    "test_activations(-2.0)  # Negative input\n",
    "test_activations(0.0)   # Zero input\n",
    "test_activations(2.0)   # Positive input\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° Key Differences:\")\n",
    "print(\"   ‚Ä¢ Sigmoid: Always positive, bounded (0, 1)\")\n",
    "print(\"   ‚Ä¢ ReLU: Kills negatives, unbounded positive\")\n",
    "print(\"   ‚Ä¢ Tanh: Centered at zero, bounded (-1, 1)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Mistakes ‚ö†Ô∏è\n",
    "\n",
    "Let's learn from common errors in forward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö†Ô∏è  COMMON MISTAKES IN FORWARD PROPAGATION\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Mistake #1: Forgetting activation functions\n",
    "print(\"\\n‚ùå MISTAKE #1: Forgetting Activation Functions\")\n",
    "print(\"-\"*70)\n",
    "print(\"\\nWithout activation, you just have:\")\n",
    "print(\"   z1 = X @ W1 + b1\")\n",
    "print(\"   z2 = z1 @ W2 + b2\")\n",
    "print(\"\\nThis is just: z2 = X @ (W1 @ W2) + (b1 @ W2 + b2)\")\n",
    "print(\"Which simplifies to: z2 = X @ W_combined + b_combined\")\n",
    "print(\"\\nüí° Result: Your deep network is just a single linear layer!\")\n",
    "print(\"   You MUST use activation functions for non-linearity!\")\n",
    "\n",
    "# Mistake #2: Wrong matrix dimensions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n‚ùå MISTAKE #2: Dimension Mismatches\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "test_input = np.array([[1, 2]])  # Shape: (1, 2)\n",
    "wrong_W1 = np.random.randn(4, 2)  # Shape: (4, 2) - WRONG!\n",
    "\n",
    "print(f\"\\nInput shape: {test_input.shape}\")\n",
    "print(f\"Wrong W1 shape: {wrong_W1.shape}\")\n",
    "print(\"\\nTrying: test_input @ wrong_W1...\")\n",
    "\n",
    "try:\n",
    "    result = np.dot(test_input, wrong_W1)\n",
    "    print(\"This shouldn't work!\")\n",
    "except ValueError as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "\n",
    "print(\"\\nüí° Remember: For X @ W to work:\")\n",
    "print(\"   X shape: (num_samples, num_features)\")\n",
    "print(\"   W shape: (num_features, num_neurons)\")\n",
    "print(\"   Output: (num_samples, num_neurons)\")\n",
    "\n",
    "# Mistake #3: Using wrong activation for output\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n‚ùå MISTAKE #3: Wrong Output Activation\")\n",
    "print(\"-\"*70)\n",
    "print(\"\\nFor BINARY classification:\")\n",
    "print(\"   ‚úÖ USE: Sigmoid (outputs 0-1, interpreted as probabilities)\")\n",
    "print(\"   ‚ùå DON'T USE: ReLU (can output huge values > 1)\")\n",
    "print(\"   ‚ùå DON'T USE: Tanh (outputs can be negative)\")\n",
    "\n",
    "print(\"\\nFor MULTI-CLASS classification:\")\n",
    "print(\"   ‚úÖ USE: Softmax (outputs sum to 1, like probabilities)\")\n",
    "\n",
    "print(\"\\nFor REGRESSION (predicting numbers):\")\n",
    "print(\"   ‚úÖ USE: Linear (no activation) or ReLU (if output must be positive)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways üìö\n",
    "\n",
    "### üéâ What We Learned Today:\n",
    "\n",
    "1. **Forward Propagation** = Data flowing through network layers:\n",
    "   ```\n",
    "   Input ‚Üí Layer 1 ‚Üí Activation ‚Üí Layer 2 ‚Üí Activation ‚Üí Output\n",
    "   ```\n",
    "\n",
    "2. **Step-by-Step Process**:\n",
    "   - Compute weighted sum: `z = inputs @ weights + bias`\n",
    "   - Apply activation: `a = activation(z)`\n",
    "   - Use layer output as input to next layer\n",
    "   - Repeat until final output\n",
    "\n",
    "3. **Each Layer Transforms Data**:\n",
    "   - Input layer: Raw data\n",
    "   - Hidden layers: Detect features and patterns\n",
    "   - Output layer: Final prediction\n",
    "\n",
    "4. **Activation Functions Are Critical**:\n",
    "   - They add non-linearity\n",
    "   - Without them, network is just linear algebra\n",
    "   - Different activations for different purposes\n",
    "\n",
    "5. **We Built a Complete Network**:\n",
    "   - Initialized weights and biases\n",
    "   - Implemented forward propagation\n",
    "   - Tested with XOR problem\n",
    "   - Visualized decision boundaries\n",
    "\n",
    "6. **Important Formulas**:\n",
    "   ```\n",
    "   Layer computation: a = activation(X @ W + b)\n",
    "   Chain layers: a2 = activation(activation(X @ W1 + b1) @ W2 + b2)\n",
    "   ```\n",
    "\n",
    "### üîÆ What's Next?\n",
    "\n",
    "Our network can make predictions, but they're random because the weights are random!\n",
    "\n",
    "**The Big Question**: How do we find the RIGHT weights?\n",
    "\n",
    "To answer this, we need to:\n",
    "1. **Measure how wrong we are** ‚Üê Next notebook: Loss Functions!\n",
    "2. Adjust weights to be less wrong (Backpropagation - coming later)\n",
    "3. Repeat until network is accurate (Training - coming later)\n",
    "\n",
    "In **Notebook 6: Loss Functions**, we'll learn:\n",
    "- How to measure prediction errors\n",
    "- Different loss functions for different problems\n",
    "- Why we need loss functions for learning\n",
    "\n",
    "### üí™ Practice Challenge:\n",
    "\n",
    "Before moving on, try:\n",
    "1. Create a network with 3 inputs, 5 hidden neurons, and 2 outputs\n",
    "2. Run forward propagation on random test data\n",
    "3. Visualize the activations at each layer\n",
    "4. Experiment with different activation function combinations\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: Forward propagation is like running data through an assembly line. Each layer processes and transforms the data, and the activation functions add the non-linear magic that makes neural networks powerful! üöÄ\n",
    "\n",
    "Now you understand how data flows through a network. Next, we'll learn how to measure and improve the network's predictions! üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
    