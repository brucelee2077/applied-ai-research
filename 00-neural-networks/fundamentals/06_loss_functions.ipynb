{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# üìè Loss Functions - Measuring How Wrong We Are\n",
    "\n",
    "Welcome back! In our journey so far, we've learned:\n",
    "- What neural networks are (Notebook 1)\n",
    "- How neurons work (Notebook 2)\n",
    "- Activation functions (Notebook 3)\n",
    "- Creating layers (Notebook 4)\n",
    "- Forward propagation (Notebook 5)\n",
    "\n",
    "Now we can make predictions... but they're terrible! Why? Because our weights are random.\n",
    "\n",
    "**The Big Question**: How do we measure how wrong our predictions are?\n",
    "\n",
    "**The Answer**: Loss Functions! üéØ\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- What loss functions are and why we need them\n",
    "- Mean Squared Error (MSE) for regression\n",
    "- Binary Cross-Entropy for binary classification\n",
    "- Categorical Cross-Entropy for multi-class problems\n",
    "- How to choose the right loss function\n",
    "- Visualizing loss landscapes\n",
    "\n",
    "**Prerequisites:** Notebooks 1-5, understanding of predictions and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our essential tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üì¶ NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§î Part 1: Why Do We Need Loss Functions?\n",
    "\n",
    "### üéì The Report Card Analogy\n",
    "\n",
    "Think of a loss function like a teacher grading your test:\n",
    "- **Your Answer**: What the network predicted\n",
    "- **Correct Answer**: What the actual target is\n",
    "- **Score**: How far off you were (the loss)\n",
    "\n",
    "The **lower the loss**, the **better the predictions**!\n",
    "\n",
    "### üó∫Ô∏è The GPS Analogy\n",
    "\n",
    "Imagine you're trying to reach a destination:\n",
    "- **Current Position**: Your prediction\n",
    "- **Destination**: The target value\n",
    "- **Distance**: The loss (how far you are from the goal)\n",
    "\n",
    "Your goal: **Minimize the distance** (loss) to reach the destination!\n",
    "\n",
    "### üìä Why Loss Functions Are Critical\n",
    "\n",
    "Our network needs loss functions to:\n",
    "1. **Quantify errors**: Turn \"bad prediction\" into a specific number\n",
    "2. **Compare predictions**: Know which weights are better\n",
    "3. **Guide learning**: Tell the network which direction to improve\n",
    "4. **Track progress**: Monitor if training is working\n",
    "\n",
    "Without loss functions, the network has no way to improve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a simple example to understand the concept\n",
    "\n",
    "print(\"üéØ SIMPLE EXAMPLE: Why We Need Loss Functions\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Scenario: Predicting house prices\n",
    "actual_price = 300000  # The real house price is $300,000\n",
    "\n",
    "# Let's try three different predictions\n",
    "prediction_1 = 295000  # Pretty good!\n",
    "prediction_2 = 350000  # Too high\n",
    "prediction_3 = 200000  # Way too low\n",
    "\n",
    "print(\"\\nüìç ACTUAL HOUSE PRICE: $300,000\")\n",
    "print(\"\\nüîÆ Our predictions:\")\n",
    "print(f\"   Prediction 1: ${prediction_1:,}\")\n",
    "print(f\"   Prediction 2: ${prediction_2:,}\")\n",
    "print(f\"   Prediction 3: ${prediction_3:,}\")\n",
    "\n",
    "# Calculate errors (how far off we are)\n",
    "error_1 = prediction_1 - actual_price\n",
    "error_2 = prediction_2 - actual_price\n",
    "error_3 = prediction_3 - actual_price\n",
    "\n",
    "print(\"\\n‚ùå Raw errors (prediction - actual):\")\n",
    "print(f\"   Error 1: ${error_1:,} (off by ${abs(error_1):,})\")\n",
    "print(f\"   Error 2: ${error_2:,} (off by ${abs(error_2):,})\")\n",
    "print(f\"   Error 3: ${error_3:,} (off by ${abs(error_3):,})\")\n",
    "\n",
    "# Problem: Negative and positive errors can cancel out!\n",
    "average_error = (error_1 + error_2 + error_3) / 3\n",
    "print(f\"\\n‚ö†Ô∏è  Average error: ${average_error:,.2f}\")\n",
    "print(\"    Problem: Positive and negative errors cancel out!\")\n",
    "\n",
    "# Solution: Square the errors (all become positive)\n",
    "squared_error_1 = error_1 ** 2\n",
    "squared_error_2 = error_2 ** 2\n",
    "squared_error_3 = error_3 ** 2\n",
    "\n",
    "print(\"\\n‚úÖ Squared errors (always positive):\")\n",
    "print(f\"   Squared Error 1: ${squared_error_1:,}\")\n",
    "print(f\"   Squared Error 2: ${squared_error_2:,}\")\n",
    "print(f\"   Squared Error 3: ${squared_error_3:,}\")\n",
    "\n",
    "# Calculate mean squared error\n",
    "mean_squared_error = (squared_error_1 + squared_error_2 + squared_error_3) / 3\n",
    "print(f\"\\nüìä Mean Squared Error (MSE): ${mean_squared_error:,.2f}\")\n",
    "print(\"    This is our LOSS - we want to minimize it!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° KEY INSIGHT:\")\n",
    "print(\"   ‚Ä¢ Loss functions convert prediction errors into a single number\")\n",
    "print(\"   ‚Ä¢ This number tells us how 'bad' our predictions are\")\n",
    "print(\"   ‚Ä¢ Lower loss = better predictions\")\n",
    "print(\"   ‚Ä¢ The goal of training: MINIMIZE THE LOSS!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Part 2: Mean Squared Error (MSE)\n",
    "\n",
    "### üéØ Use Case: Regression Problems\n",
    "\n",
    "**When to use MSE**: Predicting **continuous numbers** like:\n",
    "- House prices ($200,000, $350,000, etc.)\n",
    "- Temperature (72¬∞F, 85¬∞F, etc.)\n",
    "- Stock prices ($150.25, $200.50, etc.)\n",
    "- Age (25, 47, 63, etc.)\n",
    "\n",
    "### üìê The Formula\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{predicted}} - y_{\\text{actual}})^2$$\n",
    "\n",
    "In plain English:\n",
    "1. For each prediction: calculate (predicted - actual)\n",
    "2. Square it (makes all errors positive)\n",
    "3. Take the average of all squared errors\n",
    "\n",
    "### üîç Why Square the Errors?\n",
    "\n",
    "1. **Makes all errors positive**: -5 and +5 both become 25\n",
    "2. **Penalizes large errors more**: Error of 10 ‚Üí 100, but error of 2 ‚Üí 4\n",
    "3. **Mathematically nice**: Easy to take derivatives (needed for learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Mean Squared Error with detailed explanations\n",
    "\n",
    "def mean_squared_error(y_true, y_pred, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error (MSE) for regression problems.\n",
    "    \n",
    "    MSE measures the average squared difference between predictions and actual values.\n",
    "    Lower MSE = better predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Actual values (ground truth)\n",
    "    - y_pred: Predicted values\n",
    "    - verbose: If True, print detailed calculations\n",
    "    \n",
    "    Returns:\n",
    "    - mse: Mean Squared Error (scalar)\n",
    "    \n",
    "    Formula: MSE = (1/n) √ó Œ£(predicted - actual)¬≤\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays for easy math\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"üìä CALCULATING MSE - Step by Step\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nNumber of samples: {len(y_true)}\")\n",
    "        print(f\"\\nActual values:    {y_true}\")\n",
    "        print(f\"Predicted values: {y_pred}\")\n",
    "    \n",
    "    # Step 1: Calculate errors (prediction - actual)\n",
    "    errors = y_pred - y_true\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 1 - Errors (pred - actual): {errors}\")\n",
    "    \n",
    "    # Step 2: Square the errors (makes them all positive)\n",
    "    squared_errors = errors ** 2\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Step 2 - Squared errors:         {squared_errors}\")\n",
    "    \n",
    "    # Step 3: Calculate the mean (average) of squared errors\n",
    "    mse = np.mean(squared_errors)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 3 - Mean of squared errors: {mse:.4f}\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"\\n‚úÖ MEAN SQUARED ERROR (MSE): {mse:.4f}\")\n",
    "        print(\"   Lower is better!\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "print(\"‚úÖ MSE function defined!\")\n",
    "print(\"   Use this for: regression problems (predicting continuous numbers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Good predictions (low MSE)\n",
    "print(\"\\nüéØ EXAMPLE 1: Good Predictions\\n\")\n",
    "\n",
    "actual_temps = [72, 75, 68, 80, 77]  # Actual temperatures\n",
    "good_predictions = [71, 76, 69, 79, 78]  # Pretty close!\n",
    "\n",
    "mse_good = mean_squared_error(actual_temps, good_predictions, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Example 2: Bad predictions (high MSE)\n",
    "print(\"\\nüéØ EXAMPLE 2: Bad Predictions\\n\")\n",
    "\n",
    "bad_predictions = [65, 82, 60, 90, 70]  # Way off!\n",
    "\n",
    "mse_bad = mean_squared_error(actual_temps, bad_predictions, verbose=True)\n",
    "\n",
    "# Compare\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nGood predictions MSE: {mse_good:.4f}\")\n",
    "print(f\"Bad predictions MSE:  {mse_bad:.4f}\")\n",
    "print(f\"\\nThe bad predictions have {mse_bad/mse_good:.1f}x higher loss!\")\n",
    "print(\"\\nüí° Lower MSE = Better predictions\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how MSE changes with prediction errors\n",
    "\n",
    "print(\"\\nüìà VISUALIZING MSE\\n\")\n",
    "\n",
    "# Let's say the actual value is 5\n",
    "actual_value = 5.0\n",
    "\n",
    "# Try many different predictions from 0 to 10\n",
    "predictions = np.linspace(0, 10, 100)\n",
    "\n",
    "# Calculate MSE for each prediction\n",
    "mse_values = [(pred - actual_value)**2 for pred in predictions]\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: MSE vs Prediction\n",
    "ax1.plot(predictions, mse_values, 'b-', linewidth=2.5, label='MSE')\n",
    "ax1.axvline(x=actual_value, color='red', linestyle='--', linewidth=2, label=f'Actual value = {actual_value}')\n",
    "ax1.scatter([actual_value], [0], color='red', s=200, zorder=5, marker='*', edgecolors='black', linewidth=2)\n",
    "ax1.set_xlabel('Predicted Value', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('MSE (Loss)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('MSE Changes with Prediction\\n(Actual value = 5)', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0, 30])\n",
    "\n",
    "# Add annotations\n",
    "ax1.annotate('Perfect prediction!\\nMSE = 0', xy=(5, 0), xytext=(5, 10),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "            fontsize=10, ha='center', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "ax1.annotate('Far from target\\nMSE is HIGH', xy=(9, 16), xytext=(8.5, 25),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            fontsize=10, ha='center', bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "# Plot 2: Comparing different prediction scenarios\n",
    "scenarios = ['Perfect\\n(pred=5)', 'Close\\n(pred=6)', 'Far\\n(pred=8)', 'Very Far\\n(pred=10)']\n",
    "scenario_preds = [5, 6, 8, 10]\n",
    "scenario_mse = [(p - actual_value)**2 for p in scenario_preds]\n",
    "colors = ['green', 'yellow', 'orange', 'red']\n",
    "\n",
    "bars = ax2.bar(scenarios, scenario_mse, color=colors, edgecolor='black', linewidth=2, alpha=0.7)\n",
    "ax2.set_ylabel('MSE (Loss)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('MSE for Different Predictions\\n(Actual = 5)', fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mse in zip(bars, scenario_mse):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{mse:.1f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mse_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ MSE forms a parabola (U-shape)\")\n",
    "print(\"   ‚Ä¢ Minimum at the actual value (perfect prediction)\")\n",
    "print(\"   ‚Ä¢ Grows quickly as you move away from the target\")\n",
    "print(\"   ‚Ä¢ Large errors are penalized more than small errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### üí° When to Use MSE\n",
    "\n",
    "**‚úÖ Use MSE for:**\n",
    "- Regression problems (predicting continuous values)\n",
    "- When you want to penalize large errors heavily\n",
    "- When your data doesn't have extreme outliers\n",
    "- House price prediction\n",
    "- Temperature forecasting\n",
    "- Stock price prediction\n",
    "\n",
    "**‚ùå Don't use MSE for:**\n",
    "- Classification problems (use cross-entropy instead)\n",
    "- When your data has extreme outliers (they'll dominate the loss)\n",
    "- When you want to interpret loss in the same units as your data (use MAE instead)\n",
    "\n",
    "**‚ö†Ô∏è Things to Watch Out For:**\n",
    "- MSE is sensitive to outliers (large errors are squared!)\n",
    "- Units are squared (e.g., predicting prices in dollars ‚Üí MSE in dollars¬≤)\n",
    "- Large MSE values might need scaling for numerical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÄ Part 3: Binary Cross-Entropy (BCE)\n",
    "\n",
    "### üéØ Use Case: Binary Classification\n",
    "\n",
    "**When to use BCE**: Classifying into **TWO classes**:\n",
    "- Spam vs Not Spam\n",
    "- Cat vs Dog\n",
    "- Fraud vs Legitimate\n",
    "- Disease vs Healthy\n",
    "\n",
    "### üìê The Formula\n",
    "\n",
    "$$\\text{BCE} = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\cdot \\log(p_i) + (1-y_i) \\cdot \\log(1-p_i)]$$\n",
    "\n",
    "Where:\n",
    "- $y$ = actual label (0 or 1)\n",
    "- $p$ = predicted probability (0 to 1)\n",
    "\n",
    "### ü§î Why Not Use MSE for Classification?\n",
    "\n",
    "MSE doesn't work well for classification because:\n",
    "1. **Probability interpretation**: We want to measure how confident predictions are\n",
    "2. **Better gradients**: BCE gives clearer signals for learning\n",
    "3. **Proper scoring**: Rewards confident correct predictions, heavily penalizes confident wrong predictions\n",
    "\n",
    "### üí≠ Intuition\n",
    "\n",
    "Binary Cross-Entropy measures **how surprised we are** by the prediction:\n",
    "- Predict 0.99 when actual is 1 ‚Üí Small loss (good!)\n",
    "- Predict 0.01 when actual is 1 ‚Üí Huge loss (terrible!)\n",
    "- Predict 0.50 ‚Üí Medium loss (uncertain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Binary Cross-Entropy with detailed explanations\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred, epsilon=1e-15, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate Binary Cross-Entropy (BCE) for binary classification.\n",
    "    \n",
    "    BCE measures how well predicted probabilities match actual binary labels.\n",
    "    Lower BCE = better predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Actual labels (0 or 1)\n",
    "    - y_pred: Predicted probabilities (between 0 and 1)\n",
    "    - epsilon: Small number to prevent log(0) errors\n",
    "    - verbose: If True, print detailed calculations\n",
    "    \n",
    "    Returns:\n",
    "    - bce: Binary Cross-Entropy (scalar)\n",
    "    \n",
    "    Formula: BCE = -[y√ólog(p) + (1-y)√ólog(1-p)]\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Clip predictions to prevent log(0) which is undefined\n",
    "    # This prevents numerical instability\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"üìä CALCULATING BINARY CROSS-ENTROPY - Step by Step\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nNumber of samples: {len(y_true)}\")\n",
    "        print(f\"\\nActual labels (0 or 1):      {y_true}\")\n",
    "        print(f\"Predicted probabilities (0-1): {y_pred}\")\n",
    "    \n",
    "    # Calculate loss for each sample\n",
    "    # When y_true=1: loss = -log(y_pred)\n",
    "    # When y_true=0: loss = -log(1-y_pred)\n",
    "    loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nPer-sample losses: {loss}\")\n",
    "        print(\"\\nBreakdown for each sample:\")\n",
    "        for i in range(len(y_true)):\n",
    "            if y_true[i] == 1:\n",
    "                print(f\"  Sample {i}: y=1, p={y_pred[i]:.3f} ‚Üí loss=-log({y_pred[i]:.3f})={loss[i]:.4f}\")\n",
    "            else:\n",
    "                print(f\"  Sample {i}: y=0, p={y_pred[i]:.3f} ‚Üí loss=-log({1-y_pred[i]:.3f})={loss[i]:.4f}\")\n",
    "    \n",
    "    # Take the average\n",
    "    bce = np.mean(loss)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"\\n‚úÖ BINARY CROSS-ENTROPY (BCE): {bce:.4f}\")\n",
    "        print(\"   Lower is better!\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    return bce\n",
    "\n",
    "print(\"‚úÖ Binary Cross-Entropy function defined!\")\n",
    "print(\"   Use this for: binary classification (2 classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Email spam detection\n",
    "\n",
    "print(\"\\nüìß EXAMPLE: Email Spam Detection\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"Task: Classify emails as Spam (1) or Not Spam (0)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Actual labels\n",
    "actual_labels = [1, 0, 1, 1, 0]  # 1=Spam, 0=Not Spam\n",
    "\n",
    "# Scenario 1: Good predictions (confident and correct)\n",
    "print(\"\\nüéØ SCENARIO 1: Good Predictions (Confident & Correct)\\n\")\n",
    "good_predictions = [0.95, 0.05, 0.90, 0.88, 0.10]  # High confidence, mostly correct\n",
    "\n",
    "bce_good = binary_cross_entropy(actual_labels, good_predictions, verbose=True)\n",
    "\n",
    "# Scenario 2: Bad predictions (confident but wrong)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüéØ SCENARIO 2: Bad Predictions (Confident but WRONG)\\n\")\n",
    "bad_predictions = [0.10, 0.90, 0.15, 0.20, 0.85]  # Confidently wrong!\n",
    "\n",
    "bce_bad = binary_cross_entropy(actual_labels, bad_predictions, verbose=True)\n",
    "\n",
    "# Scenario 3: Uncertain predictions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüéØ SCENARIO 3: Uncertain Predictions (All around 0.5)\\n\")\n",
    "uncertain_predictions = [0.50, 0.50, 0.50, 0.50, 0.50]  # No confidence\n",
    "\n",
    "bce_uncertain = binary_cross_entropy(actual_labels, uncertain_predictions, verbose=True)\n",
    "\n",
    "# Compare all scenarios\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä COMPARISON OF ALL SCENARIOS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nGood predictions (confident & correct): BCE = {bce_good:.4f} ‚úÖ BEST\")\n",
    "print(f\"Uncertain predictions (all ~0.5):       BCE = {bce_uncertain:.4f}\")\n",
    "print(f\"Bad predictions (confident but wrong):  BCE = {bce_bad:.4f} ‚ùå WORST\")\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   ‚Ä¢ Confident correct predictions ‚Üí Low BCE (good!)\")\n",
    "print(\"   ‚Ä¢ Uncertain predictions ‚Üí Medium BCE\")\n",
    "print(\"   ‚Ä¢ Confident wrong predictions ‚Üí High BCE (bad!)\")\n",
    "print(\"\\n   BCE heavily penalizes confident mistakes!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Binary Cross-Entropy\n",
    "\n",
    "print(\"\\nüìà VISUALIZING BINARY CROSS-ENTROPY\\n\")\n",
    "\n",
    "# Create prediction values from 0.01 to 0.99\n",
    "predictions = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# Calculate BCE for two cases:\n",
    "# Case 1: Actual label is 1 (positive class)\n",
    "bce_when_actual_is_1 = -np.log(predictions)\n",
    "\n",
    "# Case 2: Actual label is 0 (negative class)\n",
    "bce_when_actual_is_0 = -np.log(1 - predictions)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: BCE when actual = 1\n",
    "axes[0].plot(predictions, bce_when_actual_is_1, 'r-', linewidth=2.5)\n",
    "axes[0].set_xlabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('BCE when Actual = 1 (Positive Class)\\nLower prediction = Higher loss', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 5])\n",
    "axes[0].axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Annotations\n",
    "axes[0].annotate('Predict 0.99\\nLoss ‚âà 0', xy=(0.99, 0.01), xytext=(0.7, 1.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "                fontsize=10, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "axes[0].annotate('Predict 0.01\\nLoss ‚âà 4.6', xy=(0.01, 4.6), xytext=(0.2, 3),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                fontsize=10, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "# Plot 2: BCE when actual = 0\n",
    "axes[1].plot(predictions, bce_when_actual_is_0, 'b-', linewidth=2.5)\n",
    "axes[1].set_xlabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('BCE when Actual = 0 (Negative Class)\\nHigher prediction = Higher loss', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 5])\n",
    "axes[1].axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Annotations\n",
    "axes[1].annotate('Predict 0.01\\nLoss ‚âà 0', xy=(0.01, 0.01), xytext=(0.3, 1.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "                fontsize=10, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "axes[1].annotate('Predict 0.99\\nLoss ‚âà 4.6', xy=(0.99, 4.6), xytext=(0.7, 3),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                fontsize=10, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "# Plot 3: Comparison heatmap\n",
    "# Create grid of actual vs predicted\n",
    "actual_vals = [0, 1]\n",
    "pred_vals = np.linspace(0.01, 0.99, 50)\n",
    "loss_grid = np.zeros((2, 50))\n",
    "\n",
    "for i, actual in enumerate(actual_vals):\n",
    "    if actual == 1:\n",
    "        loss_grid[i, :] = -np.log(pred_vals)\n",
    "    else:\n",
    "        loss_grid[i, :] = -np.log(1 - pred_vals)\n",
    "\n",
    "im = axes[2].imshow(loss_grid, aspect='auto', cmap='RdYlGn_r', vmin=0, vmax=5)\n",
    "axes[2].set_xlabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Actual Label', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('BCE Loss Heatmap\\nRed = High Loss, Green = Low Loss', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[2].set_yticks([0, 1])\n",
    "axes[2].set_yticklabels(['0', '1'])\n",
    "axes[2].set_xticks([0, 25, 49])\n",
    "axes[2].set_xticklabels(['0.0', '0.5', '1.0'])\n",
    "plt.colorbar(im, ax=axes[2], label='Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bce_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ When actual=1: Want predictions close to 1 (red curve)\")\n",
    "print(\"   ‚Ä¢ When actual=0: Want predictions close to 0 (blue curve)\")\n",
    "print(\"   ‚Ä¢ Loss shoots up when you're confidently wrong!\")\n",
    "print(\"   ‚Ä¢ Loss is moderate when uncertain (prediction ‚âà 0.5)\")\n",
    "print(\"   ‚Ä¢ Heatmap shows: Green=good, Red=bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### üí° When to Use Binary Cross-Entropy\n",
    "\n",
    "**‚úÖ Use BCE for:**\n",
    "- Binary classification (exactly 2 classes)\n",
    "- When your output is a probability (0 to 1)\n",
    "- Use with sigmoid activation in output layer\n",
    "- Examples: Spam detection, fraud detection, medical diagnosis\n",
    "\n",
    "**‚ùå Don't use BCE for:**\n",
    "- Regression problems (use MSE)\n",
    "- Multi-class problems with >2 classes (use categorical cross-entropy)\n",
    "- Multi-label classification (can use BCE, but apply per label)\n",
    "\n",
    "**‚ö†Ô∏è Things to Watch Out For:**\n",
    "- Predictions must be between 0 and 1 (use sigmoid activation!)\n",
    "- Use epsilon clipping to prevent log(0) errors\n",
    "- Make sure your labels are 0 or 1 (not -1 and 1)\n",
    "- Can handle class imbalance with weighted BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "## üé® Part 4: Categorical Cross-Entropy (CCE)\n",
    "\n",
    "### üéØ Use Case: Multi-Class Classification\n",
    "\n",
    "**When to use CCE**: Classifying into **3 or more classes**:\n",
    "- Digit recognition (0-9, 10 classes)\n",
    "- Animal classification (cat, dog, bird, fish, etc.)\n",
    "- Sentiment analysis (positive, neutral, negative)\n",
    "- Language identification (English, Spanish, French, etc.)\n",
    "\n",
    "### üìê The Formula\n",
    "\n",
    "$$\\text{CCE} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i,c} \\cdot \\log(p_{i,c})$$\n",
    "\n",
    "Where:\n",
    "- $n$ = number of samples\n",
    "- $C$ = number of classes\n",
    "- $y_{i,c}$ = 1 if sample $i$ belongs to class $c$, else 0 (one-hot encoded)\n",
    "- $p_{i,c}$ = predicted probability that sample $i$ belongs to class $c$\n",
    "\n",
    "### üí≠ Intuition\n",
    "\n",
    "CCE is like BCE, but for multiple classes:\n",
    "- Network outputs probability for **each class**\n",
    "- Probabilities must **sum to 1** (achieved with softmax)\n",
    "- Loss measures how well predictions match the true class\n",
    "\n",
    "### üè∑Ô∏è One-Hot Encoding\n",
    "\n",
    "For multi-class problems, labels are often one-hot encoded:\n",
    "- Class 0: `[1, 0, 0, 0]`\n",
    "- Class 1: `[0, 1, 0, 0]`\n",
    "- Class 2: `[0, 0, 1, 0]`\n",
    "- Class 3: `[0, 0, 0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement softmax activation (needed for multi-class output)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax activation function.\n",
    "    \n",
    "    Converts raw scores (logits) into probabilities that sum to 1.\n",
    "    Used as the final activation for multi-class classification.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input array of shape (num_samples, num_classes)\n",
    "    \n",
    "    Returns:\n",
    "    - Probability distribution over classes (sums to 1 for each sample)\n",
    "    \n",
    "    Formula: softmax(x_i) = exp(x_i) / Œ£exp(x_j)\n",
    "    \"\"\"\n",
    "    # Subtract max for numerical stability (prevents overflow)\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Quick test of softmax\n",
    "print(\"üß™ Testing Softmax:\\n\")\n",
    "\n",
    "# Raw scores (logits) for 3 classes\n",
    "logits = np.array([[2.0, 1.0, 0.1]])  # One sample, 3 classes\n",
    "print(f\"Input logits:  {logits[0]}\")\n",
    "\n",
    "probs = softmax(logits)\n",
    "print(f\"Output probs:  {probs[0]}\")\n",
    "print(f\"Sum of probs:  {np.sum(probs[0]):.6f} (should be 1.0)\")\n",
    "\n",
    "print(\"\\n‚úÖ Softmax function working correctly!\")\n",
    "print(\"   ‚Ä¢ Converts scores to probabilities\")\n",
    "print(\"   ‚Ä¢ All probabilities are between 0 and 1\")\n",
    "print(\"   ‚Ä¢ Probabilities sum to exactly 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Categorical Cross-Entropy\n",
    "\n",
    "def categorical_cross_entropy(y_true, y_pred, epsilon=1e-15, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate Categorical Cross-Entropy (CCE) for multi-class classification.\n",
    "    \n",
    "    CCE measures how well predicted probability distributions match \n",
    "    the true class labels (one-hot encoded).\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Actual labels as one-hot vectors, shape (num_samples, num_classes)\n",
    "    - y_pred: Predicted probabilities, shape (num_samples, num_classes)\n",
    "    - epsilon: Small number to prevent log(0) errors\n",
    "    - verbose: If True, print detailed calculations\n",
    "    \n",
    "    Returns:\n",
    "    - cce: Categorical Cross-Entropy (scalar)\n",
    "    \n",
    "    Formula: CCE = -Œ£Œ£ y√ólog(p)\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Clip predictions to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"üìä CALCULATING CATEGORICAL CROSS-ENTROPY - Step by Step\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nNumber of samples: {len(y_true)}\")\n",
    "        print(f\"Number of classes: {y_true.shape[1]}\")\n",
    "        print(f\"\\nActual labels (one-hot):\")\n",
    "        print(y_true)\n",
    "        print(f\"\\nPredicted probabilities:\")\n",
    "        print(y_pred)\n",
    "        \n",
    "        # Verify predictions sum to 1\n",
    "        sums = np.sum(y_pred, axis=1)\n",
    "        print(f\"\\nProbability sums (should be ~1.0): {sums}\")\n",
    "    \n",
    "    # Calculate loss for each sample\n",
    "    # Only the probability of the true class matters (y_true is one-hot)\n",
    "    sample_losses = -np.sum(y_true * np.log(y_pred), axis=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nPer-sample losses: {sample_losses}\")\n",
    "        print(\"\\nDetailed breakdown:\")\n",
    "        for i in range(len(y_true)):\n",
    "            true_class = np.argmax(y_true[i])\n",
    "            pred_prob = y_pred[i, true_class]\n",
    "            print(f\"  Sample {i}: True class = {true_class}, \"\n",
    "                  f\"Predicted prob = {pred_prob:.4f}, \"\n",
    "                  f\"Loss = -log({pred_prob:.4f}) = {sample_losses[i]:.4f}\")\n",
    "    \n",
    "    # Take the average\n",
    "    cce = np.mean(sample_losses)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"\\n‚úÖ CATEGORICAL CROSS-ENTROPY (CCE): {cce:.4f}\")\n",
    "        print(\"   Lower is better!\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    return cce\n",
    "\n",
    "print(\"‚úÖ Categorical Cross-Entropy function defined!\")\n",
    "print(\"   Use this for: multi-class classification (3+ classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Handwritten digit classification (0-9)\n",
    "\n",
    "print(\"\\nüî¢ EXAMPLE: Handwritten Digit Classification (0-9)\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"Task: Classify images into 10 digit classes (0-9)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create example data: 5 samples, 10 classes\n",
    "# True labels (one-hot encoded)\n",
    "y_true = np.array([\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # Sample 0: digit \"0\"\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],  # Sample 1: digit \"1\"\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],  # Sample 2: digit \"2\"\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],  # Sample 3: digit \"3\"\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],  # Sample 4: digit \"4\"\n",
    "])\n",
    "\n",
    "# Scenario 1: Good predictions\n",
    "print(\"\\nüéØ SCENARIO 1: Good Predictions (High confidence on correct class)\\n\")\n",
    "\n",
    "# High probability on the correct class\n",
    "good_logits = np.array([\n",
    "    [5.0, 0.5, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],  # Confident it's \"0\"\n",
    "    [0.1, 4.5, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],  # Confident it's \"1\"\n",
    "    [0.1, 0.1, 4.8, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],  # Confident it's \"2\"\n",
    "    [0.1, 0.1, 0.1, 5.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],  # Confident it's \"3\"\n",
    "    [0.1, 0.1, 0.1, 0.1, 4.7, 0.1, 0.1, 0.1, 0.1, 0.1],  # Confident it's \"4\"\n",
    "])\n",
    "\n",
    "good_probs = softmax(good_logits)\n",
    "cce_good = categorical_cross_entropy(y_true, good_probs, verbose=True)\n",
    "\n",
    "# Scenario 2: Uncertain predictions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüéØ SCENARIO 2: Uncertain Predictions (Equal probability for all classes)\\n\")\n",
    "\n",
    "# All classes have equal probability\n",
    "uncertain_logits = np.ones((5, 10))  # All logits are 1.0\n",
    "uncertain_probs = softmax(uncertain_logits)\n",
    "\n",
    "cce_uncertain = categorical_cross_entropy(y_true, uncertain_probs, verbose=True)\n",
    "\n",
    "# Scenario 3: Bad predictions (wrong class)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüéØ SCENARIO 3: Bad Predictions (High confidence on WRONG class)\\n\")\n",
    "\n",
    "# High probability on the wrong class\n",
    "bad_logits = np.array([\n",
    "    [0.1, 5.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],  # Predicts \"1\" but actual is \"0\"\n",
    "    [5.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],  # Predicts \"0\" but actual is \"1\"\n",
    "    [0.1, 0.1, 0.1, 5.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],  # Predicts \"3\" but actual is \"2\"\n",
    "    [0.1, 0.1, 5.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],  # Predicts \"2\" but actual is \"3\"\n",
    "    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 5.0, 0.1, 0.1, 0.1],  # Predicts \"6\" but actual is \"4\"\n",
    "])\n",
    "\n",
    "bad_probs = softmax(bad_logits)\n",
    "cce_bad = categorical_cross_entropy(y_true, bad_probs, verbose=True)\n",
    "\n",
    "# Compare all scenarios\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä COMPARISON OF ALL SCENARIOS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nGood predictions (correct & confident):  CCE = {cce_good:.4f} ‚úÖ BEST\")\n",
    "print(f\"Uncertain predictions (uniform probs):   CCE = {cce_uncertain:.4f}\")\n",
    "print(f\"Bad predictions (wrong & confident):     CCE = {cce_bad:.4f} ‚ùå WORST\")\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   ‚Ä¢ Confident correct predictions ‚Üí Low CCE\")\n",
    "print(\"   ‚Ä¢ Uncertain predictions ‚Üí Medium CCE (‚âà log(num_classes))\")\n",
    "print(\"   ‚Ä¢ Confident wrong predictions ‚Üí High CCE\")\n",
    "print(f\"   ‚Ä¢ Random guessing gives CCE ‚âà {np.log(10):.4f} for 10 classes\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Categorical Cross-Entropy\n",
    "\n",
    "print(\"\\nüìà VISUALIZING CATEGORICAL CROSS-ENTROPY\\n\")\n",
    "\n",
    "# Example: 3 classes for simplicity\n",
    "num_classes = 3\n",
    "predictions_range = np.linspace(0.01, 0.99, 50)\n",
    "\n",
    "# Calculate loss when true class is class 0\n",
    "losses_for_class_0 = -np.log(predictions_range)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Loss vs prediction for true class\n",
    "axes[0].plot(predictions_range, losses_for_class_0, 'b-', linewidth=2.5)\n",
    "axes[0].set_xlabel('Predicted Probability for True Class', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('CCE: Loss vs Probability\\n(for the true class)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 5])\n",
    "\n",
    "# Annotations\n",
    "axes[0].annotate('High confidence\\n(p=0.99)\\nLow loss!', xy=(0.99, 0.01), xytext=(0.7, 2),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "                fontsize=10, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "axes[0].annotate('Low confidence\\n(p=0.01)\\nHigh loss!', xy=(0.01, 4.6), xytext=(0.2, 3.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                fontsize=10, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "# Plot 2: Example probability distributions\n",
    "scenarios = ['Perfect\\n[0.98, 0.01, 0.01]', 'Uncertain\\n[0.33, 0.33, 0.34]', 'Wrong\\n[0.01, 0.01, 0.98]']\n",
    "probs_scenarios = [\n",
    "    [0.98, 0.01, 0.01],  # Confident and correct (true class is 0)\n",
    "    [0.33, 0.33, 0.34],  # Uncertain\n",
    "    [0.01, 0.01, 0.98],  # Confident but wrong\n",
    "]\n",
    "true_label = [1, 0, 0]  # True class is 0\n",
    "\n",
    "losses = []\n",
    "for probs in probs_scenarios:\n",
    "    loss = -np.log(probs[0])  # Loss for true class (class 0)\n",
    "    losses.append(loss)\n",
    "\n",
    "colors = ['green', 'yellow', 'red']\n",
    "bars = axes[1].bar(range(3), losses, color=colors, edgecolor='black', linewidth=2, alpha=0.7)\n",
    "axes[1].set_xticks(range(3))\n",
    "axes[1].set_xticklabels(scenarios, fontsize=9)\n",
    "axes[1].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('CCE for Different Predictions\\n(True class = 0)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, loss in zip(bars, losses):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{loss:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 3: Probability distribution visualization\n",
    "x_pos = np.arange(3)\n",
    "width = 0.25\n",
    "\n",
    "for i, (probs, scenario) in enumerate(zip(probs_scenarios, ['Good', 'Uncertain', 'Bad'])):\n",
    "    offset = (i - 1) * width\n",
    "    axes[2].bar(x_pos + offset, probs, width, label=scenario, alpha=0.7, edgecolor='black')\n",
    "\n",
    "axes[2].axhline(y=true_label[0], color='red', linestyle='--', linewidth=2, alpha=0.5, label='True class')\n",
    "axes[2].set_ylabel('Probability', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Predicted Probability Distributions\\n(True class = 0)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xticks(x_pos)\n",
    "axes[2].set_xticklabels(['Class 0\\n(TRUE)', 'Class 1', 'Class 2'])\n",
    "axes[2].legend(fontsize=9)\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cce_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ Loss is low when probability for TRUE class is high\")\n",
    "print(\"   ‚Ä¢ Loss is high when probability for TRUE class is low\")\n",
    "print(\"   ‚Ä¢ Network should put most probability mass on correct class\")\n",
    "print(\"   ‚Ä¢ Softmax ensures all probabilities sum to 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### üí° When to Use Categorical Cross-Entropy\n",
    "\n",
    "**‚úÖ Use CCE for:**\n",
    "- Multi-class classification (3+ classes)\n",
    "- When each sample belongs to exactly ONE class\n",
    "- Use with softmax activation in output layer\n",
    "- Examples: MNIST digits, ImageNet, language classification\n",
    "\n",
    "**‚ùå Don't use CCE for:**\n",
    "- Regression problems (use MSE)\n",
    "- Binary classification (use BCE - it's simpler)\n",
    "- Multi-label problems where samples can have multiple classes\n",
    "\n",
    "**‚ö†Ô∏è Things to Watch Out For:**\n",
    "- Labels must be one-hot encoded\n",
    "- Predictions must sum to 1 (use softmax!)\n",
    "- Use epsilon clipping to prevent log(0) errors\n",
    "- For many classes, consider label smoothing to prevent overconfidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Part 5: Choosing the Right Loss Function\n",
    "\n",
    "### üó∫Ô∏è Decision Guide\n",
    "\n",
    "```\n",
    "What's your task?\n",
    "‚îÇ\n",
    "‚îú‚îÄ Predicting CONTINUOUS VALUES (numbers)\n",
    "‚îÇ  ‚îî‚îÄ> Use Mean Squared Error (MSE)\n",
    "‚îÇ     Examples: House prices, temperature, age\n",
    "‚îÇ\n",
    "‚îú‚îÄ Classifying into 2 CLASSES\n",
    "‚îÇ  ‚îî‚îÄ> Use Binary Cross-Entropy (BCE)\n",
    "‚îÇ     Examples: Spam/Not Spam, Cat/Dog, Fraud/Legitimate\n",
    "‚îÇ     Output: Sigmoid activation\n",
    "‚îÇ\n",
    "‚îî‚îÄ Classifying into 3+ CLASSES\n",
    "   ‚îî‚îÄ> Use Categorical Cross-Entropy (CCE)\n",
    "      Examples: Digit recognition (0-9), Image classification\n",
    "      Output: Softmax activation\n",
    "```\n",
    "\n",
    "### üìã Quick Reference Table\n",
    "\n",
    "| Loss Function | Problem Type | Output Activation | Output Range | Labels Format |\n",
    "|---------------|--------------|-------------------|--------------|---------------|\n",
    "| **MSE** | Regression | Linear (none) | Any real number | Continuous values |\n",
    "| **BCE** | Binary Classification | Sigmoid | 0 to 1 | 0 or 1 |\n",
    "| **CCE** | Multi-class Classification | Softmax | 0 to 1 (sum=1) | One-hot vectors |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison of all loss functions\n",
    "\n",
    "print(\"\\nüìä COMPREHENSIVE LOSS FUNCTION COMPARISON\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example 1: Regression problem\n",
    "print(\"\\n1Ô∏è‚É£  REGRESSION PROBLEM: Predicting House Prices\")\n",
    "print(\"-\"*80)\n",
    "actual_prices = np.array([300000, 450000, 250000, 550000])\n",
    "predicted_prices = np.array([310000, 445000, 265000, 530000])\n",
    "\n",
    "mse = mean_squared_error(actual_prices, predicted_prices)\n",
    "print(f\"Actual prices:    {actual_prices}\")\n",
    "print(f\"Predicted prices: {predicted_prices}\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root MSE (RMSE):    ${np.sqrt(mse):,.2f}\")\n",
    "print(\"‚úÖ Use: MSE (measuring continuous values)\")\n",
    "\n",
    "# Example 2: Binary classification\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n2Ô∏è‚É£  BINARY CLASSIFICATION: Email Spam Detection\")\n",
    "print(\"-\"*80)\n",
    "actual_labels_binary = np.array([1, 0, 1, 0, 1])\n",
    "predicted_probs_binary = np.array([0.92, 0.08, 0.87, 0.15, 0.94])\n",
    "\n",
    "bce = binary_cross_entropy(actual_labels_binary, predicted_probs_binary)\n",
    "print(f\"Actual labels (1=Spam, 0=Not): {actual_labels_binary}\")\n",
    "print(f\"Predicted probabilities:       {predicted_probs_binary}\")\n",
    "print(f\"Binary Cross-Entropy: {bce:.4f}\")\n",
    "accuracy = np.mean((predicted_probs_binary > 0.5) == actual_labels_binary) * 100\n",
    "print(f\"Accuracy: {accuracy:.1f}%\")\n",
    "print(\"‚úÖ Use: Binary Cross-Entropy (2 classes)\")\n",
    "\n",
    "# Example 3: Multi-class classification\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n3Ô∏è‚É£  MULTI-CLASS CLASSIFICATION: Digit Recognition (0-9)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# 3 samples, 4 classes (simplified for display)\n",
    "actual_labels_multi = np.array([\n",
    "    [1, 0, 0, 0],  # Class 0\n",
    "    [0, 1, 0, 0],  # Class 1\n",
    "    [0, 0, 0, 1],  # Class 3\n",
    "])\n",
    "\n",
    "# Simulated predictions (after softmax)\n",
    "predicted_probs_multi = np.array([\n",
    "    [0.88, 0.05, 0.04, 0.03],  # Confident it's class 0\n",
    "    [0.02, 0.91, 0.04, 0.03],  # Confident it's class 1\n",
    "    [0.03, 0.04, 0.02, 0.91],  # Confident it's class 3\n",
    "])\n",
    "\n",
    "cce = categorical_cross_entropy(actual_labels_multi, predicted_probs_multi)\n",
    "print(f\"Number of classes: 4\")\n",
    "print(f\"Number of samples: 3\")\n",
    "print(f\"\\nTrue classes: {np.argmax(actual_labels_multi, axis=1)}\")\n",
    "print(f\"Predicted classes: {np.argmax(predicted_probs_multi, axis=1)}\")\n",
    "print(f\"\\nCategorical Cross-Entropy: {cce:.4f}\")\n",
    "accuracy_multi = np.mean(np.argmax(predicted_probs_multi, axis=1) == np.argmax(actual_labels_multi, axis=1)) * 100\n",
    "print(f\"Accuracy: {accuracy_multi:.1f}%\")\n",
    "print(\"‚úÖ Use: Categorical Cross-Entropy (3+ classes)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüí° KEY TAKEAWAY:\")\n",
    "print(\"   Different problems need different loss functions!\")\n",
    "print(\"   Choose based on your task type, not personal preference.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "## üó∫Ô∏è Part 6: Loss Landscapes\n",
    "\n",
    "### üèîÔ∏è What is a Loss Landscape?\n",
    "\n",
    "Imagine the loss function as a **terrain** or **landscape**:\n",
    "- **Height**: Loss value (higher = worse predictions)\n",
    "- **Position**: Different weight configurations\n",
    "- **Valleys**: Areas of low loss (good weights)\n",
    "- **Peaks**: Areas of high loss (bad weights)\n",
    "- **Goal**: Find the lowest valley (minimize loss)\n",
    "\n",
    "Training is like hiking downhill to find the lowest point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a simple loss landscape\n",
    "\n",
    "print(\"\\nüèîÔ∏è VISUALIZING LOSS LANDSCAPE\\n\")\n",
    "\n",
    "# Create a simple 2D loss landscape\n",
    "# Imagine we have only 2 weights: w1 and w2\n",
    "\n",
    "# Create a grid of weight values\n",
    "w1 = np.linspace(-3, 3, 100)\n",
    "w2 = np.linspace(-3, 3, 100)\n",
    "W1, W2 = np.meshgrid(w1, w2)\n",
    "\n",
    "# Define a simple loss function (paraboloid - bowl shape)\n",
    "# Loss is minimized when both weights are at (1, 1)\n",
    "Loss = (W1 - 1)**2 + (W2 - 1)**2\n",
    "\n",
    "# Create 3D visualization\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Plot 1: 3D surface\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surf = ax1.plot_surface(W1, W2, Loss, cmap='viridis', alpha=0.8, edgecolor='none')\n",
    "ax1.set_xlabel('Weight 1', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Weight 2', fontsize=11, fontweight='bold')\n",
    "ax1.set_zlabel('Loss', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('3D Loss Landscape\\n(Bowl-shaped)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Mark the minimum\n",
    "ax1.scatter([1], [1], [0], color='red', s=200, marker='*', \n",
    "           edgecolors='black', linewidth=2, zorder=10, label='Optimal weights')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Contour plot (top-down view)\n",
    "ax2 = fig.add_subplot(132)\n",
    "contours = ax2.contour(W1, W2, Loss, levels=20, cmap='viridis')\n",
    "ax2.clabel(contours, inline=True, fontsize=8)\n",
    "ax2.scatter([1], [1], color='red', s=200, marker='*', \n",
    "           edgecolors='black', linewidth=2, zorder=10, label='Minimum (optimal)')\n",
    "ax2.set_xlabel('Weight 1', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Weight 2', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Loss Contour Plot\\n(Top-down view)', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Show a path from random start to minimum\n",
    "ax3 = fig.add_subplot(133)\n",
    "contours = ax3.contour(W1, W2, Loss, levels=20, cmap='viridis', alpha=0.6)\n",
    "ax3.set_xlabel('Weight 1', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Weight 2', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Training Path\\n(Finding the minimum)', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Simulate a training path (gradient descent)\n",
    "# Start from a random point\n",
    "start_w1, start_w2 = -2, 2\n",
    "current_w1, current_w2 = start_w1, start_w2\n",
    "path_w1, path_w2 = [current_w1], [current_w2]\n",
    "\n",
    "# Take steps toward the minimum\n",
    "learning_rate = 0.1\n",
    "for i in range(30):\n",
    "    # Gradient (derivative of loss with respect to weights)\n",
    "    grad_w1 = 2 * (current_w1 - 1)\n",
    "    grad_w2 = 2 * (current_w2 - 1)\n",
    "    \n",
    "    # Update weights (move opposite to gradient - downhill!)\n",
    "    current_w1 = current_w1 - learning_rate * grad_w1\n",
    "    current_w2 = current_w2 - learning_rate * grad_w2\n",
    "    \n",
    "    path_w1.append(current_w1)\n",
    "    path_w2.append(current_w2)\n",
    "\n",
    "# Plot the path\n",
    "ax3.plot(path_w1, path_w2, 'r-', linewidth=2, marker='o', markersize=4, label='Training path')\n",
    "ax3.scatter([start_w1], [start_w2], color='blue', s=200, marker='o', \n",
    "           edgecolors='black', linewidth=2, zorder=10, label='Start (random)')\n",
    "ax3.scatter([1], [1], color='green', s=200, marker='*', \n",
    "           edgecolors='black', linewidth=2, zorder=10, label='End (optimal)')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_landscape.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Concepts:\")\n",
    "print(\"   ‚Ä¢ Loss landscape = terrain of all possible weight combinations\")\n",
    "print(\"   ‚Ä¢ Valleys = good weights (low loss)\")\n",
    "print(\"   ‚Ä¢ Peaks = bad weights (high loss)\")\n",
    "print(\"   ‚Ä¢ Training = navigating from random start to lowest valley\")\n",
    "print(\"   ‚Ä¢ Gradient = direction of steepest ascent (we go opposite direction!)\")\n",
    "print(\"\\n   In real networks: millions of dimensions, complex landscapes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Part 7: Common Mistakes and Pitfalls\n",
    "\n",
    "Let's learn from common errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö†Ô∏è  COMMON MISTAKES WITH LOSS FUNCTIONS\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Mistake #1: Using wrong loss for the task\n",
    "print(\"\\n‚ùå MISTAKE #1: Using Wrong Loss Function\")\n",
    "print(\"-\"*70)\n",
    "print(\"\\nBad: Using MSE for classification\")\n",
    "print(\"   Problem: MSE doesn't work well with probabilities\")\n",
    "print(\"   It can give misleading gradients for learning\")\n",
    "print(\"\\nGood: Use Cross-Entropy for classification\")\n",
    "print(\"   Binary class ‚Üí Binary Cross-Entropy\")\n",
    "print(\"   Multi-class ‚Üí Categorical Cross-Entropy\")\n",
    "\n",
    "# Mistake #2: Not matching loss with output activation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n‚ùå MISTAKE #2: Mismatched Output Activation\")\n",
    "print(\"-\"*70)\n",
    "print(\"\\nCommon mismatches:\")\n",
    "print(\"   ‚ùå BCE with linear output (needs sigmoid!)\")\n",
    "print(\"   ‚ùå CCE with sigmoid output (needs softmax!)\")\n",
    "print(\"   ‚ùå MSE with softmax output (just use linear!)\")\n",
    "print(\"\\nCorrect pairings:\")\n",
    "print(\"   ‚úÖ MSE + Linear (or no activation)\")\n",
    "print(\"   ‚úÖ BCE + Sigmoid\")\n",
    "print(\"   ‚úÖ CCE + Softmax\")\n",
    "\n",
    "# Mistake #3: Forgetting to clip predictions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n‚ùå MISTAKE #3: Not Clipping Predictions (Numerical Instability)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Demonstrate the problem\n",
    "print(\"\\nDemonstration of log(0) error:\")\n",
    "try:\n",
    "    bad_prediction = 0.0\n",
    "    loss = -np.log(bad_prediction)  # log(0) is undefined!\n",
    "    print(f\"Loss: {loss}\")\n",
    "except:\n",
    "    print(\"ERROR: log(0) = -inf (mathematical error!)\")\n",
    "\n",
    "print(\"\\nSolution: Clip predictions to avoid log(0)\")\n",
    "epsilon = 1e-15\n",
    "safe_prediction = np.clip(0.0, epsilon, 1-epsilon)\n",
    "loss = -np.log(safe_prediction)\n",
    "print(f\"Clipped prediction: {safe_prediction}\")\n",
    "print(f\"Loss: {loss:.2f} (finite and safe!)\")\n",
    "print(\"\\nüí° Always clip predictions: np.clip(pred, 1e-15, 1-1e-15)\")\n",
    "\n",
    "# Mistake #4: Not normalizing/scaling targets for MSE\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n‚ùå MISTAKE #4: Large MSE Values (Scale Issues)\")\n",
    "print(\"-\"*70)\n",
    "print(\"\\nProblem: Predicting house prices without scaling\")\n",
    "actual_prices = np.array([300000, 450000, 250000])\n",
    "predicted_prices = np.array([310000, 445000, 260000])\n",
    "mse = mean_squared_error(actual_prices, predicted_prices)\n",
    "print(f\"Actual: {actual_prices}\")\n",
    "print(f\"Predicted: {predicted_prices}\")\n",
    "print(f\"MSE: {mse:,.0f} (HUGE number!)\")\n",
    "print(\"   Problem: Large loss values can cause training instability\")\n",
    "\n",
    "print(\"\\nSolution: Scale/normalize the targets\")\n",
    "# Scale prices to reasonable range\n",
    "scaled_actual = actual_prices / 100000  # Divide by 100k\n",
    "scaled_predicted = predicted_prices / 100000\n",
    "scaled_mse = mean_squared_error(scaled_actual, scaled_predicted)\n",
    "print(f\"Scaled actual: {scaled_actual}\")\n",
    "print(f\"Scaled predicted: {scaled_predicted}\")\n",
    "print(f\"Scaled MSE: {scaled_mse:.6f} (much better!)\")\n",
    "print(\"\\nüí° Always normalize/scale your data before training!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö Summary and Key Takeaways\n",
    "\n",
    "### üéâ What We Learned Today:\n",
    "\n",
    "1. **Why Loss Functions?**\n",
    "   - Measure how wrong predictions are\n",
    "   - Turn errors into a single number\n",
    "   - Guide network learning\n",
    "   - Lower loss = better predictions\n",
    "\n",
    "2. **Mean Squared Error (MSE)**:\n",
    "   - Formula: `MSE = (1/n) √ó Œ£(pred - actual)¬≤`\n",
    "   - Use for: Regression (continuous values)\n",
    "   - Penalizes large errors heavily\n",
    "   - Sensitive to outliers\n",
    "\n",
    "3. **Binary Cross-Entropy (BCE)**:\n",
    "   - Formula: `BCE = -[y√ólog(p) + (1-y)√ólog(1-p)]`\n",
    "   - Use for: Binary classification (2 classes)\n",
    "   - Pair with: Sigmoid activation\n",
    "   - Heavily penalizes confident mistakes\n",
    "\n",
    "4. **Categorical Cross-Entropy (CCE)**:\n",
    "   - Formula: `CCE = -Œ£Œ£ y√ólog(p)`\n",
    "   - Use for: Multi-class classification (3+ classes)\n",
    "   - Pair with: Softmax activation\n",
    "   - Works with one-hot encoded labels\n",
    "\n",
    "5. **Choosing the Right Loss**:\n",
    "   - Regression ‚Üí MSE\n",
    "   - Binary classification ‚Üí BCE\n",
    "   - Multi-class classification ‚Üí CCE\n",
    "\n",
    "6. **Loss Landscapes**:\n",
    "   - Visualization of all possible weight configurations\n",
    "   - Training = finding the lowest valley\n",
    "   - Gradient = direction to move\n",
    "\n",
    "7. **Common Mistakes**:\n",
    "   - Using wrong loss for task\n",
    "   - Mismatched output activation\n",
    "   - Not clipping predictions (numerical instability)\n",
    "   - Not scaling data for MSE\n",
    "\n",
    "### üîÆ What's Next?\n",
    "\n",
    "Now we can:\n",
    "- ‚úÖ Make predictions (forward propagation)\n",
    "- ‚úÖ Measure how wrong we are (loss functions)\n",
    "\n",
    "**The Big Question**: How do we actually IMPROVE the predictions?\n",
    "\n",
    "**The Answer**: Backpropagation! üîô\n",
    "\n",
    "In **Notebook 7: Backpropagation**, we'll learn:\n",
    "- How to calculate gradients (derivatives of loss)\n",
    "- How to update weights to reduce loss\n",
    "- The chain rule and how it powers deep learning\n",
    "- Implementing backpropagation from scratch\n",
    "\n",
    "### üí™ Practice Challenge:\n",
    "\n",
    "Before moving on, try:\n",
    "1. Implement MAE (Mean Absolute Error): `MAE = (1/n) √ó Œ£|pred - actual|`\n",
    "2. Compare MAE vs MSE on the same dataset\n",
    "3. Create a visualization showing how different loss functions behave\n",
    "4. Try using the wrong loss function and see what happens\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: Loss functions are the report card for your neural network. They tell you exactly how good (or bad) your predictions are. In the next notebook, we'll learn how to use this information to actually improve the network! üöÄ\n",
    "\n",
    "Ready to learn the magic of backpropagation? Let's go! üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
