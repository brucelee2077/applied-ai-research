{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Activation Functions - Adding Non-Linearity\n",
    "\n",
    "Welcome to the third notebook! We've learned about neurons and how they compute weighted sums. Now we're going to add the **secret ingredient** that makes neural networks powerful: **activation functions**!\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- **Why** we need activation functions (the non-linearity problem)\n",
    "- The most important activation functions and how they work\n",
    "- How to implement each one from scratch\n",
    "- When to use which activation function\n",
    "- How activation functions enable neural networks to learn complex patterns\n",
    "\n",
    "**Prerequisites:** Notebooks 1 and 2, basic understanding of neurons and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# For nice plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§î The Problem: Why We Need Activation Functions\n",
    "\n",
    "Let's start with a fundamental question: **Why can't we just stack neurons without activation functions?**\n",
    "\n",
    "### The Linear Limitation\n",
    "\n",
    "Remember, a neuron without activation is just:\n",
    "```\n",
    "output = w‚ÇÅ¬∑x‚ÇÅ + w‚ÇÇ¬∑x‚ÇÇ + ... + b\n",
    "```\n",
    "\n",
    "This is a **linear function**. It creates straight lines (or flat planes in higher dimensions).\n",
    "\n",
    "### What Happens When We Stack Linear Functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-layer network (no activation):\n",
      "  Input: x = 5.0\n",
      "  Layer 1: h1 = 2.0¬∑5.0 + 3.0 = 13.0\n",
      "  Layer 2: output = 1.5¬∑13.0 + -2.0 = 17.5\n",
      "\n",
      "Equivalent single-layer network:\n",
      "  output = 3.0¬∑5.0 + 2.5 = 17.5\n",
      "\n",
      "üîç Key Insight:\n",
      "  Two layers give same result as one layer!\n",
      "  Both outputs: 17.5 = 17.5\n",
      "\n",
      "  üí° Stacking linear functions = Another linear function!\n",
      "  üí° Adding more layers doesn't help without activation functions!\n"
     ]
    }
   ],
   "source": [
    "# Let's see what happens when we stack linear transformations\n",
    "\n",
    "# Input\n",
    "x = 5.0\n",
    "\n",
    "# Layer 1: Linear transformation\n",
    "w1 = 2.0\n",
    "b1 = 3.0\n",
    "h1 = w1 * x + b1  # h1 = 2*5 + 3 = 13\n",
    "\n",
    "# Layer 2: Another linear transformation\n",
    "w2 = 1.5\n",
    "b2 = -2.0\n",
    "output = w2 * h1 + b2  # output = 1.5*13 - 2 = 17.5\n",
    "\n",
    "print(\"Two-layer network (no activation):\")\n",
    "print(f\"  Input: x = {x}\")\n",
    "print(f\"  Layer 1: h1 = {w1}¬∑{x} + {b1} = {h1}\")\n",
    "print(f\"  Layer 2: output = {w2}¬∑{h1} + {b2} = {output}\")\n",
    "\n",
    "# Now let's collapse this into a single layer\n",
    "# y = w‚ÇÇ¬∑(w‚ÇÅ¬∑x + b‚ÇÅ) + b‚ÇÇ\n",
    "# y = (w‚ÇÇ¬∑w‚ÇÅ)¬∑x + (w‚ÇÇ¬∑b‚ÇÅ + b‚ÇÇ)\n",
    "w_combined = w2 * w1  # Combined weight\n",
    "b_combined = w2 * b1 + b2  # Combined bias\n",
    "output_direct = w_combined * x + b_combined\n",
    "\n",
    "print(\"\\nEquivalent single-layer network:\")\n",
    "print(f\"  output = {w_combined}¬∑{x} + {b_combined} = {output_direct}\")\n",
    "\n",
    "print(\"\\nüîç Key Insight:\")\n",
    "print(f\"  Two layers give same result as one layer!\")\n",
    "print(f\"  Both outputs: {output} = {output_direct}\")\n",
    "print(f\"\\n  üí° Stacking linear functions = Another linear function!\")\n",
    "print(f\"  üí° Adding more layers doesn't help without activation functions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé® Visualizing the Problem\n",
    "\n",
    "Let's see this visually with a classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a non-linear dataset (circle pattern)\n",
    "np.random.seed(42)\n",
    "n_points = 200\n",
    "\n",
    "# Inner circle (class 0)\n",
    "r_inner = np.random.uniform(0, 1, n_points//2)\n",
    "theta_inner = np.random.uniform(0, 2*np.pi, n_points//2)\n",
    "x_inner = r_inner * np.cos(theta_inner)\n",
    "y_inner = r_inner * np.sin(theta_inner)\n",
    "\n",
    "# Outer circle (class 1)\n",
    "r_outer = np.random.uniform(1.5, 2.5, n_points//2)\n",
    "theta_outer = np.random.uniform(0, 2*np.pi, n_points//2)\n",
    "x_outer = r_outer * np.cos(theta_outer)\n",
    "y_outer = r_outer * np.sin(theta_outer)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_inner, y_inner, c='red', alpha=0.6, s=30, label='Class 0 (inner)', edgecolors='k', linewidth=0.5)\n",
    "plt.scatter(x_outer, y_outer, c='green', alpha=0.6, s=30, label='Class 1 (outer)', edgecolors='k', linewidth=0.5)\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Non-Linear Data (Concentric Circles)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_inner, y_inner, c='red', alpha=0.6, s=30, edgecolors='k', linewidth=0.5)\n",
    "plt.scatter(x_outer, y_outer, c='green', alpha=0.6, s=30, edgecolors='k', linewidth=0.5)\n",
    "# Try to draw a straight line to separate them\n",
    "x_line = np.linspace(-3, 3, 100)\n",
    "y_line = 0.5 * x_line + 0.2  # Some random line\n",
    "plt.plot(x_line, y_line, 'b--', linewidth=3, label='Best linear boundary')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Linear Boundary FAILS!', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚ùå Problem: No straight line can separate these circles!\")\n",
    "print(\"‚úÖ Solution: We need NON-LINEAR decision boundaries!\")\n",
    "print(\"‚ö° How? Add ACTIVATION FUNCTIONS to introduce non-linearity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üí° Analogies: Understanding Activation Functions\n",
    "\n",
    "### Analogy 1: Light Switch vs Dimmer Switch\n",
    "\n",
    "**Step Function (Binary Activation)**\n",
    "- Like a **light switch**: either ON or OFF\n",
    "- No middle ground\n",
    "- Simple but too rigid\n",
    "\n",
    "**Sigmoid/Tanh (Smooth Activation)**\n",
    "- Like a **dimmer switch**: smoothly adjusts from off to on\n",
    "- Many values in between\n",
    "- More flexible!\n",
    "\n",
    "**ReLU (Rectified Linear)**\n",
    "- Like a **one-way valve**: lets positive flow through, blocks negative\n",
    "- Simple and effective\n",
    "- Most popular today!\n",
    "\n",
    "### Analogy 2: Signal Processing\n",
    "\n",
    "Think of activation functions as **filters**:\n",
    "- **Step**: \"Only pass strong signals\" (threshold)\n",
    "- **Sigmoid**: \"Squash everything into a range\" (normalize)\n",
    "- **ReLU**: \"Keep positive, discard negative\" (one-way gate)\n",
    "- **Tanh**: \"Center everything around zero\" (balanced output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Activation Function #1: Step Function\n",
    "\n",
    "The **step function** was one of the first activation functions used in neural networks.\n",
    "\n",
    "### Formula\n",
    "$$f(x) = \\begin{cases} \n",
    "1 & \\text{if } x \\geq 0 \\\\\n",
    "0 & \\text{if } x < 0\n",
    "\\end{cases}$$\n",
    "\n",
    "In words: **\"If input is positive or zero, output 1. Otherwise, output 0.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    \"\"\"\n",
    "    Step function (also called Heaviside function).\n",
    "    \n",
    "    Output is binary: 0 or 1\n",
    "    - If input >= 0: return 1 (neuron 'fires')\n",
    "    - If input < 0: return 0 (neuron stays quiet)\n",
    "    \n",
    "    Args:\n",
    "        x: Input value or array\n",
    "    \n",
    "    Returns:\n",
    "        Binary output (0 or 1)\n",
    "    \"\"\"\n",
    "    # Use numpy's where: if condition is true, return 1, else return 0\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "# Test it\n",
    "test_values = np.array([-2, -1, 0, 1, 2])\n",
    "step_outputs = step_function(test_values)\n",
    "\n",
    "print(\"Step Function Test:\")\n",
    "for val, out in zip(test_values, step_outputs):\n",
    "    print(f\"  input: {val:>3} ‚Üí output: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "Congratulations! You now understand activation functions! Here's what we covered:\n",
    "\n",
    "### Why We Need Activation Functions\n",
    "- Stacking linear functions just creates another linear function\n",
    "- Without activation, deep networks collapse to a single layer\n",
    "- Activation functions add **non-linearity** to enable learning complex patterns\n",
    "\n",
    "### The Main Activation Functions\n",
    "\n",
    "| Function | Formula | Range | Best Use |\n",
    "|----------|---------|-------|----------|\n",
    "| **Step** | 0 if x<0, 1 if x‚â•0 | {0, 1} | Historical only |\n",
    "| **Sigmoid** | 1/(1+e^(-x)) | (0, 1) | Output layer (binary) |\n",
    "| **Tanh** | (e^x - e^(-x))/(e^x + e^(-x)) | (-1, 1) | RNNs, hidden layers |\n",
    "| **ReLU** | max(0, x) | [0, ‚àû) | **Hidden layers (DEFAULT)** |\n",
    "| **Leaky ReLU** | max(Œ±x, x) | (-‚àû, ‚àû) | Fix dying ReLU |\n",
    "\n",
    "### Practical Guidelines\n",
    "\n",
    "**For Hidden Layers:**\n",
    "1. Start with **ReLU** (default choice)\n",
    "2. If ReLU fails, try **Leaky ReLU**\n",
    "3. For RNNs, use **Tanh**\n",
    "\n",
    "**For Output Layer:**\n",
    "1. Binary classification ‚Üí **Sigmoid**\n",
    "2. Multi-class classification ‚Üí **Softmax** (covered later)\n",
    "3. Regression ‚Üí **No activation** (or ReLU if outputs must be positive)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ What's Next?\n",
    "\n",
    "Excellent work! You've now mastered the three fundamental building blocks:\n",
    "\n",
    "1. ‚úÖ **What neural networks are** (Notebook 1)\n",
    "2. ‚úÖ **How a single neuron works** (Notebook 2)\n",
    "3. ‚úÖ **Activation functions for non-linearity** (Notebook 3)\n",
    "\n",
    "In the next notebook, you'll learn how to combine multiple neurons into **layers** and build complete neural networks!\n",
    "\n",
    "**Ready to build networks?** ‚Üí [Continue to Notebook 4: Neural Network Layers](04_neural_network_layer.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Great job completing Notebook 3! You're making excellent progress! üåü*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
