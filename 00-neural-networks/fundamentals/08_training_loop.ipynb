
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ The Training Loop: Practice Makes Perfect\n",
    "\n",
    "**Putting Everything Together: Building a Complete Training System**\n",
    "\n",
    "Welcome to the final fundamental notebook! You've learned all the pieces - now it's time to put them together into a complete training system.\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ What We'll Learn\n",
    "\n",
    "1. **Training Concepts**: Epochs, batches, iterations\n",
    "2. **Complete Training Loop**: From raw data to trained model\n",
    "3. **Weight Initialization**: Starting off right\n",
    "4. **Hyperparameters**: Learning rate, batch size, epochs\n",
    "5. **Monitoring Progress**: Loss curves, accuracy\n",
    "6. **Train/Validation Split**: Detecting overfitting\n",
    "7. **Best Practices**: Common mistakes and how to avoid them\n",
    "\n",
    "---\n",
    "\n",
    "## üéµ The Musical Analogy: Learning an Instrument\n",
    "\n",
    "Training a neural network is like learning to play the piano:\n",
    "\n",
    "### One Practice Session ‚â† Mastery\n",
    "\n",
    "- **Single weight update** (from Notebook 7) = Playing a song once\n",
    "- **Training loop** = Practicing the song many times over weeks\n",
    "\n",
    "### The Practice Routine\n",
    "\n",
    "1. **Practice session** (Epoch):\n",
    "   - Go through your entire songbook once\n",
    "   - Each time you play all songs = 1 epoch\n",
    "\n",
    "2. **Breaking it down** (Batches):\n",
    "   - Instead of playing 100 songs at once (overwhelming!)\n",
    "   - Practice 10 songs at a time (manageable batches)\n",
    "   - Take a break, adjust technique, repeat\n",
    "\n",
    "3. **Repeated practice** (Iterations):\n",
    "   - Each small practice session = 1 iteration\n",
    "   - More iterations = more improvement opportunities\n",
    "\n",
    "4. **Progress tracking** (Validation):\n",
    "   - Perform for friends (validation set)\n",
    "   - See if you're really getting better\n",
    "   - Not just memorizing, but actually learning!\n",
    "\n",
    "### üí° Key Insight\n",
    "\n",
    "**Learning requires repetition!** One update isn't enough. We need to:\n",
    "- See the data multiple times\n",
    "- Gradually adjust weights\n",
    "- Track progress over time\n",
    "- Know when to stop\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # For numerical operations and array handling\n",
    "import matplotlib.pyplot as plt  # For creating beautiful visualizations\n",
    "from matplotlib.animation import FuncAnimation  # For animated training visualizations\n",
    "from IPython.display import HTML  # For displaying animations in notebook\n",
    "import time  # For tracking training time\n",
    "from typing import Tuple, List, Dict  # For type hints (better code documentation)\n",
    "\n",
    "# Set random seed for reproducibility (same results every time)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')  # Professional plot style\n",
    "plt.rcParams['figure.figsize'] = (12, 6)  # Default figure size\n",
    "plt.rcParams['font.size'] = 11  # Readable font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Part 1: Understanding Training Terminology\n",
    "\n",
    "Let's clarify the key terms you'll hear everywhere in deep learning:\n",
    "\n",
    "### üîÑ Epoch\n",
    "**One complete pass through the entire training dataset**\n",
    "\n",
    "- If you have 1000 training examples, seeing all 1000 once = 1 epoch\n",
    "- Typical training: 10-100+ epochs (or more for complex problems)\n",
    "- More epochs = more learning opportunities (but risk overfitting)\n",
    "\n",
    "### üì¶ Batch\n",
    "**A subset of training data processed together**\n",
    "\n",
    "- Instead of updating weights after EACH example (slow!)\n",
    "- Or after ALL examples (uses too much memory!)\n",
    "- We update after a \"batch\" of examples (just right! üêª)\n",
    "\n",
    "**Batch sizes:**\n",
    "- **Batch size = 1**: Stochastic Gradient Descent (SGD) - noisy but fast updates\n",
    "- **Batch size = all data**: Batch Gradient Descent - smooth but slow\n",
    "- **Batch size = 32, 64, 128, etc.**: Mini-batch Gradient Descent - best of both! ‚≠ê\n",
    "\n",
    "### üî¢ Iteration\n",
    "**One weight update (one forward + backward pass on a batch)**\n",
    "\n",
    "- Number of iterations per epoch = Total examples / Batch size\n",
    "- Example: 1000 examples, batch size 100 ‚Üí 10 iterations per epoch\n",
    "\n",
    "### üìä Simple Example\n",
    "\n",
    "```\n",
    "Dataset: 1000 examples\n",
    "Batch size: 100\n",
    "Epochs: 10\n",
    "\n",
    "Iterations per epoch: 1000 / 100 = 10\n",
    "Total iterations: 10 epochs √ó 10 iterations = 100 iterations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize these concepts\n",
    "\n",
    "def visualize_training_concepts():\n",
    "    \"\"\"Create a visual explanation of epochs, batches, and iterations\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Configuration\n",
    "    total_examples = 100  # Total training examples\n",
    "    batch_size = 20  # Process 20 at a time\n",
    "    num_epochs = 3  # Train for 3 epochs\n",
    "    \n",
    "    iterations_per_epoch = total_examples // batch_size  # 100 / 20 = 5\n",
    "    total_iterations = iterations_per_epoch * num_epochs  # 5 * 3 = 15\n",
    "    \n",
    "    # Plot 1: Showing one epoch (all data once)\n",
    "    ax1 = axes[0]\n",
    "    data_indices = np.arange(total_examples)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, total_examples))\n",
    "    ax1.bar(data_indices, np.ones(total_examples), color=colors, width=1.0, edgecolor='none')\n",
    "    ax1.set_xlim(-1, total_examples)\n",
    "    ax1.set_ylim(0, 1.2)\n",
    "    ax1.set_ylabel('Data Point', fontsize=12)\n",
    "    ax1.set_title(f'1 EPOCH = Seeing all {total_examples} examples once', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks([0, 25, 50, 75, 99])\n",
    "    ax1.set_yticks([])\n",
    "    \n",
    "    # Plot 2: Showing batches within an epoch\n",
    "    ax2 = axes[1]\n",
    "    for batch_idx in range(iterations_per_epoch):\n",
    "        start = batch_idx * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_indices = np.arange(start, end)\n",
    "        \n",
    "        # Different color for each batch\n",
    "        batch_color = plt.cm.Set3(batch_idx)\n",
    "        ax2.bar(batch_indices, np.ones(batch_size), color=batch_color, \n",
    "               width=1.0, edgecolor='black', linewidth=2)\n",
    "        \n",
    "        # Label each batch\n",
    "        mid_point = (start + end) / 2\n",
    "        ax2.text(mid_point, 0.5, f'Batch {batch_idx+1}', \n",
    "                ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax2.set_xlim(-1, total_examples)\n",
    "    ax2.set_ylim(0, 1.2)\n",
    "    ax2.set_ylabel('Batch', fontsize=12)\n",
    "    ax2.set_title(f'1 EPOCH = {iterations_per_epoch} BATCHES (each batch = {batch_size} examples)',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks([0, 25, 50, 75, 99])\n",
    "    ax2.set_yticks([])\n",
    "    \n",
    "    # Plot 3: Showing multiple epochs (iterations over time)\n",
    "    ax3 = axes[2]\n",
    "    iteration_numbers = np.arange(1, total_iterations + 1)\n",
    "    \n",
    "    # Color by epoch\n",
    "    colors = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_color = plt.cm.Set1(epoch)\n",
    "        colors.extend([epoch_color] * iterations_per_epoch)\n",
    "    \n",
    "    ax3.bar(iteration_numbers, np.ones(total_iterations), color=colors, \n",
    "           width=0.8, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    # Mark epoch boundaries\n",
    "    for epoch in range(1, num_epochs):\n",
    "        ax3.axvline(x=epoch * iterations_per_epoch + 0.5, \n",
    "                   color='red', linestyle='--', linewidth=3, alpha=0.7)\n",
    "    \n",
    "    # Label epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        mid_iter = epoch * iterations_per_epoch + iterations_per_epoch / 2 + 0.5\n",
    "        ax3.text(mid_iter, 0.5, f'Epoch {epoch+1}',\n",
    "                ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax3.set_xlim(0, total_iterations + 1)\n",
    "    ax3.set_ylim(0, 1.2)\n",
    "    ax3.set_xlabel('Iteration Number (Weight Updates)', fontsize=12)\n",
    "    ax3.set_ylabel('Epoch', fontsize=12)\n",
    "    ax3.set_title(f'{num_epochs} EPOCHS = {total_iterations} ITERATIONS (weight updates)',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax3.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä TRAINING CONFIGURATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total training examples: {total_examples}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Number of epochs: {num_epochs}\")\n",
    "    print()\n",
    "    print(\"COMPUTED VALUES:\")\n",
    "    print(f\"Iterations per epoch: {iterations_per_epoch}\")\n",
    "    print(f\"Total iterations: {total_iterations}\")\n",
    "    print()\n",
    "    print(\"WHAT THIS MEANS:\")\n",
    "    print(f\"‚Ä¢ Each epoch, we see all {total_examples} examples\")\n",
    "    print(f\"‚Ä¢ We process {batch_size} examples at a time (mini-batches)\")\n",
    "    print(f\"‚Ä¢ We update weights {iterations_per_epoch} times per epoch\")\n",
    "    print(f\"‚Ä¢ Total: {total_iterations} weight updates across all epochs\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "visualize_training_concepts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé≤ Part 2: Weight Initialization - Starting Off Right\n",
    "\n",
    "Before training, we need to initialize our weights. **This matters more than you might think!**\n",
    "\n",
    "### ‚ùå Bad Initialization: All Zeros\n",
    "\n",
    "```python\n",
    "# DON'T DO THIS!\n",
    "weights = np.zeros((n_in, n_out))\n",
    "```\n",
    "\n",
    "**Problem**: All neurons learn the same thing (symmetry problem)\n",
    "- All neurons get same gradient\n",
    "- All neurons update identically\n",
    "- Network doesn't learn diverse features\n",
    "\n",
    "### ‚ö†Ô∏è Bad Initialization: Too Large\n",
    "\n",
    "```python\n",
    "# Also bad!\n",
    "weights = np.random.randn(n_in, n_out) * 10  # Too large\n",
    "```\n",
    "\n",
    "**Problem**: Exploding gradients, unstable training\n",
    "\n",
    "### ‚úÖ Good Initialization: Xavier/He\n",
    "\n",
    "**Xavier (Glorot) Initialization** - for sigmoid/tanh:\n",
    "```python\n",
    "std = np.sqrt(2.0 / (n_in + n_out))\n",
    "weights = np.random.randn(n_in, n_out) * std\n",
    "```\n",
    "\n",
    "**He Initialization** - for ReLU:\n",
    "```python\n",
    "std = np.sqrt(2.0 / n_in)\n",
    "weights = np.random.randn(n_in, n_out) * std\n",
    "```\n",
    "\n",
    "These keep activations and gradients in a good range!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the effect of different initializations\n",
    "\n",
    "def compare_initializations():\n",
    "    \"\"\"Compare different weight initialization strategies\"\"\"\n",
    "    \n",
    "    n_in = 100  # Input size\n",
    "    n_out = 100  # Output size\n",
    "    n_samples = 10000  # How many times to initialize\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "    \n",
    "    # Different initialization methods\n",
    "    methods = [\n",
    "        ('All Zeros', lambda: np.zeros((n_in, n_out))),\n",
    "        ('Too Large', lambda: np.random.randn(n_in, n_out) * 5),\n",
    "        ('Too Small', lambda: np.random.randn(n_in, n_out) * 0.01),\n",
    "        ('Standard Normal', lambda: np.random.randn(n_in, n_out)),\n",
    "        ('Xavier', lambda: np.random.randn(n_in, n_out) * np.sqrt(2.0 / (n_in + n_out))),\n",
    "        ('He', lambda: np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in))\n",
    "    ]\n",
    "    \n",
    "    for idx, (name, init_func) in enumerate(methods):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        # Initialize weights\n",
    "        weights = init_func()\n",
    "        \n",
    "        # Plot histogram\n",
    "        ax.hist(weights.flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "        ax.set_title(name, fontsize=13, fontweight='bold')\n",
    "        ax.set_xlabel('Weight Value', fontsize=11)\n",
    "        ax.set_ylabel('Frequency', fontsize=11)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean = np.mean(weights)\n",
    "        std = np.std(weights)\n",
    "        ax.text(0.95, 0.95, f'Mean: {mean:.4f}\\nStd: {std:.4f}',\n",
    "               transform=ax.transAxes, fontsize=10,\n",
    "               verticalalignment='top', horizontalalignment='right',\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        # Mark if this is good or bad\n",
    "        if name in ['Xavier', 'He']:\n",
    "            ax.set_facecolor('#e8f5e9')  # Light green background\n",
    "            ax.text(0.5, 1.05, '‚úÖ GOOD', transform=ax.transAxes,\n",
    "                   ha='center', fontsize=12, fontweight='bold', color='green')\n",
    "        else:\n",
    "            ax.set_facecolor('#ffebee')  # Light red background\n",
    "            ax.text(0.5, 1.05, '‚ùå AVOID', transform=ax.transAxes,\n",
    "                   ha='center', fontsize=12, fontweight='bold', color='red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Key Takeaways:\")\n",
    "    print(\"‚Ä¢ All zeros: No learning (symmetry problem)\")\n",
    "    print(\"‚Ä¢ Too large/small: Unstable gradients\")\n",
    "    print(\"‚Ä¢ Xavier: Good for sigmoid/tanh activations\")\n",
    "    print(\"‚Ä¢ He: Good for ReLU activations\")\n",
    "    print(\"\\nüéØ Use Xavier or He initialization for best results!\")\n",
    "\n",
    "compare_initializations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Part 3: Building a Complete Neural Network with Training Loop\n",
    "\n",
    "Now let's build a complete, production-quality neural network class with:\n",
    "- Proper initialization\n",
    "- Mini-batch training\n",
    "- Progress tracking\n",
    "- Validation support\n",
    "- Early stopping\n",
    "- Extensive logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions (from previous notebooks)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation: outputs between 0 and 1\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation: max(0, x)\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivative of ReLU: 1 if x > 0, else 0\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "print(\"‚úÖ Activation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"A complete 2-layer neural network with full training capabilities\n",
    "    \n",
    "    Features:\n",
    "    - Configurable architecture\n",
    "    - Multiple activation functions\n",
    "    - Mini-batch training\n",
    "    - Progress tracking\n",
    "    - Validation support\n",
    "    - Early stopping\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, \n",
    "                 learning_rate=0.01, activation='relu'):\n",
    "        \"\"\"Initialize the neural network\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of neurons in hidden layer\n",
    "            output_size: Number of output neurons\n",
    "            learning_rate: Step size for gradient descent (default: 0.01)\n",
    "            activation: 'relu' or 'sigmoid' for hidden layer (default: 'relu')\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Set activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = relu\n",
    "            self.activation_derivative = relu_derivative\n",
    "            # Use He initialization for ReLU\n",
    "            init_std_w1 = np.sqrt(2.0 / input_size)\n",
    "            init_std_w2 = np.sqrt(2.0 / hidden_size)\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_derivative = sigmoid_derivative\n",
    "            # Use Xavier initialization for sigmoid\n",
    "            init_std_w1 = np.sqrt(2.0 / (input_size + hidden_size))\n",
    "            init_std_w2 = np.sqrt(2.0 / (hidden_size + output_size))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "        \n",
    "        # Initialize weights with appropriate initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * init_std_w1\n",
    "        self.b1 = np.zeros(hidden_size)  # Biases start at zero\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * init_std_w2\n",
    "        self.b2 = np.zeros(output_size)\n",
    "        \n",
    "        # Storage for intermediate values (needed for backprop)\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],  # Loss on training data\n",
    "            'val_loss': [],    # Loss on validation data\n",
    "            'train_acc': [],   # Accuracy on training data\n",
    "            'val_acc': []      # Accuracy on validation data\n",
    "        }\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation: compute predictions\n",
    "        \n",
    "        Args:\n",
    "            X: Input data (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            y_pred: Predictions (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Hidden layer: z1 = X¬∑W1 + b1, then activation\n",
    "        z1 = np.dot(X, self.W1) + self.b1  # Weighted sum\n",
    "        h = self.activation(z1)  # Apply activation function\n",
    "        \n",
    "        # Output layer: z2 = h¬∑W2 + b2, then sigmoid (for binary classification)\n",
    "        z2 = np.dot(h, self.W2) + self.b2  # Weighted sum\n",
    "        y_pred = sigmoid(z2)  # Sigmoid for final output (probability)\n",
    "        \n",
    "        # Cache intermediate values for backpropagation\n",
    "        self.cache = {\n",
    "            'X': X,    # Input\n",
    "            'z1': z1,  # Hidden layer weighted sum\n",
    "            'h': h,    # Hidden layer activation\n",
    "            'z2': z2,  # Output layer weighted sum\n",
    "            'y_pred': y_pred  # Final prediction\n",
    "        }\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"Compute Mean Squared Error loss\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Predicted values\n",
    "            y_true: True values\n",
    "        \n",
    "        Returns:\n",
    "            loss: Average loss across all samples\n",
    "        \"\"\"\n",
    "        # MSE = mean of (prediction - true)¬≤\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def compute_accuracy(self, y_pred, y_true):\n",
    "        \"\"\"Compute classification accuracy\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Predicted probabilities\n",
    "            y_true: True labels (0 or 1)\n",
    "        \n",
    "        Returns:\n",
    "            accuracy: Percentage of correct predictions\n",
    "        \"\"\"\n",
    "        # Convert probabilities to binary predictions (threshold at 0.5)\n",
    "        predictions = (y_pred > 0.5).astype(int)\n",
    "        # Calculate percentage of correct predictions\n",
    "        return np.mean(predictions == y_true) * 100\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        \"\"\"Backward propagation: compute gradients\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels\n",
    "        \n",
    "        Returns:\n",
    "            gradients: Dictionary of gradients for all parameters\n",
    "        \"\"\"\n",
    "        # Retrieve cached values from forward pass\n",
    "        X = self.cache['X']\n",
    "        z1 = self.cache['z1']\n",
    "        h = self.cache['h']\n",
    "        z2 = self.cache['z2']\n",
    "        y_pred = self.cache['y_pred']\n",
    "        \n",
    "        batch_size = X.shape[0]  # Number of examples in batch\n",
    "        \n",
    "        # --- Backward pass through output layer ---\n",
    "        \n",
    "        # Gradient of loss w.r.t. predictions: ‚àÇL/‚àÇy\n",
    "        dL_dy = 2 * (y_pred - y_true) / batch_size\n",
    "        \n",
    "        # Gradient of loss w.r.t. z2 (before sigmoid): ‚àÇL/‚àÇz2\n",
    "        # Chain rule: ‚àÇL/‚àÇz2 = ‚àÇL/‚àÇy √ó ‚àÇy/‚àÇz2\n",
    "        dy_dz2 = sigmoid_derivative(z2)\n",
    "        dL_dz2 = dL_dy * dy_dz2\n",
    "        \n",
    "        # Gradients for W2 and b2\n",
    "        dL_dW2 = np.dot(h.T, dL_dz2)  # ‚àÇL/‚àÇW2 = h·µÄ ¬∑ ‚àÇL/‚àÇz2\n",
    "        dL_db2 = np.sum(dL_dz2, axis=0)  # Sum over batch\n",
    "        \n",
    "        # --- Backward pass through hidden layer ---\n",
    "        \n",
    "        # Gradient of loss w.r.t. hidden activations: ‚àÇL/‚àÇh\n",
    "        dL_dh = np.dot(dL_dz2, self.W2.T)  # Chain through W2\n",
    "        \n",
    "        # Gradient of loss w.r.t. z1 (before activation): ‚àÇL/‚àÇz1\n",
    "        # Chain rule: ‚àÇL/‚àÇz1 = ‚àÇL/‚àÇh √ó ‚àÇh/‚àÇz1\n",
    "        dh_dz1 = self.activation_derivative(z1)\n",
    "        dL_dz1 = dL_dh * dh_dz1\n",
    "        \n",
    "        # Gradients for W1 and b1\n",
    "        dL_dW1 = np.dot(X.T, dL_dz1)  # ‚àÇL/‚àÇW1 = X·µÄ ¬∑ ‚àÇL/‚àÇz1\n",
    "        dL_db1 = np.sum(dL_dz1, axis=0)  # Sum over batch\n",
    "        \n",
    "        # Return all gradients\n",
    "        return {\n",
    "            'dW1': dL_dW1,\n",
    "            'db1': dL_db1,\n",
    "            'dW2': dL_dW2,\n",
    "            'db2': dL_db2\n",
    "        }\n",
    "    \n",
    "    def update_weights(self, gradients):\n",
    "        \"\"\"Update weights using gradient descent\n",
    "        \n",
    "        Args:\n",
    "            gradients: Dictionary of gradients from backward pass\n",
    "        \"\"\"\n",
    "        # Update rule: weight = weight - learning_rate √ó gradient\n",
    "        self.W1 -= self.learning_rate * gradients['dW1']\n",
    "        self.b1 -= self.learning_rate * gradients['db1']\n",
    "        self.W2 -= self.learning_rate * gradients['dW2']\n",
    "        self.b2 -= self.learning_rate * gradients['db2']\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        \"\"\"Perform one training step (forward, backward, update)\n",
    "        \n",
    "        Args:\n",
    "            X: Input batch\n",
    "            y: True labels for batch\n",
    "        \n",
    "        Returns:\n",
    "            loss: Loss value for this batch\n",
    "        \"\"\"\n",
    "        # 1. Forward pass: compute predictions\n",
    "        y_pred = self.forward(X)\n",
    "        \n",
    "        # 2. Compute loss\n",
    "        loss = self.compute_loss(y_pred, y)\n",
    "        \n",
    "        # 3. Backward pass: compute gradients\n",
    "        gradients = self.backward(y)\n",
    "        \n",
    "        # 4. Update weights\n",
    "        self.update_weights(gradients)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def create_batches(self, X, y, batch_size):\n",
    "        \"\"\"Split data into mini-batches\n",
    "        \n",
    "        Args:\n",
    "            X: All input data\n",
    "            y: All labels\n",
    "            batch_size: Size of each batch\n",
    "        \n",
    "        Yields:\n",
    "            (X_batch, y_batch): One batch at a time\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]  # Total number of samples\n",
    "        \n",
    "        # Shuffle data at the start of each epoch\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        # Split into batches\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            # Get batch (handle last batch which might be smaller)\n",
    "            end_idx = min(i + batch_size, n_samples)\n",
    "            X_batch = X_shuffled[i:end_idx]\n",
    "            y_batch = y_shuffled[i:end_idx]\n",
    "            \n",
    "            yield X_batch, y_batch\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
    "              epochs=100, batch_size=32, verbose=True, \n",
    "              early_stopping_patience=None):\n",
    "        \"\"\"Train the neural network\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training data (n_samples, n_features)\n",
    "            y_train: Training labels (n_samples, n_outputs)\n",
    "            X_val: Validation data (optional)\n",
    "            y_val: Validation labels (optional)\n",
    "            epochs: Number of times to iterate through training data\n",
    "            batch_size: Number of samples per batch\n",
    "            verbose: If True, print progress\n",
    "            early_stopping_patience: Stop if val loss doesn't improve for this many epochs\n",
    "        \n",
    "        Returns:\n",
    "            history: Dictionary with training metrics\n",
    "        \"\"\"\n",
    "        n_samples = X_train.shape[0]  # Total training samples\n",
    "        iterations_per_epoch = int(np.ceil(n_samples / batch_size))  # Batches per epoch\n",
    "        \n",
    "        # Early stopping tracking\n",
    "        best_val_loss = float('inf')  # Best validation loss seen\n",
    "        patience_counter = 0  # How many epochs without improvement\n",
    "        \n",
    "        # Training start time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"=\"*70)\n",
    "            print(\"üöÄ STARTING TRAINING\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"Training samples: {n_samples}\")\n",
    "            print(f\"Batch size: {batch_size}\")\n",
    "            print(f\"Iterations per epoch: {iterations_per_epoch}\")\n",
    "            print(f\"Total epochs: {epochs}\")\n",
    "            print(f\"Total iterations: {epochs * iterations_per_epoch}\")\n",
    "            if early_stopping_patience:\n",
    "                print(f\"Early stopping patience: {early_stopping_patience} epochs\")\n",
    "            print(\"=\"*70)\n",
    "            print()\n",
    "        \n",
    "        # Training loop - iterate through epochs\n",
    "        for epoch in range(epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            epoch_losses = []  # Track losses for this epoch\n",
    "            \n",
    "            # Iterate through mini-batches\n",
    "            for X_batch, y_batch in self.create_batches(X_train, y_train, batch_size):\n",
    "                # Perform one training step\n",
    "                batch_loss = self.train_step(X_batch, y_batch)\n",
    "                epoch_losses.append(batch_loss)\n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            avg_train_loss = np.mean(epoch_losses)\n",
    "            \n",
    "            # Compute training accuracy\n",
    "            train_pred = self.forward(X_train)\n",
    "            train_acc = self.compute_accuracy(train_pred, y_train)\n",
    "            \n",
    "            # Store training metrics\n",
    "            self.history['train_loss'].append(avg_train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            \n",
    "            # Validation metrics (if validation data provided)\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_pred = self.forward(X_val)\n",
    "                val_loss = self.compute_loss(val_pred, y_val)\n",
    "                val_acc = self.compute_accuracy(val_pred, y_val)\n",
    "                \n",
    "                self.history['val_loss'].append(val_loss)\n",
    "                self.history['val_acc'].append(val_acc)\n",
    "                \n",
    "                # Early stopping check\n",
    "                if early_stopping_patience is not None:\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        patience_counter = 0  # Reset counter\n",
    "                    else:\n",
    "                        patience_counter += 1  # Increment counter\n",
    "                        \n",
    "                        # Stop if patience exceeded\n",
    "                        if patience_counter >= early_stopping_patience:\n",
    "                            if verbose:\n",
    "                                print(f\"\\n‚èπÔ∏è  Early stopping at epoch {epoch+1}\")\n",
    "                                print(f\"Validation loss hasn't improved for {early_stopping_patience} epochs\")\n",
    "                            break\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % max(1, epochs // 10) == 0:\n",
    "                epoch_time = time.time() - epoch_start_time\n",
    "                print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                      f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                      f\"Train Acc: {train_acc:5.2f}%\", end=\"\")\n",
    "                \n",
    "                if X_val is not None:\n",
    "                    print(f\" | Val Loss: {val_loss:.4f} | \"\n",
    "                          f\"Val Acc: {val_acc:5.2f}%\", end=\"\")\n",
    "                \n",
    "                print(f\" | Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Training complete\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        if verbose:\n",
    "            print()\n",
    "            print(\"=\"*70)\n",
    "            print(\"‚úÖ TRAINING COMPLETE\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"Total time: {total_time:.2f}s\")\n",
    "            print(f\"Final training loss: {self.history['train_loss'][-1]:.4f}\")\n",
    "            print(f\"Final training accuracy: {self.history['train_acc'][-1]:.2f}%\")\n",
    "            if X_val is not None:\n",
    "                print(f\"Final validation loss: {self.history['val_loss'][-1]:.4f}\")\n",
    "                print(f\"Final validation accuracy: {self.history['val_acc'][-1]:.2f}%\")\n",
    "            print(\"=\"*70)\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\n",
    "        \n",
    "        Args:\n",
    "            X: Input data\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Binary predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        return (probabilities > 0.5).astype(int)\n",
    "\n",
    "print(\"‚úÖ Complete Neural Network class implemented!\")\n",
    "print(\"\\nThis class includes:\")\n",
    "print(\"  ‚úì Proper weight initialization (Xavier/He)\")\n",
    "print(\"  ‚úì Mini-batch training\")\n",
    "print(\"  ‚úì Training/validation split\")\n",
    "print(\"  ‚úì Progress tracking\")\n",
    "print(\"  ‚úì Early stopping\")\n",
    "print(\"  ‚úì Comprehensive logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 4: Training on a Real Problem - XOR\n",
    "\n",
    "Let's use our complete training system to solve the XOR problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XOR dataset\n",
    "X_xor = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "y_xor = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])\n",
    "\n",
    "print(\"XOR Problem:\")\n",
    "print(\"=\"*40)\n",
    "print(\"Input 1 | Input 2 | Output\")\n",
    "print(\"-\"*40)\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"   {X_xor[i,0]}    |    {X_xor[i,1]}    |   {y_xor[i,0]}\")\n",
    "print(\"=\"*40)\n",
    "print()\n",
    "\n",
    "# Create network\n",
    "np.random.seed(42)  # For reproducibility\n",
    "network = NeuralNetwork(\n",
    "    input_size=2,\n",
    "    hidden_size=8,  # 8 hidden neurons\n",
    "    output_size=1,\n",
    "    learning_rate=0.1,\n",
    "    activation='relu'  # ReLU activation\n",
    ")\n",
    "\n",
    "# Train the network\n",
    "history = network.train(\n",
    "    X_train=X_xor,\n",
    "    y_train=y_xor,\n",
    "    epochs=1000,\n",
    "    batch_size=4,  # Use all 4 examples per batch\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test the network\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ TESTING TRAINED NETWORK\")\n",
    "print(\"=\"*70)\n",
    "predictions = network.forward(X_xor)\n",
    "\n",
    "print(\"Input 1 | Input 2 | Target | Prediction | Rounded | Correct?\")\n",
    "print(\"-\"*70)\n",
    "for i in range(len(X_xor)):\n",
    "    x1, x2 = X_xor[i]\n",
    "    target = y_xor[i, 0]\n",
    "    pred = predictions[i, 0]\n",
    "    rounded = round(pred)\n",
    "    correct = \"‚úì\" if rounded == target else \"‚úó\"\n",
    "    print(f\"   {x1}    |    {x2}    |   {target}    |   {pred:.4f}   |    {rounded}    |   {correct}\")\n",
    "\n",
    "accuracy = network.compute_accuracy(predictions, y_xor)\n",
    "print(\"=\"*70)\n",
    "print(f\"Final Accuracy: {accuracy:.1f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss over epochs\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "ax1.plot(epochs, history['train_loss'], 'b-', linewidth=2, label='Training Loss')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "ax1.set_title('Loss Decreases Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')  # Log scale to see details\n",
    "\n",
    "# Plot 2: Decision boundary\n",
    "# Create a grid of points\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 200),\n",
    "                     np.linspace(-0.5, 1.5, 200))\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Predict for all grid points\n",
    "Z = network.forward(grid_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "contour = ax2.contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.8)\n",
    "ax2.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=3)\n",
    "\n",
    "# Plot the XOR points\n",
    "scatter = ax2.scatter(X_xor[:, 0], X_xor[:, 1], c=y_xor.flatten(),\n",
    "                     cmap='RdYlBu_r', s=300, edgecolors='black', linewidths=3)\n",
    "\n",
    "# Add labels\n",
    "for i, (x, y, label) in enumerate(zip(X_xor[:, 0], X_xor[:, 1], y_xor.flatten())):\n",
    "    ax2.text(x, y-0.15, f'({int(x)},{int(y)})‚Üí{int(label)}',\n",
    "             fontsize=11, ha='center', fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('Input 1', fontsize=12)\n",
    "ax2.set_ylabel('Input 2', fontsize=12)\n",
    "ax2.set_title('Decision Boundary: Network Learned XOR!', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim(-0.5, 1.5)\n",
    "ax2.set_ylim(-0.5, 1.5)\n",
    "plt.colorbar(contour, ax=ax2, label='Prediction')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Understanding the plots:\")\n",
    "print(\"Left: Loss curve - steady decrease means learning is working\")\n",
    "print(\"Right: Decision boundary - the network separates the classes correctly!\")\n",
    "print(\"  ‚Ä¢ Blue regions = network predicts 0\")\n",
    "print(\"  ‚Ä¢ Red regions = network predicts 1\")\n",
    "print(\"  ‚Ä¢ Black line = decision boundary (50% confidence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ Part 5: A More Complex Dataset - Circles\n",
    "\n",
    "Let's create a more challenging problem: classifying points inside vs outside a circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circle_data(n_samples=200, noise=0.1):\n",
    "    \"\"\"Generate a dataset with two concentric circles\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples to generate\n",
    "        noise: Amount of noise to add\n",
    "    \n",
    "    Returns:\n",
    "        X: Features (n_samples, 2)\n",
    "        y: Labels (n_samples, 1) - 0 for inner circle, 1 for outer\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate inner circle (class 0)\n",
    "    n_inner = n_samples // 2\n",
    "    theta_inner = np.random.uniform(0, 2*np.pi, n_inner)\n",
    "    r_inner = np.random.uniform(0, 0.5, n_inner)\n",
    "    X_inner = np.column_stack([\n",
    "        r_inner * np.cos(theta_inner) + np.random.normal(0, noise, n_inner),\n",
    "        r_inner * np.sin(theta_inner) + np.random.normal(0, noise, n_inner)\n",
    "    ])\n",
    "    y_inner = np.zeros((n_inner, 1))\n",
    "    \n",
    "    # Generate outer circle (class 1)\n",
    "    n_outer = n_samples - n_inner\n",
    "    theta_outer = np.random.uniform(0, 2*np.pi, n_outer)\n",
    "    r_outer = np.random.uniform(0.7, 1.0, n_outer)\n",
    "    X_outer = np.column_stack([\n",
    "        r_outer * np.cos(theta_outer) + np.random.normal(0, noise, n_outer),\n",
    "        r_outer * np.sin(theta_outer) + np.random.normal(0, noise, n_outer)\n",
    "    ])\n",
    "    y_outer = np.ones((n_outer, 1))\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    X = np.vstack([X_inner, X_outer])\n",
    "    y = np.vstack([y_inner, y_outer])\n",
    "    \n",
    "    shuffle_idx = np.random.permutation(n_samples)\n",
    "    X = X[shuffle_idx]\n",
    "    y = y[shuffle_idx]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate dataset\n",
    "X_circles, y_circles = generate_circle_data(n_samples=400, noise=0.08)\n",
    "\n",
    "# Split into train and validation sets (80/20 split)\n",
    "split_idx = int(0.8 * len(X_circles))\n",
    "X_train = X_circles[:split_idx]\n",
    "y_train = y_circles[:split_idx]\n",
    "X_val = X_circles[split_idx:]\n",
    "y_val = y_circles[split_idx:]\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print()\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_train[y_train.flatten() == 0, 0], \n",
    "           X_train[y_train.flatten() == 0, 1],\n",
    "           c='blue', label='Class 0 (inner)', s=50, alpha=0.6, edgecolors='black')\n",
    "plt.scatter(X_train[y_train.flatten() == 1, 0], \n",
    "           X_train[y_train.flatten() == 1, 1],\n",
    "           c='red', label='Class 1 (outer)', s=50, alpha=0.6, edgecolors='black')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Circles Dataset (Training Set)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° This is a non-linear problem!\")\n",
    "print(\"A single neuron can't solve this - we need a multi-layer network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network on circles dataset\n",
    "\n",
    "print(\"Training network on circles dataset...\\n\")\n",
    "\n",
    "# Create network with more hidden neurons for complex pattern\n",
    "np.random.seed(42)\n",
    "network_circles = NeuralNetwork(\n",
    "    input_size=2,\n",
    "    hidden_size=16,  # More neurons for complex pattern\n",
    "    output_size=1,\n",
    "    learning_rate=0.1,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "# Train with validation set and early stopping\n",
    "history_circles = network_circles.train(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    verbose=True,\n",
    "    early_stopping_patience=50  # Stop if no improvement for 50 epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress and results\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Training and validation loss\n",
    "ax1 = axes[0, 0]\n",
    "epochs = range(1, len(history_circles['train_loss']) + 1)\n",
    "ax1.plot(epochs, history_circles['train_loss'], 'b-', linewidth=2, label='Training Loss')\n",
    "ax1.plot(epochs, history_circles['val_loss'], 'r-', linewidth=2, label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "ax1.set_title('Loss Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training and validation accuracy\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(epochs, history_circles['train_acc'], 'b-', linewidth=2, label='Training Accuracy')\n",
    "ax2.plot(epochs, history_circles['val_acc'], 'r-', linewidth=2, label='Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Decision boundary on training set\n",
    "ax3 = axes[1, 0]\n",
    "xx, yy = np.meshgrid(np.linspace(-1.2, 1.2, 200),\n",
    "                     np.linspace(-1.2, 1.2, 200))\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = network_circles.forward(grid_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "contour = ax3.contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.8)\n",
    "ax3.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "ax3.scatter(X_train[y_train.flatten() == 0, 0], X_train[y_train.flatten() == 0, 1],\n",
    "           c='blue', s=30, alpha=0.6, edgecolors='black')\n",
    "ax3.scatter(X_train[y_train.flatten() == 1, 0], X_train[y_train.flatten() == 1, 1],\n",
    "           c='red', s=30, alpha=0.6, edgecolors='black')\n",
    "ax3.set_xlabel('Feature 1', fontsize=12)\n",
    "ax3.set_ylabel('Feature 2', fontsize=12)\n",
    "ax3.set_title('Training Set Decision Boundary', fontsize=14, fontweight='bold')\n",
    "ax3.axis('equal')\n",
    "plt.colorbar(contour, ax=ax3, label='Prediction')\n",
    "\n",
    "# Plot 4: Decision boundary on validation set\n",
    "ax4 = axes[1, 1]\n",
    "contour = ax4.contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.8)\n",
    "ax4.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "ax4.scatter(X_val[y_val.flatten() == 0, 0], X_val[y_val.flatten() == 0, 1],\n",
    "           c='blue', s=30, alpha=0.6, edgecolors='black', label='Class 0')\n",
    "ax4.scatter(X_val[y_val.flatten() == 1, 0], X_val[y_val.flatten() == 1, 1],\n",
    "           c='red', s=30, alpha=0.6, edgecolors='black', label='Class 1')\n",
    "ax4.set_xlabel('Feature 1', fontsize=12)\n",
    "ax4.set_ylabel('Feature 2', fontsize=12)\n",
    "ax4.set_title('Validation Set Decision Boundary', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.axis('equal')\n",
    "plt.colorbar(contour, ax=ax4, label='Prediction')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"‚Ä¢ Top left: Both training and validation loss decrease - good sign!\")\n",
    "print(\"‚Ä¢ Top right: Accuracy improves for both sets\")\n",
    "print(\"‚Ä¢ Bottom: The network learned the circular pattern\")\n",
    "print(\"‚Ä¢ If validation curves match training: model generalizes well! ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéõÔ∏è Part 6: Hyperparameter Tuning Experiments\n",
    "\n",
    "Let's explore how different hyperparameters affect training!\n",
    "\n",
    "### Experiment 1: Learning Rate Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "results = {}\n",
    "\n",
    "print(\"Testing different learning rates...\\n\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Training with learning rate = {lr}...\")\n",
    "    \n",
    "    # Create and train network\n",
    "    np.random.seed(42)  # Same initialization for fair comparison\n",
    "    net = NeuralNetwork(\n",
    "        input_size=2,\n",
    "        hidden_size=8,\n",
    "        output_size=1,\n",
    "        learning_rate=lr,\n",
    "        activation='relu'\n",
    "    )\n",
    "    \n",
    "    history = net.train(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        verbose=False  # Suppress output\n",
    "    )\n",
    "    \n",
    "    results[lr] = history\n",
    "    print(f\"  Final val accuracy: {history['val_acc'][-1]:.2f}%\")\n",
    "\n",
    "print(\"\\nDone!\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for lr, history in results.items():\n",
    "    epochs = range(1, len(history['val_loss']) + 1)\n",
    "    ax1.plot(epochs, history['val_loss'], linewidth=2, label=f'LR = {lr}')\n",
    "    ax2.plot(epochs, history['val_acc'], linewidth=2, label=f'LR = {lr}')\n",
    "\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax1.set_title('Effect of Learning Rate on Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Effect of Learning Rate on Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ What we learned:\")\n",
    "print(\"‚Ä¢ Too small (0.001): Slow learning, might not converge\")\n",
    "print(\"‚Ä¢ Just right (0.01-0.1): Fast, stable convergence\")\n",
    "print(\"‚Ä¢ Too large (0.5): Might be unstable or overshoot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Batch Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different batch sizes\n",
    "\n",
    "batch_sizes = [8, 16, 32, 64]\n",
    "batch_results = {}\n",
    "\n",
    "print(\"Testing different batch sizes...\\n\")\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"Training with batch size = {bs}...\")\n",
    "    \n",
    "    # Create and train network\n",
    "    np.random.seed(42)\n",
    "    net = NeuralNetwork(\n",
    "        input_size=2,\n",
    "        hidden_size=8,\n",
    "        output_size=1,\n",
    "        learning_rate=0.1,\n",
    "        activation='relu'\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = net.train(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        epochs=200,\n",
    "        batch_size=bs,\n",
    "        verbose=False\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    batch_results[bs] = {'history': history, 'time': train_time}\n",
    "    print(f\"  Time: {train_time:.2f}s | Final val acc: {history['val_acc'][-1]:.2f}%\")\n",
    "\n",
    "print(\"\\nDone!\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for bs, data in batch_results.items():\n",
    "    history = data['history']\n",
    "    epochs = range(1, len(history['val_loss']) + 1)\n",
    "    ax1.plot(epochs, history['val_loss'], linewidth=2, label=f'Batch = {bs}')\n",
    "    ax2.plot(epochs, history['val_acc'], linewidth=2, label=f'Batch = {bs}')\n",
    "\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax1.set_title('Effect of Batch Size on Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Effect of Batch Size on Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ What we learned:\")\n",
    "print(\"‚Ä¢ Smaller batches (8-16): More noisy but can escape local minima\")\n",
    "print(\"‚Ä¢ Medium batches (32): Good balance of speed and stability\")\n",
    "print(\"‚Ä¢ Larger batches (64+): Smoother updates but might miss fine details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Part 7: Common Mistakes and How to Avoid Them\n",
    "\n",
    "### Mistake 1: Not Shuffling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate importance of shuffling\n",
    "\n",
    "# Create sorted XOR data (all class 0, then all class 1)\n",
    "X_sorted = np.array([[0, 0], [1, 1], [0, 1], [1, 0]])  # Sorted by class\n",
    "y_sorted = np.array([[0], [0], [1], [1]])  # All 0s, then all 1s\n",
    "\n",
    "print(\"‚ö†Ô∏è WARNING: Training on sorted (non-shuffled) data\\n\")\n",
    "\n",
    "# Train without shuffling (modify create_batches to not shuffle)\n",
    "np.random.seed(42)\n",
    "net_no_shuffle = NeuralNetwork(2, 8, 1, learning_rate=0.1, activation='relu')\n",
    "\n",
    "# Manually train without shuffling\n",
    "losses_no_shuffle = []\n",
    "for epoch in range(100):\n",
    "    # Process in order (no shuffling!)\n",
    "    for i in range(0, len(X_sorted), 2):\n",
    "        X_batch = X_sorted[i:i+2]\n",
    "        y_batch = y_sorted[i:i+2]\n",
    "        loss = net_no_shuffle.train_step(X_batch, y_batch)\n",
    "    losses_no_shuffle.append(loss)\n",
    "\n",
    "print(\"‚úÖ Now training with shuffling (correct way)\\n\")\n",
    "\n",
    "# Train with shuffling (normal way)\n",
    "np.random.seed(42)\n",
    "net_shuffle = NeuralNetwork(2, 8, 1, learning_rate=0.1, activation='relu')\n",
    "history_shuffle = net_shuffle.train(\n",
    "    X_sorted, y_sorted, epochs=100, batch_size=2, verbose=False\n",
    ")\n",
    "\n",
    "# Compare\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_no_shuffle, 'r-', linewidth=2, label='No Shuffling ‚ùå')\n",
    "plt.plot(history_shuffle['train_loss'], 'g-', linewidth=2, label='With Shuffling ‚úì')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Shuffling Makes Training More Stable', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Takeaway: ALWAYS shuffle your data!\")\n",
    "print(\"Without shuffling, the network sees patterns in the order,\")\n",
    "print(\"not the actual data patterns we want it to learn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Not Normalizing Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with very different scales\n",
    "np.random.seed(42)\n",
    "X_unnormalized = np.random.randn(100, 2)\n",
    "X_unnormalized[:, 0] *= 1000  # First feature: large scale\n",
    "X_unnormalized[:, 1] *= 0.01  # Second feature: tiny scale\n",
    "y_unnormalized = ((X_unnormalized[:, 0] > 0) & (X_unnormalized[:, 1] > 0)).astype(int).reshape(-1, 1)\n",
    "\n",
    "# Normalize data\n",
    "X_normalized = (X_unnormalized - X_unnormalized.mean(axis=0)) / X_unnormalized.std(axis=0)\n",
    "\n",
    "print(\"Training on UNNORMALIZED data...\")\n",
    "np.random.seed(42)\n",
    "net_unnorm = NeuralNetwork(2, 8, 1, learning_rate=0.01, activation='relu')\n",
    "history_unnorm = net_unnorm.train(\n",
    "    X_unnormalized, y_unnormalized, epochs=100, batch_size=10, verbose=False\n",
    ")\n",
    "\n",
    "print(\"Training on NORMALIZED data...\")\n",
    "np.random.seed(42)\n",
    "net_norm = NeuralNetwork(2, 8, 1, learning_rate=0.01, activation='relu')\n",
    "history_norm = net_norm.train(\n",
    "    X_normalized, y_unnormalized, epochs=100, batch_size=10, verbose=False\n",
    ")\n",
    "\n",
    "# Compare\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_unnorm['train_loss'], 'r-', linewidth=2, label='Unnormalized ‚ùå')\n",
    "plt.plot(history_norm['train_loss'], 'g-', linewidth=2, label='Normalized ‚úì')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Normalization Speeds Up Training', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Takeaway: Normalize your inputs!\")\n",
    "print(\"Formula: X_norm = (X - mean) / std\")\n",
    "print(\"This gives features similar scales ‚Üí more stable training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Training Too Long (Overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overfitting with a very small training set\n",
    "X_tiny = X_train[:30]  # Only 30 training samples\n",
    "y_tiny = y_train[:30]\n",
    "\n",
    "print(\"Training with small dataset to demonstrate overfitting...\\n\")\n",
    "\n",
    "# Train for many epochs\n",
    "np.random.seed(42)\n",
    "net_overfit = NeuralNetwork(2, 32, 1, learning_rate=0.1, activation='relu')  # Large network\n",
    "history_overfit = net_overfit.train(\n",
    "    X_train=X_tiny,\n",
    "    y_train=y_tiny,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    epochs=500,\n",
    "    batch_size=10,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Plot training vs validation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs = range(1, len(history_overfit['train_loss']) + 1)\n",
    "\n",
    "ax1.plot(epochs, history_overfit['train_loss'], 'b-', linewidth=2, label='Training')\n",
    "ax1.plot(epochs, history_overfit['val_loss'], 'r-', linewidth=2, label='Validation')\n",
    "ax1.axvline(x=50, color='green', linestyle='--', linewidth=2, label='Should stop here')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Overfitting: Val Loss Increases', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(epochs, history_overfit['train_acc'], 'b-', linewidth=2, label='Training')\n",
    "ax2.plot(epochs, history_overfit['val_acc'], 'r-', linewidth=2, label='Validation')\n",
    "ax2.axvline(x=50, color='green', linestyle='--', linewidth=2, label='Should stop here')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Overfitting: Gap Between Train and Val', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüö® Signs of Overfitting:\")\n",
    "print(\"‚Ä¢ Training loss keeps decreasing\")\n",
    "print(\"‚Ä¢ Validation loss starts increasing (the gap widens)\")\n",
    "print(\"‚Ä¢ Training accuracy high, validation accuracy lower\")\n",
    "print(\"\\nüí° Solution: Use early stopping or regularization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Final Summary\n",
    "\n",
    "Congratulations! You've learned everything needed to train neural networks from scratch!\n",
    "\n",
    "### üéì What You've Mastered\n",
    "\n",
    "#### Core Concepts\n",
    "‚úÖ **Epochs**: Complete passes through the data\n",
    "\n",
    "‚úÖ **Batches**: Mini-batch gradient descent for efficiency\n",
    "\n",
    "‚úÖ **Iterations**: Individual weight updates\n",
    "\n",
    "‚úÖ **Training Loop**: The full pipeline from data to trained model\n",
    "\n",
    "#### Best Practices\n",
    "‚úÖ **Proper Initialization**: Xavier/He for stable gradients\n",
    "\n",
    "‚úÖ **Data Shuffling**: Prevents order-based biases\n",
    "\n",
    "‚úÖ **Normalization**: Speeds up training\n",
    "\n",
    "‚úÖ **Train/Val Split**: Detect overfitting\n",
    "\n",
    "‚úÖ **Early Stopping**: Prevent overfitting automatically\n",
    "\n",
    "‚úÖ **Hyperparameter Tuning**: Learning rate, batch size, etc.\n",
    "\n",
    "#### Common Mistakes to Avoid\n",
    "‚ùå All-zero initialization\n",
    "\n",
    "‚ùå Not shuffling data\n",
    "\n",
    "‚ùå Not normalizing inputs\n",
    "\n",
    "‚ùå Training too long (overfitting)\n",
    "\n",
    "‚ùå Learning rate too high/low\n",
    "\n",
    "### üé¨ The Complete Picture\n",
    "\n",
    "You can now:\n",
    "1. **Design** a network architecture\n",
    "2. **Initialize** weights properly\n",
    "3. **Prepare** data (shuffle, normalize, split)\n",
    "4. **Train** using mini-batch gradient descent\n",
    "5. **Monitor** progress (loss curves, accuracy)\n",
    "6. **Validate** to prevent overfitting\n",
    "7. **Tune** hyperparameters for best results\n",
    "\n",
    "### üöÄ What's Next?\n",
    "\n",
    "You've built neural networks **from scratch**! But in practice, we use frameworks like:\n",
    "- **PyTorch**: Most popular for research\n",
    "- **TensorFlow/Keras**: Great for production\n",
    "\n",
    "These frameworks:\n",
    "- Handle backpropagation automatically\n",
    "- Optimize using GPUs\n",
    "- Provide pre-built layers and models\n",
    "- Include advanced optimizers (Adam, RMSprop)\n",
    "\n",
    "**But understanding the fundamentals you've learned makes you a MUCH better deep learning practitioner!** üåü\n",
    "\n",
    "---\n",
    "\n",
    "## üí™ Final Challenge\n",
    "\n",
    "Can you improve the circles classifier?\n",
    "\n",
    "Try:\n",
    "1. Different network sizes\n",
    "2. Different learning rates\n",
    "3. Different batch sizes\n",
    "4. Different numbers of epochs\n",
    "5. Adding a third layer?\n",
    "\n",
    "Experiment and see what works best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experimentation space!\n",
    "# Try different configurations and see what happens\n",
    "\n",
    "# Example:\n",
    "# np.random.seed(42)\n",
    "# my_network = NeuralNetwork(\n",
    "#     input_size=2,\n",
    "#     hidden_size=???  # Try different sizes!\n",
    "#     output_size=1,\n",
    "#     learning_rate=???  # Try different rates!\n",
    "#     activation='relu'\n",
    "# )\n",
    "#\n",
    "# history = my_network.train(\n",
    "#     X_train=X_train,\n",
    "#     y_train=y_train,\n",
    "#     X_val=X_val,\n",
    "#     y_val=y_val,\n",
    "#     epochs=???,  # Try different numbers!\n",
    "#     batch_size=???,  # Try different sizes!\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "print(\"Happy experimenting! üß™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the **Neural Networks Fundamentals** series!\n",
    "\n",
    "From knowing nothing to building and training neural networks from scratch - that's an incredible journey! üöÄ\n",
    "\n",
    "### Your Journey:\n",
    "1. ‚úÖ **Notebook 1-3**: Understanding neurons, activations, and basic concepts\n",
    "2. ‚úÖ **Notebook 4-6**: Building layers, forward propagation, and loss functions\n",
    "3. ‚úÖ **Notebook 7**: Backpropagation - the learning algorithm\n",
    "4. ‚úÖ **Notebook 8**: Training loop - putting it all together\n",
    "\n",
    "### You now understand:\n",
    "- How neural networks work (not magic!)\n",
    "- How they learn (backpropagation + gradient descent)\n",
    "- How to train them (the complete pipeline)\n",
    "- What can go wrong (and how to fix it)\n",
    "\n",
    "### Keep Learning! üìö\n",
    "\n",
    "Next topics to explore:\n",
    "- **Convolutional Neural Networks** (CNNs) for images\n",
    "- **Recurrent Neural Networks** (RNNs) for sequences\n",
    "- **Transformers** for modern AI\n",
    "- **PyTorch/TensorFlow** for practical applications\n",
    "\n",
    "You have a **solid foundation**. Everything else builds on what you've learned here!\n",
    "\n",
    "**You did it!** üåüüéäüéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}