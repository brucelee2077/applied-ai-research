{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• PyTorch Equivalent: 500 Lines ‚Üí 50 Lines\n",
    "\n",
    "## Welcome to the Power of Deep Learning Frameworks!\n",
    "\n",
    "**In the last notebook**, you built a complete neural network from scratch:\n",
    "- ‚úÖ ~500 lines of code\n",
    "- ‚úÖ Manual forward propagation\n",
    "- ‚úÖ Manual backpropagation (chain rule, gradients, etc.)\n",
    "- ‚úÖ Manual weight updates\n",
    "- ‚úÖ Manual batch processing\n",
    "\n",
    "**In this notebook**, we'll rebuild the SAME network in PyTorch:\n",
    "- üöÄ ~50 lines of code\n",
    "- üöÄ Automatic differentiation (no manual gradients!)\n",
    "- üöÄ Built-in optimizers\n",
    "- üöÄ GPU acceleration\n",
    "- üöÄ Production-ready tools\n",
    "\n",
    "## Why Learn Fundamentals First?\n",
    "\n",
    "**You now have a HUGE advantage:**\n",
    "- You understand what PyTorch does behind the scenes\n",
    "- You can debug models effectively\n",
    "- You can implement custom architectures\n",
    "- You can optimize performance\n",
    "\n",
    "**Frameworks make you productive, fundamentals make you effective!**\n",
    "\n",
    "## What is PyTorch?\n",
    "\n",
    "PyTorch is a deep learning framework that provides:\n",
    "1. **Tensors**: NumPy-like arrays that run on GPUs\n",
    "2. **Autograd**: Automatic differentiation (computes gradients for you!)\n",
    "3. **nn.Module**: Building blocks for neural networks\n",
    "4. **Optimizers**: SGD, Adam, RMSprop, and more\n",
    "5. **Utils**: Data loading, training loops, saving/loading models\n",
    "\n",
    "Let's see it in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "If you haven't installed PyTorch yet:\n",
    "\n",
    "```bash\n",
    "# CPU version (lighter, good for learning)\n",
    "pip install torch torchvision\n",
    "\n",
    "# GPU version (faster training, requires CUDA)\n",
    "# Visit pytorch.org for your specific GPU setup\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch  # Core PyTorch library\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch.optim as optim  # Optimizers (SGD, Adam, etc.)\n",
    "from torch.utils.data import DataLoader, TensorDataset  # Data utilities\n",
    "import numpy as np  # For data preprocessing\n",
    "import matplotlib.pyplot as plt  # For visualizations\n",
    "from sklearn.datasets import fetch_openml  # To download MNIST\n",
    "from sklearn.model_selection import train_test_split  # For data splitting\n",
    "from sklearn.metrics import confusion_matrix  # For evaluation\n",
    "import seaborn as sns  # For beautiful plots\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import time  # To measure training time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)  # PyTorch random seed\n",
    "np.random.seed(42)  # NumPy random seed\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Preprocess Data\n",
    "\n",
    "Same MNIST dataset, but we'll convert to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "print(\"üì• Loading MNIST dataset...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "\n",
    "# Extract and normalize data\n",
    "X = mnist.data.to_numpy() / 255.0  # Normalize to [0, 1]\n",
    "y = mnist.target.to_numpy().astype(int)  # Convert labels to integers\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded: {X.shape[0]:,} images\")\n",
    "\n",
    "# Split into train/val/test (same as before)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=10000, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=10000, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data split:\")\n",
    "print(f\"   Training:   {X_train.shape[0]:,}\")\n",
    "print(f\"   Validation: {X_val.shape[0]:,}\")\n",
    "print(f\"   Test:       {X_test.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "# PyTorch uses tensors instead of NumPy arrays\n",
    "X_train_tensor = torch.FloatTensor(X_train)  # Convert to PyTorch float tensor\n",
    "y_train_tensor = torch.LongTensor(y_train)   # Convert to PyTorch long tensor (for labels)\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "print(\"‚úÖ Converted to PyTorch tensors\")\n",
    "print(f\"   X_train shape: {X_train_tensor.shape}\")\n",
    "print(f\"   y_train shape: {y_train_tensor.shape}\")\n",
    "print(f\"   Data type: {X_train_tensor.dtype} (inputs), {y_train_tensor.dtype} (labels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for efficient batch processing\n",
    "# DataLoader automatically handles batching, shuffling, and parallel loading\n",
    "batch_size = 32  # Same as before\n",
    "\n",
    "# Create datasets (combines inputs and labels)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create data loaders (handles batching and shuffling)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Shuffle training data\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)     # Don't shuffle validation\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)   # Don't shuffle test\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoaders created\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Neural Network in PyTorch\n",
    "\n",
    "### Compare: NumPy vs PyTorch\n",
    "\n",
    "**NumPy version (from notebook 9):**\n",
    "```python\n",
    "# Initialize weights manually\n",
    "self.W1 = np.random.randn(784, 128) * np.sqrt(2.0/784)\n",
    "self.b1 = np.zeros((1, 128))\n",
    "# ... repeat for each layer\n",
    "\n",
    "# Forward pass (manual)\n",
    "Z1 = np.dot(X, W1) + b1\n",
    "A1 = relu(Z1)\n",
    "# ... repeat for each layer\n",
    "\n",
    "# Backward pass (manual chain rule)\n",
    "dZ3 = A3 - y_true\n",
    "dW3 = np.dot(A2.T, dZ3) / m\n",
    "# ... lots more gradient calculations\n",
    "```\n",
    "\n",
    "**PyTorch version:**\n",
    "```python\n",
    "# That's it! PyTorch handles initialization, forward, and backward!\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "```\n",
    "\n",
    "Let's build it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network using nn.Module\n",
    "class MNISTNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for MNIST classification.\n",
    "    \n",
    "    Same architecture as our NumPy version:\n",
    "    784 ‚Üí 128 ‚Üí 64 ‚Üí 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()  # Initialize parent class\n",
    "        \n",
    "        # Define layers\n",
    "        # nn.Linear automatically initializes weights and handles matrix multiplication!\n",
    "        self.fc1 = nn.Linear(784, 128)  # First fully connected layer (784 ‚Üí 128)\n",
    "        self.relu1 = nn.ReLU()           # ReLU activation\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)   # Second fully connected layer (128 ‚Üí 64)\n",
    "        self.relu2 = nn.ReLU()           # ReLU activation\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 10)    # Output layer (64 ‚Üí 10)\n",
    "        # Note: No softmax here! CrossEntropyLoss includes it\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        PyTorch automatically tracks operations for backprop!\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)      # Linear transformation\n",
    "        x = self.relu1(x)    # ReLU activation\n",
    "        \n",
    "        x = self.fc2(x)      # Linear transformation\n",
    "        x = self.relu2(x)    # ReLU activation\n",
    "        \n",
    "        x = self.fc3(x)      # Output layer\n",
    "        return x             # Return logits (no softmax needed)\n",
    "\n",
    "# Create model and move to device (CPU or GPU)\n",
    "model = MNISTNet().to(device)  # .to(device) moves model to GPU if available\n",
    "\n",
    "print(\"üß† Neural Network created!\")\n",
    "print(f\"   Architecture: 784 ‚Üí 128 ‚Üí 64 ‚Üí 10\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"\\nModel structure:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())  # numel() = number of elements\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nParameter breakdown:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"   {name:20s}: {param.shape}  ({param.numel():,} params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Loss Function and Optimizer\n",
    "\n",
    "### More PyTorch Magic!\n",
    "\n",
    "**NumPy version:**\n",
    "```python\n",
    "# Manual cross-entropy loss\n",
    "loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "\n",
    "# Manual gradient descent\n",
    "W1 -= learning_rate * dW1\n",
    "b1 -= learning_rate * db1\n",
    "# ... repeat for all parameters\n",
    "```\n",
    "\n",
    "**PyTorch version:**\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = optim.Adam(model.parameters())  # Optimizer (better than plain SGD!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "# CrossEntropyLoss combines softmax + negative log likelihood\n",
    "# This is more numerically stable than doing them separately!\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "# Adam is an advanced optimizer (better than SGD in most cases)\n",
    "# It adapts learning rate for each parameter!\n",
    "learning_rate = 0.001  # Adam typically uses smaller learning rate than SGD\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"‚úÖ Loss function and optimizer ready\")\n",
    "print(f\"   Loss: CrossEntropyLoss (includes softmax)\")\n",
    "print(f\"   Optimizer: Adam\")\n",
    "print(f\"   Learning rate: {learning_rate}\")\n",
    "print(f\"\\nüí° Adam vs SGD:\")\n",
    "print(\"   - Adapts learning rate per parameter\")\n",
    "print(\"   - Uses momentum (remembers past gradients)\")\n",
    "print(\"   - Usually converges faster than plain SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training Loop\n",
    "\n",
    "### The PyTorch Training Pattern\n",
    "\n",
    "All PyTorch training loops follow this pattern:\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        # 1. Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 2. Backward pass\n",
    "        optimizer.zero_grad()  # Clear old gradients\n",
    "        loss.backward()        # Compute gradients (autograd!)\n",
    "        optimizer.step()       # Update weights\n",
    "```\n",
    "\n",
    "That's it! PyTorch handles all the calculus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 20  # Same as NumPy version\n",
    "\n",
    "# Lists to track metrics\n",
    "train_losses = []      # Training loss per epoch\n",
    "val_losses = []        # Validation loss per epoch\n",
    "train_accuracies = []  # Training accuracy per epoch\n",
    "val_accuracies = []    # Validation accuracy per epoch\n",
    "\n",
    "print(\"üéØ Starting training...\")\n",
    "print(f\"   Epochs: {num_epochs}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Optimizer: Adam (lr={learning_rate})\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set model to training mode (enables dropout, batch norm, etc.)\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "        # Move data to device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # Get predictions\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss.backward()        # Compute gradients (autograd magic!)\n",
    "        optimizer.step()       # Update weights\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss += loss.item() * inputs.size(0)  # Accumulate loss\n",
    "        _, predicted = torch.max(outputs.data, 1)   # Get predicted class\n",
    "        train_total += labels.size(0)               # Count samples\n",
    "        train_correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "    \n",
    "    # Calculate average training metrics\n",
    "    train_loss = train_loss / train_total\n",
    "    train_acc = 100.0 * train_correct / train_total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, batch norm, etc.)\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    # No gradient computation during validation (saves memory and computation)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # Move data to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass only\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Track metrics\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate average validation metrics\n",
    "    val_loss = val_loss / val_total\n",
    "    val_acc = 100.0 * val_correct / val_total\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Record end time\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"‚úÖ Training complete in {training_time:.2f} seconds!\")\n",
    "print(f\"   Final training accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "print(f\"   Final validation accuracy: {val_accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Training Progress\n",
    "\n",
    "Same visualization as before - let's compare with NumPy version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(train_losses, label='Training Loss', linewidth=2, marker='o')\n",
    "ax1.plot(val_losses, label='Validation Loss', linewidth=2, marker='s')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Loss Over Time (PyTorch)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(train_accuracies, label='Training Accuracy', linewidth=2, marker='o')\n",
    "ax2.plot(val_accuracies, label='Validation Accuracy', linewidth=2, marker='s')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Accuracy Over Time (PyTorch)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà PyTorch training curves\")\n",
    "print(\"   Notice: Potentially faster convergence due to Adam optimizer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "model.eval()  # Set to evaluation mode\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_predictions = []  # Store all predictions\n",
    "all_labels = []       # Store all true labels\n",
    "\n",
    "with torch.no_grad():  # No gradient computation\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Track metrics\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Store for confusion matrix\n",
    "        all_predictions.extend(predicted.cpu().numpy())  # Move to CPU and convert to numpy\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = 100.0 * test_correct / test_total\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ TEST SET PERFORMANCE (PyTorch)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n   Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"   Correct predictions: {test_correct:,} / {test_total:,}\")\n",
    "print(f\"   Wrong predictions: {test_total - test_correct:,}\")\n",
    "\n",
    "if test_accuracy > 95:\n",
    "    print(\"\\n   üéâ EXCELLENT! Over 95% accuracy!\")\n",
    "elif test_accuracy > 90:\n",
    "    print(\"\\n   üëç GOOD! Over 90% accuracy!\")\n",
    "else:\n",
    "    print(\"\\n   üìö Room for improvement. Try training longer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=list(range(10)), yticklabels=list(range(10)),\n",
    "            cbar_kws={'label': 'Number of predictions'})\n",
    "plt.xlabel('Predicted Label', fontsize=13)\n",
    "plt.ylabel('True Label', fontsize=13)\n",
    "plt.title('Confusion Matrix - MNIST Test Set (PyTorch)', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Compare this with the NumPy version!\")\n",
    "print(\"   Similar patterns? That's because it's the same network architecture!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Side-by-Side Comparison\n",
    "\n",
    "### NumPy vs PyTorch: What Changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Aspect | NumPy Version | PyTorch Version |\n",
    "|--------|---------------|------------------|\n",
    "| **Lines of code** | ~500 lines | ~50 lines |\n",
    "| **Weight initialization** | Manual (He init) | Automatic |\n",
    "| **Forward pass** | Manual matrix ops | `model(inputs)` |\n",
    "| **Loss computation** | Manual cross-entropy | `criterion(outputs, labels)` |\n",
    "| **Backward pass** | Manual gradients (chain rule) | `loss.backward()` (autograd!) |\n",
    "| **Weight updates** | Manual gradient descent | `optimizer.step()` |\n",
    "| **Batch processing** | Manual batch creation | `DataLoader` |\n",
    "| **GPU support** | ‚ùå Not available | ‚úÖ `.to(device)` |\n",
    "| **Advanced optimizers** | ‚ùå Only SGD | ‚úÖ Adam, RMSprop, etc. |\n",
    "| **Code clarity** | Lots of implementation details | Focus on architecture |\n",
    "\n",
    "### What PyTorch Automated:\n",
    "\n",
    "1. **Automatic Differentiation (Autograd)**\n",
    "   - No more manual chain rule!\n",
    "   - No more gradient calculations!\n",
    "   - Just call `loss.backward()`\n",
    "\n",
    "2. **Built-in Layers**\n",
    "   - `nn.Linear` handles weights + biases + matrix multiplication\n",
    "   - `nn.ReLU`, `nn.Sigmoid`, etc. for activations\n",
    "   - Automatic weight initialization\n",
    "\n",
    "3. **Optimizers**\n",
    "   - Adam, SGD, RMSprop, AdaGrad, etc.\n",
    "   - Built-in momentum, learning rate schedules\n",
    "   - Just call `optimizer.step()`\n",
    "\n",
    "4. **Data Loading**\n",
    "   - `DataLoader` for efficient batching\n",
    "   - Automatic shuffling\n",
    "   - Parallel data loading\n",
    "\n",
    "5. **GPU Acceleration**\n",
    "   - Move model and data to GPU with `.to(device)`\n",
    "   - 10-100x faster training!\n",
    "   - Same code works on CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Advanced PyTorch Features\n",
    "\n",
    "Let's explore some powerful PyTorch capabilities!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Different Optimizers\n",
    "\n",
    "PyTorch provides many optimization algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different optimizer options\n",
    "print(\"üîß Available Optimizers in PyTorch:\\n\")\n",
    "\n",
    "print(\"1. SGD (Stochastic Gradient Descent)\")\n",
    "print(\"   optimizer = optim.SGD(model.parameters(), lr=0.01)\")\n",
    "print(\"   - Basic gradient descent\")\n",
    "print(\"   - Can add momentum: momentum=0.9\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"2. Adam (Adaptive Moment Estimation)\")\n",
    "print(\"   optimizer = optim.Adam(model.parameters(), lr=0.001)\")\n",
    "print(\"   - Adapts learning rate per parameter\")\n",
    "print(\"   - Usually converges faster\")\n",
    "print(\"   - Good default choice!\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"3. RMSprop (Root Mean Square Propagation)\")\n",
    "print(\"   optimizer = optim.RMSprop(model.parameters(), lr=0.001)\")\n",
    "print(\"   - Good for recurrent neural networks\")\n",
    "print(\"   - Adapts learning rate\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"4. AdaGrad (Adaptive Gradient)\")\n",
    "print(\"   optimizer = optim.Adagrad(model.parameters(), lr=0.01)\")\n",
    "print(\"   - Good for sparse data\")\n",
    "print(\"   - Adapts learning rate per feature\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"üí° Tip: Adam is usually a good starting point!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Learning Rate Schedulers\n",
    "\n",
    "Automatically adjust learning rate during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Learning rate scheduler\n",
    "print(\"üìâ Learning Rate Schedulers:\\n\")\n",
    "\n",
    "print(\"1. StepLR - Reduce LR every N epochs\")\n",
    "print(\"   scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\")\n",
    "print(\"   - Reduces LR by 10x every 10 epochs\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"2. ReduceLROnPlateau - Reduce when loss plateaus\")\n",
    "print(\"   scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\")\n",
    "print(\"   - Automatically reduces LR when validation loss stops improving\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"3. CosineAnnealingLR - Cosine annealing\")\n",
    "print(\"   scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\")\n",
    "print(\"   - Gradually reduces LR following a cosine curve\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Usage in training loop:\")\n",
    "print(\"   for epoch in range(num_epochs):\")\n",
    "print(\"       train(...)\")\n",
    "print(\"       validate(...)\")\n",
    "print(\"       scheduler.step()  # Update learning rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Regularization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example network with regularization\n",
    "class RegularizedNet(nn.Module):\n",
    "    \"\"\"Network with dropout for regularization\"\"\"\n",
    "    def __init__(self):\n",
    "        super(RegularizedNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)  # Drop 20% of neurons during training\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)  # Apply dropout\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)  # Apply dropout\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "print(\"‚úÖ Regularization Techniques:\\n\")\n",
    "print(\"1. Dropout\")\n",
    "print(\"   - Randomly drops neurons during training\")\n",
    "print(\"   - Prevents overfitting\")\n",
    "print(\"   - nn.Dropout(p=0.2) drops 20% of neurons\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"2. L2 Regularization (Weight Decay)\")\n",
    "print(\"   - Add to optimizer: optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\")\n",
    "print(\"   - Penalizes large weights\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"3. Batch Normalization\")\n",
    "print(\"   - nn.BatchNorm1d(128) normalizes activations\")\n",
    "print(\"   - Stabilizes training\")\n",
    "print(\"   - Can speed up convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(\"üíæ Saving and Loading Models:\\n\")\n",
    "\n",
    "# Method 1: Save entire model\n",
    "torch.save(model, 'mnist_model.pth')\n",
    "print(\"‚úÖ Saved entire model to mnist_model.pth\")\n",
    "\n",
    "# Method 2: Save only state dict (recommended)\n",
    "torch.save(model.state_dict(), 'mnist_model_state.pth')\n",
    "print(\"‚úÖ Saved model state dict to mnist_model_state.pth\")\n",
    "\n",
    "print(\"\\nLoading models:\")\n",
    "print(\"\")\n",
    "print(\"# Method 1: Load entire model\")\n",
    "print(\"model = torch.load('mnist_model.pth')\")\n",
    "print(\"\")\n",
    "print(\"# Method 2: Load state dict (recommended)\")\n",
    "print(\"model = MNISTNet()\")\n",
    "print(\"model.load_state_dict(torch.load('mnist_model_state.pth'))\")\n",
    "print(\"model.eval()  # Set to evaluation mode\")\n",
    "\n",
    "print(\"\\nüí° Best practice: Save state_dict (more flexible)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Performance Comparison\n",
    "\n",
    "Let's compare our NumPy and PyTorch implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison summary\n",
    "print(\"=\"*70)\n",
    "print(\"üìä NUMPY vs PYTORCH COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. IMPLEMENTATION COMPLEXITY\")\n",
    "print(\"   NumPy:   ~500 lines of code\")\n",
    "print(\"   PyTorch: ~50 lines of core code (10x less!)\")\n",
    "print(\"   Winner: üèÜ PyTorch\")\n",
    "\n",
    "print(\"\\n2. DEVELOPMENT TIME\")\n",
    "print(\"   NumPy:   Hours to implement and debug\")\n",
    "print(\"   PyTorch: Minutes to implement\")\n",
    "print(\"   Winner: üèÜ PyTorch\")\n",
    "\n",
    "print(\"\\n3. ACCURACY\")\n",
    "print(\"   NumPy:   ~95-97% (depends on hyperparameters)\")\n",
    "print(f\"   PyTorch: {test_accuracy:.2f}%\")\n",
    "print(\"   Winner: ü§ù TIE (same architecture)\")\n",
    "\n",
    "print(\"\\n4. TRAINING SPEED\")\n",
    "if device.type == 'cuda':\n",
    "    print(\"   NumPy:   CPU only\")\n",
    "    print(\"   PyTorch: GPU accelerated (10-100x faster!)\")\n",
    "    print(\"   Winner: üèÜ PyTorch\")\n",
    "else:\n",
    "    print(\"   NumPy:   CPU optimized\")\n",
    "    print(\"   PyTorch: CPU (similar speed, but can use GPU!)\")\n",
    "    print(\"   Winner: üèÜ PyTorch (GPU potential)\")\n",
    "\n",
    "print(\"\\n5. FLEXIBILITY\")\n",
    "print(\"   NumPy:   Full control, but tedious\")\n",
    "print(\"   PyTorch: Easy to modify, experiment, and scale\")\n",
    "print(\"   Winner: üèÜ PyTorch\")\n",
    "\n",
    "print(\"\\n6. LEARNING VALUE\")\n",
    "print(\"   NumPy:   Deep understanding of fundamentals\")\n",
    "print(\"   PyTorch: Industry-standard tools and practices\")\n",
    "print(\"   Winner: üèÜ BOTH! (NumPy first, then PyTorch)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ OVERALL: PyTorch wins for productivity,\")\n",
    "print(\"           NumPy wins for understanding!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Quick Experiment - Different Architectures\n",
    "\n",
    "With PyTorch, it's easy to experiment! Let's try a deeper network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a deeper network\n",
    "class DeeperNet(nn.Module):\n",
    "    \"\"\"Deeper network: 784 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 10\"\"\"\n",
    "    def __init__(self):\n",
    "        super(DeeperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "print(\"üß† Deeper Network Created!\")\n",
    "print(\"   Architecture: 784 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 10\")\n",
    "print(\"\\nüí° With PyTorch, experimenting with architectures is EASY!\")\n",
    "print(\"   Try different:\")\n",
    "print(\"   - Layer sizes\")\n",
    "print(\"   - Number of layers\")\n",
    "print(\"   - Activation functions\")\n",
    "print(\"   - Regularization techniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Best Practices for PyTorch\n",
    "\n",
    "### 1. Code Organization\n",
    "\n",
    "```python\n",
    "# Good structure:\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define layers here\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define forward pass\n",
    "        return x\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    # Training logic\n",
    "    pass\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    # Validation logic\n",
    "    pass\n",
    "```\n",
    "\n",
    "### 2. Reproducibility\n",
    "\n",
    "```python\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# For GPU reproducibility\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "```\n",
    "\n",
    "### 3. Memory Management\n",
    "\n",
    "```python\n",
    "# Clear gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Use torch.no_grad() for inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "# Move tensors to CPU when done\n",
    "predictions = outputs.cpu().numpy()\n",
    "```\n",
    "\n",
    "### 4. Debugging\n",
    "\n",
    "```python\n",
    "# Check shapes\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Check for NaN\n",
    "assert not torch.isnan(loss), \"Loss is NaN!\"\n",
    "\n",
    "# Visualize gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: {param.grad.norm()}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ What's Next?\n",
    "\n",
    "You've completed the Neural Networks Fundamentals series! Here's where to go next:\n",
    "\n",
    "### 1. Convolutional Neural Networks (CNNs)\n",
    "üìÇ Path: `00-neural-networks/cnn/`\n",
    "\n",
    "**What you'll learn:**\n",
    "- Convolution layers for image processing\n",
    "- Pooling layers for dimensionality reduction\n",
    "- CNN architectures (LeNet, AlexNet, VGG, ResNet)\n",
    "- Transfer learning with pre-trained models\n",
    "\n",
    "**Why it matters:**\n",
    "- State-of-the-art for computer vision\n",
    "- Object detection, image classification, segmentation\n",
    "- Used in self-driving cars, medical imaging, etc.\n",
    "\n",
    "### 2. Recurrent Neural Networks (RNNs)\n",
    "üìÇ Path: `00-neural-networks/rnn/`\n",
    "\n",
    "**What you'll learn:**\n",
    "- RNNs for sequential data\n",
    "- LSTMs and GRUs\n",
    "- Sequence-to-sequence models\n",
    "- Attention mechanisms\n",
    "\n",
    "**Why it matters:**\n",
    "- Natural language processing\n",
    "- Time series prediction\n",
    "- Speech recognition, translation\n",
    "\n",
    "### 3. Transformers\n",
    "üìÇ Path: `01-transformers/`\n",
    "\n",
    "**What you'll learn:**\n",
    "- Self-attention mechanism\n",
    "- Transformer architecture\n",
    "- BERT, GPT, and other transformer models\n",
    "- Fine-tuning pre-trained models\n",
    "\n",
    "**Why it matters:**\n",
    "- Powers ChatGPT, BERT, GPT-4\n",
    "- State-of-the-art for NLP\n",
    "- Also used for vision (Vision Transformers)\n",
    "\n",
    "### 4. Fine-Tuning\n",
    "üìÇ Path: `02-fine-tuning/`\n",
    "\n",
    "**What you'll learn:**\n",
    "- Transfer learning principles\n",
    "- Fine-tuning pre-trained models\n",
    "- LoRA and QLoRA for efficient fine-tuning\n",
    "- Domain adaptation\n",
    "\n",
    "**Why it matters:**\n",
    "- Train powerful models with limited data\n",
    "- Adapt models to your specific task\n",
    "- Cost-effective and time-efficient\n",
    "\n",
    "### 5. RAG (Retrieval-Augmented Generation)\n",
    "üìÇ Path: `03-rag/`\n",
    "\n",
    "**What you'll learn:**\n",
    "- Combining retrieval and generation\n",
    "- Vector databases\n",
    "- Semantic search\n",
    "- Building RAG systems\n",
    "\n",
    "**Why it matters:**\n",
    "- Ground LLMs in factual data\n",
    "- Build chatbots with up-to-date knowledge\n",
    "- Reduce hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "## You've Completed the Neural Networks Fundamentals!\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "**Notebooks 1-3: Building Blocks**\n",
    "- ‚úÖ Understanding neural networks conceptually\n",
    "- ‚úÖ Single neurons and weights\n",
    "- ‚úÖ Activation functions (Sigmoid, ReLU, Softmax)\n",
    "\n",
    "**Notebooks 4-6: Forward Flow**\n",
    "- ‚úÖ Multi-layer architectures\n",
    "- ‚úÖ Forward propagation\n",
    "- ‚úÖ Loss functions\n",
    "\n",
    "**Notebooks 7-8: Learning**\n",
    "- ‚úÖ Backpropagation and chain rule\n",
    "- ‚úÖ Training loops and optimization\n",
    "\n",
    "**Notebooks 9-10: Real World**\n",
    "- ‚úÖ Complete MNIST implementation from scratch\n",
    "- ‚úÖ PyTorch equivalent (10x less code!)\n",
    "\n",
    "### Your Superpowers:\n",
    "\n",
    "1. **Deep Understanding**: You know what happens inside neural networks\n",
    "2. **PyTorch Mastery**: You can build and train models efficiently\n",
    "3. **Debugging Skills**: You can troubleshoot issues because you understand the math\n",
    "4. **Foundation**: You're ready for advanced topics (CNNs, RNNs, Transformers)\n",
    "\n",
    "### Remember:\n",
    "\n",
    "- **Theory ‚Üí Practice ‚Üí Mastery**\n",
    "- You started from scratch and built real neural networks\n",
    "- You understand both the \"what\" and the \"why\"\n",
    "- PyTorch makes you productive, fundamentals make you effective\n",
    "\n",
    "### Keep Learning!\n",
    "\n",
    "The journey doesn't end here. Neural networks are just the beginning:\n",
    "- Explore CNNs for computer vision\n",
    "- Learn RNNs for sequential data\n",
    "- Master Transformers (the future of AI)\n",
    "- Build real-world applications\n",
    "\n",
    "**You're now equipped with the knowledge to understand and build modern AI systems!**\n",
    "\n",
    "## üåü Happy Learning! üåü"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}