{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Fundamentals - Practice Exercises\n",
    "\n",
    "This notebook contains hands-on exercises to reinforce concepts from the 10 core tutorial notebooks. Work through these exercises to deepen your understanding of neural networks from first principles.\n",
    "\n",
    "**Instructions:**\n",
    "- Each exercise corresponds to one or more tutorial notebooks\n",
    "- Try to solve problems independently before checking hints\n",
    "- Solutions are available in `solutions.ipynb`\n",
    "- Experiment and explore beyond the basic requirements\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from viz_utils import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Single Neuron Variations (Notebooks 1-2)\n",
    "\n",
    "**Objective:** Implement different types of single neurons to understand basic building blocks.\n",
    "\n",
    "### Task 1.1: Implement a Linear Neuron\n",
    "\n",
    "Create a neuron that performs only linear transformation (no activation function):\n",
    "$$y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_neuron(inputs, weights, bias):\n",
    "    \"\"\"\n",
    "    Implement a linear neuron.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    inputs : np.ndarray, shape (n_features,)\n",
    "        Input features\n",
    "    weights : np.ndarray, shape (n_features,)\n",
    "        Neuron weights\n",
    "    bias : float\n",
    "        Neuron bias\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : float\n",
    "        Linear combination of inputs\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "test_inputs = np.array([1.0, 2.0, 3.0])\n",
    "test_weights = np.array([0.5, -0.3, 0.8])\n",
    "test_bias = 0.1\n",
    "\n",
    "result = linear_neuron(test_inputs, test_weights, test_bias)\n",
    "print(f\"Linear neuron output: {result}\")\n",
    "print(f\"Expected output: {np.dot(test_inputs, test_weights) + test_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>üí° Hint (Click to expand)</b></summary>\n",
    "\n",
    "Use `np.dot()` for the weighted sum of inputs, then add the bias.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Multi-Input Neuron with Sigmoid\n",
    "\n",
    "Create a neuron that takes 5 inputs and uses sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_neuron(inputs, weights, bias):\n",
    "    \"\"\"\n",
    "    Implement a sigmoid-activated neuron.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    inputs : np.ndarray\n",
    "        Input features\n",
    "    weights : np.ndarray\n",
    "        Neuron weights (same shape as inputs)\n",
    "    bias : float\n",
    "        Neuron bias\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : float\n",
    "        Sigmoid-activated output in [0, 1]\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test with 5 inputs\n",
    "test_inputs = np.random.randn(5)\n",
    "test_weights = np.random.randn(5) * 0.1\n",
    "test_bias = 0.0\n",
    "\n",
    "output = sigmoid_neuron(test_inputs, test_weights, test_bias)\n",
    "print(f\"Sigmoid neuron output: {output:.4f}\")\n",
    "print(f\"Output should be in [0, 1]: {0 <= output <= 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Implement New Activation Function (Notebook 3)\n",
    "\n",
    "**Objective:** Understand activation functions by implementing new ones.\n",
    "\n",
    "### Task 2.1: Implement ELU (Exponential Linear Unit)\n",
    "\n",
    "ELU is defined as:\n",
    "$$\\text{ELU}(x) = \\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha(e^x - 1) & \\text{if } x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "where $\\alpha$ is typically 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(x, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Implement the ELU activation function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : np.ndarray\n",
    "        Input values\n",
    "    alpha : float, default=1.0\n",
    "        ELU hyperparameter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : np.ndarray\n",
    "        ELU-activated values\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def elu_derivative(x, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Implement the derivative of ELU.\n",
    "    \n",
    "    Derivative is:\n",
    "    - 1 if x > 0\n",
    "    - alpha * exp(x) if x <= 0\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test implementation\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = elu(x)\n",
    "dy = elu_derivative(x)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(x, y, 'b-', linewidth=2, label='ELU')\n",
    "ax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax1.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax1.set_title('ELU Activation Function')\n",
    "ax1.set_xlabel('Input')\n",
    "ax1.set_ylabel('Output')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(x, dy, 'r-', linewidth=2, label='ELU Derivative')\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.set_title('ELU Derivative')\n",
    "ax2.set_xlabel('Input')\n",
    "ax2.set_ylabel('Gradient')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>üí° Hint (Click to expand)</b></summary>\n",
    "\n",
    "Use `np.where()` to apply different formulas for positive and negative values. For negative values, use `np.exp(x)` to compute the exponential.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Compare ELU with ReLU\n",
    "\n",
    "Discuss the advantages of ELU over ReLU based on your plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "*(Write your observations here about how ELU handles negative values differently from ReLU and why this might be beneficial)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Build 3-Layer Network Manually (Notebook 4)\n",
    "\n",
    "**Objective:** Understand layer composition by building a network from scratch.\n",
    "\n",
    "### Task 3.1: Implement a Dense Layer Class\n",
    "\n",
    "Create a reusable `DenseLayer` class that can be stacked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    \"\"\"\n",
    "    A fully connected (dense) neural network layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the layer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_inputs : int\n",
    "            Number of input features\n",
    "        n_neurons : int\n",
    "            Number of neurons in this layer\n",
    "        activation : str\n",
    "            Activation function ('relu', 'sigmoid', or 'linear')\n",
    "        \"\"\"\n",
    "        # Initialize weights with small random values\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the layer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : np.ndarray, shape (batch_size, n_inputs)\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : np.ndarray, shape (batch_size, n_neurons)\n",
    "            Layer output after activation\n",
    "        \"\"\"\n",
    "        # Compute linear transformation: Z = X @ W + b\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Apply activation function\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        pass\n",
    "\n",
    "# Test the layer\n",
    "layer = DenseLayer(n_inputs=10, n_neurons=5, activation='relu')\n",
    "test_input = np.random.randn(3, 10)  # 3 samples, 10 features\n",
    "output = layer.forward(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected output shape: (3, 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>üí° Hint (Click to expand)</b></summary>\n",
    "\n",
    "- Initialize weights using `np.random.randn() * 0.01` for small random values\n",
    "- Initialize biases using `np.zeros()`\n",
    "- For matrix multiplication, use `@` operator or `np.dot()`\n",
    "- For ReLU: `np.maximum(0, x)`\n",
    "- For sigmoid: `1 / (1 + np.exp(-x))`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Build a 3-Layer Network\n",
    "\n",
    "Stack three dense layers: 784 ‚Üí 128 ‚Üí 64 ‚Üí 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNetwork:\n",
    "    \"\"\"\n",
    "    A 3-layer neural network for MNIST classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Create three layers\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through all layers.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray, shape (batch_size, 784)\n",
    "            Flattened MNIST images\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : np.ndarray, shape (batch_size, 10)\n",
    "            Class probabilities\n",
    "        \"\"\"\n",
    "        # Pass through each layer sequentially\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "# Test the network\n",
    "network = ThreeLayerNetwork()\n",
    "test_batch = np.random.randn(5, 784)  # 5 images\n",
    "predictions = network.forward(test_batch)\n",
    "\n",
    "print(f\"Input shape: {test_batch.shape}\")\n",
    "print(f\"Output shape: {predictions.shape}\")\n",
    "print(f\"First prediction (10 class scores): {predictions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Trace Forward Propagation (Notebook 5)\n",
    "\n",
    "**Objective:** Understand information flow by manually tracing computations.\n",
    "\n",
    "### Task 4.1: Manual Computation\n",
    "\n",
    "Given this tiny network:\n",
    "- Input: [2.0, -1.0]\n",
    "- Layer 1: 2 neurons, weights = [[0.5, -0.3], [0.2, 0.8]], biases = [0.1, -0.1]\n",
    "- Activation: ReLU\n",
    "\n",
    "Compute the output **by hand** (showing all steps), then verify with code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Manual Calculation:**\n",
    "\n",
    "```\n",
    "Input: x = [2.0, -1.0]\n",
    "\n",
    "Neuron 1:\n",
    "  z1 = (0.5)(2.0) + (-0.3)(-1.0) + 0.1 = ?\n",
    "  a1 = ReLU(z1) = ?\n",
    "\n",
    "Neuron 2:\n",
    "  z2 = (0.2)(2.0) + (0.8)(-1.0) + (-0.1) = ?\n",
    "  a2 = ReLU(z2) = ?\n",
    "\n",
    "Output: [a1, a2] = ?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify your calculation with code\n",
    "x = np.array([2.0, -1.0])\n",
    "W = np.array([[0.5, -0.3], [0.2, 0.8]])\n",
    "b = np.array([0.1, -0.1])\n",
    "\n",
    "# Compute z (pre-activation)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Apply ReLU\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"Output: {output}\")\n",
    "print(\"Does this match your hand calculation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Implement Alternative Loss Function (Notebook 6)\n",
    "\n",
    "**Objective:** Understand loss functions by implementing Mean Absolute Error.\n",
    "\n",
    "### Task 5.1: Implement MAE Loss\n",
    "\n",
    "Mean Absolute Error (MAE) is defined as:\n",
    "$$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Mean Absolute Error loss.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : np.ndarray\n",
    "        True labels\n",
    "    y_pred : np.ndarray\n",
    "        Predicted labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    loss : float\n",
    "        MAE loss value\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def mae_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute derivative of MAE with respect to predictions.\n",
    "    \n",
    "    Derivative is:\n",
    "    - -1 if y_pred < y_true\n",
    "    - +1 if y_pred > y_true\n",
    "    - 0 if y_pred == y_true\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test implementation\n",
    "y_true = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "y_pred = np.array([1.2, 1.8, 3.1, 4.5])\n",
    "\n",
    "mae = mae_loss(y_true, y_pred)\n",
    "grad = mae_derivative(y_true, y_pred)\n",
    "\n",
    "print(f\"MAE Loss: {mae:.4f}\")\n",
    "print(f\"Expected: {np.mean(np.abs(y_true - y_pred)):.4f}\")\n",
    "print(f\"\\nGradient: {grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Compare MSE and MAE\n",
    "\n",
    "When would you prefer MAE over MSE (Mean Squared Error)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "*(Discuss robustness to outliers, gradient behavior, and use cases)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Manual Gradient Computation (Notebook 7)\n",
    "\n",
    "**Objective:** Understand backpropagation by computing gradients manually.\n",
    "\n",
    "### Task 6.1: Compute Gradients for Simple Network\n",
    "\n",
    "Given:\n",
    "- Input: x = 3.0\n",
    "- Weight: w = 0.5\n",
    "- Bias: b = 0.2\n",
    "- Activation: Linear (no activation)\n",
    "- True label: y = 2.0\n",
    "- Loss: MSE = (y - ≈∑)¬≤\n",
    "\n",
    "Compute:\n",
    "1. Forward pass: ≈∑ = wx + b\n",
    "2. Loss: L\n",
    "3. ‚àÇL/‚àÇ≈∑\n",
    "4. ‚àÇL/‚àÇw (using chain rule)\n",
    "5. ‚àÇL/‚àÇb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Manual Calculation:**\n",
    "\n",
    "```\n",
    "Step 1: Forward pass\n",
    "  ≈∑ = w*x + b = 0.5*3.0 + 0.2 = ?\n",
    "\n",
    "Step 2: Compute loss\n",
    "  L = (y - ≈∑)¬≤ = (2.0 - ?)¬≤ = ?\n",
    "\n",
    "Step 3: dL/d≈∑\n",
    "  dL/d≈∑ = 2(≈∑ - y) = ?\n",
    "\n",
    "Step 4: dL/dw (chain rule: dL/dw = dL/d≈∑ * d≈∑/dw)\n",
    "  d≈∑/dw = x = 3.0\n",
    "  dL/dw = dL/d≈∑ * d≈∑/dw = ? * 3.0 = ?\n",
    "\n",
    "Step 5: dL/db\n",
    "  d≈∑/db = 1.0\n",
    "  dL/db = dL/d≈∑ * d≈∑/db = ? * 1.0 = ?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with code\n",
    "x = 3.0\n",
    "w = 0.5\n",
    "b = 0.2\n",
    "y_true = 2.0\n",
    "\n",
    "# Forward pass\n",
    "y_pred = w * x + b\n",
    "loss = (y_true - y_pred) ** 2\n",
    "\n",
    "# Gradients\n",
    "dL_dyhat = 2 * (y_pred - y_true)\n",
    "dL_dw = dL_dyhat * x  # Chain rule\n",
    "dL_db = dL_dyhat * 1.0  # Chain rule\n",
    "\n",
    "print(f\"Prediction: {y_pred}\")\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"dL/dw: {dL_dw}\")\n",
    "print(f\"dL/db: {dL_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 7: Debug Broken Training Loop (Notebook 8)\n",
    "\n",
    "**Objective:** Develop debugging skills by fixing a broken implementation.\n",
    "\n",
    "### Task 7.1: Find and Fix the Bugs\n",
    "\n",
    "The following training loop has **3 bugs**. Find and fix them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buggy_training_loop(X, y, epochs=10, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    A training loop with bugs to fix.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_classes = y.shape[1]\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    W = np.random.randn(n_features, n_classes) * 0.01\n",
    "    b = np.zeros((1, n_classes))\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        z = X @ W - b  # BUG #1: Should this be minus?\n",
    "        \n",
    "        # Softmax activation\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        predictions = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.mean(np.sum(y * np.log(predictions + 1e-8), axis=1))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        dz = predictions - y\n",
    "        dW = (X.T @ dz)  # BUG #2: Missing division by batch size\n",
    "        db = np.sum(dz, axis=0, keepdims=True) / n_samples\n",
    "        \n",
    "        # Update parameters\n",
    "        W = W - learning_rate * dW\n",
    "        b = b + learning_rate * db  # BUG #3: Should this be plus?\n",
    "        \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return W, b, losses\n",
    "\n",
    "# Test with small dataset\n",
    "X_small = np.random.randn(100, 10)\n",
    "y_small = np.eye(3)[np.random.randint(0, 3, 100)]  # 3 classes\n",
    "\n",
    "W, b, losses = buggy_training_loop(X_small, y_small, epochs=10)\n",
    "\n",
    "# Plot losses - should decrease!\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (Should Decrease!)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>üí° Hints (Click to expand)</b></summary>\n",
    "\n",
    "**Bug #1:** Check the forward pass formula. Should bias be added or subtracted?\n",
    "\n",
    "**Bug #2:** Gradient computation should average over the batch. Are you dividing by `n_samples`?\n",
    "\n",
    "**Bug #3:** Gradient descent updates go in the direction that reduces loss. Check the sign!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 8: Hyperparameter Tuning for MNIST (Notebook 9)\n",
    "\n",
    "**Objective:** Improve model performance through systematic experimentation.\n",
    "\n",
    "### Task 8.1: Tune Learning Rate\n",
    "\n",
    "Train models with different learning rates and find the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST (small subset for speed)\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist()\n",
    "\n",
    "# Use only 10,000 samples for faster experimentation\n",
    "X_train_small = X_train[:10000]\n",
    "y_train_small = y_train[:10000]\n",
    "\n",
    "print(f\"Training set: {X_train_small.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTrying learning rate: {lr}\")\n",
    "    \n",
    "    # YOUR CODE HERE:\n",
    "    # 1. Create and train a simple network\n",
    "    # 2. Track final training and validation accuracy\n",
    "    # 3. Store results\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Plot results\n",
    "# YOUR CODE HERE: Create a bar plot comparing accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8.2: Experiment with Network Architecture\n",
    "\n",
    "Try different hidden layer sizes and depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different architectures\n",
    "architectures = [\n",
    "    [784, 64, 10],          # Small\n",
    "    [784, 128, 10],         # Medium\n",
    "    [784, 256, 10],         # Large\n",
    "    [784, 128, 64, 10],     # Two hidden layers\n",
    "    [784, 256, 128, 10],    # Larger with two hidden layers\n",
    "]\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# For each architecture:\n",
    "#   1. Build and train the network\n",
    "#   2. Record accuracy and training time\n",
    "#   3. Compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What did you learn about the trade-off between model size and performance?\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: PyTorch Translation (Notebook 10)\n",
    "\n",
    "**Objective:** Learn to translate between NumPy and PyTorch implementations.\n",
    "\n",
    "### Task 9.1: Convert NumPy Model to PyTorch\n",
    "\n",
    "Implement the same 2-layer network using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MNISTNet(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of MNIST classifier.\n",
    "    \n",
    "    Architecture: 784 -> 128 -> 10\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        # YOUR CODE HERE:\n",
    "        # Define layers using nn.Linear\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE:\n",
    "        # Implement forward pass with ReLU and softmax\n",
    "        pass\n",
    "\n",
    "# Test the model\n",
    "model = MNISTNet()\n",
    "test_input = torch.randn(5, 784)\n",
    "output = model(test_input)\n",
    "\n",
    "print(f\"Model output shape: {output.shape}\")\n",
    "print(f\"Output probabilities sum to 1: {torch.allclose(output.sum(dim=1), torch.ones(5))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>üí° Hint (Click to expand)</b></summary>\n",
    "\n",
    "```python\n",
    "# Define layers\n",
    "self.fc1 = nn.Linear(784, 128)\n",
    "self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "# Forward pass\n",
    "x = torch.relu(self.fc1(x))\n",
    "x = torch.softmax(self.fc2(x), dim=1)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9.2: Train PyTorch Model\n",
    "\n",
    "Implement a training loop using PyTorch's autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch_model(model, X_train, y_train, epochs=10, lr=0.01, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train PyTorch model.\n",
    "    \"\"\"\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Training loop\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Train and compare with NumPy implementation\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 10: Bonus Challenge üåü\n",
    "\n",
    "**Objective:** Apply everything you've learned to a new dataset.\n",
    "\n",
    "### Task 10.1: Fashion-MNIST Classification\n",
    "\n",
    "Fashion-MNIST is similar to MNIST but with clothing items instead of digits.\n",
    "\n",
    "**Challenge:** Build a neural network from scratch (using NumPy) that achieves >85% accuracy on Fashion-MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNIST can be loaded similarly to MNIST\n",
    "# For this exercise, you can use MNIST as a substitute or\n",
    "# download Fashion-MNIST from: https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Load Fashion-MNIST data\n",
    "# 2. Design your network architecture\n",
    "# 3. Implement training loop with proper validation\n",
    "# 4. Tune hyperparameters\n",
    "# 5. Visualize results and misclassifications\n",
    "\n",
    "print(\"Good luck! Remember to:\")\n",
    "print(\"- Try different architectures\")\n",
    "print(\"- Tune learning rate and batch size\")\n",
    "print(\"- Use validation set for early stopping\")\n",
    "print(\"- Visualize training progress\")\n",
    "print(\"- Analyze mistakes to improve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "After completing these exercises, reflect on your learning:\n",
    "\n",
    "1. **What concept was most challenging to implement from scratch?**\n",
    "   \n",
    "   *Your answer:*\n",
    "\n",
    "2. **How does implementing neural networks from scratch help you understand PyTorch/TensorFlow better?**\n",
    "   \n",
    "   *Your answer:*\n",
    "\n",
    "3. **What was the most surprising thing you learned about neural networks?**\n",
    "   \n",
    "   *Your answer:*\n",
    "\n",
    "4. **What would you like to explore next?**\n",
    "   \n",
    "   *Your answer:*\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Congratulations on completing the exercises! Here are some suggestions:\n",
    "\n",
    "1. üìö **Review** the solutions in `solutions.ipynb`\n",
    "2. üî¨ **Experiment** with variations on these exercises\n",
    "3. üèóÔ∏è **Build** your own project using these fundamentals\n",
    "4. üìñ **Study** more advanced topics (CNNs, RNNs, Transformers)\n",
    "5. ü§ù **Share** what you've learned with others\n",
    "\n",
    "Remember: The best way to learn is by doing. Keep experimenting!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}