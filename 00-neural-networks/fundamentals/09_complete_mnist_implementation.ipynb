{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Complete MNIST Implementation from Scratch\n",
    "\n",
    "## You've Learned All the Pieces - Let's Build Something Real!\n",
    "\n",
    "**What you've accomplished so far:**\n",
    "- ‚úÖ Neurons and activation functions\n",
    "- ‚úÖ Multi-layer networks\n",
    "- ‚úÖ Forward propagation\n",
    "- ‚úÖ Loss functions\n",
    "- ‚úÖ Backpropagation\n",
    "- ‚úÖ Training loops\n",
    "\n",
    "**Now**: Let's put it ALL together to solve a real problem!\n",
    "\n",
    "## The Problem: Handwritten Digit Recognition (MNIST)\n",
    "\n",
    "MNIST is the \"Hello World\" of machine learning:\n",
    "- **Task**: Classify handwritten digits (0-9)\n",
    "- **Data**: 70,000 grayscale images of handwritten digits\n",
    "  - 60,000 for training\n",
    "  - 10,000 for testing\n",
    "- **Image size**: 28√ó28 pixels (784 pixels total)\n",
    "- **Challenge**: Handle variations in handwriting styles\n",
    "\n",
    "This is a **real benchmark** used by researchers worldwide!\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "A complete neural network from scratch with:\n",
    "- 3 layers (784 ‚Üí 128 ‚Üí 64 ‚Üí 10)\n",
    "- Mini-batch training\n",
    "- Validation monitoring\n",
    "- >95% accuracy on test set\n",
    "\n",
    "**Everything in pure NumPy** - no frameworks (yet)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np  # For all numerical computations\n",
    "import matplotlib.pyplot as plt  # For visualizations\n",
    "from sklearn.datasets import fetch_openml  # To download MNIST dataset\n",
    "from sklearn.model_selection import train_test_split  # For data splitting\n",
    "from sklearn.metrics import confusion_matrix  # For evaluation\n",
    "import time  # To measure training time\n",
    "import seaborn as sns  # For beautiful confusion matrix\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)  # Same results every time we run\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Explore MNIST Dataset\n",
    "\n",
    "First, let's load the famous MNIST dataset and see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset from OpenML\n",
    "print(\"üì• Downloading MNIST dataset...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')  # Download the dataset\n",
    "\n",
    "# Extract images and labels\n",
    "X = mnist.data.to_numpy()  # Images: 70,000 √ó 784 (each row is a flattened 28√ó28 image)\n",
    "y = mnist.target.to_numpy().astype(int)  # Labels: 70,000 values (0-9)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded!\")\n",
    "print(f\"   Images shape: {X.shape}  (70,000 images √ó 784 pixels)\")\n",
    "print(f\"   Labels shape: {y.shape}  (70,000 labels)\")\n",
    "print(f\"   Pixel values range: {X.min():.0f} to {X.max():.0f}\")\n",
    "print(f\"   Unique labels: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample images\n",
    "fig, axes = plt.subplots(3, 10, figsize=(15, 5))  # 3 rows, 10 columns\n",
    "fig.suptitle('Sample MNIST Images', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):  # Loop through each subplot\n",
    "    # Get a random image\n",
    "    idx = np.random.randint(0, len(X))  # Random index\n",
    "    image = X[idx].reshape(28, 28)  # Reshape flat array to 28√ó28 image\n",
    "    label = y[idx]  # Get corresponding label\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(image, cmap='gray')  # Show grayscale image\n",
    "    ax.set_title(f'Label: {label}', fontsize=10)  # Title with true label\n",
    "    ax.axis('off')  # Hide axes for cleaner look\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüé® These are real handwritten digits from different people!\")\n",
    "print(\"   Notice the variations in writing styles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess the Data\n",
    "\n",
    "Before training, we need to:\n",
    "1. **Normalize** pixel values (0-255 ‚Üí 0-1)\n",
    "2. **One-hot encode** labels (3 ‚Üí [0,0,0,1,0,0,0,0,0,0])\n",
    "3. **Split** into train/validation/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Normalize pixel values to [0, 1] range\n",
    "X_normalized = X / 255.0  # Divide by 255 (max pixel value)\n",
    "print(f\"‚úÖ Normalized pixel values: {X_normalized.min():.2f} to {X_normalized.max():.2f}\")\n",
    "\n",
    "# 2. One-hot encode labels\n",
    "def one_hot_encode(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Convert labels to one-hot encoded vectors.\n",
    "    \n",
    "    Example: 3 ‚Üí [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    \"\"\"\n",
    "    n_samples = len(y)  # Number of samples\n",
    "    y_encoded = np.zeros((n_samples, num_classes))  # Initialize with zeros\n",
    "    y_encoded[np.arange(n_samples), y] = 1  # Set 1 at correct index\n",
    "    return y_encoded\n",
    "\n",
    "y_encoded = one_hot_encode(y)  # Convert all labels\n",
    "print(f\"\\n‚úÖ One-hot encoded labels shape: {y_encoded.shape}\")\n",
    "print(f\"   Example: label {y[0]} ‚Üí {y_encoded[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Split data into train/validation/test sets\n",
    "\n",
    "# First split: separate test set (10,000 images)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_normalized, y_encoded, \n",
    "    test_size=10000,  # 10,000 for testing\n",
    "    random_state=42,  # For reproducibility\n",
    "    stratify=y  # Keep same class distribution\n",
    ")\n",
    "\n",
    "# Second split: separate validation set (10,000 images)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=10000,  # 10,000 for validation\n",
    "    random_state=42,\n",
    "    stratify=np.argmax(y_temp, axis=1)  # Stratify on original labels\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Data split complete!\")\n",
    "print(f\"   Training set:   {X_train.shape[0]:,} images\")\n",
    "print(f\"   Validation set: {X_val.shape[0]:,} images\")\n",
    "print(f\"   Test set:       {X_test.shape[0]:,} images\")\n",
    "print(f\"\\n   Total: {X_train.shape[0] + X_val.shape[0] + X_test.shape[0]:,} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the Neural Network Class\n",
    "\n",
    "Now for the main event - our complete neural network!\n",
    "\n",
    "**Architecture:**\n",
    "- Input layer: 784 neurons (28√ó28 pixels)\n",
    "- Hidden layer 1: 128 neurons + ReLU activation\n",
    "- Hidden layer 2: 64 neurons + ReLU activation\n",
    "- Output layer: 10 neurons + Softmax activation (one per digit)\n",
    "\n",
    "This class contains everything we've learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A complete 3-layer neural network for MNIST classification.\n",
    "    \n",
    "    This brings together everything from notebooks 1-8!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden1_size=128, hidden2_size=64, output_size=10):\n",
    "        \"\"\"\n",
    "        Initialize the network with random weights.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features (784 for MNIST)\n",
    "            hidden1_size: Number of neurons in first hidden layer\n",
    "            hidden2_size: Number of neurons in second hidden layer\n",
    "            output_size: Number of output classes (10 for digits 0-9)\n",
    "        \"\"\"\n",
    "        # Store layer sizes\n",
    "        self.input_size = input_size\n",
    "        self.hidden1_size = hidden1_size\n",
    "        self.hidden2_size = hidden2_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights using He initialization (good for ReLU)\n",
    "        # He initialization: multiply by sqrt(2/n) where n is input size\n",
    "        self.W1 = np.random.randn(input_size, hidden1_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden1_size))  # Biases start at zero\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden1_size, hidden2_size) * np.sqrt(2.0 / hidden1_size)\n",
    "        self.b2 = np.zeros((1, hidden2_size))\n",
    "        \n",
    "        self.W3 = np.random.randn(hidden2_size, output_size) * np.sqrt(2.0 / hidden2_size)\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "        \n",
    "        # For tracking training history\n",
    "        self.train_losses = []  # Training loss per epoch\n",
    "        self.val_losses = []    # Validation loss per epoch\n",
    "        self.train_accuracies = []  # Training accuracy per epoch\n",
    "        self.val_accuracies = []    # Validation accuracy per epoch\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        \"\"\"ReLU activation: max(0, Z)\"\"\"\n",
    "        return np.maximum(0, Z)  # Element-wise maximum\n",
    "    \n",
    "    def relu_derivative(self, Z):\n",
    "        \"\"\"Derivative of ReLU: 1 if Z > 0, else 0\"\"\"\n",
    "        return (Z > 0).astype(float)  # 1 where Z > 0, 0 elsewhere\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        \"\"\"\n",
    "        Softmax activation: converts logits to probabilities.\n",
    "        \n",
    "        Uses numerical stability trick: subtract max before exp\n",
    "        \"\"\"\n",
    "        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))  # Prevent overflow\n",
    "        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)  # Normalize to sum to 1\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through all layers.\n",
    "        \n",
    "        Returns:\n",
    "            A3: Output activations (probabilities for each class)\n",
    "        \"\"\"\n",
    "        # Layer 1: Input ‚Üí Hidden1\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1  # Linear transformation\n",
    "        self.A1 = self.relu(self.Z1)  # ReLU activation\n",
    "        \n",
    "        # Layer 2: Hidden1 ‚Üí Hidden2\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2  # Linear transformation\n",
    "        self.A2 = self.relu(self.Z2)  # ReLU activation\n",
    "        \n",
    "        # Layer 3: Hidden2 ‚Üí Output\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3  # Linear transformation\n",
    "        self.A3 = self.softmax(self.Z3)  # Softmax activation (probabilities)\n",
    "        \n",
    "        return self.A3  # Return predicted probabilities\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels (one-hot encoded)\n",
    "            y_pred: Predicted probabilities\n",
    "        \n",
    "        Returns:\n",
    "            Average loss over all samples\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]  # Number of samples\n",
    "        # Cross-entropy: -sum(y_true * log(y_pred))\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y_true, learning_rate):\n",
    "        \"\"\"\n",
    "        Backpropagation: compute gradients and update weights.\n",
    "        \n",
    "        This is where all the calculus from notebook 7 happens!\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # Batch size\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dZ3 = self.A3 - y_true  # Derivative of softmax + cross-entropy\n",
    "        dW3 = np.dot(self.A2.T, dZ3) / m  # Gradient for W3\n",
    "        db3 = np.sum(dZ3, axis=0, keepdims=True) / m  # Gradient for b3\n",
    "        \n",
    "        # Hidden layer 2 gradient\n",
    "        dA2 = np.dot(dZ3, self.W3.T)  # Backpropagate through W3\n",
    "        dZ2 = dA2 * self.relu_derivative(self.Z2)  # Apply ReLU derivative\n",
    "        dW2 = np.dot(self.A1.T, dZ2) / m  # Gradient for W2\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m  # Gradient for b2\n",
    "        \n",
    "        # Hidden layer 1 gradient\n",
    "        dA1 = np.dot(dZ2, self.W2.T)  # Backpropagate through W2\n",
    "        dZ1 = dA1 * self.relu_derivative(self.Z1)  # Apply ReLU derivative\n",
    "        dW1 = np.dot(X.T, dZ1) / m  # Gradient for W1\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m  # Gradient for b1\n",
    "        \n",
    "        # Update weights using gradient descent\n",
    "        self.W3 -= learning_rate * dW3  # W3 = W3 - lr * gradient\n",
    "        self.b3 -= learning_rate * db3\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Returns:\n",
    "            Predicted class labels (0-9)\n",
    "        \"\"\"\n",
    "        probabilities = self.forward(X)  # Get probabilities\n",
    "        return np.argmax(probabilities, axis=1)  # Return class with highest probability\n",
    "    \n",
    "    def accuracy(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Compute classification accuracy.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data\n",
    "            y_true: True labels (one-hot encoded)\n",
    "        \n",
    "        Returns:\n",
    "            Accuracy as a percentage\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)  # Get predictions\n",
    "        true_labels = np.argmax(y_true, axis=1)  # Convert one-hot to labels\n",
    "        return np.mean(predictions == true_labels) * 100  # Percentage correct\n",
    "\n",
    "print(\"‚úÖ NeuralNetwork class created!\")\n",
    "print(\"   This contains everything we learned:\")\n",
    "print(\"   - Forward propagation (notebook 5)\")\n",
    "print(\"   - Activation functions (notebook 3)\")\n",
    "print(\"   - Loss computation (notebook 6)\")\n",
    "print(\"   - Backpropagation (notebook 7)\")\n",
    "print(\"   - Weight updates (notebook 8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training with Mini-Batches\n",
    "\n",
    "Let's train our network using mini-batch gradient descent!\n",
    "\n",
    "**Training strategy:**\n",
    "- Batch size: 32 (process 32 images at a time)\n",
    "- Learning rate: 0.01\n",
    "- Epochs: 20 (go through entire dataset 20 times)\n",
    "- Monitor validation performance to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Split data into mini-batches for training.\n",
    "    \n",
    "    Args:\n",
    "        X: Input data\n",
    "        y: Labels\n",
    "        batch_size: Size of each batch\n",
    "    \n",
    "    Returns:\n",
    "        List of (X_batch, y_batch) tuples\n",
    "    \"\"\"\n",
    "    m = X.shape[0]  # Total number of samples\n",
    "    mini_batches = []  # List to store batches\n",
    "    \n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(m)  # Random order\n",
    "    X_shuffled = X[indices]  # Shuffle inputs\n",
    "    y_shuffled = y[indices]  # Shuffle labels (same order)\n",
    "    \n",
    "    # Create batches\n",
    "    num_complete_batches = m // batch_size  # Number of full batches\n",
    "    \n",
    "    for i in range(num_complete_batches):\n",
    "        start = i * batch_size  # Start index\n",
    "        end = start + batch_size  # End index\n",
    "        X_batch = X_shuffled[start:end]  # Get batch of inputs\n",
    "        y_batch = y_shuffled[start:end]  # Get batch of labels\n",
    "        mini_batches.append((X_batch, y_batch))  # Add to list\n",
    "    \n",
    "    # Handle remaining samples (if any)\n",
    "    if m % batch_size != 0:\n",
    "        start = num_complete_batches * batch_size\n",
    "        X_batch = X_shuffled[start:]\n",
    "        y_batch = y_shuffled[start:]\n",
    "        mini_batches.append((X_batch, y_batch))\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "print(\"‚úÖ Mini-batch creation function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural network\n",
    "model = NeuralNetwork(\n",
    "    input_size=784,    # 28√ó28 pixels\n",
    "    hidden1_size=128,  # First hidden layer\n",
    "    hidden2_size=64,   # Second hidden layer\n",
    "    output_size=10     # 10 digit classes\n",
    ")\n",
    "\n",
    "print(\"üß† Neural Network initialized!\")\n",
    "print(f\"   Architecture: {model.input_size} ‚Üí {model.hidden1_size} ‚Üí {model.hidden2_size} ‚Üí {model.output_size}\")\n",
    "print(f\"   Total parameters: {model.W1.size + model.b1.size + model.W2.size + model.b2.size + model.W3.size + model.b3.size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "batch_size = 32      # Process 32 images at a time\n",
    "learning_rate = 0.01  # Step size for gradient descent\n",
    "epochs = 20          # Number of times to go through entire dataset\n",
    "\n",
    "print(\"üéØ Starting training...\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Learning rate: {learning_rate}\")\n",
    "print(f\"   Epochs: {epochs}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Create mini-batches for this epoch\n",
    "    mini_batches = create_mini_batches(X_train, y_train, batch_size)\n",
    "    \n",
    "    # Train on each mini-batch\n",
    "    for X_batch, y_batch in tqdm(mini_batches, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False):\n",
    "        # Forward pass\n",
    "        model.forward(X_batch)\n",
    "        # Backward pass and update weights\n",
    "        model.backward(X_batch, y_batch, learning_rate)\n",
    "    \n",
    "    # Evaluate on full training and validation sets\n",
    "    train_pred = model.forward(X_train)  # Predictions on training set\n",
    "    train_loss = model.compute_loss(y_train, train_pred)  # Training loss\n",
    "    train_acc = model.accuracy(X_train, y_train)  # Training accuracy\n",
    "    \n",
    "    val_pred = model.forward(X_val)  # Predictions on validation set\n",
    "    val_loss = model.compute_loss(y_val, val_pred)  # Validation loss\n",
    "    val_acc = model.accuracy(X_val, y_val)  # Validation accuracy\n",
    "    \n",
    "    # Store metrics\n",
    "    model.train_losses.append(train_loss)\n",
    "    model.val_losses.append(val_loss)\n",
    "    model.train_accuracies.append(train_acc)\n",
    "    model.val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Record end time\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"‚úÖ Training complete in {training_time:.2f} seconds!\")\n",
    "print(f\"   Final training accuracy: {model.train_accuracies[-1]:.2f}%\")\n",
    "print(f\"   Final validation accuracy: {model.val_accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Training Progress\n",
    "\n",
    "Let's see how our network learned over time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(model.train_losses, label='Training Loss', linewidth=2, marker='o')\n",
    "ax1.plot(model.val_losses, label='Validation Loss', linewidth=2, marker='s')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Loss Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(model.train_accuracies, label='Training Accuracy', linewidth=2, marker='o')\n",
    "ax2.plot(model.val_accuracies, label='Validation Accuracy', linewidth=2, marker='s')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Training curves show:\")\n",
    "print(\"   - Loss decreases over time (good!)\")\n",
    "print(\"   - Accuracy increases over time (good!)\")\n",
    "print(\"   - Training and validation curves are close (not overfitting!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate on Test Set\n",
    "\n",
    "Now let's see how well our model performs on completely unseen data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_predictions = model.predict(X_test)  # Get predictions\n",
    "test_accuracy = model.accuracy(X_test, y_test)  # Compute accuracy\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ TEST SET PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n   Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"   Correct predictions: {int(test_accuracy * len(X_test) / 100):,} / {len(X_test):,}\")\n",
    "print(f\"   Wrong predictions: {len(X_test) - int(test_accuracy * len(X_test) / 100):,}\")\n",
    "\n",
    "if test_accuracy > 95:\n",
    "    print(\"\\n   üéâ EXCELLENT! Over 95% accuracy!\")\n",
    "elif test_accuracy > 90:\n",
    "    print(\"\\n   üëç GOOD! Over 90% accuracy!\")\n",
    "else:\n",
    "    print(\"\\n   üìö Room for improvement. Try training longer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Confusion Matrix\n",
    "\n",
    "Let's see which digits the model confuses with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "true_labels = np.argmax(y_test, axis=1)  # Convert one-hot to labels\n",
    "cm = confusion_matrix(true_labels, test_predictions)  # Compute matrix\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=range(10), yticklabels=range(10),\n",
    "            cbar_kws={'label': 'Number of predictions'})\n",
    "plt.xlabel('Predicted Label', fontsize=13)\n",
    "plt.ylabel('True Label', fontsize=13)\n",
    "plt.title('Confusion Matrix - MNIST Test Set', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Confusion Matrix Interpretation:\")\n",
    "print(\"   - Diagonal: correct predictions (darker is better)\")\n",
    "print(\"   - Off-diagonal: mistakes\")\n",
    "print(\"   - Look for patterns: which digits are confused?\")\n",
    "\n",
    "# Find most confused pairs\n",
    "cm_no_diag = cm.copy()\n",
    "np.fill_diagonal(cm_no_diag, 0)  # Remove diagonal\n",
    "max_confusion = np.unravel_index(cm_no_diag.argmax(), cm_no_diag.shape)\n",
    "print(f\"\\n   Most confused: {max_confusion[0]} ‚Üî {max_confusion[1]} ({cm_no_diag[max_confusion]} mistakes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Predictions\n",
    "\n",
    "Let's look at some correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correct and incorrect predictions\n",
    "correct_idx = np.where(test_predictions == true_labels)[0]  # Indices of correct predictions\n",
    "incorrect_idx = np.where(test_predictions != true_labels)[0]  # Indices of mistakes\n",
    "\n",
    "# Visualize correct predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('‚úÖ Correct Predictions', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = correct_idx[np.random.randint(0, len(correct_idx))]  # Random correct prediction\n",
    "    image = X_test[idx].reshape(28, 28)  # Reshape to image\n",
    "    true_label = true_labels[idx]  # True label\n",
    "    pred_label = test_predictions[idx]  # Predicted label\n",
    "    \n",
    "    # Get prediction confidence (probability)\n",
    "    probs = model.forward(X_test[idx:idx+1])[0]  # Get probabilities\n",
    "    confidence = probs[pred_label] * 100  # Confidence in prediction\n",
    "    \n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.set_title(f'True: {true_label} | Pred: {pred_label}\\nConfidence: {confidence:.1f}%', \n",
    "                 fontsize=10, color='green')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Model got {len(correct_idx):,} predictions correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize incorrect predictions\n",
    "if len(incorrect_idx) > 0:  # Only if there are mistakes\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    fig.suptitle('‚ùå Incorrect Predictions (Learning from Mistakes)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(incorrect_idx):  # If we have enough mistakes\n",
    "            idx = incorrect_idx[i]  # Get mistake index\n",
    "            image = X_test[idx].reshape(28, 28)  # Reshape to image\n",
    "            true_label = true_labels[idx]  # True label\n",
    "            pred_label = test_predictions[idx]  # Predicted label\n",
    "            \n",
    "            # Get prediction confidence\n",
    "            probs = model.forward(X_test[idx:idx+1])[0]  # Get probabilities\n",
    "            confidence = probs[pred_label] * 100  # Confidence in prediction\n",
    "            \n",
    "            ax.imshow(image, cmap='gray')\n",
    "            ax.set_title(f'True: {true_label} | Pred: {pred_label}\\nConfidence: {confidence:.1f}%', \n",
    "                         fontsize=10, color='red')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')  # Hide extra subplots\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚ùå Model made {len(incorrect_idx)} mistakes\")\n",
    "    print(\"   Notice: Some mistakes are understandable (ambiguous handwriting)!\")\n",
    "else:\n",
    "    print(\"\\nüéâ Perfect score! No mistakes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize What the Network Learned\n",
    "\n",
    "Let's peek inside and see what features the first layer learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weights of first layer\n",
    "# Each neuron in the first layer has 784 weights (one per pixel)\n",
    "# These weights show what pattern each neuron is looking for\n",
    "\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "fig.suptitle('First Layer Weights (What Each Neuron Looks For)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < 32:  # Show first 32 neurons\n",
    "        weights = model.W1[:, i].reshape(28, 28)  # Reshape weights to 28√ó28 image\n",
    "        ax.imshow(weights, cmap='RdBu', vmin=-0.5, vmax=0.5)  # Blue=negative, Red=positive\n",
    "        ax.set_title(f'Neuron {i+1}', fontsize=9)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Weight Visualization:\")\n",
    "print(\"   - Red areas: neuron activates strongly for bright pixels\")\n",
    "print(\"   - Blue areas: neuron activates strongly for dark pixels\")\n",
    "print(\"   - Each neuron learns to detect different features (edges, curves, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Activation Maps\n",
    "\n",
    "Let's see how neurons activate for a specific image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random test image\n",
    "sample_idx = np.random.randint(0, len(X_test))\n",
    "sample_image = X_test[sample_idx:sample_idx+1]  # Keep 2D shape\n",
    "sample_label = true_labels[sample_idx]\n",
    "\n",
    "# Forward pass to get activations\n",
    "model.forward(sample_image)\n",
    "\n",
    "# Create visualization\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Original image\n",
    "ax1 = plt.subplot(1, 4, 1)\n",
    "ax1.imshow(sample_image.reshape(28, 28), cmap='gray')\n",
    "ax1.set_title(f'Input Image\\n(Digit: {sample_label})', fontsize=12, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Layer 1 activations\n",
    "ax2 = plt.subplot(1, 4, 2)\n",
    "ax2.bar(range(model.hidden1_size), model.A1[0])  # Bar plot of activations\n",
    "ax2.set_title(f'Layer 1 Activations\\n({model.hidden1_size} neurons)', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Neuron Index')\n",
    "ax2.set_ylabel('Activation Value')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Layer 2 activations\n",
    "ax3 = plt.subplot(1, 4, 3)\n",
    "ax3.bar(range(model.hidden2_size), model.A2[0])  # Bar plot of activations\n",
    "ax3.set_title(f'Layer 2 Activations\\n({model.hidden2_size} neurons)', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Neuron Index')\n",
    "ax3.set_ylabel('Activation Value')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Output layer activations (probabilities)\n",
    "ax4 = plt.subplot(1, 4, 4)\n",
    "bars = ax4.bar(range(10), model.A3[0])  # Bar plot of probabilities\n",
    "# Highlight predicted class\n",
    "predicted_class = np.argmax(model.A3[0])\n",
    "bars[predicted_class].set_color('green')\n",
    "ax4.set_title(f'Output Probabilities\\n(Predicted: {predicted_class})', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Digit Class')\n",
    "ax4.set_ylabel('Probability')\n",
    "ax4.set_xticks(range(10))\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Activation Flow:\")\n",
    "print(\"   1. Input: 28√ó28 image (784 pixels)\")\n",
    "print(\"   2. Layer 1: Different neurons activate for different features\")\n",
    "print(\"   3. Layer 2: Combines Layer 1 features into higher-level patterns\")\n",
    "print(\"   4. Output: Probabilities for each digit (0-9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Performance Analysis\n",
    "\n",
    "Let's analyze which digits are hardest to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-class accuracy\n",
    "per_class_accuracy = {}\n",
    "\n",
    "for digit in range(10):\n",
    "    # Get indices for this digit\n",
    "    digit_indices = np.where(true_labels == digit)[0]\n",
    "    # Get predictions for this digit\n",
    "    digit_predictions = test_predictions[digit_indices]\n",
    "    # Compute accuracy\n",
    "    accuracy = np.mean(digit_predictions == digit) * 100\n",
    "    per_class_accuracy[digit] = accuracy\n",
    "\n",
    "# Plot per-class accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(per_class_accuracy.keys(), per_class_accuracy.values(), \n",
    "               color=['green' if v > 95 else 'orange' if v > 90 else 'red' \n",
    "                      for v in per_class_accuracy.values()])\n",
    "plt.xlabel('Digit', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('Per-Digit Classification Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(10))\n",
    "plt.ylim([80, 100])  # Zoom in to see differences\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, (digit, acc) in zip(bars, per_class_accuracy.items()):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{acc:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find hardest and easiest digits\n",
    "hardest_digit = min(per_class_accuracy, key=per_class_accuracy.get)\n",
    "easiest_digit = max(per_class_accuracy, key=per_class_accuracy.get)\n",
    "\n",
    "print(\"\\nüìä Per-Digit Analysis:\")\n",
    "print(f\"   Easiest digit: {easiest_digit} ({per_class_accuracy[easiest_digit]:.2f}% accuracy)\")\n",
    "print(f\"   Hardest digit: {hardest_digit} ({per_class_accuracy[hardest_digit]:.2f}% accuracy)\")\n",
    "print(\"\\n   Why some digits are harder:\")\n",
    "print(\"   - Similar shapes (e.g., 4 and 9, 3 and 8)\")\n",
    "print(\"   - Variation in handwriting styles\")\n",
    "print(\"   - Less training examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "## You Just Built a Real Neural Network from Scratch!\n",
    "\n",
    "**What you accomplished:**\n",
    "- ‚úÖ Loaded and preprocessed 70,000 images\n",
    "- ‚úÖ Built a 3-layer neural network (>100,000 parameters)\n",
    "- ‚úÖ Implemented forward propagation\n",
    "- ‚úÖ Implemented backpropagation through all layers\n",
    "- ‚úÖ Trained with mini-batch gradient descent\n",
    "- ‚úÖ Achieved >95% accuracy on MNIST\n",
    "- ‚úÖ Visualized what the network learned\n",
    "\n",
    "**This is REAL machine learning** - the same techniques used in production systems!\n",
    "\n",
    "## But Here's the Thing...\n",
    "\n",
    "This took about **500 lines of code** and required deep understanding of:\n",
    "- Matrix operations\n",
    "- Calculus (derivatives)\n",
    "- Optimization algorithms\n",
    "- Numerical stability\n",
    "\n",
    "### What if I told you PyTorch can do this in ~50 lines?\n",
    "\n",
    "**Next notebook**: See how deep learning frameworks make this MUCH easier!\n",
    "\n",
    "But remember: **You now understand what's happening under the hood** - that's invaluable when debugging or optimizing models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}