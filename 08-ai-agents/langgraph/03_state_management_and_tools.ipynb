{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Management and Tool Integration\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll explore:\n",
    "- Advanced state management patterns\n",
    "- Integrating external tools with agents\n",
    "- Building agents with memory\n",
    "- Checkpointing and persistence\n",
    "\n",
    "These are essential for building production-ready AI agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Advanced State Management\n",
    "\n",
    "### Complex State Schemas\n",
    "\n",
    "Real agents need to track multiple pieces of information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "class ComplexAgentState(TypedDict):\n",
    "    # Conversation history\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    \n",
    "    # Agent's internal thoughts\n",
    "    thoughts: Annotated[list[str], operator.add]\n",
    "    \n",
    "    # Tools used and their outputs\n",
    "    tool_history: Annotated[list[dict], operator.add]\n",
    "    \n",
    "    # Current task decomposition\n",
    "    current_plan: list[str]\n",
    "    \n",
    "    # Metadata\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    \n",
    "    # Error tracking\n",
    "    errors: Annotated[list[str], operator.add]\n",
    "    \n",
    "    # Final output\n",
    "    final_answer: str | None"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Reducers\n",
    "\n",
    "Custom logic for how state updates are merged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from typing import Any\n",
    "\n",
    "def merge_unique_items(existing: list, new: list) -> list:\n",
    "    \"\"\"Merge lists while keeping unique items\"\"\"\n",
    "    combined = existing + new\n",
    "    return list(dict.fromkeys(combined))  # Remove duplicates\n",
    "\n",
    "def merge_dicts_deep(existing: dict, new: dict) -> dict:\n",
    "    \"\"\"Deep merge dictionaries\"\"\"\n",
    "    result = existing.copy()\n",
    "    for key, value in new.items():\n",
    "        if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n",
    "            result[key] = merge_dicts_deep(result[key], value)\n",
    "        else:\n",
    "            result[key] = value\n",
    "    return result\n",
    "\n",
    "class SmartState(TypedDict):\n",
    "    tags: Annotated[list, merge_unique_items]\n",
    "    metadata: Annotated[dict, merge_dicts_deep]\n",
    "    counter: int  # Default: replace"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tool Integration\n",
    "\n",
    "### Defining Tools\n",
    "\n",
    "Tools are functions that agents can call to interact with the world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain_core.tools import tool\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current time.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "@tool\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "    \"\"\"\n",
    "    # Simulated web search\n",
    "    return f\"Search results for '{query}': [Result 1, Result 2, Result 3]\"\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Safely evaluate a mathematical expression.\n",
    "    \n",
    "    Args:\n",
    "        expression: Math expression like '2 + 2' or '10 * 5'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Safe evaluation (in production, use a proper parser)\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def read_file(filepath: str) -> str:\n",
    "    \"\"\"Read contents of a file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "# Collect all tools\n",
    "tools = [get_current_time, search_web, calculate, read_file]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Tool-Using Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# 1. Define state\n",
    "class ToolAgentState(TypedDict):\n",
    "    messages: Annotated[list, operator.add]\n",
    "    iterations: int\n",
    "\n",
    "# 2. Initialize LLM with tools\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# 3. Tool executor\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "# 4. Define agent node\n",
    "def call_agent(state: ToolAgentState) -> ToolAgentState:\n",
    "    \"\"\"Agent decides what to do next\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"iterations\": state[\"iterations\"] + 1\n",
    "    }\n",
    "\n",
    "# 5. Define tool execution node\n",
    "def execute_tools(state: ToolAgentState) -> ToolAgentState:\n",
    "    \"\"\"Execute tools requested by the agent\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # Execute each tool call\n",
    "    tool_messages = []\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        result = tool_executor.invoke(tool_call)\n",
    "        tool_messages.append(\n",
    "            ToolMessage(\n",
    "                content=str(result),\n",
    "                tool_call_id=tool_call[\"id\"]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return {\"messages\": tool_messages, \"iterations\": state[\"iterations\"]}\n",
    "\n",
    "# 6. Define routing\n",
    "def should_continue(state: ToolAgentState) -> str:\n",
    "    \"\"\"Determine next step\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # Check iteration limit\n",
    "    if state[\"iterations\"] >= 10:\n",
    "        return \"end\"\n",
    "    \n",
    "    # If agent called tools, execute them\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"continue\"\n",
    "    \n",
    "    # Otherwise, we're done\n",
    "    return \"end\"\n",
    "\n",
    "# 7. Build graph\n",
    "workflow = StateGraph(ToolAgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_agent)\n",
    "workflow.add_node(\"tools\", execute_tools)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# 8. Compile\n",
    "agent_app = workflow.compile()\n",
    "\n",
    "# 9. Test the agent\n",
    "print(\"Tool-Using Agent Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = agent_app.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What time is it? Then calculate 15 * 24\")],\n",
    "    \"iterations\": 0\n",
    "})\n",
    "\n",
    "print(\"\\nFinal Response:\")\n",
    "print(result[\"messages\"][-1].content)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Flow Visualization\n",
    "\n",
    "```\n",
    "User: \"What time is it? Then calculate 15 * 24\"\n",
    "   │\n",
    "   ▼\n",
    "┌──────────┐\n",
    "│  Agent   │ → Decides to call get_current_time()\n",
    "└────┬─────┘\n",
    "     │\n",
    "     ▼\n",
    "┌──────────┐\n",
    "│  Tools   │ → Executes: \"2024-01-15 14:30:00\"\n",
    "└────┬─────┘\n",
    "     │\n",
    "     ▼\n",
    "┌──────────┐\n",
    "│  Agent   │ → Decides to call calculate(\"15 * 24\")\n",
    "└────┬─────┘\n",
    "     │\n",
    "     ▼\n",
    "┌──────────┐\n",
    "│  Tools   │ → Executes: \"360\"\n",
    "└────┬─────┘\n",
    "     │\n",
    "     ▼\n",
    "┌──────────┐\n",
    "│  Agent   │ → Formulates final answer\n",
    "└────┬─────┘\n",
    "     │\n",
    "     ▼\n",
    "   END\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Persistence and Checkpointing\n",
    "\n",
    "### Why Persistence?\n",
    "\n",
    "Persistence allows agents to:\n",
    "- Resume interrupted conversations\n",
    "- Maintain long-term memory\n",
    "- Recover from failures\n",
    "- Support human-in-the-loop workflows\n",
    "\n",
    "### Using Memory Checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Create memory checkpointer\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile graph with checkpointing\n",
    "persistent_agent = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Configuration for this conversation\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation-1\"}}\n",
    "\n",
    "# First interaction\n",
    "print(\"First interaction:\")\n",
    "result1 = persistent_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(\"My name is Alice\")], \"iterations\": 0},\n",
    "    config\n",
    ")\n",
    "print(result1[\"messages\"][-1].content)\n",
    "\n",
    "# Second interaction (agent remembers context)\n",
    "print(\"\\nSecond interaction (in same thread):\")\n",
    "result2 = persistent_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(\"What's my name?\")], \"iterations\": 0},\n",
    "    config\n",
    ")\n",
    "print(result2[\"messages\"][-1].content)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLite Checkpointer (Production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# Create SQLite checkpointer\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "    # Compile with persistence\n",
    "    persistent_app = workflow.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    # Use the agent\n",
    "    config = {\"configurable\": {\"thread_id\": \"user-123\"}}\n",
    "    result = persistent_app.invoke(\n",
    "        {\"messages\": [HumanMessage(\"Hello!\")], \"iterations\": 0},\n",
    "        config\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Human-in-the-Loop\n",
    "\n",
    "### Interrupt for Approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "class HumanLoopState(TypedDict):\n",
    "    request: str\n",
    "    approved: bool | None\n",
    "    result: str\n",
    "\n",
    "def make_request(state: HumanLoopState) -> HumanLoopState:\n",
    "    \"\"\"Agent makes a request that needs approval\"\"\"\n",
    "    state[\"request\"] = \"I want to delete all files\"\n",
    "    return state\n",
    "\n",
    "def execute_if_approved(state: HumanLoopState) -> HumanLoopState:\n",
    "    \"\"\"Execute only if approved\"\"\"\n",
    "    if state[\"approved\"]:\n",
    "        state[\"result\"] = \"Files deleted\"\n",
    "    else:\n",
    "        state[\"result\"] = \"Action cancelled\"\n",
    "    return state\n",
    "\n",
    "# Build graph\n",
    "workflow = StateGraph(HumanLoopState)\n",
    "workflow.add_node(\"request\", make_request)\n",
    "workflow.add_node(\"execute\", execute_if_approved)\n",
    "\n",
    "workflow.set_entry_point(\"request\")\n",
    "workflow.add_edge(\"request\", \"execute\")\n",
    "workflow.add_edge(\"execute\", END)\n",
    "\n",
    "# Compile with interrupt\n",
    "memory = MemorySaver()\n",
    "app_with_human = workflow.compile(\n",
    "    checkpointer=memory,\n",
    "    interrupt_before=[\"execute\"]  # Pause before execute\n",
    ")\n",
    "\n",
    "# Run and pause\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "result = app_with_human.invoke(\n",
    "    {\"request\": \"\", \"approved\": None, \"result\": \"\"},\n",
    "    config\n",
    ")\n",
    "\n",
    "print(f\"Agent requests: {result['request']}\")\n",
    "print(\"Waiting for human approval...\")\n",
    "\n",
    "# Human approves/rejects\n",
    "result[\"approved\"] = False  # Human rejects\n",
    "\n",
    "# Resume execution\n",
    "final = app_with_human.invoke(result, config)\n",
    "print(f\"Result: {final['result']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Memory Strategies\n",
    "\n",
    "### Short-term vs Long-term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MemoryState(TypedDict):\n",
    "    # Short-term: Recent conversation (limited)\n",
    "    recent_messages: Annotated[list, lambda x, y: (x + y)[-10:]]  # Keep last 10\n",
    "    \n",
    "    # Long-term: Important facts (unlimited)\n",
    "    facts: Annotated[dict, merge_dicts_deep]\n",
    "    \n",
    "    # Working memory: Current task context\n",
    "    working_context: str\n",
    "\n",
    "def extract_facts(state: MemoryState) -> MemoryState:\n",
    "    \"\"\"Extract important information to long-term memory\"\"\"\n",
    "    # Use LLM to identify facts from recent messages\n",
    "    new_facts = {\n",
    "        \"user_name\": \"Alice\",\n",
    "        \"preferences\": {\"language\": \"Python\"}\n",
    "    }\n",
    "    state[\"facts\"] = new_facts\n",
    "    return state"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store Memory (for Semantic Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "class VectorMemoryState(TypedDict):\n",
    "    messages: Annotated[list, operator.add]\n",
    "    vector_store: Any  # FAISS vector store\n",
    "    relevant_memories: list[str]\n",
    "\n",
    "def retrieve_relevant_memories(state: VectorMemoryState) -> VectorMemoryState:\n",
    "    \"\"\"Search vector store for relevant past interactions\"\"\"\n",
    "    current_query = state[\"messages\"][-1].content\n",
    "    \n",
    "    # Search for similar past conversations\n",
    "    if state[\"vector_store\"]:\n",
    "        results = state[\"vector_store\"].similarity_search(current_query, k=3)\n",
    "        state[\"relevant_memories\"] = [doc.page_content for doc in results]\n",
    "    \n",
    "    return state\n",
    "\n",
    "def store_interaction(state: VectorMemoryState) -> VectorMemoryState:\n",
    "    \"\"\"Add current interaction to vector store\"\"\"\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    \n",
    "    if state[\"vector_store\"]:\n",
    "        state[\"vector_store\"].add_texts([last_message])\n",
    "    \n",
    "    return state"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Build a Calculator Agent\n",
    "Create an agent that:\n",
    "1. Can solve multi-step math problems\n",
    "2. Uses the calculate tool\n",
    "3. Shows its reasoning\n",
    "\n",
    "### Exercise 2: Add Error Handling\n",
    "Extend an agent to:\n",
    "1. Catch and handle tool errors\n",
    "2. Retry failed operations\n",
    "3. Report errors to the user\n",
    "\n",
    "### Exercise 3: Build a Research Agent\n",
    "Create an agent that:\n",
    "1. Searches for information\n",
    "2. Synthesizes multiple sources\n",
    "3. Cites its sources\n",
    "\n",
    "### Exercise 4: Implement Conversation Memory\n",
    "Build an agent that:\n",
    "1. Remembers user preferences\n",
    "2. Refers to earlier conversation\n",
    "3. Persists across sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Complex state** requires careful schema design and reducers\n",
    "- **Tools** extend agent capabilities to interact with the world\n",
    "- **Persistence** enables long-running conversations and recovery\n",
    "- **Human-in-the-loop** adds safety and control\n",
    "- **Memory strategies** balance context vs. performance\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook:\n",
    "- Advanced agent patterns (ReAct, Plan-Execute, Reflection)\n",
    "- Multi-agent systems\n",
    "- Production optimization\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: [04_advanced_patterns.ipynb](./04_advanced_patterns.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
