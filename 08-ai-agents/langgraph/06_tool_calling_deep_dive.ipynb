{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Calling Deep Dive\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook provides comprehensive coverage of tool calling in LangGraph agents:\n",
    "\n",
    "- Understanding tool call mechanics\n",
    "- Different tool calling patterns\n",
    "- Parallel tool execution\n",
    "- Error handling and retries\n",
    "- Tool call validation\n",
    "- Advanced tool patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tool Call Fundamentals\n",
    "\n",
    "### How Tool Calls Work\n",
    "\n",
    "```\n",
    "User: \"What's the weather in SF and the time in NYC?\"\n",
    "   ‚îÇ\n",
    "   ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ     LLM     ‚îÇ ‚Üí Generates tool_calls in response\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "AIMessage(\n",
    "  content=\"\",\n",
    "  tool_calls=[\n",
    "    {\"name\": \"get_weather\", \"args\": {\"city\": \"SF\"}, \"id\": \"call_1\"},\n",
    "    {\"name\": \"get_time\", \"args\": {\"city\": \"NYC\"}, \"id\": \"call_2\"}\n",
    "  ]\n",
    ")\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇTool Executor‚îÇ ‚Üí Executes each tool\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "ToolMessage(content=\"72¬∞F\", tool_call_id=\"call_1\")\n",
    "ToolMessage(content=\"3:45 PM\", tool_call_id=\"call_2\")\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ     LLM     ‚îÇ ‚Üí Synthesizes final answer\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from typing import Literal\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the current weather for a city.\n",
    "    \n",
    "    Args:\n",
    "        city: The city name\n",
    "    \"\"\"\n",
    "    # Simulated weather API\n",
    "    weather_data = {\n",
    "        \"SF\": \"72¬∞F and sunny\",\n",
    "        \"NYC\": \"65¬∞F and cloudy\",\n",
    "        \"London\": \"55¬∞F and rainy\"\n",
    "    }\n",
    "    return weather_data.get(city, \"Weather data not available\")\n",
    "\n",
    "@tool\n",
    "def get_time(city: str) -> str:\n",
    "    \"\"\"Get the current time for a city.\n",
    "    \n",
    "    Args:\n",
    "        city: The city name\n",
    "    \"\"\"\n",
    "    # Simulated time API\n",
    "    times = {\n",
    "        \"SF\": \"12:45 PM PST\",\n",
    "        \"NYC\": \"3:45 PM EST\",\n",
    "        \"London\": \"8:45 PM GMT\"\n",
    "    }\n",
    "    return times.get(city, \"Time data not available\")\n",
    "\n",
    "# Inspect tool schema\n",
    "print(\"Tool Schema:\")\n",
    "print(get_weather.name)\n",
    "print(get_weather.description)\n",
    "print(get_weather.args)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parallel Tool Execution\n",
    "\n",
    "LLMs can call multiple tools in a single response\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "tools = [get_weather, get_time]\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "# Bind tools to LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Test parallel tool calls\n",
    "response = llm_with_tools.invoke([\n",
    "    HumanMessage(content=\"What's the weather AND time in SF?\")\n",
    "])\n",
    "\n",
    "print(\"LLM Response:\")\n",
    "print(f\"Content: {response.content}\")\n",
    "print(f\"\\nTool Calls: {len(response.tool_calls)}\")\n",
    "\n",
    "for i, tool_call in enumerate(response.tool_calls, 1):\n",
    "    print(f\"\\nTool Call {i}:\")\n",
    "    print(f\"  Name: {tool_call['name']}\")\n",
    "    print(f\"  Args: {tool_call['args']}\")\n",
    "    print(f\"  ID: {tool_call['id']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Tool Call Execution Patterns\n",
    "\n",
    "### Pattern 1: Sequential Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def execute_tools_sequentially(tool_calls, tool_executor):\n",
    "    \"\"\"Execute tools one at a time\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for tool_call in tool_calls:\n",
    "        print(f\"Executing: {tool_call['name']}({tool_call['args']})\")\n",
    "        \n",
    "        result = tool_executor.invoke(tool_call)\n",
    "        \n",
    "        tool_message = ToolMessage(\n",
    "            content=str(result),\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "        \n",
    "        results.append(tool_message)\n",
    "        print(f\"  Result: {result}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "if response.tool_calls:\n",
    "    tool_results = execute_tools_sequentially(response.tool_calls, tool_executor)\n",
    "    print(f\"\\nExecuted {len(tool_results)} tools\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Parallel Execution (with threading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def execute_tools_parallel(tool_calls, tool_executor, max_workers=5):\n",
    "    \"\"\"Execute tools in parallel for speed\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    def execute_single(tool_call):\n",
    "        result = tool_executor.invoke(tool_call)\n",
    "        return ToolMessage(\n",
    "            content=str(result),\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(execute_single, tc): tc for tc in tool_calls}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            tool_call = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                print(f\"‚úì {tool_call['name']} completed\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚úó {tool_call['name']} failed: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test parallel execution\n",
    "if response.tool_calls:\n",
    "    import time\n",
    "    start = time.time()\n",
    "    results = execute_tools_parallel(response.tool_calls, tool_executor)\n",
    "    duration = time.time() - start\n",
    "    print(f\"\\nCompleted in {duration:.2f}s\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Error Handling\n",
    "\n",
    "### Graceful Tool Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@tool\n",
    "def unreliable_tool(value: int) -> str:\n",
    "    \"\"\"A tool that sometimes fails.\n",
    "    \n",
    "    Args:\n",
    "        value: An integer value\n",
    "    \"\"\"\n",
    "    if value % 2 == 0:\n",
    "        raise Exception(\"Even numbers not supported\!\")\n",
    "    return f\"Success with {value}\"\n",
    "\n",
    "def safe_tool_execution(tool_call, tool_executor):\n",
    "    \"\"\"Execute tool with error handling\"\"\"\n",
    "    try:\n",
    "        result = tool_executor.invoke(tool_call)\n",
    "        return ToolMessage(\n",
    "            content=str(result),\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error calling {tool_call['name']}: {str(e)}\"\n",
    "        print(f\"‚ö†Ô∏è  {error_msg}\")\n",
    "        \n",
    "        return ToolMessage(\n",
    "            content=error_msg,\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "# Test error handling\n",
    "test_executor = ToolExecutor([unreliable_tool])\n",
    "\n",
    "# Simulate tool call\n",
    "test_call = {\n",
    "    \"name\": \"unreliable_tool\",\n",
    "    \"args\": {\"value\": 4},\n",
    "    \"id\": \"test_123\"\n",
    "}\n",
    "\n",
    "result = safe_tool_execution(test_call, test_executor)\n",
    "print(f\"\\nResult: {result.content}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "def execute_with_retry(tool_call, tool_executor, max_retries=3, backoff=1.0):\n",
    "    \"\"\"Execute tool with exponential backoff retry\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = tool_executor.invoke(tool_call)\n",
    "            return ToolMessage(\n",
    "                content=str(result),\n",
    "                tool_call_id=tool_call[\"id\"]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = backoff * (2 ** attempt)\n",
    "                print(f\"Attempt {attempt + 1} failed, retrying in {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                error_msg = f\"Failed after {max_retries} attempts: {str(e)}\"\n",
    "                return ToolMessage(\n",
    "                    content=error_msg,\n",
    "                    tool_call_id=tool_call[\"id\"]\n",
    "                )\n",
    "\n",
    "# Test retry\n",
    "result = execute_with_retry(test_call, test_executor, max_retries=2)\n",
    "print(f\"\\nFinal result: {result.content}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Tool Call Validation\n",
    "\n",
    "### Validate Arguments Before Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "class WeatherArgs(BaseModel):\n",
    "    \"\"\"Validated arguments for weather tool\"\"\"\n",
    "    city: str = Field(description=\"City name\")\n",
    "    \n",
    "    @validator('city')\n",
    "    def city_must_be_valid(cls, v):\n",
    "        valid_cities = [\"SF\", \"NYC\", \"London\"]\n",
    "        if v not in valid_cities:\n",
    "            raise ValueError(f\"City must be one of {valid_cities}\")\n",
    "        return v\n",
    "\n",
    "@tool(args_schema=WeatherArgs)\n",
    "def validated_weather(city: str) -> str:\n",
    "    \"\"\"Get weather with validated arguments.\"\"\"\n",
    "    return f\"Weather for {city}: 72¬∞F\"\n",
    "\n",
    "# This will automatically validate\!\n",
    "try:\n",
    "    result = validated_weather.invoke({\"city\": \"InvalidCity\"})\n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {e}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Tool Patterns\n",
    "\n",
    "### Pattern 1: Tool Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@tool\n",
    "def search_location(query: str) -> dict:\n",
    "    \"\"\"Search for a location and return coordinates.\"\"\"\n",
    "    # Simulated geocoding\n",
    "    return {\"city\": \"San Francisco\", \"lat\": 37.7749, \"lon\": -122.4194}\n",
    "\n",
    "@tool\n",
    "def get_weather_by_coords(lat: float, lon: float) -> str:\n",
    "    \"\"\"Get weather by coordinates.\"\"\"\n",
    "    return f\"Weather at ({lat}, {lon}): 72¬∞F and sunny\"\n",
    "\n",
    "# Agent can chain these:\n",
    "# 1. search_location(\"San Francisco\")\n",
    "# 2. get_weather_by_coords(37.7749, -122.4194)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Conditional Tool Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "import operator\n",
    "\n",
    "class SmartToolState(dict):\n",
    "    messages: Annotated[list, operator.add]\n",
    "    available_tools: list[str]\n",
    "    tool_budget: int  # Max number of tool calls\n",
    "\n",
    "def select_tools_node(state: SmartToolState) -> SmartToolState:\n",
    "    \"\"\"Dynamically select which tools to make available\"\"\"\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    \n",
    "    # Choose tools based on query\n",
    "    if \"weather\" in last_message.lower():\n",
    "        tools = [get_weather]\n",
    "    elif \"time\" in last_message.lower():\n",
    "        tools = [get_time]\n",
    "    else:\n",
    "        tools = [get_weather, get_time]\n",
    "    \n",
    "    llm_with_selected_tools = llm.bind_tools(tools)\n",
    "    response = llm_with_selected_tools.invoke(state[\"messages\"])\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Tool Result Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from functools import lru_cache\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "class CachedToolExecutor:\n",
    "    \"\"\"Tool executor with result caching\"\"\"\n",
    "    \n",
    "    def __init__(self, tools):\n",
    "        self.executor = ToolExecutor(tools)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def _make_cache_key(self, tool_call):\n",
    "        \"\"\"Create cache key from tool call\"\"\"\n",
    "        key_str = json.dumps({\n",
    "            \"name\": tool_call[\"name\"],\n",
    "            \"args\": tool_call[\"args\"]\n",
    "        }, sort_keys=True)\n",
    "        return hashlib.md5(key_str.encode()).hexdigest()\n",
    "    \n",
    "    def invoke(self, tool_call):\n",
    "        \"\"\"Execute with caching\"\"\"\n",
    "        cache_key = self._make_cache_key(tool_call)\n",
    "        \n",
    "        if cache_key in self.cache:\n",
    "            print(f\"‚úì Cache hit for {tool_call['name']}\")\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        print(f\"‚äó Cache miss for {tool_call['name']}, executing...\")\n",
    "        result = self.executor.invoke(tool_call)\n",
    "        self.cache[cache_key] = result\n",
    "        return result\n",
    "\n",
    "# Test caching\n",
    "cached_executor = CachedToolExecutor([get_weather])\n",
    "\n",
    "call = {\"name\": \"get_weather\", \"args\": {\"city\": \"SF\"}, \"id\": \"1\"}\n",
    "\n",
    "result1 = cached_executor.invoke(call)\n",
    "result2 = cached_executor.invoke(call)  # Should hit cache"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Complete Agent with Advanced Tool Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class AdvancedToolState(TypedDict):\n",
    "    messages: Annotated[list, operator.add]\n",
    "    tool_calls_made: int\n",
    "    max_tool_calls: int\n",
    "    errors: Annotated[list, operator.add]\n",
    "\n",
    "def agent_with_tools(state: AdvancedToolState):\n",
    "    \"\"\"Agent with tool calling\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([get_weather, get_time])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def execute_tools_safely(state: AdvancedToolState):\n",
    "    \"\"\"Execute tools with error handling and limits\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_calls = last_message.tool_calls\n",
    "    \n",
    "    # Check budget\n",
    "    if state[\"tool_calls_made\"] + len(tool_calls) > state[\"max_tool_calls\"]:\n",
    "        return {\n",
    "            \"errors\": [\"Tool call budget exceeded\"],\n",
    "            \"messages\": [AIMessage(content=\"Too many tool calls\")]\n",
    "        }\n",
    "    \n",
    "    # Execute with retries and caching\n",
    "    cached_executor = CachedToolExecutor([get_weather, get_time])\n",
    "    tool_messages = []\n",
    "    \n",
    "    for tool_call in tool_calls:\n",
    "        result = execute_with_retry(tool_call, cached_executor)\n",
    "        tool_messages.append(result)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": tool_messages,\n",
    "        \"tool_calls_made\": len(tool_calls)\n",
    "    }\n",
    "\n",
    "def should_continue(state: AdvancedToolState) -> str:\n",
    "    \"\"\"Route based on tool calls\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"end\"\n",
    "\n",
    "# Build graph\n",
    "workflow = StateGraph(AdvancedToolState)\n",
    "workflow.add_node(\"agent\", agent_with_tools)\n",
    "workflow.add_node(\"tools\", execute_tools_safely)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", \"end\": END})\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "result = app.invoke({\n",
    "    \"messages\": [HumanMessage(\"What's the weather in SF and NYC?\")],\n",
    "    \"tool_calls_made\": 0,\n",
    "    \"max_tool_calls\": 10,\n",
    "    \"errors\": []\n",
    "})\n",
    "\n",
    "print(\"\\nFinal Response:\")\n",
    "print(result[\"messages\"][-1].content)\n",
    "print(f\"\\nTotal tool calls: {result['tool_calls_made']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Tool Calls** are structured requests from LLM to execute functions\n",
    "- **Parallel Execution** improves performance for independent tools\n",
    "- **Error Handling** is critical for reliable agents\n",
    "- **Retry Logic** handles transient failures\n",
    "- **Validation** prevents invalid tool calls\n",
    "- **Caching** reduces redundant API calls\n",
    "- **Tool Budgets** prevent runaway costs\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. Always handle tool errors gracefully\n",
    "2. Use parallel execution when possible\n",
    "3. Implement retry logic for network calls\n",
    "4. Cache expensive tool results\n",
    "5. Validate tool arguments\n",
    "6. Set limits on tool calls\n",
    "7. Log all tool executions\n",
    "\n",
    "---\n",
    "\n",
    "**You now have comprehensive knowledge of tool calling in LangGraph\!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
