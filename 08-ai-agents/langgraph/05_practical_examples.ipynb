{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical LangGraph Examples\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook contains complete, production-ready examples of AI agents built with LangGraph:\n",
    "\n",
    "1. **Research Assistant** - Gathers and synthesizes information\n",
    "2. **Code Generator** - Writes and tests code\n",
    "3. **Data Analyst** - Analyzes datasets and creates insights\n",
    "4. **Customer Support** - Answers questions with knowledge base\n",
    "5. **Task Planner** - Breaks down complex tasks\n",
    "\n",
    "Each example is self-contained and ready to run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Research Assistant Agent\n",
    "\n",
    "An agent that can search for information, read sources, and synthesize findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "import operator\n",
    "\n",
    "# 1. Define tools\n",
    "@tool\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    # In production, use real search API (Tavily, SerpAPI)\n",
    "    return f\"\"\"Search results for '{query}':\n",
    "    1. LangGraph is a library for building stateful AI agents\n",
    "    2. It extends LangChain with graph-based orchestration\n",
    "    3. Supports cyclical flows and complex control logic\n",
    "    \"\"\"\n",
    "\n",
    "@tool\n",
    "def read_document(url: str) -> str:\n",
    "    \"\"\"Read and extract content from a URL.\"\"\"\n",
    "    # In production, use web scraping or PDF extraction\n",
    "    return f\"Content from {url}: Detailed information about the topic...\"\n",
    "\n",
    "# 2. Define state\n",
    "class ResearchState(TypedDict):\n",
    "    question: str\n",
    "    messages: Annotated[List, operator.add]\n",
    "    search_results: List[str]\n",
    "    final_answer: str\n",
    "\n",
    "# 3. Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "tools = [search_web, read_document]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# 4. Define nodes\n",
    "def research_node(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Agent conducts research\"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if not messages:\n",
    "        messages = [\n",
    "            HumanMessage(content=f\"\"\"Research this question: {state['question']}\n",
    "            Use search_web to find information. Be thorough.\"\"\")\n",
    "        ]\n",
    "    \n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def synthesize_node(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Synthesize findings into final answer\"\"\"\n",
    "    synthesis_prompt = f\"\"\"Based on the research conducted, provide a comprehensive answer to:\n",
    "    {state['question']}\n",
    "    \n",
    "    Include:\n",
    "    1. Key findings\n",
    "    2. Supporting evidence\n",
    "    3. Conclusion\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"] + [HumanMessage(content=synthesis_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    return {\"final_answer\": response.content}\n",
    "\n",
    "# 5. Build graph\n",
    "research_workflow = StateGraph(ResearchState)\n",
    "\n",
    "research_workflow.add_node(\"research\", research_node)\n",
    "research_workflow.add_node(\"synthesize\", synthesize_node)\n",
    "\n",
    "research_workflow.set_entry_point(\"research\")\n",
    "research_workflow.add_edge(\"research\", \"synthesize\")\n",
    "research_workflow.add_edge(\"synthesize\", END)\n",
    "\n",
    "research_agent = research_workflow.compile()\n",
    "\n",
    "# 6. Test\n",
    "print(\"Research Assistant Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = research_agent.invoke({\n",
    "    \"question\": \"What is LangGraph and how is it used?\",\n",
    "    \"messages\": [],\n",
    "    \"search_results\": [],\n",
    "    \"final_answer\": \"\"\n",
    "})\n",
    "\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(result[\"final_answer\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Code Generator Agent\n",
    "\n",
    "An agent that writes, tests, and debugs code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class CodeGenState(TypedDict):\n",
    "    task: str\n",
    "    code: str\n",
    "    test_results: str\n",
    "    iterations: int\n",
    "    final_code: str\n",
    "\n",
    "def generate_code(state: CodeGenState) -> CodeGenState:\n",
    "    \"\"\"Generate code based on task\"\"\"\n",
    "    prompt = f\"\"\"Write Python code for: {state['task']}\n",
    "    \n",
    "    Requirements:\n",
    "    - Include docstrings\n",
    "    - Add type hints\n",
    "    - Handle edge cases\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    code = response.content\n",
    "    \n",
    "    return {\"code\": code, \"iterations\": 0}\n",
    "\n",
    "def test_code(state: CodeGenState) -> CodeGenState:\n",
    "    \"\"\"Test the generated code\"\"\"\n",
    "    # In production, actually execute and test the code\n",
    "    try:\n",
    "        # Simulated testing\n",
    "        exec(state[\"code\"])  # Be careful with exec in production!\n",
    "        test_results = \"All tests passed!\"\n",
    "    except Exception as e:\n",
    "        test_results = f\"Error: {str(e)}\"\n",
    "    \n",
    "    return {\"test_results\": test_results, \"iterations\": state[\"iterations\"] + 1}\n",
    "\n",
    "def debug_code(state: CodeGenState) -> CodeGenState:\n",
    "    \"\"\"Fix code issues\"\"\"\n",
    "    prompt = f\"\"\"Fix this code:\n",
    "    ```python\n",
    "    {state['code']}\n",
    "    ```\n",
    "    \n",
    "    Error: {state['test_results']}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\"code\": response.content}\n",
    "\n",
    "def should_debug(state: CodeGenState) -> str:\n",
    "    \"\"\"Decide if we need to debug\"\"\"\n",
    "    if \"passed\" in state[\"test_results\"].lower():\n",
    "        return \"finish\"\n",
    "    elif state[\"iterations\"] >= 3:\n",
    "        return \"finish\"  # Give up after 3 tries\n",
    "    else:\n",
    "        return \"debug\"\n",
    "\n",
    "def finalize_code(state: CodeGenState) -> CodeGenState:\n",
    "    \"\"\"Finalize the code\"\"\"\n",
    "    return {\"final_code\": state[\"code\"]}\n",
    "\n",
    "# Build graph\n",
    "code_workflow = StateGraph(CodeGenState)\n",
    "\n",
    "code_workflow.add_node(\"generate\", generate_code)\n",
    "code_workflow.add_node(\"test\", test_code)\n",
    "code_workflow.add_node(\"debug\", debug_code)\n",
    "code_workflow.add_node(\"finalize\", finalize_code)\n",
    "\n",
    "code_workflow.set_entry_point(\"generate\")\n",
    "code_workflow.add_edge(\"generate\", \"test\")\n",
    "code_workflow.add_conditional_edges(\n",
    "    \"test\",\n",
    "    should_debug,\n",
    "    {\"debug\": \"debug\", \"finish\": \"finalize\"}\n",
    ")\n",
    "code_workflow.add_edge(\"debug\", \"test\")\n",
    "code_workflow.add_edge(\"finalize\", END)\n",
    "\n",
    "code_agent = code_workflow.compile()\n",
    "\n",
    "# Test\n",
    "print(\"Code Generator Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = code_agent.invoke({\n",
    "    \"task\": \"Write a function to check if a string is a palindrome\",\n",
    "    \"code\": \"\",\n",
    "    \"test_results\": \"\",\n",
    "    \"iterations\": 0,\n",
    "    \"final_code\": \"\"\n",
    "})\n",
    "\n",
    "print(\"\\nFinal Code:\")\n",
    "print(result[\"final_code\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Customer Support Agent\n",
    "\n",
    "An agent that answers customer questions using a knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SupportState(TypedDict):\n",
    "    customer_question: str\n",
    "    category: str\n",
    "    knowledge_base_results: List[str]\n",
    "    response: str\n",
    "    escalate: bool\n",
    "\n",
    "# Simulated knowledge base\n",
    "KNOWLEDGE_BASE = {\n",
    "    \"billing\": \"Billing information: Charges are processed monthly. Contact billing@example.com for issues.\",\n",
    "    \"technical\": \"Technical support: Check our FAQ at example.com/faq or contact support@example.com\",\n",
    "    \"account\": \"Account management: You can update your profile at example.com/profile\"\n",
    "}\n",
    "\n",
    "def categorize_question(state: SupportState) -> SupportState:\n",
    "    \"\"\"Categorize the customer question\"\"\"\n",
    "    prompt = f\"\"\"Categorize this customer question into one of:\n",
    "    - billing\n",
    "    - technical\n",
    "    - account\n",
    "    \n",
    "    Question: {state['customer_question']}\n",
    "    \n",
    "    Respond with just the category name.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    category = response.content.strip().lower()\n",
    "    \n",
    "    return {\"category\": category}\n",
    "\n",
    "def search_knowledge_base(state: SupportState) -> SupportState:\n",
    "    \"\"\"Search knowledge base for relevant information\"\"\"\n",
    "    results = KNOWLEDGE_BASE.get(state[\"category\"], [])\n",
    "    return {\"knowledge_base_results\": [results]}\n",
    "\n",
    "def generate_response(state: SupportState) -> SupportState:\n",
    "    \"\"\"Generate customer response\"\"\"\n",
    "    prompt = f\"\"\"Generate a helpful response to this customer:\n",
    "    \n",
    "    Question: {state['customer_question']}\n",
    "    Category: {state['category']}\n",
    "    Relevant Info: {state['knowledge_base_results']}\n",
    "    \n",
    "    Be friendly, professional, and helpful.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\"response\": response.content, \"escalate\": False}\n",
    "\n",
    "# Build graph\n",
    "support_workflow = StateGraph(SupportState)\n",
    "\n",
    "support_workflow.add_node(\"categorize\", categorize_question)\n",
    "support_workflow.add_node(\"search\", search_knowledge_base)\n",
    "support_workflow.add_node(\"respond\", generate_response)\n",
    "\n",
    "support_workflow.set_entry_point(\"categorize\")\n",
    "support_workflow.add_edge(\"categorize\", \"search\")\n",
    "support_workflow.add_edge(\"search\", \"respond\")\n",
    "support_workflow.add_edge(\"respond\", END)\n",
    "\n",
    "support_agent = support_workflow.compile()\n",
    "\n",
    "# Test\n",
    "print(\"Customer Support Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = support_agent.invoke({\n",
    "    \"customer_question\": \"Why was I charged twice this month?\",\n",
    "    \"category\": \"\",\n",
    "    \"knowledge_base_results\": [],\n",
    "    \"response\": \"\",\n",
    "    \"escalate\": False\n",
    "})\n",
    "\n",
    "print(f\"\\nCategory: {result['category']}\")\n",
    "print(f\"\\nResponse: {result['response']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Tips\n",
    "\n",
    "### 1. Add Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def logged_node(state):\n",
    "    logger.info(f\"Node executing with state: {state}\")\n",
    "    # Node logic\n",
    "    result = process(state)\n",
    "    logger.info(f\"Node completed: {result}\")\n",
    "    return result"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "def monitored_node(state):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = process(state)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        # Log metrics\n",
    "        logger.info(f\"Node completed in {duration:.2f}s\")\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Node failed: {e}\")\n",
    "        raise"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=10, period=60)  # 10 calls per minute\n",
    "def call_llm(messages):\n",
    "    return llm.invoke(messages)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Modularity**: Break agents into reusable components\n",
    "- **Error Handling**: Always handle failures gracefully\n",
    "- **Testing**: Test each node independently\n",
    "- **Monitoring**: Log and track agent behavior\n",
    "- **Optimization**: Cache results, use appropriate models\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Build your own agent for a specific use case\n",
    "- Add persistence for long-running conversations\n",
    "- Implement human-in-the-loop for critical decisions\n",
    "- Deploy to production with proper monitoring\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed the LangGraph tutorials. Now go build amazing agents! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
