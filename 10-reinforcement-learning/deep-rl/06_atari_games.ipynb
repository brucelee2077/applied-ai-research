{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Atari Games: Where Deep RL Began\n",
    "\n",
    "This is where deep RL made history! In 2013, DeepMind's DQN learned to play Atari games from raw pixels, achieving superhuman performance on many games.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The historical significance of DQN on Atari\n",
    "- Why images need special preprocessing\n",
    "- Frame stacking: giving the agent \"motion vision\"\n",
    "- CNN architectures for image-based RL\n",
    "- Using Stable-Baselines3 for Atari\n",
    "- Tips for training and evaluation\n",
    "\n",
    "**Prerequisites:** Notebooks 1-5 (Deep RL fundamentals)\n",
    "\n",
    "**Time:** ~25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: A Historic Achievement\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          DQN ON ATARI: WHY IT MATTERED                        │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  BEFORE DQN (2013):                                           │\n",
    "    │    • RL worked on small, hand-crafted state spaces            │\n",
    "    │    • Needed domain experts to design features                 │\n",
    "    │    • Couldn't handle raw sensory input (images, sounds)       │\n",
    "    │                                                                │\n",
    "    │  THE DQN BREAKTHROUGH:                                        │\n",
    "    │    • Input: Raw pixels (210×160×3 = 100,800 numbers!)        │\n",
    "    │    • Output: Which button to press                           │\n",
    "    │    • No game-specific engineering                            │\n",
    "    │    • Same algorithm for ALL 49 Atari games!                  │\n",
    "    │                                                                │\n",
    "    │  RESULTS:                                                     │\n",
    "    │    • Superhuman on Breakout, Pong, Space Invaders, etc.      │\n",
    "    │    • Learned strategies humans didn't teach it               │\n",
    "    │    • Famous Breakout \"tunnel\" strategy discovered by AI      │\n",
    "    │                                                                │\n",
    "    │  IMPACT:                                                      │\n",
    "    │    • Proved deep learning + RL could scale                   │\n",
    "    │    • Led to AlphaGo, AlphaFold, ChatGPT (RLHF!)             │\n",
    "    │    • Started the modern deep RL revolution                   │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, Circle\n",
    "\n",
    "# Visualize the Atari challenge\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Left: Raw pixel input\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('The Challenge:\\nRaw Pixels → Actions', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Pixel grid representation\n",
    "np.random.seed(42)\n",
    "pixel_img = np.random.rand(8, 8, 3)\n",
    "pixel_img[3:5, 3:5] = [1, 0, 0]  # Red object\n",
    "pixel_img[6:7, 2:6] = [0, 1, 0]  # Green paddle\n",
    "\n",
    "img_box = FancyBboxPatch((1, 4), 4, 4, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#e0e0e0', edgecolor='#333', linewidth=2)\n",
    "ax1.add_patch(img_box)\n",
    "ax1.imshow(pixel_img, extent=(1.2, 4.8, 4.2, 7.8))\n",
    "ax1.text(3, 3.5, '210×160×3 pixels', ha='center', fontsize=10)\n",
    "ax1.text(3, 2.8, '= 100,800 numbers!', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Arrow\n",
    "ax1.annotate('', xy=(7, 5.5), xytext=(5.5, 5.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=3, color='#1976d2'))\n",
    "ax1.text(6.25, 6.2, 'DQN', ha='center', fontsize=12, fontweight='bold', color='#1976d2')\n",
    "\n",
    "# Output: Actions\n",
    "action_box = FancyBboxPatch((7, 4.5), 2.5, 2, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax1.add_patch(action_box)\n",
    "ax1.text(8.25, 5.8, 'Action', ha='center', fontsize=11, fontweight='bold')\n",
    "ax1.text(8.25, 5.2, '←  →  fire', ha='center', fontsize=10)\n",
    "\n",
    "# Middle: Timeline\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('The Deep RL Revolution', fontsize=14, fontweight='bold')\n",
    "\n",
    "milestones = [\n",
    "    ('2013', 'DQN Paper', '#bbdefb'),\n",
    "    ('2015', 'Nature Paper', '#c8e6c9'),\n",
    "    ('2016', 'AlphaGo', '#fff3e0'),\n",
    "    ('2017', 'Rainbow', '#e1bee7'),\n",
    "    ('2022', 'ChatGPT (RLHF)', '#ffcdd2'),\n",
    "]\n",
    "\n",
    "for i, (year, event, color) in enumerate(milestones):\n",
    "    y = 8 - i * 1.5\n",
    "    box = FancyBboxPatch((2, y-0.4), 6, 0.8, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor='#333', linewidth=1)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(5, y, f'{year}: {event}', ha='center', va='center', fontsize=10)\n",
    "\n",
    "ax2.text(5, 0.5, 'DQN started it all!', ha='center', fontsize=11, \n",
    "         style='italic', color='#666')\n",
    "\n",
    "# Right: Performance chart\n",
    "ax3 = axes[2]\n",
    "games = ['Breakout', 'Pong', 'Q*bert', 'Seaquest', 'Enduro']\n",
    "human = [31.8, 9.3, 13455, 42055, 309.6]\n",
    "dqn = [401.2, 20.9, 10596, 5286, 475.6]\n",
    "dqn_pct = [d/h*100 if h > 0 else 100 for d, h in zip(dqn, human)]\n",
    "\n",
    "colors = ['#4caf50' if p > 100 else '#ff9800' for p in dqn_pct]\n",
    "ax3.barh(games, dqn_pct, color=colors, edgecolor='black', linewidth=1)\n",
    "ax3.axvline(x=100, color='#333', linestyle='--', linewidth=2, label='Human Level')\n",
    "ax3.set_xlabel('DQN Score (% of Human)', fontsize=11)\n",
    "ax3.set_title('DQN vs Human Performance', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THE DQN ACHIEVEMENT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "DQN proved that a single algorithm could:\n",
    "  1. Take raw pixels as input (no hand-crafted features)\n",
    "  2. Learn to play 49 different games\n",
    "  3. Achieve superhuman performance on many of them\n",
    "  4. Discover novel strategies humans didn't teach it\n",
    "\n",
    "This was the \"ImageNet moment\" for reinforcement learning!\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Preprocessing Pipeline\n",
    "\n",
    "Raw Atari frames need careful preprocessing!\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              ATARI PREPROCESSING PIPELINE                      │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  RAW FRAME                                                    │\n",
    "    │  ┌─────────────┐                                              │\n",
    "    │  │             │  210 × 160 × 3 (RGB)                        │\n",
    "    │  │   COLOR     │  = 100,800 values                           │\n",
    "    │  │   IMAGE     │                                              │\n",
    "    │  └─────────────┘                                              │\n",
    "    │        ↓                                                      │\n",
    "    │  STEP 1: Convert to Grayscale                                │\n",
    "    │  ┌─────────────┐                                              │\n",
    "    │  │             │  210 × 160 × 1                               │\n",
    "    │  │   GRAY      │  = 33,600 values                            │\n",
    "    │  │   IMAGE     │  (Color doesn't help gameplay!)             │\n",
    "    │  └─────────────┘                                              │\n",
    "    │        ↓                                                      │\n",
    "    │  STEP 2: Resize to 84×84                                     │\n",
    "    │  ┌─────┐                                                      │\n",
    "    │  │     │  84 × 84 × 1                                        │\n",
    "    │  │     │  = 7,056 values                                     │\n",
    "    │  └─────┘  (Still has all important info!)                    │\n",
    "    │        ↓                                                      │\n",
    "    │  STEP 3: Stack 4 Frames (for motion)                         │\n",
    "    │  ┌─────┬─────┬─────┬─────┐                                   │\n",
    "    │  │ t-3 │ t-2 │ t-1 │  t  │  84 × 84 × 4                      │\n",
    "    │  └─────┴─────┴─────┴─────┘  = 28,224 values                  │\n",
    "    │                                                                │\n",
    "    │  This is what the DQN network sees!                          │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the preprocessing pipeline\n",
    "np.random.seed(42)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "\n",
    "# Top row: Preprocessing steps\n",
    "ax1, ax2, ax3, ax4 = axes[0]\n",
    "\n",
    "# Raw frame (simulate with random colors)\n",
    "raw_frame = np.random.rand(210, 160, 3)\n",
    "# Add some structure\n",
    "raw_frame[180:200, 70:90] = [0.2, 0.8, 0.2]  # Green paddle\n",
    "raw_frame[50:60, 80:90] = [1, 0.2, 0.2]       # Red ball\n",
    "raw_frame[10:20, :] = [0.3, 0.3, 1]           # Blue bricks\n",
    "\n",
    "ax1.imshow(raw_frame)\n",
    "ax1.set_title('1. Raw Frame\\n210×160×3 = 100,800', fontsize=11)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Grayscale\n",
    "gray_frame = 0.299*raw_frame[:,:,0] + 0.587*raw_frame[:,:,1] + 0.114*raw_frame[:,:,2]\n",
    "ax2.imshow(gray_frame, cmap='gray')\n",
    "ax2.set_title('2. Grayscale\\n210×160×1 = 33,600', fontsize=11)\n",
    "ax2.axis('off')\n",
    "\n",
    "# Resized (simulate)\n",
    "from scipy.ndimage import zoom\n",
    "resized_frame = zoom(gray_frame, (84/210, 84/160))\n",
    "ax3.imshow(resized_frame, cmap='gray')\n",
    "ax3.set_title('3. Resized\\n84×84×1 = 7,056', fontsize=11)\n",
    "ax3.axis('off')\n",
    "\n",
    "# Frame stack visualization\n",
    "ax4.set_xlim(0, 10)\n",
    "ax4.set_ylim(0, 10)\n",
    "ax4.axis('off')\n",
    "ax4.set_title('4. Frame Stack\\n84×84×4 = 28,224', fontsize=11)\n",
    "\n",
    "# Draw stacked frames\n",
    "for i, offset in enumerate([0.3, 0.2, 0.1, 0]):\n",
    "    alpha = 0.4 + 0.2 * i\n",
    "    color = f'{0.3 + 0.2*i}'\n",
    "    rect = Rectangle((2 + offset*3, 2 + offset*3), 5, 5, \n",
    "                     facecolor=color, edgecolor='black', linewidth=2, alpha=alpha)\n",
    "    ax4.add_patch(rect)\n",
    "ax4.text(5, 1, 't-3, t-2, t-1, t', ha='center', fontsize=10)\n",
    "ax4.text(5, 0.3, '(captures motion!)', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Bottom row: Why frame stacking matters\n",
    "for ax in axes[1]:\n",
    "    ax.axis('off')\n",
    "\n",
    "axes[1, 0].set_title('Why Stack Frames?', fontsize=12, fontweight='bold', loc='left')\n",
    "\n",
    "# Single frame problem\n",
    "ax5 = axes[1, 0]\n",
    "ax5.text(0.5, 0.9, 'Single Frame Problem:', transform=ax5.transAxes, fontsize=10, fontweight='bold')\n",
    "ax5.text(0.5, 0.6, '• Ball position: (50, 80)', transform=ax5.transAxes, fontsize=9)\n",
    "ax5.text(0.5, 0.4, '• But is it going ↑ or ↓?', transform=ax5.transAxes, fontsize=9)\n",
    "ax5.text(0.5, 0.2, '• Cannot tell from ONE frame!', transform=ax5.transAxes, fontsize=9, color='#d32f2f')\n",
    "\n",
    "# Stacked frames solution\n",
    "ax6 = axes[1, 1]\n",
    "ax6.text(0.5, 0.9, 'Stacked Frames Solution:', transform=ax6.transAxes, fontsize=10, fontweight='bold')\n",
    "ax6.text(0.5, 0.65, '• Frame t-3: ball at (50, 75)', transform=ax6.transAxes, fontsize=9)\n",
    "ax6.text(0.5, 0.45, '• Frame t-2: ball at (50, 77)', transform=ax6.transAxes, fontsize=9)\n",
    "ax6.text(0.5, 0.25, '• Frame t-1: ball at (50, 79)', transform=ax6.transAxes, fontsize=9)\n",
    "ax6.text(0.5, 0.05, '• Ball is moving DOWN! ↓', transform=ax6.transAxes, fontsize=9, color='#388e3c', fontweight='bold')\n",
    "\n",
    "# Analogy\n",
    "ax7 = axes[1, 2]\n",
    "ax7.text(0.5, 0.9, 'Analogy:', transform=ax7.transAxes, fontsize=10, fontweight='bold')\n",
    "ax7.text(0.5, 0.6, 'Single frame = Photo', transform=ax7.transAxes, fontsize=9)\n",
    "ax7.text(0.5, 0.4, 'Frame stack = Short Video', transform=ax7.transAxes, fontsize=9)\n",
    "ax7.text(0.5, 0.15, 'Video shows MOTION!', transform=ax7.transAxes, fontsize=9, color='#1976d2', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPREPROCESSING SUMMARY:\")\n",
    "print(\"  1. Grayscale: Color doesn't help, reduces dimensions\")\n",
    "print(\"  2. Resize: 84×84 is enough detail, faster to process\")\n",
    "print(\"  3. Frame Stack: 4 frames capture velocity/motion\")\n",
    "print(\"\\n  Final input: 84×84×4 = 28,224 values per observation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The CNN Architecture\n",
    "\n",
    "DQN uses a Convolutional Neural Network to process images!\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                    DQN CNN ARCHITECTURE                        │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  INPUT: 84 × 84 × 4 (4 stacked grayscale frames)              │\n",
    "    │         ↓                                                      │\n",
    "    │  CONV1: 32 filters, 8×8, stride 4 → 20×20×32                  │\n",
    "    │         ReLU                                                   │\n",
    "    │         ↓                                                      │\n",
    "    │  CONV2: 64 filters, 4×4, stride 2 → 9×9×64                    │\n",
    "    │         ReLU                                                   │\n",
    "    │         ↓                                                      │\n",
    "    │  CONV3: 64 filters, 3×3, stride 1 → 7×7×64                    │\n",
    "    │         ReLU                                                   │\n",
    "    │         ↓                                                      │\n",
    "    │  FLATTEN: 7×7×64 = 3,136                                      │\n",
    "    │         ↓                                                      │\n",
    "    │  FC1: 512 units, ReLU                                         │\n",
    "    │         ↓                                                      │\n",
    "    │  OUTPUT: num_actions (e.g., 4 for Breakout)                   │\n",
    "    │                                                                │\n",
    "    │  WHY CNNs?                                                    │\n",
    "    │    • Detect patterns regardless of position (translation inv) │\n",
    "    │    • Hierarchical features: edges → shapes → objects         │\n",
    "    │    • Parameter efficient: shared weights across image         │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    DQN Architecture for Atari Games.\n",
    "    \n",
    "    This is the exact architecture from the Nature paper!\n",
    "    Uses CNNs to process image input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ========================================\n",
    "        # CONVOLUTIONAL LAYERS (feature extraction)\n",
    "        # ========================================\n",
    "        self.conv = nn.Sequential(\n",
    "            # Conv1: 84×84×4 → 20×20×32\n",
    "            # 8×8 kernel with stride 4 captures large features\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv2: 20×20×32 → 9×9×64\n",
    "            # 4×4 kernel captures medium features\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv3: 9×9×64 → 7×7×64\n",
    "            # 3×3 kernel captures fine details\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # FULLY CONNECTED LAYERS (decision making)\n",
    "        # ========================================\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),  # 3,136 → 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)   # 512 → actions\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: image → Q-values.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape (batch, 4, 84, 84)\n",
    "               4 stacked frames, each 84×84 grayscale\n",
    "        \n",
    "        Returns:\n",
    "            Q-values for each action\n",
    "        \"\"\"\n",
    "        # Normalize pixel values to [0, 1]\n",
    "        x = x / 255.0\n",
    "        \n",
    "        # Extract features with convolutions\n",
    "        features = self.conv(x)\n",
    "        \n",
    "        # Flatten: (batch, 64, 7, 7) → (batch, 3136)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Compute Q-values\n",
    "        q_values = self.fc(features)\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "\n",
    "# Examine the network\n",
    "print(\"ATARI DQN ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_actions = 4  # e.g., Breakout: noop, fire, left, right\n",
    "model = AtariDQN(num_actions)\n",
    "\n",
    "print(\"\\nNetwork Structure:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, 4, 84, 84)  # Batch of 1, 4 frames, 84×84\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "print(f\"\\nInput shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape} (Q-values for {num_actions} actions)\")\n",
    "print(f\"Output: {output.numpy()[0].round(3)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what each conv layer does\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n",
    "\n",
    "# Input\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Input\\n84×84×4', fontsize=12, fontweight='bold')\n",
    "\n",
    "input_box = FancyBboxPatch((2, 2), 6, 6, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#bbdefb', edgecolor='#1976d2', linewidth=3)\n",
    "ax1.add_patch(input_box)\n",
    "ax1.text(5, 5, '4 stacked\\nframes', ha='center', va='center', fontsize=10)\n",
    "ax1.text(5, 1.5, '28,224 values', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Conv1\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('After Conv1\\n20×20×32', fontsize=12, fontweight='bold')\n",
    "\n",
    "conv1_box = FancyBboxPatch((3, 3), 4, 4, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax2.add_patch(conv1_box)\n",
    "ax2.text(5, 5, '32 feature\\nmaps', ha='center', va='center', fontsize=10)\n",
    "ax2.text(5, 2, 'Detects edges,\\nbasic shapes', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Conv2\n",
    "ax3 = axes[2]\n",
    "ax3.set_xlim(0, 10)\n",
    "ax3.set_ylim(0, 10)\n",
    "ax3.axis('off')\n",
    "ax3.set_title('After Conv2\\n9×9×64', fontsize=12, fontweight='bold')\n",
    "\n",
    "conv2_box = FancyBboxPatch((3.5, 3.5), 3, 3, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#fff3e0', edgecolor='#f57c00', linewidth=3)\n",
    "ax3.add_patch(conv2_box)\n",
    "ax3.text(5, 5, '64 feature\\nmaps', ha='center', va='center', fontsize=10)\n",
    "ax3.text(5, 2.5, 'Detects objects,\\nmotion patterns', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Conv3 + FC\n",
    "ax4 = axes[3]\n",
    "ax4.set_xlim(0, 10)\n",
    "ax4.set_ylim(0, 10)\n",
    "ax4.axis('off')\n",
    "ax4.set_title('After Conv3 → FC\\n→ Q-values', fontsize=12, fontweight='bold')\n",
    "\n",
    "conv3_box = FancyBboxPatch((4, 6), 2, 2, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#e1bee7', edgecolor='#7b1fa2', linewidth=3)\n",
    "ax4.add_patch(conv3_box)\n",
    "ax4.text(5, 7, '7×7×64', ha='center', va='center', fontsize=9)\n",
    "\n",
    "ax4.annotate('', xy=(5, 5.2), xytext=(5, 5.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "fc_box = FancyBboxPatch((3.5, 3), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=3)\n",
    "ax4.add_patch(fc_box)\n",
    "ax4.text(5, 3.75, 'Q-values', ha='center', va='center', fontsize=10)\n",
    "\n",
    "ax4.text(5, 1.5, 'High-level decisions:\\n\"Ball going left → move left\"', \n",
    "         ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Arrows between stages\n",
    "for ax in axes[:-1]:\n",
    "    ax.annotate('', xy=(9.5, 5), xytext=(8.5, 5),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWHAT EACH LAYER LEARNS:\")\n",
    "print(\"  Conv1: Low-level features (edges, corners, colors)\")\n",
    "print(\"  Conv2: Mid-level features (shapes, textures, simple objects)\")\n",
    "print(\"  Conv3: High-level features (game objects, spatial relationships)\")\n",
    "print(\"  FC: Decision making (which action maximizes future reward)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using Stable-Baselines3 for Atari\n",
    "\n",
    "For production use, don't implement from scratch - use a well-tested library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Stable-Baselines3 is available\n",
    "try:\n",
    "    from stable_baselines3 import DQN\n",
    "    from stable_baselines3.common.env_util import make_atari_env\n",
    "    from stable_baselines3.common.vec_env import VecFrameStack\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    SB3_AVAILABLE = True\n",
    "    print(\"✓ Stable-Baselines3 is installed!\")\n",
    "except ImportError:\n",
    "    SB3_AVAILABLE = False\n",
    "    print(\"Stable-Baselines3 not installed.\")\n",
    "    print(\"\\nTo install, run:\")\n",
    "    print(\"  pip install stable-baselines3[extra] gymnasium[atari] ale-py\")\n",
    "    print(\"\\nShowing conceptual code only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Atari environment with preprocessing\n",
    "if SB3_AVAILABLE:\n",
    "    try:\n",
    "        # Create the environment with all preprocessing built-in!\n",
    "        # make_atari_env handles:\n",
    "        # - Grayscale conversion\n",
    "        # - Resizing to 84×84\n",
    "        # - Reward clipping\n",
    "        # - Frame skipping\n",
    "        env = make_atari_env(\n",
    "            \"BreakoutNoFrameskip-v4\",  # No frame skip (we control it)\n",
    "            n_envs=1,                   # Number of parallel envs\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # Add frame stacking (4 frames)\n",
    "        env = VecFrameStack(env, n_stack=4)\n",
    "        \n",
    "        print(\"ATARI ENVIRONMENT CREATED\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nObservation space: {env.observation_space}\")\n",
    "        print(f\"Action space: {env.action_space}\")\n",
    "        \n",
    "        # Show actions\n",
    "        print(\"\\nAvailable actions:\")\n",
    "        print(\"  0: NOOP (no operation)\")\n",
    "        print(\"  1: FIRE (start game/launch ball)\")\n",
    "        print(\"  2: RIGHT (move paddle right)\")\n",
    "        print(\"  3: LEFT (move paddle left)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        env.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating environment: {e}\")\n",
    "        print(\"\\nYou may need to install ROM files. Try:\")\n",
    "        print(\"  pip install 'gymnasium[accept-rom-license]'\")\n",
    "        SB3_AVAILABLE = False\n",
    "else:\n",
    "    print(\"\\n(Skipping environment creation - SB3 not installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training DQN on Atari\n",
    "\n",
    "Here's how you would train a DQN agent on Breakout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training code (conceptual - don't run unless you have hours!)\n",
    "\n",
    "training_code = '''\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "\n",
    "# Create environment\n",
    "env = make_atari_env(\"BreakoutNoFrameskip-v4\", n_envs=1, seed=42)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Create DQN agent with Atari-specific hyperparameters\n",
    "model = DQN(\n",
    "    \"CnnPolicy\",              # Use CNN for images\n",
    "    env,\n",
    "    \n",
    "    # Core hyperparameters\n",
    "    learning_rate=1e-4,       # Adam learning rate\n",
    "    buffer_size=100_000,      # Replay buffer size\n",
    "    batch_size=32,            # Minibatch size\n",
    "    gamma=0.99,               # Discount factor\n",
    "    \n",
    "    # Target network\n",
    "    target_update_interval=1000,  # Update target every N steps\n",
    "    \n",
    "    # Training schedule\n",
    "    learning_starts=10_000,   # Random actions before learning\n",
    "    train_freq=4,             # Train every 4 steps\n",
    "    \n",
    "    # Exploration (epsilon-greedy)\n",
    "    exploration_fraction=0.1,     # Anneal ε over 10% of training\n",
    "    exploration_final_eps=0.01,   # Final ε value\n",
    "    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train! (This takes HOURS)\n",
    "model.learn(total_timesteps=1_000_000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"dqn_breakout\")\n",
    "\n",
    "# Later, load and use:\n",
    "# model = DQN.load(\"dqn_breakout\")\n",
    "'''\n",
    "\n",
    "print(\"TRAINING CODE FOR DQN ON ATARI\")\n",
    "print(\"=\"*70)\n",
    "print(training_code)\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNOTE: Training takes 2-10 hours depending on hardware!\")\n",
    "print(\"For production, consider using pretrained models or more compute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Hyperparameters for Atari\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              RECOMMENDED HYPERPARAMETERS                       │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  PARAMETER              VALUE      WHY                        │\n",
    "    │  ─────────────────────────────────────────────────────────────│\n",
    "    │  Buffer size            1M         Need lots of diverse exp  │\n",
    "    │  Batch size             32         Original DQN paper        │\n",
    "    │  Learning rate          1e-4       Adam optimizer            │\n",
    "    │  Gamma                  0.99       Long-term planning        │\n",
    "    │  Target update          10K        Stability                 │\n",
    "    │  Frame skip             4          Speed up training         │\n",
    "    │  Frame stack            4          Capture motion            │\n",
    "    │  ε start                1.0        Full exploration          │\n",
    "    │  ε end                  0.01       Mostly greedy             │\n",
    "    │  ε decay steps          1M         Gradual transition        │\n",
    "    │                                                                │\n",
    "    │  TRAINING TIME:                                               │\n",
    "    │  • 1M steps ≈ 2-4 hours (GPU)                                │\n",
    "    │  • 10M steps ≈ 1-2 days (GPU)                                │\n",
    "    │  • For best results: 50M+ steps                              │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training time and performance\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Training time\n",
    "ax1 = axes[0]\n",
    "steps = [100_000, 1_000_000, 10_000_000, 50_000_000]\n",
    "hours_gpu = [0.25, 2.5, 25, 125]\n",
    "hours_cpu = [2, 20, 200, 1000]\n",
    "\n",
    "x = np.arange(len(steps))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, hours_gpu, width, label='GPU', color='#4caf50', edgecolor='black')\n",
    "ax1.bar(x + width/2, hours_cpu, width, label='CPU', color='#ff9800', edgecolor='black')\n",
    "\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(['100K', '1M', '10M', '50M'])\n",
    "ax1.set_xlabel('Training Steps', fontsize=11)\n",
    "ax1.set_ylabel('Training Time (hours, log scale)', fontsize=11)\n",
    "ax1.set_title('DQN Training Time on Atari', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.axhline(y=24, color='red', linestyle='--', alpha=0.5)\n",
    "ax1.text(3.5, 30, '1 day', fontsize=9, color='red')\n",
    "\n",
    "# Right: Typical learning curve\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Simulate a typical learning curve\n",
    "np.random.seed(42)\n",
    "steps_plot = np.linspace(0, 10_000_000, 1000)\n",
    "# Sigmoid-like learning curve with noise\n",
    "base_reward = 400 / (1 + np.exp(-steps_plot / 2_000_000 + 2))\n",
    "noise = np.random.randn(1000) * 30\n",
    "rewards = base_reward + noise\n",
    "rewards = np.clip(rewards, 0, None)\n",
    "\n",
    "ax2.plot(steps_plot / 1e6, rewards, 'b-', alpha=0.3, linewidth=0.5)\n",
    "# Smoothed\n",
    "window = 50\n",
    "smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "ax2.plot(steps_plot[window-1:] / 1e6, smoothed, 'b-', linewidth=2, label='Score')\n",
    "\n",
    "ax2.axhline(y=31.8, color='red', linestyle='--', linewidth=2, label='Human Level')\n",
    "ax2.set_xlabel('Million Steps', fontsize=11)\n",
    "ax2.set_ylabel('Score (Breakout)', fontsize=11)\n",
    "ax2.set_title('Typical DQN Learning Curve', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTRAINING TIPS:\")\n",
    "print(\"  1. Start with 1M steps to verify everything works\")\n",
    "print(\"  2. Use GPU if available (10x faster!)\")\n",
    "print(\"  3. Monitor learning curves to catch problems early\")\n",
    "print(\"  4. For production: train for 10M+ steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### The DQN Achievement\n",
    "\n",
    "| Aspect | Details |\n",
    "|--------|--------|\n",
    "| **Input** | Raw pixels (84×84×4 after preprocessing) |\n",
    "| **Output** | Q-values for each action |\n",
    "| **Games** | 49 Atari games with same architecture |\n",
    "| **Performance** | Superhuman on many games |\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "\n",
    "| Step | What | Why |\n",
    "|------|------|-----|\n",
    "| Grayscale | Remove color | Not needed for gameplay |\n",
    "| Resize | 84×84 | Reduce computation |\n",
    "| Frame Stack | 4 frames | Capture motion |\n",
    "\n",
    "### Architecture\n",
    "\n",
    "| Layer | Size | Purpose |\n",
    "|-------|------|--------|\n",
    "| Conv1 | 32×8×8 | Detect edges, basic features |\n",
    "| Conv2 | 64×4×4 | Detect objects, patterns |\n",
    "| Conv3 | 64×3×3 | High-level features |\n",
    "| FC | 512 | Decision making |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why do we stack 4 frames instead of using a single frame?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "A single frame is like a photograph - it shows position but not motion. We can't tell if the ball is moving up or down. By stacking 4 consecutive frames, the network can see how objects have moved over time, like a short video. This captures velocity and trajectory information essential for playing games.\n",
    "</details>\n",
    "\n",
    "**2. Why convert to grayscale?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Color rarely provides useful information for Atari games - the important features (ball position, paddle position, etc.) are visible in grayscale. Removing color reduces the input size by 3x, making training faster and the network simpler without losing important information.\n",
    "</details>\n",
    "\n",
    "**3. Why use CNNs instead of fully-connected networks for images?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "CNNs have several advantages for images:\n",
    "1. Translation invariance: A ball in the corner uses the same features as a ball in the center\n",
    "2. Parameter efficiency: Shared weights across the image means fewer parameters\n",
    "3. Hierarchical features: Layers naturally build from edges → shapes → objects\n",
    "</details>\n",
    "\n",
    "**4. Why does training take so long (millions of steps)?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Several reasons:\n",
    "1. Sparse rewards: In Breakout, you only get reward when hitting a brick\n",
    "2. Credit assignment: The action that caused the reward may be hundreds of steps earlier\n",
    "3. Exploration: Need to try many random actions to discover good strategies\n",
    "4. Image complexity: 28K inputs per observation takes time to learn patterns\n",
    "</details>\n",
    "\n",
    "**5. What made DQN a breakthrough compared to earlier methods?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "DQN was the first to:\n",
    "1. Use deep neural networks successfully with RL (earlier attempts failed)\n",
    "2. Learn directly from raw pixels with no hand-crafted features\n",
    "3. Use the same algorithm for many different games\n",
    "4. Achieve superhuman performance on complex visual tasks\n",
    "\n",
    "The key innovations (experience replay + target networks) solved the instability problems that made earlier attempts fail.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the **Deep RL** section! You now understand:\n",
    "\n",
    "- ✅ Why function approximation is needed\n",
    "- ✅ How DQN combines deep learning with RL\n",
    "- ✅ Experience replay and target networks\n",
    "- ✅ DQN improvements (Double, Dueling, Rainbow)\n",
    "- ✅ Training DQN on Atari games\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Move on to **[Policy Gradient Methods](../policy-gradient/)** to learn about:\n",
    "- REINFORCE algorithm\n",
    "- Actor-Critic methods\n",
    "- A2C and A3C\n",
    "\n",
    "These methods directly learn policies instead of value functions!\n",
    "\n",
    "---\n",
    "\n",
    "*\"DQN proved that a single algorithm could learn superhuman performance from pixels alone. The deep RL revolution had begun.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
