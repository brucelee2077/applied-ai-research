{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Improvements: Supercharging the Original\n",
    "\n",
    "The original DQN was revolutionary, but it had problems. This notebook covers the key improvements that made DQN even better!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The overestimation problem (with a restaurant analogy!)\n",
    "- Double DQN: fixing the optimism bias\n",
    "- Dueling DQN: separating \"where\" from \"what\"\n",
    "- Other improvements: N-step, Noisy Nets\n",
    "- Rainbow DQN: combining everything\n",
    "- Implementing all improvements in PyTorch\n",
    "\n",
    "**Prerequisites:** Notebooks 1-4 (DQN fundamentals)\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Restaurant Review Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          WHY DQN NEEDS IMPROVEMENTS: THE RESTAURANT           │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Imagine you're choosing a restaurant based on reviews...     │\n",
    "    │                                                                │\n",
    "    │  THE OVERESTIMATION PROBLEM (Fixed by Double DQN):            │\n",
    "    │    You always pick the restaurant with the HIGHEST rating.    │\n",
    "    │    But ratings are noisy! That \"5-star\" place might be:       │\n",
    "    │    - Actually 4.5 stars + lucky reviews                       │\n",
    "    │    - Meanwhile a 4.8 star place got unlucky reviews           │\n",
    "    │    Always picking max picks the luckiest, not the best!       │\n",
    "    │                                                                │\n",
    "    │  THE EFFICIENCY PROBLEM (Fixed by Dueling DQN):               │\n",
    "    │    Some neighborhoods are just BETTER (safer, nicer).         │\n",
    "    │    Any restaurant there is probably good!                     │\n",
    "    │    You can learn: \"Good neighborhood\" + \"Good menu\"           │\n",
    "    │    separately, instead of rating each restaurant from scratch.│\n",
    "    │                                                                │\n",
    "    │  THE EXPLORATION PROBLEM (Fixed by Noisy Nets):               │\n",
    "    │    With ε-greedy, you randomly try ANY restaurant.            │\n",
    "    │    Wouldn't it be smarter to be uncertain about similar      │\n",
    "    │    restaurants, not completely random?                        │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, Circle, FancyArrowPatch\n",
    "\n",
    "# Visualize DQN improvements overview\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 9.5, 'Evolution of DQN Improvements', ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Original DQN\n",
    "dqn_box = FancyBboxPatch((0.5, 6), 3, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#bbdefb', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(dqn_box)\n",
    "ax.text(2, 7.8, 'Original DQN', ha='center', fontsize=11, fontweight='bold', color='#1976d2')\n",
    "ax.text(2, 7.2, '(2015)', ha='center', fontsize=9)\n",
    "ax.text(2, 6.5, '• Experience Replay\\n• Target Network', ha='center', fontsize=9)\n",
    "\n",
    "# Double DQN\n",
    "ddqn_box = FancyBboxPatch((4.5, 6), 3, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(ddqn_box)\n",
    "ax.text(6, 7.8, 'Double DQN', ha='center', fontsize=11, fontweight='bold', color='#388e3c')\n",
    "ax.text(6, 7.2, '(2015)', ha='center', fontsize=9)\n",
    "ax.text(6, 6.5, 'Fix: Overestimation', ha='center', fontsize=9)\n",
    "\n",
    "# Dueling DQN\n",
    "duel_box = FancyBboxPatch((8.5, 6), 3, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#fff3e0', edgecolor='#f57c00', linewidth=3)\n",
    "ax.add_patch(duel_box)\n",
    "ax.text(10, 7.8, 'Dueling DQN', ha='center', fontsize=11, fontweight='bold', color='#f57c00')\n",
    "ax.text(10, 7.2, '(2016)', ha='center', fontsize=9)\n",
    "ax.text(10, 6.5, 'Fix: V/A Separation', ha='center', fontsize=9)\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(4.4, 7.25), xytext=(3.6, 7.25),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(8.4, 7.25), xytext=(7.6, 7.25),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Other improvements\n",
    "others = [\n",
    "    ('Prioritized\\nReplay', 0.5, '#e1bee7', '#7b1fa2'),\n",
    "    ('N-step\\nReturns', 3.5, '#b2dfdb', '#00796b'),\n",
    "    ('Noisy\\nNets', 6.5, '#ffe0b2', '#e65100'),\n",
    "    ('Distributional\\nRL', 9.5, '#f8bbd9', '#c2185b'),\n",
    "]\n",
    "\n",
    "for name, x, color, edge in others:\n",
    "    box = FancyBboxPatch((x, 2.5), 2.5, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor=edge, linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 1.25, 3.5, name, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Rainbow\n",
    "rainbow_box = FancyBboxPatch((4, 0), 6, 1.8, boxstyle=\"round,pad=0.1\",\n",
    "                              facecolor='#fff', edgecolor='#333', linewidth=3)\n",
    "ax.add_patch(rainbow_box)\n",
    "\n",
    "# Rainbow gradient text\n",
    "colors = ['#e57373', '#ffb74d', '#fff176', '#81c784', '#64b5f6', '#9575cd']\n",
    "for i, (c, letter) in enumerate(zip(colors, 'RAINBOW')):\n",
    "    ax.text(5.5 + i*0.5, 1.1, letter, fontsize=14, fontweight='bold', color=c)\n",
    "ax.text(7, 0.5, '(2017) - Combines ALL improvements!', ha='center', fontsize=10)\n",
    "\n",
    "# Arrows to Rainbow\n",
    "for x in [1.75, 4.75, 7.75, 10.75]:\n",
    "    ax.annotate('', xy=(7, 1.9), xytext=(x, 2.4),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1, color='#666', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DQN IMPROVEMENTS TIMELINE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "2015: Original DQN - First deep RL to play Atari\n",
    "2015: Double DQN - Fixed overestimation bias\n",
    "2015: Prioritized Replay - Learn from important experiences\n",
    "2016: Dueling DQN - Separated value and advantage\n",
    "2017: Rainbow - Combined everything for 2x performance!\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1: Overestimation Bias\n",
    "\n",
    "DQN has a fundamental problem: it **overestimates** Q-values!\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              THE OVERESTIMATION PROBLEM                        │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  DQN TARGET: y = r + γ × max_a' Q(s', a')                     │\n",
    "    │                          └────────────┘                        │\n",
    "    │                          This is the problem!                  │\n",
    "    │                                                                │\n",
    "    │  WHY MAX CAUSES OVERESTIMATION:                               │\n",
    "    │                                                                │\n",
    "    │  Q-values have noise (estimation errors):                     │\n",
    "    │    True Q:  [3.0, 2.8, 2.9, 3.1]   (action 4 is actually best)│\n",
    "    │    Noisy Q: [3.5, 2.6, 3.2, 2.9]   (with estimation errors)   │\n",
    "    │                ↑                                              │\n",
    "    │    max picks: 3.5 (WRONG! This is noise, not skill!)          │\n",
    "    │                                                                │\n",
    "    │  RESULT: We consistently overestimate because max always     │\n",
    "    │  picks the action with the highest NOISE + value!            │\n",
    "    │                                                                │\n",
    "    │  E[max(X + noise)] > max(X) when there's noise!              │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overestimation bias\n",
    "np.random.seed(42)\n",
    "\n",
    "# True Q-values for 4 actions\n",
    "true_q = np.array([2.0, 2.2, 2.1, 2.3])  # Action 4 is truly best (2.3)\n",
    "n_samples = 1000\n",
    "\n",
    "# Simulate noisy Q-estimates\n",
    "noise_std = 0.5\n",
    "max_estimates = []\n",
    "selected_actions = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    noisy_q = true_q + np.random.randn(4) * noise_std\n",
    "    max_estimates.append(np.max(noisy_q))\n",
    "    selected_actions.append(np.argmax(noisy_q))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Left: Distribution of max estimates\n",
    "ax1 = axes[0]\n",
    "ax1.hist(max_estimates, bins=50, color='#ef5350', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(np.max(true_q), color='#388e3c', linewidth=3, linestyle='--', \n",
    "            label=f'True max: {np.max(true_q):.2f}')\n",
    "ax1.axvline(np.mean(max_estimates), color='#d32f2f', linewidth=3, \n",
    "            label=f'Avg estimate: {np.mean(max_estimates):.2f}')\n",
    "ax1.set_xlabel('Max Q-value Estimate', fontsize=11)\n",
    "ax1.set_ylabel('Count', fontsize=11)\n",
    "ax1.set_title('Overestimation Bias\\n(max picks highest noise!)', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Middle: Which action is selected?\n",
    "ax2 = axes[1]\n",
    "action_counts = [selected_actions.count(i) for i in range(4)]\n",
    "colors = ['#90caf9', '#90caf9', '#90caf9', '#81c784']  # Highlight true best\n",
    "bars = ax2.bar(['A1', 'A2', 'A3', 'A4 (best)'], action_counts, color=colors, edgecolor='black')\n",
    "ax2.set_ylabel('Times Selected', fontsize=11)\n",
    "ax2.set_title('Action Selection with Noise\\n(Often picks wrong action!)', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Right: Overestimation grows with more actions\n",
    "ax3 = axes[2]\n",
    "n_actions_list = [2, 4, 8, 16, 32]\n",
    "overestimation = []\n",
    "\n",
    "for n_actions in n_actions_list:\n",
    "    true_max = 2.5  # Assume all true values are similar\n",
    "    estimates = []\n",
    "    for _ in range(500):\n",
    "        noisy = np.random.randn(n_actions) * noise_std + true_max - 0.2\n",
    "        estimates.append(np.max(noisy))\n",
    "    overestimation.append(np.mean(estimates) - true_max)\n",
    "\n",
    "ax3.plot(n_actions_list, overestimation, 'ro-', linewidth=2, markersize=10)\n",
    "ax3.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Number of Actions', fontsize=11)\n",
    "ax3.set_ylabel('Overestimation', fontsize=11)\n",
    "ax3.set_title('More Actions = More Overestimation', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOVERESTIMATION ANALYSIS:\")\n",
    "print(f\"  True max Q-value: {np.max(true_q):.2f}\")\n",
    "print(f\"  Average max estimate: {np.mean(max_estimates):.2f}\")\n",
    "print(f\"  Overestimation: +{np.mean(max_estimates) - np.max(true_q):.2f}\")\n",
    "print(f\"\\n  Times correct action selected: {selected_actions.count(3)/n_samples*100:.1f}%\")\n",
    "print(f\"  (Should be higher, but noise causes wrong selections!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solution 1: Double DQN\n",
    "\n",
    "The elegant fix: **Decouple selection from evaluation!**\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                    DOUBLE DQN                                  │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  ORIGINAL DQN:                                                │\n",
    "    │    target = r + γ × max_a' Q(s', a'; θ⁻)                      │\n",
    "    │                    └─────────────────┘                         │\n",
    "    │               Same network selects AND evaluates              │\n",
    "    │                                                                │\n",
    "    │  DOUBLE DQN:                                                  │\n",
    "    │    Step 1: a* = argmax_a' Q(s', a'; θ)   ← Q-net SELECTS     │\n",
    "    │    Step 2: target = r + γ × Q(s', a*; θ⁻) ← Target EVALUATES │\n",
    "    │                                                                │\n",
    "    │  WHY THIS WORKS:                                              │\n",
    "    │    • Even if Q-net's noise picks wrong action...             │\n",
    "    │    • Target net's evaluation of that action is unbiased!     │\n",
    "    │    • The two networks have DIFFERENT noise!                  │\n",
    "    │                                                                │\n",
    "    │  ANALOGY:                                                     │\n",
    "    │    • Friend 1 (Q-net): \"Let's go to Restaurant A!\"           │\n",
    "    │    • Friend 2 (Target): \"Hmm, Restaurant A is actually 3.5⭐\"│\n",
    "    │    • Two opinions reduce the bias!                           │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Double DQN vs Standard DQN\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Simple Q-Network.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "def compute_dqn_target(reward, next_state, done, target_net, gamma):\n",
    "    \"\"\"\n",
    "    Standard DQN target.\n",
    "    \n",
    "    Uses same network for both selection AND evaluation.\n",
    "    target = r + γ × max_a' Q(s', a'; θ⁻)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Target network does both: select AND evaluate\n",
    "        next_q = target_net(next_state)\n",
    "        max_next_q = next_q.max(dim=1)[0]\n",
    "        target = reward + gamma * max_next_q * (1 - done)\n",
    "    return target\n",
    "\n",
    "\n",
    "def compute_double_dqn_target(reward, next_state, done, q_net, target_net, gamma):\n",
    "    \"\"\"\n",
    "    Double DQN target - reduces overestimation!\n",
    "    \n",
    "    Step 1: Q-network SELECTS the best action\n",
    "    Step 2: Target network EVALUATES that action\n",
    "    \n",
    "    Different networks have different noise → less bias!\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # ========================================\n",
    "        # STEP 1: Q-network SELECTS best action\n",
    "        # ========================================\n",
    "        q_values = q_net(next_state)\n",
    "        best_actions = q_values.argmax(dim=1)  # What Q-net thinks is best\n",
    "        \n",
    "        # ========================================\n",
    "        # STEP 2: Target network EVALUATES\n",
    "        # ========================================\n",
    "        next_q = target_net(next_state)\n",
    "        # Get Q-value for the actions Q-net selected\n",
    "        next_q_values = next_q.gather(1, best_actions.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        target = reward + gamma * next_q_values * (1 - done)\n",
    "    return target\n",
    "\n",
    "\n",
    "# Demonstrate the difference\n",
    "print(\"DOUBLE DQN vs STANDARD DQN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "state_dim, action_dim = 4, 4\n",
    "q_net = QNetwork(state_dim, action_dim)\n",
    "target_net = QNetwork(state_dim, action_dim)\n",
    "\n",
    "# Sample data\n",
    "batch_size = 5\n",
    "rewards = torch.zeros(batch_size)\n",
    "next_states = torch.randn(batch_size, state_dim)\n",
    "dones = torch.zeros(batch_size)\n",
    "gamma = 0.99\n",
    "\n",
    "dqn_targets = compute_dqn_target(rewards, next_states, dones, target_net, gamma)\n",
    "ddqn_targets = compute_double_dqn_target(rewards, next_states, dones, q_net, target_net, gamma)\n",
    "\n",
    "print(\"\\nTarget values for same batch:\")\n",
    "print(f\"  DQN targets:    {dqn_targets.numpy().round(3)}\")\n",
    "print(f\"  Double DQN:     {ddqn_targets.numpy().round(3)}\")\n",
    "print(f\"\\n  Difference:     {(dqn_targets - ddqn_targets).numpy().round(3)}\")\n",
    "print(\"\\nDouble DQN typically gives LOWER (more accurate) targets!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Double DQN architecture\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Standard DQN\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Standard DQN\\n(Overestimates)', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Target network does everything\n",
    "target_box = FancyBboxPatch((3, 5), 4, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=3)\n",
    "ax1.add_patch(target_box)\n",
    "ax1.text(5, 6.8, 'Target Network (θ⁻)', ha='center', fontsize=11, fontweight='bold')\n",
    "ax1.text(5, 6.0, 'SELECT: argmax Q(s\\',a\\';θ⁻)', ha='center', fontsize=9)\n",
    "ax1.text(5, 5.4, 'EVALUATE: Q(s\\',a*;θ⁻)', ha='center', fontsize=9)\n",
    "\n",
    "# Input/Output arrows\n",
    "ax1.annotate('', xy=(5, 4.9), xytext=(5, 3.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f'))\n",
    "ax1.text(5, 3, 'Target: r + γ×max Q', ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "ax1.annotate('', xy=(5, 7.6), xytext=(5, 8.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax1.text(5, 9, \"Next state s'\", ha='center', fontsize=10)\n",
    "\n",
    "ax1.text(5, 1.5, '❌ Same noise for selection\\n    and evaluation!', \n",
    "         ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "# Right: Double DQN\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Double DQN\\n(Accurate)', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Q-network for selection\n",
    "q_box = FancyBboxPatch((0.5, 5), 4, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                        facecolor='#bbdefb', edgecolor='#1976d2', linewidth=3)\n",
    "ax2.add_patch(q_box)\n",
    "ax2.text(2.5, 6.8, 'Q-Network (θ)', ha='center', fontsize=11, fontweight='bold')\n",
    "ax2.text(2.5, 6.0, 'SELECT:', ha='center', fontsize=9)\n",
    "ax2.text(2.5, 5.4, 'a* = argmax Q(s\\',a\\';θ)', ha='center', fontsize=9)\n",
    "\n",
    "# Target network for evaluation\n",
    "target_box2 = FancyBboxPatch((5.5, 5), 4, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                              facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax2.add_patch(target_box2)\n",
    "ax2.text(7.5, 6.8, 'Target Network (θ⁻)', ha='center', fontsize=11, fontweight='bold')\n",
    "ax2.text(7.5, 6.0, 'EVALUATE:', ha='center', fontsize=9)\n",
    "ax2.text(7.5, 5.4, 'Q(s\\', a*; θ⁻)', ha='center', fontsize=9)\n",
    "\n",
    "# Arrow between networks\n",
    "ax2.annotate('', xy=(5.4, 6.25), xytext=(4.6, 6.25),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax2.text(5, 6.8, 'a*', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Output\n",
    "ax2.annotate('', xy=(5, 4.9), xytext=(5, 3.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#388e3c'))\n",
    "ax2.text(5, 3, 'Target: r + γ×Q(s\\',a*)', ha='center', fontsize=10, color='#388e3c')\n",
    "\n",
    "ax2.text(5, 1.5, '✓ Different noise for\\n    selection vs evaluation!', \n",
    "         ha='center', fontsize=10, color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2: Learning Efficiency\n",
    "\n",
    "Sometimes the state matters more than the action!\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              THE EFFICIENCY PROBLEM                            │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Consider playing Pong near the end of a point:               │\n",
    "    │                                                                │\n",
    "    │  SITUATION 1: Ball going into opponent's corner               │\n",
    "    │    → You're going to WIN no matter what action you take       │\n",
    "    │    → The STATE is good, actions don't matter much             │\n",
    "    │                                                                │\n",
    "    │  SITUATION 2: Ball coming at you very fast                    │\n",
    "    │    → Your ACTION matters a lot! Move or lose!                 │\n",
    "    │    → Need to distinguish between good and bad actions         │\n",
    "    │                                                                │\n",
    "    │  INSIGHT: Sometimes V(s) is enough, sometimes A(s,a) matters! │\n",
    "    │                                                                │\n",
    "    │  SOLUTION: Learn BOTH separately, then combine:               │\n",
    "    │    Q(s, a) = V(s) + A(s, a)                                   │\n",
    "    │             ↑        ↑                                        │\n",
    "    │     \"How good is    \"How much better                         │\n",
    "    │      this state?\"    is this action?\"                        │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solution 2: Dueling DQN\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                    DUELING ARCHITECTURE                        │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │                        ┌───────────┐                          │\n",
    "    │                        │   State   │                          │\n",
    "    │                        │   Input   │                          │\n",
    "    │                        └─────┬─────┘                          │\n",
    "    │                              │                                │\n",
    "    │                        ┌─────▼─────┐                          │\n",
    "    │                        │  Shared   │                          │\n",
    "    │                        │  Layers   │                          │\n",
    "    │                        └─────┬─────┘                          │\n",
    "    │                      ┌───────┴───────┐                        │\n",
    "    │                      │               │                        │\n",
    "    │                ┌─────▼─────┐   ┌─────▼─────┐                  │\n",
    "    │                │   Value   │   │ Advantage │                  │\n",
    "    │                │  Stream   │   │  Stream   │                  │\n",
    "    │                │   V(s)    │   │   A(s,a)  │                  │\n",
    "    │                └─────┬─────┘   └─────┬─────┘                  │\n",
    "    │                      │               │                        │\n",
    "    │                      └───────┬───────┘                        │\n",
    "    │                              │                                │\n",
    "    │                   Q(s,a) = V(s) + A(s,a) - mean(A)            │\n",
    "    │                                                                │\n",
    "    │  WHY SUBTRACT MEAN(A)?                                        │\n",
    "    │    To make V(s) uniquely identifiable!                        │\n",
    "    │    Otherwise V could absorb A or vice versa.                  │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling DQN Architecture.\n",
    "    \n",
    "    Separates the Q-value into:\n",
    "    - V(s): How good is this state?\n",
    "    - A(s,a): How much better is this action than average?\n",
    "    \n",
    "    Q(s, a) = V(s) + A(s, a) - mean(A(s, .))\n",
    "    \n",
    "    Benefits:\n",
    "    - Can learn state value even when actions don't matter\n",
    "    - More sample efficient\n",
    "    - Better generalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ========================================\n",
    "        # SHARED FEATURE EXTRACTION\n",
    "        # ========================================\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # VALUE STREAM: V(s) - single number\n",
    "        # \"How good is this state overall?\"\n",
    "        # ========================================\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)  # Single value output\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # ADVANTAGE STREAM: A(s, a) - one per action\n",
    "        # \"How much better is each action than average?\"\n",
    "        # ========================================\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, action_dim)  # One per action\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass: combine value and advantage streams.\n",
    "        \n",
    "        Q(s, a) = V(s) + A(s, a) - mean(A(s, .))\n",
    "        \n",
    "        Subtracting mean(A) ensures V(s) is unique.\n",
    "        \"\"\"\n",
    "        # Extract shared features\n",
    "        features = self.features(state)\n",
    "        \n",
    "        # Compute value and advantage\n",
    "        value = self.value_stream(features)           # Shape: (batch, 1)\n",
    "        advantage = self.advantage_stream(features)   # Shape: (batch, actions)\n",
    "        \n",
    "        # ========================================\n",
    "        # COMBINE: Q = V + A - mean(A)\n",
    "        # ========================================\n",
    "        # Subtract mean advantage for identifiability\n",
    "        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        return q_values\n",
    "    \n",
    "    def get_value_and_advantage(self, state):\n",
    "        \"\"\"Helper to see V and A separately (for visualization).\"\"\"\n",
    "        features = self.features(state)\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "        return value, advantage\n",
    "\n",
    "\n",
    "# Create and examine Dueling DQN\n",
    "print(\"DUELING DQN DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "state_dim = 4\n",
    "action_dim = 4\n",
    "dueling = DuelingDQN(state_dim, action_dim)\n",
    "\n",
    "# Test on sample state\n",
    "test_state = torch.randn(1, state_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    q_values = dueling(test_state)\n",
    "    value, advantage = dueling.get_value_and_advantage(test_state)\n",
    "\n",
    "print(f\"\\nFor a sample state:\")\n",
    "print(f\"  V(s) = {value.item():.3f} (state value)\")\n",
    "print(f\"  A(s, a) = {advantage.numpy()[0].round(3)} (advantage per action)\")\n",
    "print(f\"  Q(s, a) = {q_values.numpy()[0].round(3)}\")\n",
    "\n",
    "print(f\"\\nVerify: Q = V + A - mean(A)\")\n",
    "mean_adv = advantage.mean().item()\n",
    "reconstructed = value.item() + advantage.numpy()[0] - mean_adv\n",
    "print(f\"  Computed: {reconstructed.round(3)}\")\n",
    "print(f\"  Matches!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Dueling architecture\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "# Left: Standard DQN architecture\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Standard DQN Architecture', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Input\n",
    "input_box = FancyBboxPatch((3.5, 8), 3, 1, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#bbdefb', edgecolor='#1976d2', linewidth=2)\n",
    "ax1.add_patch(input_box)\n",
    "ax1.text(5, 8.5, 'State Input', ha='center', fontsize=10)\n",
    "\n",
    "# Hidden layers\n",
    "for i, y in enumerate([6, 4]):\n",
    "    box = FancyBboxPatch((3.5, y), 3, 1, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(5, y + 0.5, f'Hidden Layer {i+1}', ha='center', fontsize=10)\n",
    "    ax1.annotate('', xy=(5, y + 1), xytext=(5, y + 1.9),\n",
    "                 arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Output\n",
    "output_box = FancyBboxPatch((3.5, 1.5), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax1.add_patch(output_box)\n",
    "ax1.text(5, 2.5, 'Q(s,a₁), Q(s,a₂), ...', ha='center', fontsize=10)\n",
    "ax1.text(5, 2, '(One output per action)', ha='center', fontsize=9, color='#666')\n",
    "ax1.annotate('', xy=(5, 3.1), xytext=(5, 3.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Right: Dueling DQN architecture\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Dueling DQN Architecture', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Input\n",
    "input_box = FancyBboxPatch((3.5, 8), 3, 1, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#bbdefb', edgecolor='#1976d2', linewidth=2)\n",
    "ax2.add_patch(input_box)\n",
    "ax2.text(5, 8.5, 'State Input', ha='center', fontsize=10)\n",
    "\n",
    "# Shared layers\n",
    "shared_box = FancyBboxPatch((3.5, 6), 3, 1, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax2.add_patch(shared_box)\n",
    "ax2.text(5, 6.5, 'Shared Features', ha='center', fontsize=10)\n",
    "ax2.annotate('', xy=(5, 7), xytext=(5, 7.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Value stream\n",
    "value_box = FancyBboxPatch((1, 4), 3, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#e1bee7', edgecolor='#7b1fa2', linewidth=2)\n",
    "ax2.add_patch(value_box)\n",
    "ax2.text(2.5, 4.9, 'Value Stream', ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.text(2.5, 4.3, 'V(s)', ha='center', fontsize=10)\n",
    "\n",
    "# Advantage stream\n",
    "adv_box = FancyBboxPatch((6, 4), 3, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#b2dfdb', edgecolor='#00796b', linewidth=2)\n",
    "ax2.add_patch(adv_box)\n",
    "ax2.text(7.5, 4.9, 'Advantage Stream', ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.text(7.5, 4.3, 'A(s,a)', ha='center', fontsize=10)\n",
    "\n",
    "# Arrows from shared to streams\n",
    "ax2.annotate('', xy=(2.5, 5.3), xytext=(4, 6),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax2.annotate('', xy=(7.5, 5.3), xytext=(6, 6),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Combination\n",
    "combine_box = FancyBboxPatch((3.5, 1.5), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                              facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax2.add_patch(combine_box)\n",
    "ax2.text(5, 2.6, 'Q = V + A - mean(A)', ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.text(5, 2, 'Q(s,a₁), Q(s,a₂), ...', ha='center', fontsize=9)\n",
    "\n",
    "# Arrows to combination\n",
    "ax2.annotate('', xy=(4, 3.1), xytext=(2.5, 3.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#7b1fa2'))\n",
    "ax2.annotate('', xy=(6, 3.1), xytext=(7.5, 3.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#00796b'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDUELING ADVANTAGE:\")\n",
    "print(\"  • Can learn V(s) even when all actions are equally good\")\n",
    "print(\"  • More efficient learning in states where actions don't matter\")\n",
    "print(\"  • Better generalization across states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Other Important Improvements\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              OTHER DQN IMPROVEMENTS                            │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  N-STEP RETURNS:                                              │\n",
    "    │    Instead of: y = r + γ × Q(s')                              │\n",
    "    │    Use: y = r₀ + γr₁ + γ²r₂ + ... + γⁿQ(sₙ)                  │\n",
    "    │    → Better credit assignment, less bias                      │\n",
    "    │                                                                │\n",
    "    │  NOISY NETWORKS:                                              │\n",
    "    │    Replace ε-greedy with learned noise in weights             │\n",
    "    │    w → w + σ × ε where ε ~ N(0,1)                            │\n",
    "    │    → Smarter exploration, state-dependent uncertainty         │\n",
    "    │                                                                │\n",
    "    │  DISTRIBUTIONAL RL (C51):                                     │\n",
    "    │    Don't predict E[return], predict the full distribution!    │\n",
    "    │    → Captures uncertainty, more stable learning               │\n",
    "    │                                                                │\n",
    "    │  PRIORITIZED REPLAY (covered in notebook 3):                  │\n",
    "    │    Sample important experiences more often                    │\n",
    "    │    → More efficient use of experience                         │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement N-step returns\n",
    "\n",
    "def compute_nstep_return(rewards, next_q_value, gamma, n_steps):\n",
    "    \"\"\"\n",
    "    Compute n-step return.\n",
    "    \n",
    "    G_t = r_t + γr_{t+1} + γ²r_{t+2} + ... + γⁿQ(s_{t+n})\n",
    "    \n",
    "    Benefits:\n",
    "    - Less bias than 1-step (uses more actual rewards)\n",
    "    - Less variance than Monte Carlo (still bootstraps)\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of n rewards [r_t, r_{t+1}, ..., r_{t+n-1}]\n",
    "        next_q_value: Q-value at state n steps ahead\n",
    "        gamma: Discount factor\n",
    "        n_steps: Number of steps to look ahead\n",
    "    \"\"\"\n",
    "    n_step_return = 0\n",
    "    \n",
    "    # Sum discounted rewards\n",
    "    for i, r in enumerate(rewards):\n",
    "        n_step_return += (gamma ** i) * r\n",
    "    \n",
    "    # Add bootstrapped value\n",
    "    n_step_return += (gamma ** n_steps) * next_q_value\n",
    "    \n",
    "    return n_step_return\n",
    "\n",
    "\n",
    "# Demonstrate n-step returns\n",
    "print(\"N-STEP RETURNS DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example: 3 steps of experience\n",
    "rewards = [1, 1, 1]  # Got reward 1 for 3 consecutive steps\n",
    "next_q = 5.0         # Q-value at state 3 steps ahead\n",
    "gamma = 0.9\n",
    "\n",
    "print(f\"\\nRewards: {rewards}\")\n",
    "print(f\"Q(s_3): {next_q}\")\n",
    "print(f\"Gamma: {gamma}\")\n",
    "\n",
    "print(\"\\nDifferent n-step returns:\")\n",
    "for n in [1, 2, 3]:\n",
    "    ret = compute_nstep_return(rewards[:n], next_q, gamma, n)\n",
    "    formula = ' + '.join([f'{gamma}^{i}×{r}' for i, r in enumerate(rewards[:n])])\n",
    "    formula += f' + {gamma}^{n}×{next_q}'\n",
    "    print(f\"  n={n}: {ret:.3f}  ({formula})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"More steps = less bias (more real rewards) but more variance\")\n",
    "print(\"Rainbow uses n=3 as a good balance!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Noisy Linear Layer for exploration.\n",
    "    \n",
    "    Instead of ε-greedy (random with probability ε),\n",
    "    we add learned noise to the weights.\n",
    "    \n",
    "    w = μ_w + σ_w * ε  where ε ~ N(0, 1)\n",
    "    \n",
    "    Benefits:\n",
    "    - Exploration is state-dependent (not random)\n",
    "    - Network learns when to explore\n",
    "    - No need to tune ε schedule\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, sigma_init=0.5):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        \n",
    "        # Initialize\n",
    "        mu_range = 1 / np.sqrt(in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(sigma_init / np.sqrt(in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(sigma_init / np.sqrt(out_features))\n",
    "        \n",
    "        # Register buffer for noise (not a parameter)\n",
    "        self.register_buffer('weight_epsilon', torch.zeros(out_features, in_features))\n",
    "        self.register_buffer('bias_epsilon', torch.zeros(out_features))\n",
    "        \n",
    "        self.reset_noise()\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        \"\"\"Sample new noise.\"\"\"\n",
    "        self.weight_epsilon.normal_()\n",
    "        self.bias_epsilon.normal_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply noisy linear transformation.\"\"\"\n",
    "        if self.training:\n",
    "            # Use noisy weights during training\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            # Use mean weights during evaluation (greedy)\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        \n",
    "        return F.linear(x, weight, bias)\n",
    "\n",
    "\n",
    "# Demonstrate Noisy Networks\n",
    "print(\"NOISY NETWORKS DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "noisy_layer = NoisyLinear(4, 4)\n",
    "test_input = torch.randn(1, 4)\n",
    "\n",
    "print(\"\\nSame input, different noise:\")\n",
    "noisy_layer.train()  # Training mode = use noise\n",
    "for i in range(3):\n",
    "    noisy_layer.reset_noise()  # New noise\n",
    "    output = noisy_layer(test_input)\n",
    "    print(f\"  Sample {i+1}: {output.detach().numpy()[0].round(3)}\")\n",
    "\n",
    "print(\"\\nDuring evaluation (no noise):\")\n",
    "noisy_layer.eval()  # Eval mode = no noise\n",
    "output_eval = noisy_layer(test_input)\n",
    "print(f\"  Consistent: {output_eval.detach().numpy()[0].round(3)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOISY NETS vs ε-GREEDY:\")\n",
    "print(\"  ε-greedy: Random action with probability ε (state-blind)\")\n",
    "print(\"  Noisy nets: Uncertainty is STATE-DEPENDENT (smarter!)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Rainbow DQN: All Together\n",
    "\n",
    "Rainbow combines ALL the improvements for maximum performance!\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                    RAINBOW DQN (2017)                          │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Rainbow = DQN + ALL improvements:                            │\n",
    "    │                                                                │\n",
    "    │    1. ✓ Double DQN - Reduce overestimation                    │\n",
    "    │    2. ✓ Dueling architecture - V/A separation                 │\n",
    "    │    3. ✓ Prioritized Experience Replay - Sample efficiently    │\n",
    "    │    4. ✓ N-step returns - Better credit assignment             │\n",
    "    │    5. ✓ Distributional RL (C51) - Learn return distribution   │\n",
    "    │    6. ✓ Noisy Networks - Smarter exploration                  │\n",
    "    │                                                                │\n",
    "    │  PERFORMANCE:                                                 │\n",
    "    │    • 2x median score of any single improvement                │\n",
    "    │    • State-of-the-art on Atari in 2017                        │\n",
    "    │                                                                │\n",
    "    │  FOR PRODUCTION: Use Stable-Baselines3 or CleanRL!            │\n",
    "    │    Don't implement from scratch unless learning.              │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Rainbow components and their contributions\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "components = [\n",
    "    'DQN (baseline)',\n",
    "    '+ Double DQN',\n",
    "    '+ Prioritized Replay',\n",
    "    '+ Dueling',\n",
    "    '+ N-step',\n",
    "    '+ Distributional',\n",
    "    '+ Noisy Nets',\n",
    "    'Rainbow (all)'\n",
    "]\n",
    "\n",
    "# Approximate relative improvements (illustrative)\n",
    "scores = [100, 115, 135, 150, 170, 200, 210, 250]\n",
    "\n",
    "colors = ['#bbdefb', '#c8e6c9', '#e1bee7', '#fff3e0', '#b2dfdb', '#f8bbd9', '#ffe0b2', '#ff8a65']\n",
    "\n",
    "bars = ax.barh(components, scores, color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add score labels\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax.text(bar.get_width() + 5, bar.get_y() + bar.get_height()/2, \n",
    "            f'{score}%', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Relative Performance (DQN = 100%)', fontsize=12)\n",
    "ax.set_title('Rainbow DQN: Each Component Adds Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 280)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Highlight Rainbow\n",
    "bars[-1].set_edgecolor('#d32f2f')\n",
    "bars[-1].set_linewidth(3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHT:\")\n",
    "print(\"  • Each improvement helps on its own\")\n",
    "print(\"  • Combined, they're even better (synergy!)\")\n",
    "print(\"  • Rainbow = 2.5x baseline DQN performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### The Problems and Solutions\n",
    "\n",
    "| Problem | Solution | Key Idea |\n",
    "|---------|----------|----------|\n",
    "| Overestimation | Double DQN | Separate selection and evaluation |\n",
    "| Efficiency | Dueling DQN | Learn V(s) and A(s,a) separately |\n",
    "| Credit assignment | N-step | Use more real rewards |\n",
    "| Exploration | Noisy Nets | Learned, state-dependent noise |\n",
    "| Sample efficiency | PER | Prioritize surprising experiences |\n",
    "\n",
    "### Double DQN Formula\n",
    "\n",
    "```\n",
    "a* = argmax Q(s', a'; θ)      ← Q-network SELECTS\n",
    "target = r + γ × Q(s', a*; θ⁻)  ← Target EVALUATES\n",
    "```\n",
    "\n",
    "### Dueling DQN Formula\n",
    "\n",
    "```\n",
    "Q(s, a) = V(s) + A(s, a) - mean(A(s, .))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why does DQN overestimate Q-values?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The max operator always picks the action with the highest Q-value, but Q-values have estimation noise. Taking the max of noisy estimates gives a biased result - we're likely picking the action that got lucky with positive noise, not necessarily the truly best action. E[max(X + noise)] > max(X).\n",
    "</details>\n",
    "\n",
    "**2. How does Double DQN fix overestimation?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Double DQN decouples action selection from action evaluation:\n",
    "1. The Q-network selects the best action: a* = argmax Q(s', a'; θ)\n",
    "2. The target network evaluates that action: Q(s', a*; θ⁻)\n",
    "\n",
    "Since the two networks have different noise, even if the Q-network picks a lucky action, the target network's evaluation of that action is unbiased.\n",
    "</details>\n",
    "\n",
    "**3. What are V(s) and A(s,a) in Dueling DQN?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "- V(s) is the state value: \"How good is this state overall, regardless of action?\"\n",
    "- A(s,a) is the advantage: \"How much better is this specific action than the average action in this state?\"\n",
    "\n",
    "Q(s,a) = V(s) + A(s,a) - mean(A). This lets the network learn state value separately from action preferences, which is more efficient when actions don't matter much.\n",
    "</details>\n",
    "\n",
    "**4. Why subtract mean(A) in Dueling DQN?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "For identifiability! Without subtracting the mean, we can't uniquely determine V(s) and A(s,a) - you could add any constant to V and subtract it from A without changing Q. By forcing A to have mean 0, we ensure V(s) is uniquely identifiable as the expected value over actions.\n",
    "</details>\n",
    "\n",
    "**5. What's the advantage of Noisy Networks over ε-greedy?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Noisy Networks provide state-dependent exploration:\n",
    "- ε-greedy: Random action with fixed probability (same everywhere)\n",
    "- Noisy Nets: Uncertainty depends on the state and is learned\n",
    "\n",
    "The network can learn to be uncertain in new states and confident in familiar ones. No need to manually tune ε schedule!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "You now understand all the major DQN improvements!\n",
    "\n",
    "In the final notebook of this section, we'll see these techniques in action on Atari games.\n",
    "\n",
    "**Continue to:** [Notebook 6: Atari Games](06_atari_games.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Rainbow: \"Why choose one improvement when you can have them all?\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
