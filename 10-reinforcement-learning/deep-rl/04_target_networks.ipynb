{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Networks: The Slow Twin That Stabilizes Training\n",
    "\n",
    "Target networks are the second key innovation that made DQN stable. Without them, DQN would chase a moving target and never converge!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The moving target problem (with a soccer analogy!)\n",
    "- Why targets change when we update the Q-network\n",
    "- Hard vs soft target updates\n",
    "- Implementing target networks in PyTorch\n",
    "- The impact on training stability\n",
    "\n",
    "**Prerequisites:** Notebooks 2-3 (DQN, Experience Replay)\n",
    "\n",
    "**Time:** ~25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Soccer Analogy\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          THE MOVING TARGET PROBLEM: SOCCER ANALOGY            â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Imagine you're learning to kick a soccer ball into a goal.   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  NORMAL SITUATION (Stable Target):                            â”‚\n",
    "    â”‚    ğŸ¥… The goal stays in one place                             â”‚\n",
    "    â”‚    âš½ You kick, miss slightly to the left                     â”‚\n",
    "    â”‚    ğŸ“Š You adjust: \"Aim a bit more to the right\"              â”‚\n",
    "    â”‚    âœ“ You keep improving!                                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  DQN WITHOUT TARGET NETWORK (Moving Target):                  â”‚\n",
    "    â”‚    ğŸ¥… Every time you kick, the goal MOVES!                    â”‚\n",
    "    â”‚    âš½ You kick, it goes where the goal WAS                    â”‚\n",
    "    â”‚    ğŸ“Š But now the goal is somewhere else...                   â”‚\n",
    "    â”‚    âŒ You can never hit a moving target!                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  DQN WITH TARGET NETWORK (Semi-Stable Target):                â”‚\n",
    "    â”‚    ğŸ¥… The goal stays FIXED for 100 kicks                      â”‚\n",
    "    â”‚    âš½ You learn to hit that position                          â”‚\n",
    "    â”‚    ğŸ“Š Goal moves to new spot, you adapt                       â”‚\n",
    "    â”‚    âœ“ You can track the slowly-moving target!                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, Circle, FancyArrowPatch\n",
    "\n",
    "# Visualize the moving target problem\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Left: Stable target (normal learning)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Normal Learning\\n(Fixed Target)', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Fixed target\n",
    "target_circle = Circle((7, 5), 0.8, facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax1.add_patch(target_circle)\n",
    "ax1.text(7, 5, 'TARGET', ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "# Agent improving\n",
    "for i, (x, y) in enumerate([(2, 3), (3, 4), (4.5, 4.5), (6, 5)]):\n",
    "    alpha = 0.3 + 0.2 * i\n",
    "    ax1.scatter(x, y, s=100, c='blue', alpha=alpha, zorder=5)\n",
    "    if i < 3:\n",
    "        ax1.annotate('', xy=(x+0.3, y+0.2), xytext=(x-0.2, y-0.2),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=1, color='blue', alpha=alpha))\n",
    "\n",
    "ax1.text(5, 2, 'âœ“ Converges!', ha='center', fontsize=11, color='#388e3c', fontweight='bold')\n",
    "ax1.text(5, 1, 'Target stays put', ha='center', fontsize=10)\n",
    "\n",
    "# Middle: Moving target (unstable)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('DQN Without Target Net\\n(Moving Target)', fontsize=12, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Multiple target positions (moving)\n",
    "target_positions = [(3, 7), (7, 6), (5, 3), (8, 8)]\n",
    "for i, (tx, ty) in enumerate(target_positions):\n",
    "    alpha = 0.2 + 0.2 * i\n",
    "    circle = Circle((tx, ty), 0.5, facecolor='#ffcdd2', edgecolor='#d32f2f', \n",
    "                    linewidth=2, alpha=alpha)\n",
    "    ax2.add_patch(circle)\n",
    "    if i > 0:\n",
    "        ax2.annotate('', xy=(tx, ty), xytext=target_positions[i-1],\n",
    "                    arrowprops=dict(arrowstyle='->', lw=2, color='red', alpha=0.5,\n",
    "                                   connectionstyle='arc3,rad=0.2'))\n",
    "\n",
    "# Chasing agent (never catches up)\n",
    "ax2.scatter([2, 4, 6, 7], [6, 5, 4, 7], s=80, c='blue', zorder=5)\n",
    "\n",
    "ax2.text(5, 1.5, 'âŒ Never converges!', ha='center', fontsize=11, color='#d32f2f', fontweight='bold')\n",
    "ax2.text(5, 0.5, 'Target moves every update', ha='center', fontsize=10)\n",
    "\n",
    "# Right: Target network (stable enough)\n",
    "ax3 = axes[2]\n",
    "ax3.set_xlim(0, 10)\n",
    "ax3.set_ylim(0, 10)\n",
    "ax3.axis('off')\n",
    "ax3.set_title('DQN With Target Net\\n(Periodic Updates)', fontsize=12, fontweight='bold', color='#1976d2')\n",
    "\n",
    "# Two fixed positions with arrows\n",
    "circle1 = Circle((3, 6), 0.6, facecolor='#bbdefb', edgecolor='#1976d2', linewidth=2)\n",
    "ax3.add_patch(circle1)\n",
    "ax3.text(3, 6, 'T1', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "circle2 = Circle((7, 5), 0.6, facecolor='#90caf9', edgecolor='#1976d2', linewidth=3)\n",
    "ax3.add_patch(circle2)\n",
    "ax3.text(7, 5, 'T2', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Arrow showing periodic update\n",
    "ax3.annotate('', xy=(6.3, 5.2), xytext=(3.7, 5.8),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#1976d2', linestyle='--'))\n",
    "ax3.text(5, 6.5, 'Update every\\nN steps', ha='center', fontsize=9, color='#1976d2')\n",
    "\n",
    "# Agent converging to each\n",
    "ax3.scatter([1.5, 2.2, 2.7], [5.5, 5.8, 6], s=60, c='blue', alpha=0.5)\n",
    "ax3.scatter([5, 6, 6.8], [5.5, 5.3, 5.1], s=60, c='blue')\n",
    "\n",
    "ax3.text(5, 1.5, 'âœ“ Converges!', ha='center', fontsize=11, color='#1976d2', fontweight='bold')\n",
    "ax3.text(5, 0.5, 'Target fixed between updates', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THE MOVING TARGET PROBLEM\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "In Q-learning, the target is: y = r + Î³ Ã— max_a' Q(s', a'; Î¸)\n",
    "\n",
    "THE PROBLEM:\n",
    "  - The target uses Q(s', a'; Î¸)\n",
    "  - When we update Î¸, the target CHANGES!\n",
    "  - We're chasing a target that moves every time we step!\n",
    "\n",
    "THE SOLUTION: Use TWO networks\n",
    "  - Q-network Î¸: Updated every step (for predictions)\n",
    "  - Target network Î¸â»: Updated slowly (for stable targets)\n",
    "  \n",
    "  y = r + Î³ Ã— max_a' Q(s', a'; Î¸â»)  â† Uses FROZEN network!\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why Targets Move: A Mathematical View\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              THE MATHEMATICS OF MOVING TARGETS                 â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WITHOUT TARGET NETWORK:                                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Step 1: Compute target                                       â”‚\n",
    "    â”‚    y = r + Î³ Ã— max Q(s', a'; Î¸)                              â”‚\n",
    "    â”‚                            â†‘                                   â”‚\n",
    "    â”‚                    uses current Î¸                              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Step 2: Update network                                       â”‚\n",
    "    â”‚    Î¸ â† Î¸ - Î± Ã— âˆ‡(y - Q(s, a; Î¸))Â²                            â”‚\n",
    "    â”‚    â†‘                                                          â”‚\n",
    "    â”‚    Î¸ changed!                                                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Step 3: But wait...                                          â”‚\n",
    "    â”‚    The target y now gives a DIFFERENT value                   â”‚\n",
    "    â”‚    because it depends on the changed Î¸!                       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WITH TARGET NETWORK:                                         â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Step 1: Compute target (using FROZEN Î¸â»)                     â”‚\n",
    "    â”‚    y = r + Î³ Ã— max Q(s', a'; Î¸â»)                             â”‚\n",
    "    â”‚                            â†‘                                   â”‚\n",
    "    â”‚                    uses frozen Î¸â»                              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Step 2: Update main network                                  â”‚\n",
    "    â”‚    Î¸ â† Î¸ - Î± Ã— âˆ‡(y - Q(s, a; Î¸))Â²                            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Step 3: Target is STABLE!                                    â”‚\n",
    "    â”‚    Î¸â» hasn't changed, so y stays the same                     â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Periodically: Copy Î¸ to Î¸â»                                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Q-network and target network\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Simple Q-Network for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "# Create Q-network and target network\n",
    "state_dim = 4\n",
    "action_dim = 2\n",
    "\n",
    "q_network = QNetwork(state_dim, action_dim)\n",
    "target_network = QNetwork(state_dim, action_dim)\n",
    "\n",
    "# ========================================\n",
    "# KEY: Initialize target to match Q-network\n",
    "# ========================================\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "print(\"TWO NETWORKS CREATED\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. Q-NETWORK (Main Network)\")\n",
    "print(\"   - Used for action selection\")\n",
    "print(\"   - Updated EVERY training step\")\n",
    "print(\"   - Learns continuously\")\n",
    "\n",
    "print(\"\\n2. TARGET NETWORK (Frozen Copy)\")\n",
    "print(\"   - Used for computing targets\")\n",
    "print(\"   - Updated only PERIODICALLY\")\n",
    "print(\"   - Provides stable targets\")\n",
    "\n",
    "# Verify they start identical\n",
    "test_state = torch.randn(1, state_dim)\n",
    "with torch.no_grad():\n",
    "    q_output = q_network(test_state)\n",
    "    target_output = target_network(test_state)\n",
    "\n",
    "print(f\"\\nQ-network output: {q_output.numpy()[0]}\")\n",
    "print(f\"Target output:    {target_output.numpy()[0]}\")\n",
    "print(f\"\\nIdentical at start: {torch.allclose(q_output, target_output)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Hard vs Soft Updates\n",
    "\n",
    "There are two ways to update the target network:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              HARD vs SOFT UPDATES                              â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  HARD UPDATE (Original DQN):                                  â”‚\n",
    "    â”‚    Every N steps (e.g., N=1000):                              â”‚\n",
    "    â”‚      Î¸â» â† Î¸                                                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    \"Suddenly replace the target network\"                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    âœ“ Simple to implement                                      â”‚\n",
    "    â”‚    âœ“ Target is completely stable between copies               â”‚\n",
    "    â”‚    âœ— Big jump when copy happens                               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  SOFT UPDATE (Polyak Averaging):                              â”‚\n",
    "    â”‚    Every step:                                                â”‚\n",
    "    â”‚      Î¸â» â† Ï„ Ã— Î¸ + (1 - Ï„) Ã— Î¸â»                               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    Where Ï„ is small (e.g., 0.005)                             â”‚\n",
    "    â”‚    \"Slowly blend toward the Q-network\"                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    âœ“ Smoother updates, no big jumps                           â”‚\n",
    "    â”‚    âœ“ Always tracking the Q-network slowly                     â”‚\n",
    "    â”‚    âœ— Target changes slightly every step                       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_update(target_net, q_net):\n",
    "    \"\"\"\n",
    "    Hard Update: Completely copy weights from Q-network to target.\n",
    "    \n",
    "    Î¸â» â† Î¸\n",
    "    \n",
    "    Called every N steps (e.g., every 1000 training steps).\n",
    "    \"\"\"\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "\n",
    "def soft_update(target_net, q_net, tau=0.005):\n",
    "    \"\"\"\n",
    "    Soft Update (Polyak Averaging): Slowly blend weights.\n",
    "    \n",
    "    Î¸â» â† Ï„ Ã— Î¸ + (1 - Ï„) Ã— Î¸â»\n",
    "    \n",
    "    Called every step. tau controls the blending rate:\n",
    "    - tau = 1.0: Same as hard update\n",
    "    - tau = 0.005: Target moves 0.5% toward Q-network each step\n",
    "    \"\"\"\n",
    "    for target_param, q_param in zip(target_net.parameters(), q_net.parameters()):\n",
    "        # Blend: target = tau * q + (1 - tau) * target\n",
    "        target_param.data.copy_(tau * q_param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "# Demonstrate the difference\n",
    "print(\"COMPARING HARD VS SOFT UPDATES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get initial weight\n",
    "initial_weight = q_network.network[0].weight.data[0, 0].item()\n",
    "print(f\"\\nInitial Q-network weight: {initial_weight:.4f}\")\n",
    "\n",
    "# Modify Q-network (simulate training)\n",
    "with torch.no_grad():\n",
    "    q_network.network[0].weight.data[0, 0] = 5.0\n",
    "\n",
    "new_weight = q_network.network[0].weight.data[0, 0].item()\n",
    "print(f\"After 'training', Q-network weight: {new_weight:.4f}\")\n",
    "\n",
    "# Apply hard update to a fresh target\n",
    "target_hard = QNetwork(state_dim, action_dim)\n",
    "target_hard.network[0].weight.data[0, 0] = initial_weight  # Start at initial\n",
    "hard_update(target_hard, q_network)\n",
    "print(f\"\\nHARD UPDATE result: {target_hard.network[0].weight.data[0, 0].item():.4f}\")\n",
    "print(\"  â†’ Jumps immediately to 5.0\")\n",
    "\n",
    "# Apply soft updates to a fresh target\n",
    "target_soft = QNetwork(state_dim, action_dim)\n",
    "target_soft.network[0].weight.data[0, 0] = initial_weight  # Start at initial\n",
    "\n",
    "print(\"\\nSOFT UPDATE (tau=0.1, 10 steps):\")\n",
    "for step in range(10):\n",
    "    soft_update(target_soft, q_network, tau=0.1)\n",
    "    if step % 3 == 0 or step == 9:\n",
    "        current = target_soft.network[0].weight.data[0, 0].item()\n",
    "        print(f\"  Step {step+1}: {current:.4f}\")\n",
    "\n",
    "print(\"  â†’ Gradually approaches 5.0\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hard vs soft updates\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Simulate updates over time\n",
    "n_steps = 200\n",
    "initial_value = 0.0\n",
    "target_value = 5.0\n",
    "\n",
    "# Hard update (every 50 steps)\n",
    "ax1 = axes[0]\n",
    "hard_values = []\n",
    "current_hard = initial_value\n",
    "\n",
    "for step in range(n_steps):\n",
    "    if step > 0 and step % 50 == 0:\n",
    "        current_hard = target_value  # Hard copy\n",
    "    hard_values.append(current_hard)\n",
    "\n",
    "ax1.plot(range(n_steps), hard_values, 'b-', linewidth=2, label='Target Network')\n",
    "ax1.axhline(y=target_value, color='gray', linestyle='--', alpha=0.5, label='Q-Network')\n",
    "ax1.set_xlabel('Training Step', fontsize=11)\n",
    "ax1.set_ylabel('Weight Value', fontsize=11)\n",
    "ax1.set_title('Hard Update (every 50 steps)', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark update points\n",
    "for step in [50, 100, 150]:\n",
    "    ax1.axvline(x=step, color='red', linestyle=':', alpha=0.5)\n",
    "    ax1.scatter([step], [target_value], c='red', s=100, zorder=5)\n",
    "\n",
    "# Soft update\n",
    "ax2 = axes[1]\n",
    "soft_values = []\n",
    "current_soft = initial_value\n",
    "tau = 0.01\n",
    "\n",
    "for step in range(n_steps):\n",
    "    current_soft = tau * target_value + (1 - tau) * current_soft\n",
    "    soft_values.append(current_soft)\n",
    "\n",
    "ax2.plot(range(n_steps), soft_values, 'g-', linewidth=2, label='Target Network')\n",
    "ax2.axhline(y=target_value, color='gray', linestyle='--', alpha=0.5, label='Q-Network')\n",
    "ax2.set_xlabel('Training Step', fontsize=11)\n",
    "ax2.set_ylabel('Weight Value', fontsize=11)\n",
    "ax2.set_title(f'Soft Update (Ï„ = {tau})', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHard Update: Sudden jumps every N steps\")\n",
    "print(\"Soft Update: Smooth, gradual approach\")\n",
    "print(\"\\nSoft updates are often preferred for smoother training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Impact on Training Stability\n",
    "\n",
    "Let's visualize why target networks help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate TD targets with and without target network\n",
    "np.random.seed(42)\n",
    "\n",
    "n_updates = 100\n",
    "true_value = 5.0\n",
    "gamma = 0.99\n",
    "\n",
    "# Without target network\n",
    "q_value_no_target = 0.0\n",
    "q_history_no_target = [q_value_no_target]\n",
    "target_history_no_target = []\n",
    "\n",
    "for _ in range(n_updates):\n",
    "    reward = np.random.randn() * 0.5\n",
    "    # Target uses same Q-value (moving target!)\n",
    "    target = reward + gamma * q_value_no_target\n",
    "    target_history_no_target.append(target)\n",
    "    # Update toward target\n",
    "    q_value_no_target += 0.1 * (target - q_value_no_target) + np.random.randn() * 0.3\n",
    "    q_history_no_target.append(q_value_no_target)\n",
    "\n",
    "# With target network (hard update every 20 steps)\n",
    "q_value_with_target = 0.0\n",
    "target_q = 0.0  # Target network Q-value\n",
    "q_history_with_target = [q_value_with_target]\n",
    "target_history_with_target = []\n",
    "\n",
    "for step in range(n_updates):\n",
    "    reward = np.random.randn() * 0.5\n",
    "    # Target uses FROZEN target Q-value\n",
    "    target = reward + gamma * target_q\n",
    "    target_history_with_target.append(target)\n",
    "    # Update toward target\n",
    "    q_value_with_target += 0.1 * (target - q_value_with_target) + np.random.randn() * 0.3\n",
    "    q_history_with_target.append(q_value_with_target)\n",
    "    \n",
    "    # Hard update every 20 steps\n",
    "    if (step + 1) % 20 == 0:\n",
    "        target_q = q_value_with_target\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot(q_history_no_target, 'r-', linewidth=2, label='Q-value')\n",
    "ax1.plot(target_history_no_target, 'r--', alpha=0.5, label='Target')\n",
    "ax1.set_xlabel('Training Step', fontsize=11)\n",
    "ax1.set_ylabel('Value', fontsize=11)\n",
    "ax1.set_title('WITHOUT Target Network\\n(Targets move with Q)', fontsize=12, fontweight='bold', color='#d32f2f')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(q_history_with_target, 'g-', linewidth=2, label='Q-value')\n",
    "ax2.plot(target_history_with_target, 'b--', alpha=0.5, label='Target (from target net)')\n",
    "for step in range(20, n_updates, 20):\n",
    "    ax2.axvline(x=step, color='gray', linestyle=':', alpha=0.3)\n",
    "ax2.set_xlabel('Training Step', fontsize=11)\n",
    "ax2.set_ylabel('Value', fontsize=11)\n",
    "ax2.set_title('WITH Target Network\\n(Targets fixed between copies)', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Without target network: More erratic, targets chase Q-values\")\n",
    "print(\"- With target network: Smoother, targets provide stable reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Putting It Together: DQN Target Computation\n",
    "\n",
    "Here's how targets are computed in DQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dqn_targets(q_network, target_network, \n",
    "                        rewards, next_states, dones, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute DQN targets using the target network.\n",
    "    \n",
    "    target = r + Î³ Ã— max_a' Q(s', a'; Î¸â»)\n",
    "                               â†‘\n",
    "                       Uses TARGET network!\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Use TARGET network for computing max Q(s', a')\n",
    "        next_q_values = target_network(next_states)\n",
    "        max_next_q = next_q_values.max(dim=1)[0]\n",
    "        \n",
    "        # Compute targets\n",
    "        # Note: (1 - dones) zeros out future value for terminal states\n",
    "        targets = rewards + gamma * max_next_q * (1 - dones)\n",
    "    \n",
    "    return targets\n",
    "\n",
    "\n",
    "# Demonstrate\n",
    "print(\"TARGET COMPUTATION IN DQN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reset networks\n",
    "q_network = QNetwork(state_dim, action_dim)\n",
    "target_network = QNetwork(state_dim, action_dim)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "# Sample batch\n",
    "batch_size = 4\n",
    "next_states = torch.randn(batch_size, state_dim)\n",
    "rewards = torch.tensor([1.0, 0.0, 1.0, 10.0])\n",
    "dones = torch.tensor([0.0, 0.0, 0.0, 1.0])  # Last one is terminal\n",
    "\n",
    "# Compute targets\n",
    "targets = compute_dqn_targets(q_network, target_network, rewards, next_states, dones)\n",
    "\n",
    "print(\"\\nBatch Information:\")\n",
    "print(f\"  Rewards: {rewards.tolist()}\")\n",
    "print(f\"  Dones:   {dones.tolist()}\")\n",
    "\n",
    "print(f\"\\nComputed Targets: {targets.tolist()}\")\n",
    "print(\"\\nNote: Terminal state (done=1) has target = reward only\")\n",
    "print(f\"  (no future value since episode ended)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### The Problem\n",
    "\n",
    "| Issue | Description |\n",
    "|-------|-------------|\n",
    "| **Moving Targets** | Target depends on Q-network that's being updated |\n",
    "| **Instability** | Updates chase a constantly-changing goal |\n",
    "| **Non-convergence** | Network oscillates instead of improving |\n",
    "\n",
    "### The Solution\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| **Target Network (Î¸â»)** | Provides stable targets for training |\n",
    "| **Periodic Updates** | Slowly sync with Q-network |\n",
    "\n",
    "### Update Methods\n",
    "\n",
    "| Method | Formula | When to Use |\n",
    "|--------|---------|-------------|\n",
    "| **Hard** | Î¸â» â† Î¸ (every N steps) | Original DQN |\n",
    "| **Soft** | Î¸â» â† Ï„Î¸ + (1-Ï„)Î¸â» (every step) | DDPG, SAC |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why do targets \"move\" in Q-learning without a target network?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The target y = r + Î³ Ã— max Q(s', a'; Î¸) depends on the Q-network weights Î¸. When we update Î¸ to minimize the loss, the Q-values for next states change, which changes the target. So both the prediction and target change together!\n",
    "</details>\n",
    "\n",
    "**2. How does a target network solve this problem?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The target network Î¸â» is a separate copy that's updated slowly (or periodically). Targets are computed using Î¸â», which stays fixed while the main network Î¸ updates. This provides a stable goal to train toward.\n",
    "</details>\n",
    "\n",
    "**3. What's the difference between hard and soft updates?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Hard update: Î¸â» â† Î¸ every N steps (complete copy)\n",
    "Soft update: Î¸â» â† Ï„Î¸ + (1-Ï„)Î¸â» every step (gradual blend)\n",
    "\n",
    "Hard updates cause sudden changes every N steps. Soft updates are smoother but targets change slightly every step.\n",
    "</details>\n",
    "\n",
    "**4. What happens to the target when an episode ends (done=True)?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "For terminal states, the target is just the reward: y = r (no Î³ Ã— max Q term). There's no future value because the episode has ended. We multiply by (1 - done) to zero out the future value term.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "You now understand both key innovations of DQN:\n",
    "1. **Experience Replay** - Breaks correlations\n",
    "2. **Target Networks** - Stabilizes targets\n",
    "\n",
    "In the next notebook, we'll explore improvements to DQN!\n",
    "\n",
    "**Continue to:** [Notebook 5: DQN Improvements](05_dqn_improvements.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Target networks: \"Give the learner a stable goal to chase.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
