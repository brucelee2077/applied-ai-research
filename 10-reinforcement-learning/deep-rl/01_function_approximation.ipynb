{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximation: Scaling RL to the Real World\n",
    "\n",
    "Welcome to the bridge between classic RL and deep RL! This notebook explains why we need function approximation and how it transforms RL.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The curse of dimensionality (with a library analogy!)\n",
    "- Why tabular methods break at scale\n",
    "- Function approximation: from memorization to generalization\n",
    "- Linear vs neural network approximation\n",
    "- The deadly triad: why combining these things is dangerous\n",
    "- How to train Q-networks with gradient descent\n",
    "\n",
    "**Prerequisites:** Classic algorithms section, neural network basics\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Library Analogy\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          THE LIBRARY ANALOGY: WHY TABLES DON'T SCALE          â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Imagine you're building a library of book reviews:           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TABULAR APPROACH (Q-Table):                                  â”‚\n",
    "    â”‚    ğŸ“š Store a separate review for EVERY possible book        â”‚\n",
    "    â”‚       \"The Great Gatsby\" â†’ \"â˜…â˜…â˜…â˜…â˜… Great American novel\"      â”‚\n",
    "    â”‚       \"1984\" â†’ \"â˜…â˜…â˜…â˜…â˜… Chilling dystopia\"                     â”‚\n",
    "    â”‚       \"Harry Potter 1\" â†’ \"â˜…â˜…â˜…â˜…â˜† Magical adventure\"           â”‚\n",
    "    â”‚       ...                                                     â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    PROBLEM: There are MILLIONS of books!                      â”‚\n",
    "    â”‚    You can't review every single one!                         â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  FUNCTION APPROXIMATION APPROACH:                             â”‚\n",
    "    â”‚    ğŸ§  Learn PATTERNS that predict good reviews:               â”‚\n",
    "    â”‚       \"Books with compelling characters â†’ Higher rating\"      â”‚\n",
    "    â”‚       \"Books by award-winning authors â†’ Usually good\"         â”‚\n",
    "    â”‚       \"Books in genres I like â†’ Higher predicted rating\"      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    Now you can predict ratings for books you've NEVER read!  â”‚\n",
    "    â”‚    This is GENERALIZATION!                                    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**The same principle applies to RL: Instead of memorizing Q-values for every state-action pair, we learn patterns that generalize!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, Circle, FancyArrowPatch\n",
    "\n",
    "# Visualize tabular vs function approximation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Tabular (memorization)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('TABULAR Q-LEARNING\\n\"Memorization\"', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Draw a table\n",
    "for row in range(5):\n",
    "    for col in range(4):\n",
    "        rect = Rectangle((1 + col*2, 6.5 - row), 1.8, 0.8, \n",
    "                         facecolor='#ffcdd2' if row == 0 else '#fff',\n",
    "                         edgecolor='black', linewidth=1)\n",
    "        ax1.add_patch(rect)\n",
    "\n",
    "# Table headers\n",
    "ax1.text(2, 7.1, 'State', ha='center', fontsize=9, fontweight='bold')\n",
    "ax1.text(4, 7.1, 'UP', ha='center', fontsize=9, fontweight='bold')\n",
    "ax1.text(6, 7.1, 'DOWN', ha='center', fontsize=9, fontweight='bold')\n",
    "ax1.text(8, 7.1, 'LEFT', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Table content\n",
    "states = ['(0,0)', '(0,1)', '(0,2)', '...']\n",
    "for i, s in enumerate(states):\n",
    "    ax1.text(2, 6.1 - i, s, ha='center', fontsize=9)\n",
    "    ax1.text(4, 6.1 - i, f'{np.random.randn():.1f}', ha='center', fontsize=8)\n",
    "    ax1.text(6, 6.1 - i, f'{np.random.randn():.1f}', ha='center', fontsize=8)\n",
    "    ax1.text(8, 6.1 - i, f'{np.random.randn():.1f}', ha='center', fontsize=8)\n",
    "\n",
    "ax1.text(5, 2.5, 'One value for EVERY\\n(state, action) pair', ha='center', fontsize=11)\n",
    "ax1.text(5, 1.5, 'âŒ Cannot handle large/continuous states', ha='center', fontsize=10, color='#d32f2f')\n",
    "ax1.text(5, 0.8, 'âŒ No generalization to new states', ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "# Right: Function approximation (generalization)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('FUNCTION APPROXIMATION\\n\"Generalization\"', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Draw a neural network\n",
    "layer_sizes = [4, 5, 4, 2]\n",
    "layer_x = [2, 4, 6, 8]\n",
    "colors = ['#bbdefb', '#fff3e0', '#fff3e0', '#c8e6c9']\n",
    "\n",
    "for l, (x, size, color) in enumerate(zip(layer_x, layer_sizes, colors)):\n",
    "    for n in range(size):\n",
    "        y = 5 + (n - (size-1)/2) * 0.8\n",
    "        circle = Circle((x, y), 0.25, facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "        ax2.add_patch(circle)\n",
    "        \n",
    "        # Draw connections to next layer\n",
    "        if l < len(layer_sizes) - 1:\n",
    "            next_size = layer_sizes[l+1]\n",
    "            for n2 in range(next_size):\n",
    "                y2 = 5 + (n2 - (next_size-1)/2) * 0.8\n",
    "                ax2.plot([x+0.25, layer_x[l+1]-0.25], [y, y2], \n",
    "                        'gray', linewidth=0.3, alpha=0.5)\n",
    "\n",
    "# Labels\n",
    "ax2.text(2, 3.2, 'State', ha='center', fontsize=10)\n",
    "ax2.text(5, 3.2, 'Hidden Layers', ha='center', fontsize=10)\n",
    "ax2.text(8, 3.2, 'Q-values', ha='center', fontsize=10)\n",
    "\n",
    "ax2.text(5, 2.2, 'Neural network LEARNS patterns', ha='center', fontsize=11)\n",
    "ax2.text(5, 1.5, 'âœ“ Works with any state representation', ha='center', fontsize=10, color='#388e3c')\n",
    "ax2.text(5, 0.8, 'âœ“ Generalizes to unseen states!', ha='center', fontsize=10, color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THE FUNDAMENTAL SHIFT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "TABULAR (Q-Table):\n",
    "  Q: (state, action) â†’ lookup in table â†’ value\n",
    "  \"What value did I store for this exact state-action pair?\"\n",
    "\n",
    "FUNCTION APPROXIMATION:\n",
    "  Q: (state, action) â†’ neural network â†’ value\n",
    "  \"Based on patterns I've learned, what should this value be?\"\n",
    "\n",
    "The key insight: Similar states should have similar values!\n",
    "Neural networks naturally learn this similarity structure.\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Curse of Dimensionality\n",
    "\n",
    "Why exactly do tables fail? It's a numbers game:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              THE CURSE OF DIMENSIONALITY                       â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  NUMBER OF STATES = (values per dimension)^(number of dims)   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Example: Each dimension can have 10 values                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Dims    States          Memory Needed                        â”‚\n",
    "    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚\n",
    "    â”‚    1     10              80 bytes                             â”‚\n",
    "    â”‚    2     100             800 bytes                            â”‚\n",
    "    â”‚    3     1,000           8 KB                                 â”‚\n",
    "    â”‚    4     10,000          80 KB                                â”‚\n",
    "    â”‚    5     100,000         800 KB                               â”‚\n",
    "    â”‚   10     10 billion      80 GB â† Already impractical!        â”‚\n",
    "    â”‚   20     10^20           10^12 GB â† More than all data!      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  REAL-WORLD EXAMPLES:                                         â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  CartPole (4 continuous dims): Infinite states!              â”‚\n",
    "    â”‚  Atari (210Ã—160Ã—3 = 100,800 dims): 256^100800 states!        â”‚\n",
    "    â”‚  Robotics (joint angles, velocities): Continuous!            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the exponential growth\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Exponential growth\n",
    "ax1 = axes[0]\n",
    "dimensions = np.arange(1, 15)\n",
    "states_per_dim = 10\n",
    "total_states = states_per_dim ** dimensions\n",
    "\n",
    "ax1.semilogy(dimensions, total_states, 'b-o', linewidth=2, markersize=8)\n",
    "ax1.axhline(y=1e6, color='orange', linestyle='--', linewidth=2, label='1 million states')\n",
    "ax1.axhline(y=1e9, color='red', linestyle='--', linewidth=2, label='1 billion states')\n",
    "ax1.axhline(y=1e12, color='darkred', linestyle='--', linewidth=2, label='1 trillion states')\n",
    "\n",
    "ax1.fill_between(dimensions, total_states, 1, alpha=0.2, color='blue')\n",
    "\n",
    "ax1.set_xlabel('Number of State Dimensions', fontsize=12)\n",
    "ax1.set_ylabel('Total States (log scale)', fontsize=12)\n",
    "ax1.set_title('The Curse of Dimensionality\\n(10 values per dimension)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(dimensions)\n",
    "\n",
    "# Right: Real-world comparison\n",
    "ax2 = axes[1]\n",
    "problems = ['4Ã—4 Grid', '10Ã—10 Grid', 'CartPole\\n(discretized)', 'Atari\\n(one frame)', 'Robot\\nArm']\n",
    "states = [16, 100, 10000, 1e10, np.inf]\n",
    "colors = ['#4caf50', '#8bc34a', '#ffeb3b', '#ff9800', '#f44336']\n",
    "\n",
    "# Use log scale, cap infinity\n",
    "states_display = [16, 100, 10000, 1e10, 1e15]\n",
    "bars = ax2.bar(problems, states_display, color=colors, edgecolor='black', linewidth=2)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# Annotations\n",
    "annotations = ['Easy!\\n(table)', 'OK\\n(table)', 'Hard\\n(need FA)', 'Impossible\\n(need FA)', 'Infinite\\n(need FA)']\n",
    "for bar, ann in zip(bars, annotations):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, height * 2, ann, \n",
    "             ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax2.set_ylabel('Number of States (log scale)', fontsize=12)\n",
    "ax2.set_title('State Space Size by Problem', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nReal-world RL problems have HUGE or CONTINUOUS state spaces!\")\n",
    "print(\"We CANNOT use tables. We MUST use function approximation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Solution: Q(s, a) â‰ˆ Q(s, a; Î¸)\n",
    "\n",
    "Instead of storing Q-values, we **approximate** them with a function:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              FROM TABLE TO FUNCTION                            â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Q-TABLE:                                                     â”‚\n",
    "    â”‚    Q(state, action) = lookup[state][action]                   â”‚\n",
    "    â”‚    Memory: O(|States| Ã— |Actions|) â† HUGE!                    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  FUNCTION APPROXIMATION:                                      â”‚\n",
    "    â”‚    Q(state, action) â‰ˆ f(state, action; Î¸)                    â”‚\n",
    "    â”‚    Memory: O(|Parameters|) â† Much smaller!                    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Where Î¸ are learnable parameters (weights and biases)        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TYPES OF FUNCTION APPROXIMATORS:                             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  1. LINEAR: Q(s,a) = Î¸áµ€ Ã— features(s,a)                       â”‚\n",
    "    â”‚     Simple but limited expressiveness                         â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  2. NEURAL NETWORK: Q(s,a) = NN(s; Î¸)[a]                     â”‚\n",
    "    â”‚     Powerful, can learn any function!                         â”‚\n",
    "    â”‚     This is what DQN uses!                                    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Visualize the function approximation concept\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 9.5, 'Function Approximation: The Core Idea', ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Input\n",
    "input_box = FancyBboxPatch((0.5, 4), 3, 3, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(input_box)\n",
    "ax.text(2, 6.5, 'STATE', ha='center', fontsize=12, fontweight='bold', color='#1976d2')\n",
    "ax.text(2, 5.8, 's = [sâ‚, sâ‚‚, ..., sâ‚™]', ha='center', fontsize=10)\n",
    "ax.text(2, 5.0, 'e.g., position,', ha='center', fontsize=9, color='#666')\n",
    "ax.text(2, 4.4, 'velocity, angle...', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Function approximator\n",
    "func_box = FancyBboxPatch((4.5, 3), 5, 5, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#fff3e0', edgecolor='#f57c00', linewidth=3)\n",
    "ax.add_patch(func_box)\n",
    "ax.text(7, 7.5, 'FUNCTION', ha='center', fontsize=12, fontweight='bold', color='#f57c00')\n",
    "ax.text(7, 7.0, 'APPROXIMATOR', ha='center', fontsize=12, fontweight='bold', color='#f57c00')\n",
    "ax.text(7, 6.0, 'f(s; Î¸)', ha='center', fontsize=14, style='italic')\n",
    "ax.text(7, 5.0, 'Î¸ = learnable', ha='center', fontsize=10)\n",
    "ax.text(7, 4.4, 'parameters', ha='center', fontsize=10)\n",
    "ax.text(7, 3.6, '(weights & biases)', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Output\n",
    "output_box = FancyBboxPatch((10.5, 4), 3, 3, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#e8f5e9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(output_box)\n",
    "ax.text(12, 6.5, 'Q-VALUES', ha='center', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax.text(12, 5.7, 'Q(s, aâ‚)', ha='center', fontsize=10)\n",
    "ax.text(12, 5.1, 'Q(s, aâ‚‚)', ha='center', fontsize=10)\n",
    "ax.text(12, 4.5, '...', ha='center', fontsize=10)\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(4.3, 5.5), xytext=(3.6, 5.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#1976d2'))\n",
    "ax.annotate('', xy=(10.3, 5.5), xytext=(9.6, 5.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#388e3c'))\n",
    "\n",
    "# Benefits at bottom\n",
    "ax.text(7, 1.8, 'BENEFITS:', ha='center', fontsize=12, fontweight='bold')\n",
    "ax.text(7, 1.2, 'âœ“ Compact: Few parameters vs. millions of states', ha='center', fontsize=10)\n",
    "ax.text(7, 0.6, 'âœ“ Generalizes: Similar states â†’ Similar values', ha='center', fontsize=10)\n",
    "ax.text(7, 0.0, 'âœ“ Continuous: Works with any state representation', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building a Q-Network\n",
    "\n",
    "A Q-Network takes a state as input and outputs Q-values for all actions:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                   Q-NETWORK ARCHITECTURE                       â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Input: State vector [sâ‚, sâ‚‚, sâ‚ƒ, sâ‚„]                         â”‚\n",
    "    â”‚                â†“                                               â”‚\n",
    "    â”‚  Hidden Layer 1: Linear(4 â†’ 64) + ReLU                        â”‚\n",
    "    â”‚                â†“                                               â”‚\n",
    "    â”‚  Hidden Layer 2: Linear(64 â†’ 64) + ReLU                       â”‚\n",
    "    â”‚                â†“                                               â”‚\n",
    "    â”‚  Output: Linear(64 â†’ num_actions) â†’ [Q(s,aâ‚), Q(s,aâ‚‚), ...]   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WHY THIS ARCHITECTURE:                                       â”‚\n",
    "    â”‚  â€¢ Input layer: One neuron per state dimension                â”‚\n",
    "    â”‚  â€¢ Hidden layers: Learn complex patterns                      â”‚\n",
    "    â”‚  â€¢ Output layer: One Q-value per action                       â”‚\n",
    "    â”‚  â€¢ No activation on output: Q can be any real number          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Q-Network for function approximation.\n",
    "    \n",
    "    This replaces the Q-table with a neural network!\n",
    "    \n",
    "    Input: State (e.g., 4 dimensions for CartPole)\n",
    "    Output: Q-value for each action (e.g., 2 for left/right)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The neural network layers\n",
    "        self.network = nn.Sequential(\n",
    "            # Input â†’ Hidden 1\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Hidden 1 â†’ Hidden 2\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Hidden 2 â†’ Output (Q-values)\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "            # NOTE: No activation! Q-values can be any number\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass: state â†’ Q-values for all actions.\n",
    "        \n",
    "        Args:\n",
    "            state: Tensor of shape (batch_size, state_dim)\n",
    "        Returns:\n",
    "            Q-values: Tensor of shape (batch_size, action_dim)\n",
    "        \"\"\"\n",
    "        return self.network(state)\n",
    "\n",
    "\n",
    "# Create and examine a Q-network for CartPole\n",
    "state_dim = 4   # CartPole has 4 state dimensions\n",
    "action_dim = 2  # CartPole has 2 actions (left, right)\n",
    "\n",
    "q_net = QNetwork(state_dim, action_dim)\n",
    "\n",
    "print(\"Q-NETWORK FOR CARTPOLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nNetwork architecture:\")\n",
    "print(q_net)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in q_net.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Compare to tabular\n",
    "discretized_states = 1000\n",
    "table_size = discretized_states * action_dim\n",
    "print(f\"\\nCOMPARISON:\")\n",
    "print(f\"  Q-Network: {total_params:,} parameters\")\n",
    "print(f\"  Q-Table (1000 discretized states): {table_size:,} values\")\n",
    "print(f\"  Ratio: Network is ~{table_size/total_params:.1f}x smaller!\")\n",
    "print(\"\\nAnd the network handles CONTINUOUS states!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how the Q-network works\n",
    "\n",
    "print(\"HOW THE Q-NETWORK WORKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample states (like CartPole observations)\n",
    "sample_states = [\n",
    "    [0.0, 0.0, 0.0, 0.0],    # Perfectly balanced\n",
    "    [0.5, 0.1, 0.1, 0.2],    # Slightly tilted right\n",
    "    [-0.5, -0.1, -0.1, -0.2] # Slightly tilted left\n",
    "]\n",
    "\n",
    "print(\"\\nState format: [cart_pos, cart_vel, pole_angle, pole_vel]\\n\")\n",
    "print(f\"{'State':<40} {'Q(LEFT)':<12} {'Q(RIGHT)':<12} {'Best':<10}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "for state in sample_states:\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q_values = q_net(state_tensor)\n",
    "    \n",
    "    q_left = q_values[0, 0].item()\n",
    "    q_right = q_values[0, 1].item()\n",
    "    best = \"LEFT\" if q_left > q_right else \"RIGHT\"\n",
    "    \n",
    "    print(f\"{str(state):<40} {q_left:<12.4f} {q_right:<12.4f} {best:<10}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*75)\n",
    "print(\"NOTE: These are random values before training!\")\n",
    "print(\"After training, similar states will have similar Q-values.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training with Gradient Descent\n",
    "\n",
    "How do we train the Q-network? We minimize the TD error using gradient descent!\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              TRAINING THE Q-NETWORK                            â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  RECALL Q-LEARNING UPDATE (tabular):                          â”‚\n",
    "    â”‚    Q(s,a) â† Q(s,a) + Î± Ã— [r + Î³Â·max Q(s',a') - Q(s,a)]        â”‚\n",
    "    â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\n",
    "    â”‚                                    TD target                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  FOR NEURAL NETWORKS, WE USE GRADIENT DESCENT:                â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  1. Compute TD target: y = r + Î³ Ã— max Q(s', a'; Î¸)           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  2. Compute prediction: Q(s, a; Î¸)                            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  3. Compute loss: L = (y - Q(s, a; Î¸))Â²                       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  4. Update: Î¸ â† Î¸ - Î± Ã— âˆ‡_Î¸ L                                 â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  This is the same idea as tabular, but uses backpropagation! â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_loss(q_net, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute the Q-learning loss for a batch of transitions.\n",
    "    \n",
    "    This is the CORE of training a Q-network!\n",
    "    \n",
    "    Loss = mean( (target - prediction)Â² )\n",
    "         = mean( (r + Î³Â·max Q(s',a') - Q(s,a))Â² )\n",
    "    \n",
    "    Args:\n",
    "        q_net: The Q-network\n",
    "        states: Batch of current states\n",
    "        actions: Batch of actions taken\n",
    "        rewards: Batch of rewards received\n",
    "        next_states: Batch of resulting states\n",
    "        dones: Batch of done flags (1 if terminal, 0 otherwise)\n",
    "        gamma: Discount factor\n",
    "    \n",
    "    Returns:\n",
    "        loss: The MSE loss between predictions and targets\n",
    "    \"\"\"\n",
    "    # ========================================\n",
    "    # STEP 1: Get current Q-values\n",
    "    # ========================================\n",
    "    # Q(s, a) for all actions\n",
    "    all_q_values = q_net(states)\n",
    "    # Select Q(s, a) for the actions we actually took\n",
    "    q_values = all_q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 2: Compute targets\n",
    "    # ========================================\n",
    "    with torch.no_grad():  # Don't compute gradients for targets!\n",
    "        # Q(s', a') for all actions\n",
    "        next_q_values = q_net(next_states)\n",
    "        # max Q(s', a')\n",
    "        max_next_q = next_q_values.max(dim=1)[0]\n",
    "        # Target: r + Î³ * max Q(s', a') * (1 - done)\n",
    "        targets = rewards + gamma * max_next_q * (1 - dones)\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 3: Compute loss\n",
    "    # ========================================\n",
    "    loss = nn.functional.mse_loss(q_values, targets)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# Demonstrate with a sample batch\n",
    "print(\"Q-LEARNING LOSS COMPUTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Create sample batch\n",
    "states = torch.randn(batch_size, state_dim)\n",
    "actions = torch.randint(0, action_dim, (batch_size,))\n",
    "rewards = torch.tensor([1.0, 1.0, 1.0, 0.0])  # Normal rewards + terminal\n",
    "next_states = torch.randn(batch_size, state_dim)\n",
    "dones = torch.tensor([0.0, 0.0, 0.0, 1.0])  # Last one is terminal\n",
    "\n",
    "# Compute loss\n",
    "loss = compute_q_loss(q_net, states, actions, rewards, next_states, dones)\n",
    "\n",
    "print(f\"\\nSample batch of {batch_size} transitions:\")\n",
    "print(f\"  States shape: {states.shape}\")\n",
    "print(f\"  Actions: {actions.tolist()}\")\n",
    "print(f\"  Rewards: {rewards.tolist()}\")\n",
    "print(f\"  Dones: {dones.tolist()}\")\n",
    "print(f\"\\nComputed loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"The loss measures how far our Q-predictions are from targets.\")\n",
    "print(\"Training minimizes this loss, making predictions more accurate!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate a single training step\n",
    "\n",
    "print(\"SINGLE TRAINING STEP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(q_net.parameters(), lr=0.001)\n",
    "\n",
    "# Record Q-value before training\n",
    "test_state = torch.FloatTensor([[0.1, 0.0, 0.05, 0.0]])\n",
    "with torch.no_grad():\n",
    "    q_before = q_net(test_state).numpy()[0]\n",
    "\n",
    "print(f\"\\nQ-values BEFORE training:\")\n",
    "print(f\"  Q(LEFT): {q_before[0]:.4f}\")\n",
    "print(f\"  Q(RIGHT): {q_before[1]:.4f}\")\n",
    "\n",
    "# Perform multiple training steps\n",
    "n_steps = 100\n",
    "for _ in range(n_steps):\n",
    "    # Generate fake batch\n",
    "    states = torch.randn(32, state_dim)\n",
    "    actions = torch.randint(0, action_dim, (32,))\n",
    "    rewards = torch.ones(32)\n",
    "    next_states = torch.randn(32, state_dim)\n",
    "    dones = torch.zeros(32)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_q_loss(q_net, states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Record Q-value after training\n",
    "with torch.no_grad():\n",
    "    q_after = q_net(test_state).numpy()[0]\n",
    "\n",
    "print(f\"\\nQ-values AFTER {n_steps} training steps:\")\n",
    "print(f\"  Q(LEFT): {q_after[0]:.4f}\")\n",
    "print(f\"  Q(RIGHT): {q_after[1]:.4f}\")\n",
    "\n",
    "print(f\"\\nChange:\")\n",
    "print(f\"  Q(LEFT): {q_after[0] - q_before[0]:+.4f}\")\n",
    "print(f\"  Q(RIGHT): {q_after[1] - q_before[1]:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"The Q-values changed because of gradient descent!\")\n",
    "print(\"With proper training data, they would converge to true values.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Deadly Triad: Why This Is Dangerous\n",
    "\n",
    "Combining three things can cause training to **diverge** (explode!):\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                   THE DEADLY TRIAD                             â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚\n",
    "    â”‚           â”‚    FUNCTION   â”‚                                   â”‚\n",
    "    â”‚           â”‚ APPROXIMATION â”‚                                   â”‚\n",
    "    â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚\n",
    "    â”‚                   â”‚                                           â”‚\n",
    "    â”‚         DANGER    â”‚                                           â”‚\n",
    "    â”‚           ZONE    â–¼                                           â”‚\n",
    "    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚\n",
    "    â”‚   â”‚                               â”‚                           â”‚\n",
    "    â”‚   â”‚    COMBINING ALL THREE        â”‚                           â”‚\n",
    "    â”‚   â”‚     CAN CAUSE DIVERGENCE!     â”‚                           â”‚\n",
    "    â”‚   â”‚                               â”‚                           â”‚\n",
    "    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚\n",
    "    â”‚         â–²                   â–²                                 â”‚\n",
    "    â”‚         â”‚                   â”‚                                 â”‚\n",
    "    â”‚   â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                           â”‚\n",
    "    â”‚   â”‚BOOTSTRAPPINGâ”‚     â”‚ OFF-POLICY â”‚                          â”‚\n",
    "    â”‚   â”‚ (TD Learning)â”‚    â”‚ (Q-Learning)â”‚                          â”‚\n",
    "    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WHY IT'S DANGEROUS:                                          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  1. CORRELATED DATA: Sequential experiences are similar      â”‚\n",
    "    â”‚     â†’ Network overfits to recent experiences                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  2. NON-STATIONARY TARGETS: Targets depend on Q-network      â”‚\n",
    "    â”‚     â†’ When network updates, targets change too!              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  3. BOOTSTRAPPING ERROR: Small errors compound over time     â”‚\n",
    "    â”‚     â†’ Errors in Q(s') affect Q(s), which affects Q(s'')...   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the deadly triad\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 9.5, 'The Deadly Triad', ha='center', fontsize=18, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Three circles forming a triangle\n",
    "positions = [(3, 6), (11, 6), (7, 2)]\n",
    "labels = ['Function\\nApproximation', 'Off-Policy\\nLearning', 'Bootstrapping']\n",
    "examples = ['(Neural Networks)', '(Q-Learning)', '(TD Learning)']\n",
    "colors = ['#bbdefb', '#c8e6c9', '#fff3e0']\n",
    "edges = ['#1976d2', '#388e3c', '#f57c00']\n",
    "\n",
    "for (x, y), label, example, color, edge in zip(positions, labels, examples, colors, edges):\n",
    "    circle = Circle((x, y), 1.5, facecolor=color, edgecolor=edge, linewidth=3)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x, y + 0.3, label, ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "    ax.text(x, y - 0.5, example, ha='center', va='center', fontsize=9, color='#666')\n",
    "\n",
    "# Danger zone in center\n",
    "danger_circle = Circle((7, 5), 1.2, facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=3)\n",
    "ax.add_patch(danger_circle)\n",
    "ax.text(7, 5.3, 'âš ï¸', ha='center', fontsize=24)\n",
    "ax.text(7, 4.6, 'DIVERGENCE', ha='center', fontsize=9, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Lines connecting\n",
    "for (x1, y1), (x2, y2) in [(positions[0], positions[1]), \n",
    "                           (positions[1], positions[2]),\n",
    "                           (positions[2], positions[0])]:\n",
    "    ax.plot([x1, x2], [y1, y2], 'r--', linewidth=2, alpha=0.5)\n",
    "\n",
    "# Solutions at bottom\n",
    "ax.text(7, -0.5, 'SOLUTIONS (coming in next notebooks!):', \n",
    "        ha='center', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax.text(4, -1.2, 'â€¢ Experience Replay', ha='center', fontsize=10)\n",
    "ax.text(7, -1.2, 'â€¢ Target Networks', ha='center', fontsize=10)\n",
    "ax.text(10, -1.2, 'â€¢ Careful tuning', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDQN solves this with two key innovations:\")\n",
    "print(\"1. EXPERIENCE REPLAY: Breaks correlation in training data\")\n",
    "print(\"2. TARGET NETWORK: Stabilizes the targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### Why Function Approximation?\n",
    "\n",
    "| Problem | Tabular | Function Approx |\n",
    "|---------|---------|----------------|\n",
    "| Small discrete states | âœ“ Works | âœ“ Works |\n",
    "| Large discrete states | âœ— Too much memory | âœ“ Works |\n",
    "| Continuous states | âœ— Impossible | âœ“ Works |\n",
    "| Generalization | âœ— None | âœ“ Similar states â†’ Similar values |\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Q-Network** | Neural network that approximates Q(s, a) |\n",
    "| **Gradient Descent** | Minimize (target - prediction)Â² |\n",
    "| **Deadly Triad** | FA + Bootstrapping + Off-policy = Danger |\n",
    "\n",
    "### The Core Equation\n",
    "\n",
    "```\n",
    "Loss = (r + Î³ Ã— max Q(s', a'; Î¸) - Q(s, a; Î¸))Â²\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    TD Error squared\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why can't we use a Q-table for continuous state spaces?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Continuous state spaces have infinitely many possible states. You can't create a table row for every possible real-valued state. For example, a position of 1.00001 is different from 1.00002, and there are infinitely many values between them. Function approximation maps these continuous states to Q-values using a finite number of parameters.\n",
    "</details>\n",
    "\n",
    "**2. What are the benefits of function approximation over tables?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "1. **Generalization**: Similar states get similar Q-values automatically. The network learns patterns, not individual states.\n",
    "2. **Compact**: Far fewer parameters than states. A network with 1000 parameters can handle billions of states.\n",
    "3. **Continuous**: Works with any state representation - no discretization needed.\n",
    "</details>\n",
    "\n",
    "**3. What is the \"deadly triad\" and why is it dangerous?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The deadly triad is the combination of:\n",
    "1. Function approximation (neural networks)\n",
    "2. Bootstrapping (TD learning - using estimates to update estimates)\n",
    "3. Off-policy learning (learning about a different policy than we follow)\n",
    "\n",
    "It's dangerous because: training data is correlated (sequential), targets are non-stationary (depend on the changing network), and small errors can compound. This can cause training to diverge!\n",
    "</details>\n",
    "\n",
    "**4. How do we train a Q-network?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "1. Compute the TD target: y = r + Î³ Ã— max Q(s', a')\n",
    "2. Compute the prediction: Q(s, a)\n",
    "3. Compute the loss: (y - Q(s, a))Â²\n",
    "4. Update weights using gradient descent: Î¸ â† Î¸ - Î± Ã— âˆ‡L\n",
    "\n",
    "This is the same as tabular Q-learning, but using backpropagation!\n",
    "</details>\n",
    "\n",
    "**5. What solutions does DQN use for the deadly triad?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "DQN uses two key innovations:\n",
    "1. **Experience Replay**: Store experiences in a buffer and sample randomly. This breaks the correlation between consecutive experiences.\n",
    "2. **Target Network**: Use a separate, slowly-updated network for computing targets. This stabilizes the targets during training.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "You now understand why we need function approximation and the challenges it brings!\n",
    "\n",
    "In the next notebook, we'll build a complete **DQN from scratch** that solves these challenges.\n",
    "\n",
    "**Continue to:** [Notebook 2: DQN From Scratch](02_dqn_from_scratch.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Function approximation is the bridge from toy problems to real-world RL!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
