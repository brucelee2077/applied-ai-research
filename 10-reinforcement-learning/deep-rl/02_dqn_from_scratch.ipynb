{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN From Scratch: Teaching a Neural Network to Play Games\n",
    "\n",
    "Welcome to DQN - the algorithm that taught computers to play Atari games at superhuman level! This notebook will build a complete DQN from scratch with clear explanations.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- Why we need neural networks for Q-learning (the scaling problem)\n",
    "- What DQN is and how it works (with a brain upgrade analogy!)\n",
    "- Experience replay (the memory bank trick)\n",
    "- Target networks (the slow twin trick)\n",
    "- How to implement DQN from scratch in PyTorch\n",
    "- Train an agent to balance a pole!\n",
    "\n",
    "**Prerequisites:** Notebooks in `classic-algorithms/` (Q-learning basics)\n",
    "\n",
    "**Time:** ~45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: Why Deep Q-Learning?\n",
    "\n",
    "### The Problem with Tables\n",
    "\n",
    "Remember Q-learning? We stored Q-values in a table:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                  THE TABLE PROBLEM                      â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Q-Table for a 4x4 Grid:   Easy! Only 16 states        â”‚\n",
    "    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚\n",
    "    â”‚  â”‚ State â”‚ UP â”‚ DN â”‚ ...â”‚                               â”‚\n",
    "    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                               â”‚\n",
    "    â”‚  â”‚ (0,0) â”‚ 2.3â”‚ 1.2â”‚ ...â”‚                               â”‚\n",
    "    â”‚  â”‚ (0,1) â”‚ 3.1â”‚ 2.5â”‚ ...â”‚                               â”‚\n",
    "    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Q-Table for Atari:      IMPOSSIBLE!                   â”‚\n",
    "    â”‚  â€¢ Screen: 210 Ã— 160 pixels Ã— 3 colors                 â”‚\n",
    "    â”‚  â€¢ Each pixel: 256 possible values                     â”‚\n",
    "    â”‚  â€¢ Total states: 256^(210Ã—160Ã—3) â‰ˆ âˆ                   â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  We can't store a table with infinite rows!            â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### The Solution: Neural Networks!\n",
    "\n",
    "Instead of memorizing every state, we **learn patterns**:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚            FROM TABLE TO NEURAL NETWORK                 â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Q-TABLE (Memorization):                               â”‚\n",
    "    â”‚    state â†’ lookup in table â†’ Q-values                  â”‚\n",
    "    â”‚    \"I've seen this exact state before\"                 â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Q-NETWORK (Generalization):                           â”‚\n",
    "    â”‚    state â†’ neural network â†’ Q-values                   â”‚\n",
    "    â”‚    \"This looks SIMILAR to states I've seen\"            â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  â”Œâ”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "    â”‚  â”‚Stateâ”‚ â”€â”€â”€> â”‚   Neural    â”‚ â”€â”€â”€> â”‚Q-values â”‚         â”‚\n",
    "    â”‚  â”‚Imageâ”‚      â”‚   Network   â”‚      â”‚for each â”‚         â”‚\n",
    "    â”‚  â””â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ action  â”‚         â”‚\n",
    "    â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Brain Upgrade Analogy\n",
    "\n",
    "Think of DQN as **upgrading your brain**:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚            THE BRAIN UPGRADE ANALOGY                    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  TABULAR Q-LEARNING = Flashcard Memory                 â”‚\n",
    "    â”‚    ğŸ“ \"If I see THIS exact problem, the answer is X\"   â”‚\n",
    "    â”‚    â€¢ Must see every problem to know the answer         â”‚\n",
    "    â”‚    â€¢ Can't handle new problems                         â”‚\n",
    "    â”‚    â€¢ Memory fills up quickly                           â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  DQN = Pattern Recognition Brain                       â”‚\n",
    "    â”‚    ğŸ§  \"Problems with THESE features usually need X\"     â”‚\n",
    "    â”‚    â€¢ Learns general patterns                           â”‚\n",
    "    â”‚    â€¢ Can handle new situations                         â”‚\n",
    "    â”‚    â€¢ Much more efficient!                              â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Example: Recognizing faces                            â”‚\n",
    "    â”‚    Flashcard: Store every possible face image          â”‚\n",
    "    â”‚    Brain: Learn \"eyes go here, nose goes here\"         â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch, Circle\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Try to import gymnasium, fall back to gym\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym\n",
    "\n",
    "# Visualize the DQN architecture\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(7, 9.5, 'Deep Q-Network (DQN) Architecture', \n",
    "        ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Input: State\n",
    "state_box = FancyBboxPatch((0.5, 3.5), 2.5, 3, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(state_box)\n",
    "ax.text(1.75, 5.5, 'STATE', ha='center', fontsize=14, fontweight='bold', color='#1976d2')\n",
    "ax.text(1.75, 4.8, '(observation)', ha='center', fontsize=10, color='#1976d2')\n",
    "ax.text(1.75, 4.2, 'e.g., cart pos,\\npole angle...', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Neural Network\n",
    "nn_box = FancyBboxPatch((4.5, 2.5), 5, 5, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#fff3e0', edgecolor='#f57c00', linewidth=3)\n",
    "ax.add_patch(nn_box)\n",
    "ax.text(7, 7, 'NEURAL NETWORK', ha='center', fontsize=14, fontweight='bold', color='#f57c00')\n",
    "\n",
    "# Draw neurons\n",
    "layers = [\n",
    "    [(5.5, 5.5), (5.5, 4.5), (5.5, 3.5)],  # Input layer\n",
    "    [(7, 5.8), (7, 5), (7, 4.2), (7, 3.4)],  # Hidden layer\n",
    "    [(8.5, 5.5), (8.5, 4.5), (8.5, 3.5)]   # Output layer\n",
    "]\n",
    "\n",
    "colors = ['#bbdefb', '#ffcc80', '#c8e6c9']\n",
    "for layer_idx, layer in enumerate(layers):\n",
    "    for (x, y) in layer:\n",
    "        circle = Circle((x, y), 0.25, facecolor=colors[layer_idx], \n",
    "                        edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "# Draw connections (simplified)\n",
    "for (x1, y1) in layers[0]:\n",
    "    for (x2, y2) in layers[1]:\n",
    "        ax.plot([x1+0.25, x2-0.25], [y1, y2], 'gray', linewidth=0.5, alpha=0.5)\n",
    "for (x1, y1) in layers[1]:\n",
    "    for (x2, y2) in layers[2]:\n",
    "        ax.plot([x1+0.25, x2-0.25], [y1, y2], 'gray', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "ax.text(5.5, 2.8, 'Input', ha='center', fontsize=9)\n",
    "ax.text(7, 2.8, 'Hidden', ha='center', fontsize=9)\n",
    "ax.text(8.5, 2.8, 'Output', ha='center', fontsize=9)\n",
    "\n",
    "# Output: Q-values\n",
    "output_box = FancyBboxPatch((10.5, 3), 3, 4, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#e8f5e9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(output_box)\n",
    "ax.text(12, 6.5, 'Q-VALUES', ha='center', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "ax.text(12, 5.6, 'Q(s, left) = 2.3', ha='center', fontsize=11, color='#666')\n",
    "ax.text(12, 4.8, 'Q(s, right) = 5.1 â†', ha='center', fontsize=11, color='#388e3c', fontweight='bold')\n",
    "ax.text(12, 3.8, 'Best action: RIGHT', ha='center', fontsize=10, color='#388e3c', style='italic')\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(4.3, 5), xytext=(3.1, 5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#1976d2'))\n",
    "ax.annotate('', xy=(10.3, 5), xytext=(9.7, 5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#388e3c'))\n",
    "\n",
    "# Bottom explanation\n",
    "ax.text(7, 1.5, 'The neural network takes a state as input and outputs Q-values for ALL actions',\n",
    "        ha='center', fontsize=11, color='#333')\n",
    "ax.text(7, 0.8, 'Action selection: Pick the action with the highest Q-value!',\n",
    "        ha='center', fontsize=11, color='#333', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHT: From Table to Function\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nQ-Table:   Q: (state, action) â†’ one value (lookup)\")\n",
    "print(\"Q-Network: Q: state â†’ [value for each action] (computation)\")\n",
    "print(\"\\nThe neural network LEARNS the mapping through training!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The CartPole Environment: Our Test Game\n",
    "\n",
    "We'll train our DQN to play **CartPole** - a classic control problem:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                    CARTPOLE GAME                        â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚                      â•±â•²                                 â”‚\n",
    "    â”‚                     â•±  â•²   â† Pole (keep it balanced!)   â”‚\n",
    "    â”‚                    â•±    â•²                               â”‚\n",
    "    â”‚                   â•±      â•²                              â”‚\n",
    "    â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚\n",
    "    â”‚              â”‚   Cart    â”‚                              â”‚\n",
    "    â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚\n",
    "    â”‚    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                 â”‚\n",
    "    â”‚              â† LEFT    RIGHT â†’                          â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  GOAL: Keep the pole balanced as long as possible!     â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  STATE (what the agent sees):                          â”‚\n",
    "    â”‚    1. Cart position (-2.4 to 2.4)                      â”‚\n",
    "    â”‚    2. Cart velocity                                    â”‚\n",
    "    â”‚    3. Pole angle (-12Â° to 12Â°)                         â”‚\n",
    "    â”‚    4. Pole angular velocity                            â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  ACTIONS (what the agent can do):                      â”‚\n",
    "    â”‚    0 = Push cart LEFT                                  â”‚\n",
    "    â”‚    1 = Push cart RIGHT                                 â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  REWARD: +1 for each timestep the pole stays up       â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  SOLVED: Average reward â‰¥ 195 over 100 episodes        â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and explore the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "print(\"CARTPOLE ENVIRONMENT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nObservation space: {env.observation_space}\")\n",
    "print(f\"  Shape: {env.observation_space.shape}\")\n",
    "print(f\"  Meaning: [cart_pos, cart_vel, pole_angle, pole_vel]\")\n",
    "print(f\"\\nAction space: {env.action_space}\")\n",
    "print(f\"  0 = Push LEFT\")\n",
    "print(f\"  1 = Push RIGHT\")\n",
    "\n",
    "# Show a sample observation\n",
    "state, _ = env.reset()\n",
    "print(f\"\\nSample starting state: {state}\")\n",
    "print(f\"  Cart position:  {state[0]:.4f}\")\n",
    "print(f\"  Cart velocity:  {state[1]:.4f}\")\n",
    "print(f\"  Pole angle:     {state[2]:.4f} rad ({np.degrees(state[2]):.2f}Â°)\")\n",
    "print(f\"  Pole velocity:  {state[3]:.4f}\")\n",
    "\n",
    "env.close()\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Building the Q-Network\n",
    "\n",
    "Our Q-Network is a simple feedforward neural network:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚               Q-NETWORK ARCHITECTURE                    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Input Layer (4 neurons):                              â”‚\n",
    "    â”‚    [cart_pos, cart_vel, pole_angle, pole_vel]          â”‚\n",
    "    â”‚                         â†“                               â”‚\n",
    "    â”‚  Hidden Layer 1 (128 neurons) + ReLU:                  â”‚\n",
    "    â”‚    Learn intermediate features                         â”‚\n",
    "    â”‚                         â†“                               â”‚\n",
    "    â”‚  Hidden Layer 2 (128 neurons) + ReLU:                  â”‚\n",
    "    â”‚    Learn more complex patterns                         â”‚\n",
    "    â”‚                         â†“                               â”‚\n",
    "    â”‚  Output Layer (2 neurons):                             â”‚\n",
    "    â”‚    [Q(s, LEFT), Q(s, RIGHT)]                           â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network: A neural network that estimates Q-values.\n",
    "    \n",
    "    Input: State (e.g., 4 values for CartPole)\n",
    "    Output: Q-value for EACH action (e.g., 2 values for LEFT and RIGHT)\n",
    "    \n",
    "    Architecture:\n",
    "        state (4) â†’ Linear(128) â†’ ReLU â†’ Linear(128) â†’ ReLU â†’ Linear(2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store dimensions for later use\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Build the network\n",
    "        self.network = nn.Sequential(\n",
    "            # Layer 1: Input â†’ Hidden\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Layer 2: Hidden â†’ Hidden\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Layer 3: Hidden â†’ Output (Q-values)\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "            # No activation! Q-values can be any number\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: state â†’ Q-values.\n",
    "        \n",
    "        Args:\n",
    "            x: Batch of states, shape (batch_size, state_dim)\n",
    "        \n",
    "        Returns:\n",
    "            Q-values for each action, shape (batch_size, action_dim)\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.0):\n",
    "        \"\"\"\n",
    "        Choose an action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (numpy array)\n",
    "            epsilon: Probability of random action (exploration)\n",
    "        \n",
    "        Returns:\n",
    "            Action to take (integer)\n",
    "        \"\"\"\n",
    "        # Exploration: Random action\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        # Exploitation: Best action based on Q-values\n",
    "        with torch.no_grad():  # No gradients needed for action selection\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Add batch dimension\n",
    "            q_values = self.forward(state_tensor)\n",
    "            return q_values.argmax().item()  # Return action with highest Q-value\n",
    "\n",
    "\n",
    "# Create and test the network\n",
    "print(\"Q-NETWORK DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create network for CartPole (4 inputs, 2 outputs)\n",
    "q_net = QNetwork(state_dim=4, action_dim=2)\n",
    "print(f\"\\nNetwork architecture:\")\n",
    "print(q_net)\n",
    "\n",
    "# Test with a sample state\n",
    "sample_state = np.array([0.01, 0.02, -0.03, 0.04])\n",
    "state_tensor = torch.FloatTensor(sample_state).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    q_values = q_net(state_tensor)\n",
    "\n",
    "print(f\"\\nSample state: {sample_state}\")\n",
    "print(f\"Q-values output: {q_values.numpy()[0]}\")\n",
    "print(f\"  Q(s, LEFT):  {q_values[0, 0]:.4f}\")\n",
    "print(f\"  Q(s, RIGHT): {q_values[0, 1]:.4f}\")\n",
    "print(f\"  Best action: {'LEFT' if q_values.argmax() == 0 else 'RIGHT'}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in q_net.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(\"(This is much smaller than storing a table for continuous states!)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Experience Replay - The Memory Bank Trick\n",
    "\n",
    "### The Problem: Correlated Data\n",
    "\n",
    "When learning in sequence, consecutive experiences are very similar:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚            THE PROBLEM WITH ONLINE LEARNING             â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Consecutive experiences are CORRELATED:               â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Time 1: Cart at position 0.1, pole angle 0.05        â”‚\n",
    "    â”‚  Time 2: Cart at position 0.12, pole angle 0.06       â”‚\n",
    "    â”‚  Time 3: Cart at position 0.14, pole angle 0.07       â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  These are almost IDENTICAL! If we train on these     â”‚\n",
    "    â”‚  in order, the network overfits to this one situation â”‚\n",
    "    â”‚  and forgets everything else!                          â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Analogy: Studying for a test                          â”‚\n",
    "    â”‚    Bad: Read chapter 5, chapter 5, chapter 5... (Ã—100) â”‚\n",
    "    â”‚    Good: Random mix of all chapters!                   â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### The Solution: Memory Bank\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚            EXPERIENCE REPLAY: THE MEMORY BANK           â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  1. STORE experiences in a memory bank (buffer)        â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚     Memory Bank (holds thousands of experiences)       â”‚\n",
    "    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
    "    â”‚     â”‚ (sâ‚,aâ‚,râ‚,s'â‚) â”‚ (sâ‚‚,aâ‚‚,râ‚‚,s'â‚‚) â”‚ ... â”‚        â”‚\n",
    "    â”‚     â”‚ (sâ‚ƒ,aâ‚ƒ,râ‚ƒ,s'â‚ƒ) â”‚ (sâ‚„,aâ‚„,râ‚„,s'â‚„) â”‚ ... â”‚        â”‚\n",
    "    â”‚     â”‚ (sâ‚…,aâ‚…,râ‚…,s'â‚…) â”‚ (sâ‚†,aâ‚†,râ‚†,s'â‚†) â”‚ ... â”‚        â”‚\n",
    "    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  2. SAMPLE random mini-batches for training            â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚     Random Sample:                                     â”‚\n",
    "    â”‚     [exp_42, exp_1337, exp_99, exp_500, ...]          â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚     These are from DIFFERENT times and situations!    â”‚\n",
    "    â”‚     Much more diverse training data!                   â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience Replay Buffer - The Memory Bank!\n",
    "    \n",
    "    Stores transitions (s, a, r, s', done) and allows random sampling.\n",
    "    Uses a deque (double-ended queue) that automatically removes old\n",
    "    experiences when capacity is reached.\n",
    "    \n",
    "    Analogy: A library with limited shelf space.\n",
    "    - New books go on the shelf\n",
    "    - Old books are removed when shelf is full\n",
    "    - You can randomly pick any book to read!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity: Maximum number of experiences to store.\n",
    "                      Once full, oldest experiences are removed.\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store a new experience in the buffer.\n",
    "        \n",
    "        Args:\n",
    "            state: The state before the action\n",
    "            action: The action taken\n",
    "            reward: The reward received\n",
    "            next_state: The resulting state\n",
    "            done: Whether the episode ended\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of experiences.\n",
    "        \n",
    "        This is the key insight! Random sampling breaks correlations!\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of tensors: (states, actions, rewards, next_states, dones)\n",
    "        \"\"\"\n",
    "        # Randomly sample batch_size experiences\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Unzip the batch into separate lists\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(dones)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Demonstrate the replay buffer\n",
    "print(\"REPLAY BUFFER DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "buffer = ReplayBuffer(capacity=1000)\n",
    "\n",
    "# Add some fake experiences\n",
    "print(\"\\nAdding 100 experiences to the buffer...\")\n",
    "for i in range(100):\n",
    "    state = np.random.randn(4)\n",
    "    action = random.randint(0, 1)\n",
    "    reward = 1.0\n",
    "    next_state = np.random.randn(4)\n",
    "    done = False\n",
    "    buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"Buffer size: {len(buffer)} / {buffer.capacity}\")\n",
    "\n",
    "# Sample a batch\n",
    "print(\"\\nSampling a batch of 5 experiences...\")\n",
    "states, actions, rewards, next_states, dones = buffer.sample(5)\n",
    "\n",
    "print(f\"States shape: {states.shape}\")\n",
    "print(f\"Actions: {actions.tolist()}\")\n",
    "print(f\"Rewards: {rewards.tolist()}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"KEY BENEFITS OF EXPERIENCE REPLAY:\")\n",
    "print(\"-\"*60)\n",
    "print(\"1. BREAKS CORRELATION: Random sampling removes temporal patterns\")\n",
    "print(\"2. REUSES DATA: Same experience can be sampled multiple times\")\n",
    "print(\"3. STABILIZES TRAINING: More diverse mini-batches\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Target Network - The Slow Twin Trick\n",
    "\n",
    "### The Problem: Moving Target\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚            THE MOVING TARGET PROBLEM                    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Q-learning update:                                    â”‚\n",
    "    â”‚    Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]   â”‚\n",
    "    â”‚                           â†‘                            â”‚\n",
    "    â”‚                     This is our TARGET                 â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  THE PROBLEM:                                          â”‚\n",
    "    â”‚    We use the SAME network for:                        â”‚\n",
    "    â”‚    1. Predicting Q(s,a)  (what we're training)        â”‚\n",
    "    â”‚    2. Computing target max Q(s',a')                    â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚    When we update the network, BOTH change!            â”‚\n",
    "    â”‚    It's like chasing a target that keeps moving!       â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Analogy: Trying to hit a moving target               â”‚\n",
    "    â”‚    Every time you aim and shoot, the target moves!    â”‚\n",
    "    â”‚    Very hard to converge!                              â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### The Solution: A Slow Twin\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚            TARGET NETWORK: THE SLOW TWIN                â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Use TWO networks:                                     â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "    â”‚  â”‚  Q-Network      â”‚    â”‚ Target Network   â”‚            â”‚\n",
    "    â”‚  â”‚  (Main/Online)  â”‚    â”‚ (Slow Twin)      â”‚            â”‚\n",
    "    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚\n",
    "    â”‚  â”‚ Updated EVERY   â”‚    â”‚ Updated every   â”‚            â”‚\n",
    "    â”‚  â”‚ training step   â”‚    â”‚ N steps (slowly) â”‚            â”‚\n",
    "    â”‚  â”‚                 â”‚    â”‚                 â”‚            â”‚\n",
    "    â”‚  â”‚ Used for:       â”‚    â”‚ Used for:       â”‚            â”‚\n",
    "    â”‚  â”‚ - Action select â”‚    â”‚ - Computing     â”‚            â”‚\n",
    "    â”‚  â”‚ - Q(s,a)        â”‚    â”‚   TARGET values â”‚            â”‚\n",
    "    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "    â”‚           â†‘                     â†‘                       â”‚\n",
    "    â”‚           â”‚         Copy every N steps                 â”‚\n",
    "    â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  The target network is a FROZEN COPY that only updates â”‚\n",
    "    â”‚  periodically, giving a STABLE target to chase!        â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the target network concept\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Without target network (unstable)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 8)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('WITHOUT Target Network\\n(Unstable!)', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Single network\n",
    "net_box = FancyBboxPatch((3, 3), 4, 3, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "ax1.add_patch(net_box)\n",
    "ax1.text(5, 5, 'Q-Network', ha='center', fontsize=12, fontweight='bold')\n",
    "ax1.text(5, 4.2, 'Predicts Q(s,a)', ha='center', fontsize=10)\n",
    "ax1.text(5, 3.5, 'AND targets!', ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "# Arrows showing circular dependency\n",
    "ax1.annotate('', xy=(7.2, 5.5), xytext=(7.2, 2.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f', \n",
    "                            connectionstyle='arc3,rad=0.3'))\n",
    "ax1.annotate('', xy=(2.8, 2.5), xytext=(2.8, 5.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f',\n",
    "                            connectionstyle='arc3,rad=0.3'))\n",
    "\n",
    "ax1.text(5, 1.5, 'Updates change both prediction AND target!', \n",
    "         ha='center', fontsize=10, color='#d32f2f')\n",
    "ax1.text(5, 0.8, 'â†’ Chasing a moving target = UNSTABLE', \n",
    "         ha='center', fontsize=10, color='#d32f2f', fontweight='bold')\n",
    "\n",
    "# Right: With target network (stable)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 8)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('WITH Target Network\\n(Stable!)', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Q-Network\n",
    "q_box = FancyBboxPatch((0.5, 3), 3.5, 3, boxstyle=\"round,pad=0.1\",\n",
    "                        facecolor='#bbdefb', edgecolor='#1976d2', linewidth=2)\n",
    "ax2.add_patch(q_box)\n",
    "ax2.text(2.25, 5, 'Q-Network', ha='center', fontsize=11, fontweight='bold', color='#1976d2')\n",
    "ax2.text(2.25, 4.2, 'Updates every', ha='center', fontsize=9)\n",
    "ax2.text(2.25, 3.6, 'training step', ha='center', fontsize=9)\n",
    "\n",
    "# Target Network\n",
    "target_box = FancyBboxPatch((6, 3), 3.5, 3, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax2.add_patch(target_box)\n",
    "ax2.text(7.75, 5, 'Target Network', ha='center', fontsize=11, fontweight='bold', color='#388e3c')\n",
    "ax2.text(7.75, 4.2, 'Updates every', ha='center', fontsize=9)\n",
    "ax2.text(7.75, 3.6, 'N steps (slow)', ha='center', fontsize=9)\n",
    "\n",
    "# Copy arrow\n",
    "ax2.annotate('', xy=(5.8, 4.5), xytext=(4.2, 4.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666', linestyle='--'))\n",
    "ax2.text(5, 5.2, 'Copy every N steps', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "ax2.text(5, 1.5, 'Target stays FIXED between copies!', \n",
    "         ha='center', fontsize=10, color='#388e3c')\n",
    "ax2.text(5, 0.8, 'â†’ Stable target = STABLE training', \n",
    "         ha='center', fontsize=10, color='#388e3c', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: The Complete DQN Agent\n",
    "\n",
    "Now let's put it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Complete DQN Agent with:\n",
    "    - Q-Network (for predictions)\n",
    "    - Target Network (for stable targets)\n",
    "    - Experience Replay (for breaking correlations)\n",
    "    \n",
    "    This is the algorithm that DeepMind used to play Atari games!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_size=10000, batch_size=64, target_update=100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_dim: Size of state observation\n",
    "            action_dim: Number of possible actions\n",
    "            lr: Learning rate for optimizer\n",
    "            gamma: Discount factor for future rewards\n",
    "            buffer_size: Capacity of replay buffer\n",
    "            batch_size: Number of samples per training batch\n",
    "            target_update: How often to update target network\n",
    "        \"\"\"\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        \n",
    "        # ========================================\n",
    "        # CREATE TWO NETWORKS\n",
    "        # ========================================\n",
    "        # Main Q-Network (updated every step)\n",
    "        self.q_network = QNetwork(state_dim, action_dim)\n",
    "        \n",
    "        # Target Network (updated slowly)\n",
    "        self.target_network = QNetwork(state_dim, action_dim)\n",
    "        # Start with same weights as Q-network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # ========================================\n",
    "        # OPTIMIZER AND REPLAY BUFFER\n",
    "        # ========================================\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Step counter for target network updates\n",
    "        self.steps = 0\n",
    "    \n",
    "    def act(self, state, epsilon=0.0):\n",
    "        \"\"\"Choose action using epsilon-greedy policy.\"\"\"\n",
    "        return self.q_network.get_action(state, epsilon)\n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer.\"\"\"\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Perform one training step.\n",
    "        \n",
    "        This is where the learning happens!\n",
    "        \"\"\"\n",
    "        # Need enough experiences to sample a batch\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # ========================================\n",
    "        # STEP 1: Sample a batch from memory\n",
    "        # ========================================\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        # ========================================\n",
    "        # STEP 2: Compute current Q-values\n",
    "        # ========================================\n",
    "        # Q(s, a) for the actions we actually took\n",
    "        q_values = self.q_network(states)  # Shape: (batch, num_actions)\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()  # Q(s, a)\n",
    "        \n",
    "        # ========================================\n",
    "        # STEP 3: Compute target Q-values\n",
    "        # ========================================\n",
    "        # Use TARGET network for stable targets!\n",
    "        with torch.no_grad():  # Don't compute gradients for targets\n",
    "            next_q_values = self.target_network(next_states)\n",
    "            max_next_q = next_q_values.max(dim=1)[0]  # max Q(s', a')\n",
    "            \n",
    "            # Target: r + Î³ * max Q(s', a') * (1 - done)\n",
    "            # The (1 - done) zeroes out future value for terminal states\n",
    "            targets = rewards + self.gamma * max_next_q * (1 - dones)\n",
    "        \n",
    "        # ========================================\n",
    "        # STEP 4: Compute loss and update\n",
    "        # ========================================\n",
    "        # Loss: Mean squared error between prediction and target\n",
    "        loss = nn.functional.mse_loss(q_values, targets)\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # ========================================\n",
    "        # STEP 5: Update target network periodically\n",
    "        # ========================================\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update == 0:\n",
    "            # Copy weights from Q-network to target network\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "print(\"DQN AGENT COMPONENTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    DQN AGENT SUMMARY                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                          â”‚\n",
    "â”‚  1. Q-NETWORK (Main)                                    â”‚\n",
    "â”‚     â†’ Predicts Q-values for action selection            â”‚\n",
    "â”‚     â†’ Updated every training step                       â”‚\n",
    "â”‚                                                          â”‚\n",
    "â”‚  2. TARGET NETWORK (Slow Twin)                          â”‚\n",
    "â”‚     â†’ Provides stable targets for training              â”‚\n",
    "â”‚     â†’ Copied from Q-network every N steps               â”‚\n",
    "â”‚                                                          â”‚\n",
    "â”‚  3. REPLAY BUFFER (Memory Bank)                         â”‚\n",
    "â”‚     â†’ Stores past experiences                           â”‚\n",
    "â”‚     â†’ Enables random sampling for training              â”‚\n",
    "â”‚                                                          â”‚\n",
    "â”‚  TRAINING LOOP:                                         â”‚\n",
    "â”‚     1. Act in environment (epsilon-greedy)              â”‚\n",
    "â”‚     2. Store experience in buffer                       â”‚\n",
    "â”‚     3. Sample random batch from buffer                  â”‚\n",
    "â”‚     4. Compute loss: (Q(s,a) - target)Â²                 â”‚\n",
    "â”‚     5. Update Q-network with gradient descent           â”‚\n",
    "â”‚     6. Periodically copy to target network              â”‚\n",
    "â”‚                                                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Training the DQN Agent\n",
    "\n",
    "Now let's train our agent to balance the pole!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env_name='CartPole-v1', n_episodes=500, \n",
    "              epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "              verbose=True):\n",
    "    \"\"\"\n",
    "    Train a DQN agent on the specified environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Gymnasium environment name\n",
    "        n_episodes: Number of episodes to train\n",
    "        epsilon_start: Starting exploration rate\n",
    "        epsilon_end: Minimum exploration rate\n",
    "        epsilon_decay: How fast to reduce exploration\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        agent: Trained DQN agent\n",
    "        rewards_history: List of episode rewards\n",
    "        losses_history: List of training losses\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    # Create agent\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "    \n",
    "    # Training tracking\n",
    "    rewards_history = []\n",
    "    losses_history = []\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"TRAINING DQN ON CARTPOLE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Episodes: {n_episodes}\")\n",
    "        print(f\"Epsilon: {epsilon_start} â†’ {epsilon_end}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # Reset environment\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_losses = []\n",
    "        \n",
    "        for t in range(500):  # Max 500 steps per episode\n",
    "            # ========================================\n",
    "            # 1. Choose action (epsilon-greedy)\n",
    "            # ========================================\n",
    "            action = agent.act(state, epsilon)\n",
    "            \n",
    "            # ========================================\n",
    "            # 2. Take action in environment\n",
    "            # ========================================\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # ========================================\n",
    "            # 3. Store experience\n",
    "            # ========================================\n",
    "            agent.store(state, action, reward, next_state, float(done))\n",
    "            \n",
    "            # ========================================\n",
    "            # 4. Train the agent\n",
    "            # ========================================\n",
    "            loss = agent.update()\n",
    "            if loss is not None:\n",
    "                episode_losses.append(loss)\n",
    "            \n",
    "            # ========================================\n",
    "            # 5. Move to next state\n",
    "            # ========================================\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Track results\n",
    "        rewards_history.append(total_reward)\n",
    "        if episode_losses:\n",
    "            losses_history.append(np.mean(episode_losses))\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-50:])\n",
    "            print(f\"Episode {episode+1:4d} | \"\n",
    "                  f\"Avg Reward (last 50): {avg_reward:6.1f} | \"\n",
    "                  f\"Epsilon: {epsilon:.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Training complete!\")\n",
    "        final_avg = np.mean(rewards_history[-100:])\n",
    "        print(f\"Final average reward (last 100): {final_avg:.1f}\")\n",
    "        print(f\"Solved threshold: 195.0\")\n",
    "        print(f\"Status: {'SOLVED!' if final_avg >= 195 else 'Keep training...'}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    return agent, rewards_history, losses_history\n",
    "\n",
    "# Train the agent!\n",
    "agent, rewards, losses = train_dqn(n_episodes=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Rewards\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rewards, alpha=0.3, color='blue', label='Raw')\n",
    "\n",
    "# Smoothed rewards\n",
    "window = 20\n",
    "if len(rewards) >= window:\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(rewards)), smoothed, color='blue', \n",
    "             linewidth=2, label=f'Smoothed (window={window})')\n",
    "\n",
    "# Solved threshold\n",
    "ax1.axhline(y=195, color='green', linestyle='--', linewidth=2, label='Solved (195)')\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward', fontsize=12)\n",
    "ax1.set_title('DQN Learning Curve', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Training loss\n",
    "ax2 = axes[1]\n",
    "if losses:\n",
    "    ax2.plot(losses, alpha=0.5, color='red')\n",
    "    # Smoothed loss\n",
    "    if len(losses) >= window:\n",
    "        smoothed_loss = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "        ax2.plot(range(window-1, len(losses)), smoothed_loss, color='red', linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontsize=12)\n",
    "ax2.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTRAINING ANALYSIS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. REWARDS (left graph):\")\n",
    "print(\"   - Should increase over time as agent learns\")\n",
    "print(\"   - Goal: Reach and maintain â‰¥195 (solved!)\")\n",
    "print(\"\\n2. LOSS (right graph):\")\n",
    "print(\"   - High initially (random predictions)\")\n",
    "print(\"   - Should decrease as Q-values become accurate\")\n",
    "print(\"   - Some noise is normal (stochastic training)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Watch the Trained Agent Play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env_name='CartPole-v1', n_episodes=10):\n",
    "    \"\"\"\n",
    "    Evaluate the trained agent (no exploration, pure exploitation).\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for t in range(500):\n",
    "            # No exploration (epsilon=0)\n",
    "            action = agent.act(state, epsilon=0.0)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    env.close()\n",
    "    return rewards\n",
    "\n",
    "# Evaluate the trained agent\n",
    "print(\"EVALUATING TRAINED AGENT\")\n",
    "print(\"=\"*60)\n",
    "print(\"Running 10 episodes with NO exploration (epsilon=0)...\\n\")\n",
    "\n",
    "eval_rewards = evaluate_agent(agent, n_episodes=10)\n",
    "\n",
    "for i, r in enumerate(eval_rewards):\n",
    "    status = \"ğŸ‰\" if r >= 195 else \"ğŸ˜”\"\n",
    "    print(f\"Episode {i+1:2d}: {r:5.0f} steps {status}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"Average: {np.mean(eval_rewards):.1f} steps\")\n",
    "print(f\"Best:    {max(eval_rewards):.0f} steps\")\n",
    "print(f\"Worst:   {min(eval_rewards):.0f} steps\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "if np.mean(eval_rewards) >= 195:\n",
    "    print(\"\\nğŸ‰ CONGRATULATIONS! The agent has solved CartPole!\")\n",
    "    print(\"   It can balance the pole for 195+ steps on average!\")\n",
    "else:\n",
    "    print(\"\\nğŸ“ˆ The agent is learning but needs more training.\")\n",
    "    print(\"   Try training for more episodes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Complete DQN Algorithm\n",
    "\n",
    "Let's summarize the full algorithm:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                  DQN ALGORITHM                          â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Initialize:                                           â”‚\n",
    "    â”‚    - Q-network with random weights Î¸                   â”‚\n",
    "    â”‚    - Target network with weights Î¸â» = Î¸                â”‚\n",
    "    â”‚    - Replay buffer D                                   â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  For each episode:                                     â”‚\n",
    "    â”‚    s = initial state                                   â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚    For each step:                                      â”‚\n",
    "    â”‚      1. Choose a = Îµ-greedy(Q(s, Â·; Î¸))               â”‚\n",
    "    â”‚      2. Execute a, observe r, s'                       â”‚\n",
    "    â”‚      3. Store (s, a, r, s', done) in D                 â”‚\n",
    "    â”‚      4. Sample mini-batch from D                       â”‚\n",
    "    â”‚      5. Compute target: y = r + Î³Â·max Q(s', a'; Î¸â»)   â”‚\n",
    "    â”‚      6. Minimize loss: (Q(s, a; Î¸) - y)Â²              â”‚\n",
    "    â”‚      7. Every N steps: Î¸â» â† Î¸                          â”‚\n",
    "    â”‚      8. s â† s'                                         â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚    Decay Îµ                                             â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### What is DQN?\n",
    "\n",
    "DQN = Q-Learning + Neural Networks + Two Key Tricks\n",
    "\n",
    "| Component | Purpose | Analogy |\n",
    "|-----------|---------|----------|\n",
    "| Q-Network | Approximate Q-values | Brain that generalizes |\n",
    "| Experience Replay | Break correlations | Random study from notes |\n",
    "| Target Network | Stable training targets | Slow-moving goal post |\n",
    "\n",
    "### Why Each Component Matters\n",
    "\n",
    "1. **Neural Network instead of Table**\n",
    "   - Handles continuous/large state spaces\n",
    "   - Generalizes to unseen states\n",
    "\n",
    "2. **Experience Replay**\n",
    "   - Breaks temporal correlation\n",
    "   - Reuses data efficiently\n",
    "   - Stabilizes training\n",
    "\n",
    "3. **Target Network**\n",
    "   - Provides stable training targets\n",
    "   - Prevents oscillation/divergence\n",
    "\n",
    "### Historical Note\n",
    "\n",
    "DQN was published by DeepMind in 2015 and was the first algorithm to:\n",
    "- Play Atari games directly from pixels\n",
    "- Achieve superhuman performance on many games\n",
    "- Show that deep learning + RL could work together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why can't we use a Q-table for Atari games?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The state space is too large! An Atari screen is 210Ã—160Ã—3 pixels with 256 possible values per pixel. This means there are 256^(210Ã—160Ã—3) â‰ˆ 10^300000 possible states - far more than atoms in the universe! We can't store a table that big.\n",
    "</details>\n",
    "\n",
    "**2. What problem does experience replay solve?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Experience replay breaks the correlation between consecutive experiences. Without it, the agent trains on very similar, sequential data which causes it to overfit to recent experiences and forget older ones. Random sampling from the buffer provides more diverse, uncorrelated training data.\n",
    "</details>\n",
    "\n",
    "**3. Why do we need a target network?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Without a target network, both the predictions Q(s,a) and the targets r + Î³Â·max Q(s',a') change every training step since they use the same network. This is like chasing a moving target, causing unstable training. The target network provides fixed targets that only change periodically, stabilizing learning.\n",
    "</details>\n",
    "\n",
    "**4. What is epsilon in epsilon-greedy and why does it decay?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Epsilon is the probability of taking a random action (exploration). It starts high (e.g., 1.0 = 100% random) so the agent explores many states. It decays over time because as the agent learns better Q-values, it should exploit its knowledge more and explore less.\n",
    "</details>\n",
    "\n",
    "**5. What does the loss function measure in DQN?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The loss measures the squared difference between the predicted Q-value Q(s,a) and the target y = r + Î³Â·max Q(s',a'). Minimizing this loss makes the Q-network's predictions more accurate according to the Bellman equation.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Excellent work! You've implemented DQN from scratch!\n",
    "\n",
    "In the next notebooks, we'll dive deeper into:\n",
    "- **Experience Replay** variations (Prioritized Experience Replay)\n",
    "- **Target Networks** and soft updates\n",
    "- **DQN Improvements** (Double DQN, Dueling DQN)\n",
    "\n",
    "**Continue to:** [Notebook 3: Experience Replay](03_experience_replay.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*DQN was a breakthrough that showed deep learning and RL could work together. You've just implemented the same algorithm that learned to play Atari games at superhuman level!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
