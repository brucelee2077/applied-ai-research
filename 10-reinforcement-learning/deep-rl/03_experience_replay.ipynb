{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay: The Memory Bank That Saves DQN\n",
    "\n",
    "Experience replay is one of the two key innovations that made DQN actually work! This notebook dives deep into why it's essential.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- Why correlated data is deadly for neural networks (with a studying analogy!)\n",
    "- How experience replay breaks these correlations\n",
    "- Implementing a replay buffer from scratch\n",
    "- Prioritized experience replay: learning from mistakes\n",
    "- Comparing DQN with and without replay\n",
    "\n",
    "**Prerequisites:** Notebook 2 (DQN From Scratch)\n",
    "\n",
    "**Time:** ~25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Study Habits Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE STUDY HABITS ANALOGY                              │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Imagine you're studying for a big exam covering 10 chapters. │\n",
    "    │                                                                │\n",
    "    │  BAD STRATEGY (Online Learning):                              │\n",
    "    │    Day 1: Study Ch 1, Ch 1, Ch 1, Ch 1, Ch 1, ...            │\n",
    "    │    Day 2: Study Ch 2, Ch 2, Ch 2, Ch 2, Ch 2, ...            │\n",
    "    │    Day 3: Study Ch 3, Ch 3, Ch 3, Ch 3, Ch 3, ...            │\n",
    "    │                                                                │\n",
    "    │    PROBLEM: By Day 10, you've forgotten Chapter 1!           │\n",
    "    │    You only remember what you studied recently.               │\n",
    "    │                                                                │\n",
    "    │  GOOD STRATEGY (Experience Replay):                           │\n",
    "    │    Take notes in a notebook (replay buffer)                   │\n",
    "    │    Each study session: randomly review notes from ALL chapters│\n",
    "    │    \"Chapter 5 note, Chapter 2 note, Chapter 8 note, ...\"     │\n",
    "    │                                                                │\n",
    "    │    RESULT: You remember everything evenly!                    │\n",
    "    │    No correlation between what you study each session.        │\n",
    "    │                                                                │\n",
    "    │  Experience Replay = Taking Notes + Random Review             │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, Circle\n",
    "\n",
    "# Visualize the correlation problem\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Online learning (correlated)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('ONLINE LEARNING\\n(Sequential Data)', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Show sequential samples\n",
    "for i in range(5):\n",
    "    x = 1 + i * 1.7\n",
    "    # All boxes same color to show correlation\n",
    "    color = f'#{(int(100 + i*30)):02x}8080'\n",
    "    box = FancyBboxPatch((x, 5), 1.2, 1.2, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(x + 0.6, 5.6, f's{i+1}', ha='center', fontsize=11, fontweight='bold')\n",
    "    if i < 4:\n",
    "        ax1.annotate('', xy=(x + 1.3, 5.6), xytext=(x + 1.2, 5.6),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "ax1.text(5, 4, 'Consecutive samples are HIGHLY SIMILAR', ha='center', fontsize=11, color='#d32f2f')\n",
    "ax1.text(5, 3.2, '❌ Violates i.i.d. assumption', ha='center', fontsize=10)\n",
    "ax1.text(5, 2.5, '❌ Network overfits to recent data', ha='center', fontsize=10)\n",
    "ax1.text(5, 1.8, '❌ Forgets older experiences quickly', ha='center', fontsize=10)\n",
    "\n",
    "# Right: Experience replay (uncorrelated)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('EXPERIENCE REPLAY\\n(Random Sampling)', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Memory buffer\n",
    "buffer_box = FancyBboxPatch((1, 6), 8, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "ax2.add_patch(buffer_box)\n",
    "ax2.text(5, 8.7, 'REPLAY BUFFER (Memory)', ha='center', fontsize=11, fontweight='bold', color='#1976d2')\n",
    "\n",
    "# Experiences in buffer (diverse colors)\n",
    "colors = ['#e57373', '#81c784', '#64b5f6', '#ffb74d', '#ba68c8', '#4db6ac']\n",
    "for i, color in enumerate(colors):\n",
    "    x = 1.5 + i * 1.2\n",
    "    box = FancyBboxPatch((x, 6.5), 0.9, 0.9, boxstyle=\"round,pad=0.02\",\n",
    "                          facecolor=color, edgecolor='black', linewidth=1)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(x + 0.45, 7, f'e{i+1}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Random sampling\n",
    "ax2.text(5, 5.3, '↓ Random Sample ↓', ha='center', fontsize=10, color='#388e3c')\n",
    "\n",
    "# Sampled batch (random order/colors)\n",
    "sampled_colors = [colors[2], colors[5], colors[0], colors[4]]\n",
    "ax2.text(5, 4.5, 'Training Batch:', ha='center', fontsize=10, fontweight='bold')\n",
    "for i, color in enumerate(sampled_colors):\n",
    "    x = 2 + i * 1.5\n",
    "    box = FancyBboxPatch((x, 3.5), 0.9, 0.9, boxstyle=\"round,pad=0.02\",\n",
    "                          facecolor=color, edgecolor='black', linewidth=1)\n",
    "    ax2.add_patch(box)\n",
    "\n",
    "ax2.text(5, 2.5, '✓ Diverse, uncorrelated samples', ha='center', fontsize=10, color='#388e3c')\n",
    "ax2.text(5, 1.8, '✓ Reuses past experiences', ha='center', fontsize=10, color='#388e3c')\n",
    "ax2.text(5, 1.1, '✓ Stable training!', ha='center', fontsize=10, color='#388e3c', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THE CORRELATION PROBLEM\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Neural networks expect i.i.d. (independent and identically distributed) data.\n",
    "\n",
    "In RL, consecutive experiences are HIGHLY CORRELATED:\n",
    "  - State at time t is very similar to state at time t+1\n",
    "  - All experiences in one episode come from the same situation\n",
    "\n",
    "This causes the network to:\n",
    "  1. Overfit to recent experiences\n",
    "  2. Forget how to handle older situations (\"catastrophic forgetting\")\n",
    "  3. Oscillate wildly during training\n",
    "\n",
    "SOLUTION: Store experiences and randomly sample from them!\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implementing a Replay Buffer\n",
    "\n",
    "A replay buffer is like a library with limited shelf space:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                    REPLAY BUFFER                               │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  STORAGE:                                                     │\n",
    "    │    Each experience = (state, action, reward, next_state, done)│\n",
    "    │    Capacity = max number of experiences to store              │\n",
    "    │    When full: remove oldest experience (FIFO)                 │\n",
    "    │                                                                │\n",
    "    │  OPERATIONS:                                                  │\n",
    "    │    push(s, a, r, s', done) → Store new experience            │\n",
    "    │    sample(batch_size) → Random sample of experiences         │\n",
    "    │                                                                │\n",
    "    │  TYPICAL SIZES:                                               │\n",
    "    │    CartPole: 10,000 - 100,000                                 │\n",
    "    │    Atari: 1,000,000                                           │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Standard Uniform Replay Buffer.\n",
    "    \n",
    "    Stores experiences and allows random sampling for training.\n",
    "    Uses a deque (double-ended queue) for efficient O(1) operations.\n",
    "    \n",
    "    The key insight: Random sampling breaks temporal correlations!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity: Maximum number of experiences to store.\n",
    "                      Older experiences are automatically removed.\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store a new experience.\n",
    "        \n",
    "        If buffer is full, oldest experience is automatically removed.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of experiences.\n",
    "        \n",
    "        This is where the magic happens - random sampling means:\n",
    "        - Experiences from different episodes in same batch\n",
    "        - Experiences from different times in same batch\n",
    "        - Approximately i.i.d. training data!\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (states, actions, rewards, next_states, dones) tensors\n",
    "        \"\"\"\n",
    "        # Random sample (this breaks correlations!)\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Unpack and convert to tensors\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(dones)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Demonstrate the buffer\n",
    "print(\"REPLAY BUFFER DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "buffer = ReplayBuffer(capacity=1000)\n",
    "\n",
    "# Simulate collecting experiences\n",
    "print(\"\\nSimulating 100 experiences...\")\n",
    "for i in range(100):\n",
    "    state = np.array([i * 0.01, np.sin(i * 0.1), np.cos(i * 0.1), i * 0.001])\n",
    "    action = i % 2\n",
    "    reward = 1.0 if i % 10 == 0 else 0.0\n",
    "    next_state = np.array([(i+1) * 0.01, np.sin((i+1) * 0.1), np.cos((i+1) * 0.1), (i+1) * 0.001])\n",
    "    done = (i == 99)\n",
    "    buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"Buffer size: {len(buffer)} / {buffer.capacity}\")\n",
    "\n",
    "# Sample a batch\n",
    "print(\"\\nSampling a batch of 5 experiences...\")\n",
    "states, actions, rewards, next_states, dones = buffer.sample(5)\n",
    "\n",
    "print(f\"\\nSampled States (first column shows diversity):\")\n",
    "for i, s in enumerate(states):\n",
    "    print(f\"  Sample {i+1}: state[0] = {s[0].item():.3f}\")\n",
    "\n",
    "print(\"\\nNotice: The state values are NOT sequential!\")\n",
    "print(\"This is the key benefit - random sampling breaks correlations.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why Random Sampling Matters: Visualization\n",
    "\n",
    "Let's see the difference in what the network \"sees\" during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference in training data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate 100 timesteps of experience\n",
    "timesteps = np.arange(100)\n",
    "state_values = np.sin(timesteps * 0.1) + 0.1 * np.random.randn(100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Left: All experiences\n",
    "ax1 = axes[0]\n",
    "ax1.plot(timesteps, state_values, 'b-', alpha=0.7, linewidth=1)\n",
    "ax1.scatter(timesteps, state_values, c='blue', s=20, alpha=0.7)\n",
    "ax1.set_xlabel('Time Step', fontsize=11)\n",
    "ax1.set_ylabel('State Value', fontsize=11)\n",
    "ax1.set_title('All Collected Experiences', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Middle: Online learning (sequential batch)\n",
    "ax2 = axes[1]\n",
    "# Take consecutive samples (last 8)\n",
    "online_indices = np.arange(92, 100)\n",
    "ax2.plot(timesteps, state_values, 'gray', alpha=0.2, linewidth=1)\n",
    "ax2.scatter(timesteps, state_values, c='gray', s=10, alpha=0.2)\n",
    "ax2.scatter(online_indices, state_values[online_indices], c='red', s=80, \n",
    "           edgecolors='black', linewidth=2, zorder=5, label='Training Batch')\n",
    "ax2.axvspan(92, 99, alpha=0.2, color='red')\n",
    "ax2.set_xlabel('Time Step', fontsize=11)\n",
    "ax2.set_title('Online Learning\\n(Consecutive Samples)', fontsize=12, fontweight='bold', color='#d32f2f')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Right: Replay (random batch)\n",
    "ax3 = axes[2]\n",
    "# Random samples\n",
    "replay_indices = np.random.choice(100, 8, replace=False)\n",
    "ax3.plot(timesteps, state_values, 'gray', alpha=0.2, linewidth=1)\n",
    "ax3.scatter(timesteps, state_values, c='gray', s=10, alpha=0.2)\n",
    "ax3.scatter(replay_indices, state_values[replay_indices], c='green', s=80,\n",
    "           edgecolors='black', linewidth=2, zorder=5, label='Training Batch')\n",
    "for idx in replay_indices:\n",
    "    ax3.axvline(x=idx, alpha=0.1, color='green')\n",
    "ax3.set_xlabel('Time Step', fontsize=11)\n",
    "ax3.set_title('Experience Replay\\n(Random Samples)', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the actual values\n",
    "print(\"\\nSAMPLED VALUES COMPARISON:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Online (consecutive): {state_values[online_indices].round(2)}\")\n",
    "print(f\"  → Very similar values! High correlation.\")\n",
    "print(f\"\\nReplay (random): {state_values[replay_indices].round(2)}\")\n",
    "print(f\"  → Diverse values! Low correlation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prioritized Experience Replay: Learning From Mistakes\n",
    "\n",
    "Not all experiences are equally valuable! Some teach us more than others.\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │            PRIORITIZED EXPERIENCE REPLAY                       │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  IDEA: Sample experiences that are more \"surprising\" more often│\n",
    "    │                                                                │\n",
    "    │  What makes an experience surprising?                         │\n",
    "    │    High TD Error = |target - prediction|                      │\n",
    "    │                                                                │\n",
    "    │    If TD error is high → We predicted poorly                  │\n",
    "    │    → This experience has more to teach us!                    │\n",
    "    │                                                                │\n",
    "    │  PRIORITY FORMULA:                                            │\n",
    "    │    priority = |TD error| + ε   (ε is small constant)          │\n",
    "    │                                                                │\n",
    "    │  SAMPLING:                                                    │\n",
    "    │    P(sample experience i) ∝ priority_i^α                      │\n",
    "    │    α controls how much to prioritize (0 = uniform, 1 = full)  │\n",
    "    │                                                                │\n",
    "    │  ANALOGY:                                                     │\n",
    "    │    Studying for an exam: spend more time on problems you      │\n",
    "    │    got WRONG, not the ones you already know!                  │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"\n",
    "    Prioritized Experience Replay Buffer.\n",
    "    \n",
    "    Samples experiences with high TD error more frequently.\n",
    "    This focuses learning on \"surprising\" experiences.\n",
    "    \n",
    "    Key insight: Experiences we predict poorly are more informative!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity, alpha=0.6, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity: Maximum buffer size\n",
    "            alpha: How much to prioritize (0=uniform, 1=fully prioritized)\n",
    "            epsilon: Small constant added to priorities (ensures non-zero)\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.priorities = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store experience with maximum priority.\n",
    "        \n",
    "        New experiences get max priority so they're sampled at least once.\n",
    "        \"\"\"\n",
    "        max_priority = max(self.priorities) if self.priorities else 1.0\n",
    "        \n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "            self.priorities.append(max_priority)\n",
    "        else:\n",
    "            self.buffer[self.position] = experience\n",
    "            self.priorities[self.position] = max_priority\n",
    "        \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample experiences with probability proportional to priority.\n",
    "        \n",
    "        P(i) = priority_i^alpha / sum(priority_j^alpha)\n",
    "        \"\"\"\n",
    "        # Compute sampling probabilities\n",
    "        priorities = np.array(self.priorities) ** self.alpha\n",
    "        probabilities = priorities / priorities.sum()\n",
    "        \n",
    "        # Sample indices according to priorities\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        \n",
    "        # Get experiences\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(dones),\n",
    "            indices,\n",
    "            torch.FloatTensor(probabilities[indices])  # For importance sampling\n",
    "        )\n",
    "    \n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        \"\"\"\n",
    "        Update priorities based on new TD errors.\n",
    "        \n",
    "        Called after training step with the computed TD errors.\n",
    "        \"\"\"\n",
    "        for idx, td_error in zip(indices, td_errors):\n",
    "            self.priorities[idx] = abs(td_error) + self.epsilon\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Demonstrate prioritized replay\n",
    "print(\"PRIORITIZED REPLAY DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "per_buffer = PrioritizedReplayBuffer(capacity=100, alpha=0.6)\n",
    "\n",
    "# Add experiences\n",
    "for i in range(100):\n",
    "    state = np.random.randn(4)\n",
    "    per_buffer.push(state, i % 2, np.random.randn(), np.random.randn(4), False)\n",
    "\n",
    "# Simulate updating priorities (some experiences are \"surprising\")\n",
    "for i in range(0, 100, 10):\n",
    "    per_buffer.priorities[i] = 5.0  # High priority (big TD error)\n",
    "\n",
    "print(\"\\nAfter setting high priority for indices 0, 10, 20, 30, ...\")\n",
    "print(f\"\\nSampling 20 experiences:\")\n",
    "\n",
    "# Sample and count how often high-priority experiences appear\n",
    "high_priority_count = 0\n",
    "for _ in range(100):\n",
    "    _, _, _, _, _, indices, _ = per_buffer.sample(20)\n",
    "    high_priority_count += sum(1 for i in indices if i % 10 == 0)\n",
    "\n",
    "print(f\"High-priority experiences sampled: {high_priority_count} / 2000\")\n",
    "print(f\"Expected if uniform: ~200 (10% of experiences)\")\n",
    "print(f\"\\nWith prioritization, surprising experiences are sampled MORE OFTEN!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prioritized vs uniform sampling\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Create sample priorities\n",
    "np.random.seed(42)\n",
    "n_experiences = 50\n",
    "priorities = np.random.exponential(1, n_experiences)\n",
    "# Make a few experiences very high priority\n",
    "priorities[5] = 10\n",
    "priorities[22] = 8\n",
    "priorities[38] = 12\n",
    "\n",
    "# Left: Priorities\n",
    "ax1 = axes[0]\n",
    "colors = ['red' if p > 5 else 'steelblue' for p in priorities]\n",
    "ax1.bar(range(n_experiences), priorities, color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax1.set_xlabel('Experience Index', fontsize=11)\n",
    "ax1.set_ylabel('Priority (|TD Error|)', fontsize=11)\n",
    "ax1.set_title('Experience Priorities\\n(Red = High TD Error)', fontsize=12, fontweight='bold')\n",
    "ax1.axhline(y=np.mean(priorities), color='gray', linestyle='--', label=f'Mean: {np.mean(priorities):.1f}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Right: Sampling probabilities comparison\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Uniform probabilities\n",
    "uniform_probs = np.ones(n_experiences) / n_experiences\n",
    "\n",
    "# Prioritized probabilities (alpha=0.6)\n",
    "alpha = 0.6\n",
    "prioritized_probs = (priorities ** alpha) / (priorities ** alpha).sum()\n",
    "\n",
    "x = np.arange(n_experiences)\n",
    "width = 0.35\n",
    "\n",
    "ax2.bar(x - width/2, uniform_probs * 100, width, label='Uniform', color='#90caf9', edgecolor='black')\n",
    "ax2.bar(x + width/2, prioritized_probs * 100, width, label='Prioritized', color='#ef5350', edgecolor='black')\n",
    "\n",
    "ax2.set_xlabel('Experience Index', fontsize=11)\n",
    "ax2.set_ylabel('Sampling Probability (%)', fontsize=11)\n",
    "ax2.set_title('Uniform vs Prioritized Sampling', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHigh-priority experiences (5, 22, 38) are sampled much more often!\")\n",
    "print(f\"Uniform probability: {uniform_probs[5]*100:.1f}%\")\n",
    "print(f\"Prioritized probability: {prioritized_probs[5]*100:.1f}% (experience 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### The Problem\n",
    "\n",
    "| Issue | Description |\n",
    "|-------|-------------|\n",
    "| **Correlation** | Consecutive experiences are nearly identical |\n",
    "| **Forgetting** | Network overfits to recent data, forgets past |\n",
    "| **Instability** | Training oscillates wildly |\n",
    "\n",
    "### The Solution\n",
    "\n",
    "| Component | How It Helps |\n",
    "|-----------|-------------|\n",
    "| **Replay Buffer** | Stores past experiences for later use |\n",
    "| **Random Sampling** | Breaks temporal correlations |\n",
    "| **Data Reuse** | Same experience can train multiple times |\n",
    "\n",
    "### Uniform vs Prioritized\n",
    "\n",
    "| Type | Sampling | When to Use |\n",
    "|------|----------|-------------|\n",
    "| **Uniform** | All equal probability | Simple, stable |\n",
    "| **Prioritized** | High TD-error more likely | Better sample efficiency |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why is correlated training data bad for neural networks?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Neural networks assume i.i.d. (independent and identically distributed) training data. Correlated data causes the network to overfit to recent patterns while forgetting older ones (\"catastrophic forgetting\"). The gradient updates become biased toward the current situation rather than the overall task.\n",
    "</details>\n",
    "\n",
    "**2. How does experience replay break correlations?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "By storing experiences in a buffer and randomly sampling from it, we mix experiences from different times and episodes in each training batch. A batch might contain experiences from the beginning of training alongside recent ones, making the data approximately i.i.d.\n",
    "</details>\n",
    "\n",
    "**3. What does \"priority\" mean in prioritized experience replay?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Priority is based on TD error (|target - prediction|). Experiences with high TD error were poorly predicted, meaning they have more to teach the network. By sampling these more often, we focus learning on the most informative experiences.\n",
    "</details>\n",
    "\n",
    "**4. Why do new experiences get max priority in PER?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "New experiences haven't been trained on yet, so we don't know their TD error. Giving them max priority ensures they'll be sampled at least once, after which their priority will be updated based on actual TD error.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Experience replay is half the story. The other key innovation is **Target Networks**!\n",
    "\n",
    "**Continue to:** [Notebook 4: Target Networks](04_target_networks.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Experience replay: \"Learn from the past, not just the present.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
