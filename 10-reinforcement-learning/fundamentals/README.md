# Reinforcement Learning Fundamentals

This section covers the core concepts that form the foundation of all reinforcement learning algorithms.

## What You'll Learn

By completing this section, you'll understand:
- The RL paradigm and how it differs from supervised/unsupervised learning
- Markov Decision Processes (MDPs) - the mathematical framework for RL
- Reward functions, discounting, and returns
- Policies and value functions (V and Q)
- Bellman equations and dynamic programming

## Notebooks

| # | Notebook | Description |
|---|----------|-------------|
| 1 | [What is Reinforcement Learning?](01_what_is_reinforcement_learning.ipynb) | RL paradigm, agent-environment loop, exploration vs exploitation |
| 2 | [Markov Decision Processes](02_markov_decision_processes.ipynb) | States, actions, transitions, the Markov property |
| 3 | [Rewards and Returns](03_rewards_and_returns.ipynb) | Reward design, discounting, cumulative returns |
| 4 | [Policies and Value Functions](04_policies_and_value_functions.ipynb) | V(s), Q(s,a), deterministic vs stochastic policies |
| 5 | [Bellman Equations](05_bellman_equations.ipynb) | Bellman expectation, optimality, dynamic programming |

## Prerequisites

- Basic Python and NumPy
- No prior RL knowledge required!

## What's Next?

After completing this section, proceed to:
- [Classic Algorithms](../classic-algorithms/) - Implement Q-learning, SARSA, and more
