{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Reinforcement Learning?\n",
    "\n",
    "Welcome to your journey into Reinforcement Learning! This notebook will introduce you to RL using simple language, real-world analogies, and interactive examples.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- What reinforcement learning is (explained with everyday examples!)\n",
    "- The agent-environment interaction loop\n",
    "- Key concepts: states, actions, rewards, and policies\n",
    "- How RL differs from other types of machine learning\n",
    "- You'll build and visualize your first RL environment!\n",
    "\n",
    "**Prerequisites:** Basic Python. Absolutely no prior RL knowledge needed!\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: Learning by Doing\n",
    "\n",
    "### Analogy: Training a Dog\n",
    "\n",
    "Imagine you're training a puppy to sit:\n",
    "\n",
    "1. **You say \"sit\"** (the puppy observes the situation)\n",
    "2. **The puppy does something** - maybe it sits, maybe it jumps, maybe it does nothing\n",
    "3. **You give feedback:**\n",
    "   - If it sits: \"Good boy!\" + treat (+reward)\n",
    "   - If it doesn't: No treat (no reward)\n",
    "4. **The puppy learns** which actions lead to treats!\n",
    "\n",
    "**This is exactly how reinforcement learning works!**\n",
    "\n",
    "The puppy (agent) learns through trial and error, receiving rewards for good behavior.\n",
    "\n",
    "```\n",
    "    You say \"sit\"           Puppy sits           You give treat\n",
    "    ┌─────────┐            ┌─────────┐           ┌─────────┐\n",
    "    │  STATE  │  ────────> │ ACTION  │ ────────> │ REWARD  │\n",
    "    │ (\"sit\") │            │ (sits)  │           │ (treat) │\n",
    "    └─────────┘            └─────────┘           └─────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Three Types of Machine Learning\n",
    "\n",
    "Before diving deeper, let's see how RL compares to other approaches:\n",
    "\n",
    "### 1. Supervised Learning (Like a Teacher)\n",
    "- **Analogy:** A teacher shows you problems WITH answers\n",
    "- \"This photo is a cat. This photo is a dog. Now you try!\"\n",
    "- You learn from labeled examples\n",
    "\n",
    "### 2. Unsupervised Learning (Like Exploring)\n",
    "- **Analogy:** Finding patterns in a pile of Legos\n",
    "- \"These pieces seem to go together, those are different\"\n",
    "- No one tells you what's right or wrong\n",
    "\n",
    "### 3. Reinforcement Learning (Like a Video Game)\n",
    "- **Analogy:** Learning to play a new video game\n",
    "- No instruction manual - you try things and see what happens\n",
    "- Good moves = points, bad moves = game over\n",
    "- You learn the best strategy through experience!\n",
    "\n",
    "| Type | Learns From | Example |\n",
    "|------|-------------|--------|\n",
    "| Supervised | Labeled answers | \"This email is spam\" |\n",
    "| Unsupervised | Patterns in data | \"These customers are similar\" |\n",
    "| **Reinforcement** | **Trial and error + rewards** | **\"Moving right got me 10 points!\"** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The RL Loop: Agent and Environment\n",
    "\n",
    "Every RL problem has two main characters:\n",
    "\n",
    "### The Agent (The Learner)\n",
    "- Makes decisions\n",
    "- Takes actions\n",
    "- Learns from experience\n",
    "- **Examples:** A robot, an AI player, a trading algorithm\n",
    "\n",
    "### The Environment (The World)\n",
    "- Where the agent lives and acts\n",
    "- Responds to actions\n",
    "- Provides feedback (rewards)\n",
    "- **Examples:** A maze, a video game, the stock market\n",
    "\n",
    "They interact in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "import numpy as np\n",
    "\n",
    "# Create a beautiful visualization of the RL loop\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_facecolor('#f8f9fa')\n",
    "\n",
    "# Environment box (top)\n",
    "env_box = FancyBboxPatch((2, 6), 10, 2.5, boxstyle=\"round,pad=0.1\", \n",
    "                          facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(env_box)\n",
    "ax.text(7, 7.5, 'ENVIRONMENT', ha='center', va='center', \n",
    "        fontsize=18, fontweight='bold', color='#1976d2')\n",
    "ax.text(7, 6.8, '(The World: maze, game, robot world...)', ha='center', \n",
    "        fontsize=11, color='#1976d2', style='italic')\n",
    "\n",
    "# Agent box (bottom)\n",
    "agent_box = FancyBboxPatch((4, 1.5), 6, 2.5, boxstyle=\"round,pad=0.1\", \n",
    "                            facecolor='#e8f5e9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(agent_box)\n",
    "ax.text(7, 3, 'AGENT', ha='center', va='center', \n",
    "        fontsize=18, fontweight='bold', color='#388e3c')\n",
    "ax.text(7, 2.3, '(The Learner: robot, AI, algorithm...)', ha='center', \n",
    "        fontsize=11, color='#388e3c', style='italic')\n",
    "\n",
    "# Arrow: State (environment -> agent) - LEFT SIDE\n",
    "ax.annotate('', xy=(4.5, 4.2), xytext=(3, 6),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#7b1fa2'))\n",
    "ax.text(2.2, 5.2, 'STATE', fontsize=14, color='#7b1fa2', fontweight='bold')\n",
    "ax.text(2.2, 4.7, '\"Where am I?\"', fontsize=10, color='#7b1fa2', style='italic')\n",
    "\n",
    "# Arrow: Reward (environment -> agent) - RIGHT SIDE  \n",
    "ax.annotate('', xy=(9.5, 4.2), xytext=(11, 6),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#f57c00'))\n",
    "ax.text(11.2, 5.2, 'REWARD', fontsize=14, color='#f57c00', fontweight='bold')\n",
    "ax.text(11.2, 4.7, '\"+10 points!\"', fontsize=10, color='#f57c00', style='italic')\n",
    "\n",
    "# Arrow: Action (agent -> environment) - CENTER\n",
    "ax.annotate('', xy=(7, 6), xytext=(7, 4),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#d32f2f'))\n",
    "ax.text(7.3, 5.2, 'ACTION', fontsize=14, color='#d32f2f', fontweight='bold')\n",
    "ax.text(7.3, 4.7, '\"Go right!\"', fontsize=10, color='#d32f2f', style='italic')\n",
    "\n",
    "# Title\n",
    "ax.text(7, 9.3, 'The Reinforcement Learning Loop', \n",
    "        ha='center', fontsize=20, fontweight='bold', color='#333')\n",
    "\n",
    "# Step explanations at the bottom\n",
    "steps = [\n",
    "    \"1. Agent sees the STATE (current situation)\",\n",
    "    \"2. Agent takes an ACTION (makes a decision)\", \n",
    "    \"3. Environment gives REWARD (feedback: good or bad)\",\n",
    "    \"4. Repeat! Agent learns which actions lead to high rewards\"\n",
    "]\n",
    "for i, step in enumerate(steps):\n",
    "    ax.text(7, 0.8 - i*0.25, step, ha='center', fontsize=10, color='#555')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"THE RL LOOP - Think of it like this:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. You're playing a video game (you are the AGENT)\")\n",
    "print(\"2. The game shows you the screen (that's the STATE)\")\n",
    "print(\"3. You press a button (that's your ACTION)\")\n",
    "print(\"4. You get points or lose a life (that's the REWARD)\")\n",
    "print(\"5. The game continues... you learn what works!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Vocabulary (With Examples!)\n",
    "\n",
    "Let's learn the RL vocabulary using a familiar example: **Pac-Man**!\n",
    "\n",
    "| Term | Definition | Pac-Man Example |\n",
    "|------|------------|----------------|\n",
    "| **Agent** | The decision-maker | Pac-Man himself |\n",
    "| **Environment** | The world | The maze with ghosts and dots |\n",
    "| **State** | Current situation | Positions of Pac-Man, ghosts, remaining dots |\n",
    "| **Action** | What agent can do | Move UP, DOWN, LEFT, or RIGHT |\n",
    "| **Reward** | Feedback signal | +10 for eating dot, +200 for ghost, -500 for dying |\n",
    "| **Policy** | The strategy | \"If ghost is near, run away. Otherwise, eat dots.\" |\n",
    "| **Episode** | One complete game | From start until Pac-Man dies or wins |\n",
    "\n",
    "### More Real-World Examples\n",
    "\n",
    "**Self-Driving Car:**\n",
    "- Agent: The car's AI\n",
    "- State: Camera images, sensor data, speed, location\n",
    "- Actions: Accelerate, brake, turn left/right\n",
    "- Rewards: +1 for safe driving, -100 for accidents, +10 for reaching destination\n",
    "\n",
    "**Robot Learning to Walk:**\n",
    "- Agent: The robot\n",
    "- State: Joint angles, balance sensors\n",
    "- Actions: Move each leg motor\n",
    "- Rewards: +1 for each step forward, -10 for falling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Let's Build Our First Environment!\n",
    "\n",
    "We'll create a simple **Grid World** - a 5x5 grid where an agent must navigate to a goal.\n",
    "\n",
    "```\n",
    "┌───┬───┬───┬───┬───┐\n",
    "│ A │   │   │   │   │   A = Agent (starts here)\n",
    "├───┼───┼───┼───┼───┤\n",
    "│   │   │   │   │   │\n",
    "├───┼───┼───┼───┼───┤\n",
    "│   │   │   │   │   │\n",
    "├───┼───┼───┼───┼───┤\n",
    "│   │   │   │   │   │\n",
    "├───┼───┼───┼───┼───┤\n",
    "│   │   │   │   │ G │   G = Goal (get here!)\n",
    "└───┴───┴───┴───┴───┘\n",
    "```\n",
    "\n",
    "**The Rules:**\n",
    "- Agent can move: UP, DOWN, LEFT, or RIGHT\n",
    "- Each step costs -1 (encourages finding shortest path)\n",
    "- Reaching the goal gives +10\n",
    "- Hitting a wall = stay in place (still costs -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class SimpleGridWorld:\n",
    "    \"\"\"\n",
    "    A simple 5x5 Grid World environment.\n",
    "    \n",
    "    This is our first RL environment! The agent starts at (0,0) \n",
    "    and must reach the goal at (4,4).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.goal = (size-1, size-1)  # Bottom-right corner\n",
    "        self.action_names = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent to the starting position.\"\"\"\n",
    "        self.agent_pos = [0, 0]  # Top-left corner\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Return the current state (agent's position).\"\"\"\n",
    "        return tuple(self.agent_pos)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action and return the result.\n",
    "        \n",
    "        Actions: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "        \n",
    "        Returns: (new_state, reward, done)\n",
    "        \"\"\"\n",
    "        # Save old position\n",
    "        old_pos = self.agent_pos.copy()\n",
    "        \n",
    "        # Move based on action\n",
    "        if action == 0:    # UP\n",
    "            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)\n",
    "        elif action == 1:  # RIGHT\n",
    "            self.agent_pos[1] = min(self.size - 1, self.agent_pos[1] + 1)\n",
    "        elif action == 2:  # DOWN\n",
    "            self.agent_pos[0] = min(self.size - 1, self.agent_pos[0] + 1)\n",
    "        elif action == 3:  # LEFT\n",
    "            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)\n",
    "        \n",
    "        # Check if we reached the goal\n",
    "        done = tuple(self.agent_pos) == self.goal\n",
    "        \n",
    "        # Calculate reward\n",
    "        if done:\n",
    "            reward = 10  # Big reward for reaching goal!\n",
    "        else:\n",
    "            reward = -1  # Small penalty for each step (encourages efficiency)\n",
    "        \n",
    "        return self.get_state(), reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Display the grid with the agent's position.\"\"\"\n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        \n",
    "        # Mark goal\n",
    "        grid[self.goal] = 2\n",
    "        \n",
    "        # Mark agent\n",
    "        grid[tuple(self.agent_pos)] = 1\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        \n",
    "        # Custom colormap: white=empty, blue=agent, green=goal\n",
    "        colors = ['white', '#2196F3', '#4CAF50']\n",
    "        cmap = ListedColormap(colors)\n",
    "        \n",
    "        ax.imshow(grid, cmap=cmap)\n",
    "        \n",
    "        # Add grid lines\n",
    "        for i in range(self.size + 1):\n",
    "            ax.axhline(i - 0.5, color='black', linewidth=2)\n",
    "            ax.axvline(i - 0.5, color='black', linewidth=2)\n",
    "        \n",
    "        # Add labels\n",
    "        ax.text(self.agent_pos[1], self.agent_pos[0], 'A', \n",
    "                ha='center', va='center', fontsize=24, fontweight='bold', color='white')\n",
    "        ax.text(self.goal[1], self.goal[0], 'G', \n",
    "                ha='center', va='center', fontsize=24, fontweight='bold', color='white')\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('Grid World\\nA=Agent, G=Goal', fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and display our environment!\n",
    "env = SimpleGridWorld(size=5)\n",
    "print(\"Welcome to Grid World!\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Grid size: {env.size}x{env.size}\")\n",
    "print(f\"Agent starts at: {env.get_state()}\")\n",
    "print(f\"Goal is at: {env.goal}\")\n",
    "print(f\"Actions: {env.action_names}\")\n",
    "print(\"\\nRewards:\")\n",
    "print(\"  - Each step: -1 (so we want to be efficient!)\")\n",
    "print(\"  - Reaching goal: +10\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Let's Watch the Agent-Environment Loop in Action!\n",
    "\n",
    "We'll manually step through a few moves to see exactly how the loop works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh environment\n",
    "env = SimpleGridWorld(size=5)\n",
    "\n",
    "print(\"THE AGENT-ENVIRONMENT LOOP IN ACTION\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nWe'll manually take 4 steps to see how it works:\\n\")\n",
    "\n",
    "# Define a sequence of actions to demonstrate\n",
    "demo_actions = [1, 1, 2, 2]  # RIGHT, RIGHT, DOWN, DOWN\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for step, action in enumerate(demo_actions, 1):\n",
    "    # Get current state BEFORE the action\n",
    "    old_state = env.get_state()\n",
    "    \n",
    "    # Take the action\n",
    "    new_state, reward, done = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    # Print what happened\n",
    "    print(f\"Step {step}:\")\n",
    "    print(f\"  State (before): {old_state}\")\n",
    "    print(f\"  Action taken:   {env.action_names[action]}\")\n",
    "    print(f\"  State (after):  {new_state}\")\n",
    "    print(f\"  Reward:         {reward}\")\n",
    "    print(f\"  Done?           {done}\")\n",
    "    print(f\"  Total reward:   {total_reward}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\nAfter 4 steps, the agent has moved from (0,0) to {env.get_state()}\")\n",
    "print(f\"Total reward collected: {total_reward}\")\n",
    "print(\"\\nLet's visualize the current position:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What is a Policy?\n",
    "\n",
    "A **policy** is the agent's strategy - it tells the agent what to do in each situation.\n",
    "\n",
    "### Analogy: GPS Navigation\n",
    "\n",
    "Think of a policy like GPS directions:\n",
    "- **State:** Your current location\n",
    "- **Policy:** The GPS rules that tell you which way to turn\n",
    "- **Action:** The turn you actually make\n",
    "\n",
    "A good policy gets you to your destination quickly. A bad policy gets you lost!\n",
    "\n",
    "### Let's Compare Two Policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    \"\"\"\n",
    "    A RANDOM policy - just picks any action randomly.\n",
    "    \n",
    "    This is like closing your eyes and picking a direction!\n",
    "    Not very smart, but it's a starting point.\n",
    "    \"\"\"\n",
    "    return np.random.randint(0, 4)  # Random: UP, RIGHT, DOWN, or LEFT\n",
    "\n",
    "\n",
    "def smart_policy(state):\n",
    "    \"\"\"\n",
    "    A SMART policy - always moves toward the goal.\n",
    "    \n",
    "    This policy knows the goal is at (4,4) and always moves closer to it.\n",
    "    It's like having a GPS that knows exactly where to go!\n",
    "    \"\"\"\n",
    "    row, col = state\n",
    "    goal_row, goal_col = 4, 4\n",
    "    \n",
    "    # If we're not at the goal row, move down\n",
    "    if row < goal_row:\n",
    "        return 2  # DOWN\n",
    "    # If we're at goal row but not goal column, move right\n",
    "    elif col < goal_col:\n",
    "        return 1  # RIGHT\n",
    "    # We're at the goal!\n",
    "    else:\n",
    "        return 0  # Doesn't matter, we're done\n",
    "\n",
    "\n",
    "def run_episode(env, policy, policy_name, max_steps=50):\n",
    "    \"\"\"\n",
    "    Run one complete episode using the given policy.\n",
    "    Returns the total reward and number of steps taken.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    path = [state]  # Track the path taken\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action = policy(state)\n",
    "        state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        path.append(state)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return total_reward, steps, path, done\n",
    "\n",
    "\n",
    "# Compare the two policies!\n",
    "print(\"COMPARING POLICIES: Random vs Smart\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Smart policy\n",
    "env = SimpleGridWorld(size=5)\n",
    "smart_reward, smart_steps, smart_path, smart_success = run_episode(env, smart_policy, \"Smart\")\n",
    "\n",
    "print(\"\\nSMART POLICY (always moves toward goal):\")\n",
    "print(f\"  Path taken: {' -> '.join([str(p) for p in smart_path])}\")\n",
    "print(f\"  Steps taken: {smart_steps}\")\n",
    "print(f\"  Total reward: {smart_reward}\")\n",
    "print(f\"  Reached goal: {'Yes!' if smart_success else 'No'}\")\n",
    "\n",
    "# Random policy (run multiple times since it's random)\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"\\nRANDOM POLICY (picks random directions):\")\n",
    "\n",
    "random_rewards = []\n",
    "random_steps_list = []\n",
    "successes = 0\n",
    "\n",
    "for i in range(100):  # Run 100 episodes\n",
    "    env = SimpleGridWorld(size=5)\n",
    "    reward, steps, path, success = run_episode(env, random_policy, \"Random\")\n",
    "    random_rewards.append(reward)\n",
    "    random_steps_list.append(steps)\n",
    "    if success:\n",
    "        successes += 1\n",
    "\n",
    "print(f\"  (Results averaged over 100 episodes)\")\n",
    "print(f\"  Average steps: {np.mean(random_steps_list):.1f}\")\n",
    "print(f\"  Average reward: {np.mean(random_rewards):.1f}\")\n",
    "print(f\"  Success rate: {successes}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nCONCLUSION:\")\n",
    "print(f\"  Smart policy: {smart_steps} steps, {smart_reward} reward (optimal!)\")\n",
    "print(f\"  Random policy: ~{np.mean(random_steps_list):.0f} steps, ~{np.mean(random_rewards):.0f} reward\")\n",
    "print(\"\\n  The goal of RL is to LEARN a smart policy through experience!\")\n",
    "print(\"  We don't want to hard-code it - we want the agent to discover it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualizing the Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_path(path, title):\n",
    "    \"\"\"Visualize a path through the grid.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    \n",
    "    # Draw grid\n",
    "    for i in range(6):\n",
    "        ax.axhline(i, color='black', linewidth=2)\n",
    "        ax.axvline(i, color='black', linewidth=2)\n",
    "    \n",
    "    # Draw path\n",
    "    path_x = [p[1] + 0.5 for p in path]\n",
    "    path_y = [p[0] + 0.5 for p in path]\n",
    "    \n",
    "    # Draw line\n",
    "    ax.plot(path_x, path_y, 'b-', linewidth=3, alpha=0.5, label='Path')\n",
    "    \n",
    "    # Draw points\n",
    "    for i, (x, y) in enumerate(zip(path_x, path_y)):\n",
    "        if i == 0:\n",
    "            ax.scatter(x, y, s=300, c='blue', zorder=5, label='Start')\n",
    "            ax.text(x, y, 'S', ha='center', va='center', fontsize=14, color='white', fontweight='bold')\n",
    "        elif i == len(path) - 1:\n",
    "            ax.scatter(x, y, s=300, c='green', zorder=5, label='End')\n",
    "            ax.text(x, y, 'E', ha='center', va='center', fontsize=14, color='white', fontweight='bold')\n",
    "        else:\n",
    "            ax.scatter(x, y, s=100, c='lightblue', zorder=4, edgecolors='blue')\n",
    "            ax.text(x, y, str(i), ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Mark goal\n",
    "    ax.add_patch(plt.Rectangle((4, 4), 1, 1, color='lightgreen', alpha=0.5))\n",
    "    ax.text(4.5, 4.5, 'GOAL', ha='center', va='center', fontsize=12, fontweight='bold', color='green')\n",
    "    \n",
    "    ax.set_xlim(0, 5)\n",
    "    ax.set_ylim(5, 0)  # Inverted y-axis\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show smart policy path\n",
    "env = SimpleGridWorld(size=5)\n",
    "_, _, smart_path, _ = run_episode(env, smart_policy, \"Smart\")\n",
    "visualize_path(smart_path, f\"Smart Policy: {len(smart_path)-1} steps (Optimal!)\")\n",
    "\n",
    "# Show a random policy path\n",
    "np.random.seed(42)  # For reproducibility\n",
    "env = SimpleGridWorld(size=5)\n",
    "_, _, random_path, _ = run_episode(env, random_policy, \"Random\", max_steps=30)\n",
    "visualize_path(random_path[:31], f\"Random Policy: {len(random_path)-1} steps (Inefficient!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploration vs Exploitation: A Key Challenge\n",
    "\n",
    "One of the most important concepts in RL is the **exploration-exploitation trade-off**.\n",
    "\n",
    "### The Restaurant Analogy\n",
    "\n",
    "Imagine you're in a new city and want to find a good restaurant:\n",
    "\n",
    "**Exploitation (Use what you know):**\n",
    "- You found a decent Italian place yesterday\n",
    "- You could go there again - you know it's okay!\n",
    "- Safe choice, but maybe there's something better?\n",
    "\n",
    "**Exploration (Try something new):**\n",
    "- There's a Thai place you've never tried\n",
    "- It might be amazing... or terrible\n",
    "- Risky, but you might discover your new favorite!\n",
    "\n",
    "### The Balance\n",
    "\n",
    "- **Too much exploitation:** You miss better options\n",
    "- **Too much exploration:** You waste time on bad choices\n",
    "\n",
    "**Good RL agents balance both!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, best_action_func, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Epsilon-Greedy Policy: The simplest way to balance exploration and exploitation!\n",
    "    \n",
    "    - With probability (1 - epsilon): EXPLOIT - take the best known action\n",
    "    - With probability epsilon: EXPLORE - take a random action\n",
    "    \n",
    "    Args:\n",
    "        state: Current state\n",
    "        best_action_func: Function that returns the best action for a state\n",
    "        epsilon: Probability of exploring (0.1 = 10% exploration)\n",
    "    \"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        # EXPLORE: Random action\n",
    "        return np.random.randint(0, 4)\n",
    "    else:\n",
    "        # EXPLOIT: Best known action\n",
    "        return best_action_func(state)\n",
    "\n",
    "# Demonstrate epsilon-greedy\n",
    "print(\"EPSILON-GREEDY POLICY DEMONSTRATION\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nThis policy usually picks the best action,\")\n",
    "print(\"but sometimes explores randomly.\\n\")\n",
    "\n",
    "# Simulate 1000 decisions at state (0,0)\n",
    "# Best action from (0,0) is to go DOWN or RIGHT\n",
    "def best_action(state):\n",
    "    return 2  # DOWN is best from (0,0)\n",
    "\n",
    "for epsilon in [0.0, 0.1, 0.3, 0.5, 1.0]:\n",
    "    actions = [epsilon_greedy_policy((0,0), best_action, epsilon) for _ in range(1000)]\n",
    "    best_count = actions.count(2)  # DOWN\n",
    "    print(f\"Epsilon = {epsilon:.1f}: Best action chosen {best_count/10:.0f}% of the time\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- epsilon=0.0: Always exploit (never explore) - might miss better options\")\n",
    "print(\"- epsilon=0.1: Mostly exploit, sometimes explore - good balance!\")\n",
    "print(\"- epsilon=1.0: Always explore (random) - doesn't use knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Real-World RL Success Stories\n",
    "\n",
    "RL has achieved remarkable things! Here are some famous examples:\n",
    "\n",
    "### AlphaGo (2016)\n",
    "- **What:** AI that plays the board game Go\n",
    "- **Achievement:** Beat the world champion!\n",
    "- **Why it matters:** Go was considered too complex for computers\n",
    "\n",
    "### OpenAI Five (2019)\n",
    "- **What:** AI team that plays Dota 2\n",
    "- **Achievement:** Beat world champion esports team\n",
    "- **Why it matters:** Complex teamwork and strategy\n",
    "\n",
    "### ChatGPT (2022)\n",
    "- **What:** Conversational AI\n",
    "- **Achievement:** Helpful, harmless, and honest responses\n",
    "- **How:** RLHF - Reinforcement Learning from Human Feedback\n",
    "\n",
    "### Self-Driving Cars\n",
    "- **What:** Cars that drive themselves\n",
    "- **Companies:** Tesla, Waymo, Cruise\n",
    "- **How:** RL helps with decision-making in complex traffic\n",
    "\n",
    "### Robotics\n",
    "- **What:** Robots learning to walk, grasp objects, etc.\n",
    "- **Companies:** Boston Dynamics, OpenAI\n",
    "- **How:** RL teaches robots through trial and error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "Let's recap what we learned:\n",
    "\n",
    "### 1. What is RL?\n",
    "- Learning through **trial and error**\n",
    "- Agent takes actions, gets rewards, learns what works\n",
    "- Like training a dog or playing a video game!\n",
    "\n",
    "### 2. The RL Loop\n",
    "```\n",
    "State → Agent → Action → Environment → Reward → (repeat)\n",
    "```\n",
    "\n",
    "### 3. Key Vocabulary\n",
    "- **Agent:** The learner (makes decisions)\n",
    "- **Environment:** The world (responds to actions)\n",
    "- **State:** Current situation\n",
    "- **Action:** What the agent does\n",
    "- **Reward:** Feedback (positive or negative)\n",
    "- **Policy:** The agent's strategy\n",
    "\n",
    "### 4. Exploration vs Exploitation\n",
    "- **Explore:** Try new things to find better options\n",
    "- **Exploit:** Use what you know works\n",
    "- Good agents **balance both**!\n",
    "\n",
    "### 5. The Goal\n",
    "**Find a policy that maximizes total reward over time!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "Try to answer these questions before clicking to reveal the answers!\n",
    "\n",
    "**1. In the dog training analogy, what is the reward?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The treat! It's the positive feedback that encourages the dog (agent) to repeat the good behavior (action).\n",
    "</details>\n",
    "\n",
    "**2. What's the difference between a state and an action?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The state is the current situation (what the agent observes). The action is what the agent decides to do in response. For example: state = \"ghost is nearby\", action = \"run away\".\n",
    "</details>\n",
    "\n",
    "**3. Why is a random policy bad?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "A random policy doesn't learn from experience - it just guesses. It takes many more steps to reach the goal (if it ever does!) and collects much less reward than a smart policy.\n",
    "</details>\n",
    "\n",
    "**4. What is epsilon in epsilon-greedy?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Epsilon is the probability of exploring (taking a random action) instead of exploiting (taking the best known action). An epsilon of 0.1 means 10% exploration, 90% exploitation.\n",
    "</details>\n",
    "\n",
    "**5. What technology uses RLHF?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "ChatGPT! RLHF (Reinforcement Learning from Human Feedback) is used to make language models more helpful, harmless, and honest.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Congratulations! You now understand the fundamentals of reinforcement learning!\n",
    "\n",
    "In the next notebook, we'll learn about **Markov Decision Processes (MDPs)** - the mathematical framework that formalizes everything we discussed here.\n",
    "\n",
    "Don't worry - we'll use the same intuitive approach with lots of examples and visualizations!\n",
    "\n",
    "**Continue to:** [Notebook 2: Markov Decision Processes](02_markov_decision_processes.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Great job completing your first RL notebook! You're on your way to understanding how AI learns through experience!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
