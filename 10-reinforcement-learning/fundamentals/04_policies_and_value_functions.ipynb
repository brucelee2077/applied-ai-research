{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policies and Value Functions\n",
    "\n",
    "Welcome back! Now we'll learn about the two most important concepts in RL: **policies** (what to do) and **value functions** (how good is it).\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- What a policy is (with a GPS navigation analogy!)\n",
    "- Deterministic vs stochastic policies\n",
    "- State-value function V(s) - \"How good is being here?\"\n",
    "- Action-value function Q(s,a) - \"How good is this action?\"\n",
    "- Optimal policies and why we want them\n",
    "- How to estimate values with Monte Carlo\n",
    "\n",
    "**Prerequisites:** Notebooks 1-3\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: GPS and Property Values\n",
    "\n",
    "Two simple analogies explain policies and value functions:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │                    POLICY = GPS                         │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  GPS Navigation:                                       │\n",
    "    │    \"At this location, go THIS direction\"               │\n",
    "    │                                                         │\n",
    "    │  Policy:                                               │\n",
    "    │    \"In this state, take THIS action\"                   │\n",
    "    │                                                         │\n",
    "    │  Both map a current situation to a decision!           │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │            VALUE FUNCTION = PROPERTY VALUES             │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  Property Values:                                      │\n",
    "    │    \"This neighborhood is worth more money\"             │\n",
    "    │    → Better location = higher value                    │\n",
    "    │                                                         │\n",
    "    │  State-Value V(s):                                     │\n",
    "    │    \"This state is worth more reward\"                   │\n",
    "    │    → Better state = higher value                       │\n",
    "    │                                                         │\n",
    "    │  Both measure how \"good\" a location/state is!          │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What is a Policy?\n",
    "\n",
    "A **policy** π (pi) is the agent's strategy - it tells the agent what to do in each situation.\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │                      POLICY                             │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  A policy is a MAPPING from states to actions:         │\n",
    "    │                                                         │\n",
    "    │       State ────────> Action                           │\n",
    "    │                                                         │\n",
    "    │  Examples:                                             │\n",
    "    │                                                         │\n",
    "    │  Chess:                                                │\n",
    "    │    Board position → Best move                          │\n",
    "    │                                                         │\n",
    "    │  Self-driving car:                                     │\n",
    "    │    Camera image + sensors → Steering/acceleration      │\n",
    "    │                                                         │\n",
    "    │  Robot vacuum:                                         │\n",
    "    │    Current location + dirt sensors → Movement direction│\n",
    "    │                                                         │\n",
    "    │  THE GOAL OF RL: Find the BEST policy!                 │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Two Types of Policies\n",
    "\n",
    "### 1. Deterministic Policy\n",
    "\n",
    "Always picks the **same action** for a given state.\n",
    "\n",
    "```\n",
    "π(s) = a      \"In state s, ALWAYS do action a\"\n",
    "```\n",
    "\n",
    "**Example:** A GPS that always tells you the shortest route.\n",
    "\n",
    "### 2. Stochastic Policy\n",
    "\n",
    "Picks actions with **probabilities**.\n",
    "\n",
    "```\n",
    "π(a|s) = P(action = a | state = s)    \"In state s, do action a with probability p\"\n",
    "```\n",
    "\n",
    "**Example:** A GPS that sometimes suggests scenic routes (exploration!).\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │       DETERMINISTIC vs STOCHASTIC POLICY                │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  DETERMINISTIC:                                        │\n",
    "    │    State (1,1) → Action: RIGHT (always)               │\n",
    "    │                                                         │\n",
    "    │  STOCHASTIC (epsilon-greedy with ε=0.1):               │\n",
    "    │    State (1,1) → Action probabilities:                 │\n",
    "    │      • RIGHT: 92.5%  ← Best action (high probability)  │\n",
    "    │      • UP:     2.5%                                    │\n",
    "    │      • DOWN:   2.5%                                    │\n",
    "    │      • LEFT:   2.5%                                    │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, FancyArrowPatch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Demonstrate deterministic vs stochastic policies\n",
    "\n",
    "def deterministic_policy(state):\n",
    "    \"\"\"\n",
    "    Deterministic policy: Always takes the same action.\n",
    "    \n",
    "    Strategy: Go toward the goal at (3, 3)\n",
    "    - If not at goal column, go RIGHT\n",
    "    - If at goal column, go DOWN\n",
    "    \"\"\"\n",
    "    row, col = state\n",
    "    if col < 3:\n",
    "        return 1  # RIGHT\n",
    "    elif row < 3:\n",
    "        return 2  # DOWN\n",
    "    else:\n",
    "        return 0  # At goal, any action\n",
    "\n",
    "\n",
    "def stochastic_policy(state, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Stochastic policy (epsilon-greedy):\n",
    "    - With probability (1-epsilon): take best action\n",
    "    - With probability epsilon: take random action\n",
    "    \"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(0, 4)  # Random action\n",
    "    else:\n",
    "        return deterministic_policy(state)  # Best action\n",
    "\n",
    "\n",
    "def get_policy_distribution(state, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Get the probability distribution over actions for a stochastic policy.\n",
    "    \"\"\"\n",
    "    optimal_action = deterministic_policy(state)\n",
    "    probs = np.ones(4) * (epsilon / 4)  # Base probability for exploration\n",
    "    probs[optimal_action] += (1 - epsilon)  # High prob for optimal action\n",
    "    return probs\n",
    "\n",
    "\n",
    "# Visualize policy distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "action_names = ['Up', 'Right', 'Down', 'Left']\n",
    "colors = ['#f44336', '#4caf50', '#ff9800', '#2196f3']\n",
    "\n",
    "# Left: Deterministic policy\n",
    "ax1 = axes[0]\n",
    "state = (1, 1)\n",
    "det_probs = np.zeros(4)\n",
    "det_probs[deterministic_policy(state)] = 1.0\n",
    "\n",
    "bars = ax1.bar(action_names, det_probs, color=colors, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Probability', fontsize=12)\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.set_title(f'Deterministic Policy at State {state}\\n\"Always go RIGHT\"', \n",
    "              fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, p in enumerate(det_probs):\n",
    "    ax1.text(i, p + 0.03, f'{p:.0%}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Right: Stochastic policy\n",
    "ax2 = axes[1]\n",
    "stoch_probs = get_policy_distribution(state, epsilon=0.1)\n",
    "\n",
    "bars = ax2.bar(action_names, stoch_probs, color=colors, edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Probability', fontsize=12)\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.set_title(f'Stochastic Policy at State {state}\\nε-greedy (ε=0.1)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, p in enumerate(stoch_probs):\n",
    "    ax2.text(i, p + 0.03, f'{p:.1%}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WHY USE STOCHASTIC POLICIES?\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. EXPLORATION: Sometimes try new things to find better actions\")\n",
    "print(\"2. GAME THEORY: Unpredictable behavior is harder to exploit\")\n",
    "print(\"3. UNCERTAINTY: When unsure, hedging bets can be optimal\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## State-Value Function V(s): \"How Good Is Being Here?\"\n",
    "\n",
    "The **state-value function** V^π(s) tells us the expected total future reward starting from state s, when following policy π.\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │            STATE-VALUE FUNCTION V(s)                    │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  V^π(s) = Expected Return starting from state s        │\n",
    "    │         = E[G_t | s_t = s, following policy π]         │\n",
    "    │         = E[r_t + γr_{t+1} + γ²r_{t+2} + ... | s]      │\n",
    "    │                                                         │\n",
    "    │  In plain English:                                     │\n",
    "    │    \"How much total reward can I expect to get          │\n",
    "    │     if I start HERE and follow THIS strategy?\"         │\n",
    "    │                                                         │\n",
    "    │  ANALOGY: Property Values                              │\n",
    "    │    V(nice neighborhood) = HIGH   (good location)       │\n",
    "    │    V(bad neighborhood)  = LOW    (bad location)        │\n",
    "    │                                                         │\n",
    "    │    The \"value\" depends on what you can get from there! │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "V(s) depends on TWO things:\n",
    "1. **The state s** - some states are naturally better\n",
    "2. **The policy π** - your strategy affects your outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Simple 4x4 Grid World for demonstrating value functions.\n",
    "    \n",
    "    Layout:\n",
    "        ┌───┬───┬───┬───┐\n",
    "        │ S │   │   │   │   S = Start (0,0)\n",
    "        ├───┼───┼───┼───┤\n",
    "        │   │   │   │   │\n",
    "        ├───┼───┼───┼───┤\n",
    "        │   │   │   │   │\n",
    "        ├───┼───┼───┼───┤\n",
    "        │   │   │   │ G │   G = Goal (3,3)\n",
    "        └───┴───┴───┴───┘\n",
    "    \n",
    "    Rewards: -1 per step, +10 at goal\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.goal = (3, 3)\n",
    "        self.gamma = 0.9  # Discount factor\n",
    "        self.action_names = ['Up', 'Right', 'Down', 'Left']\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        \"\"\"Get the next state given current state and action.\"\"\"\n",
    "        row, col = state\n",
    "        \n",
    "        if action == 0:    # Up\n",
    "            row = max(0, row - 1)\n",
    "        elif action == 1:  # Right\n",
    "            col = min(3, col + 1)\n",
    "        elif action == 2:  # Down\n",
    "            row = min(3, row + 1)\n",
    "        elif action == 3:  # Left\n",
    "            col = max(0, col - 1)\n",
    "        \n",
    "        return (row, col)\n",
    "    \n",
    "    def get_reward(self, state, next_state):\n",
    "        \"\"\"Get the reward for a transition.\"\"\"\n",
    "        if next_state == self.goal:\n",
    "            return 10  # Big reward at goal!\n",
    "        return -1  # Small penalty for each step\n",
    "\n",
    "\n",
    "def estimate_value_monte_carlo(env, policy, state, n_episodes=1000, max_steps=50):\n",
    "    \"\"\"\n",
    "    Estimate V(s) using Monte Carlo simulation.\n",
    "    \n",
    "    Method: Run many episodes, average the returns.\n",
    "    \n",
    "    This is like: \"Try the strategy many times, see what you get on average\"\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        current_state = state\n",
    "        episode_return = 0\n",
    "        discount = 1.0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Get action from policy\n",
    "            action = policy(current_state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state = env.get_next_state(current_state, action)\n",
    "            reward = env.get_reward(current_state, next_state)\n",
    "            \n",
    "            # Accumulate discounted reward\n",
    "            episode_return += discount * reward\n",
    "            discount *= env.gamma\n",
    "            \n",
    "            # Check if done\n",
    "            if next_state == env.goal:\n",
    "                break\n",
    "            \n",
    "            current_state = next_state\n",
    "        \n",
    "        returns.append(episode_return)\n",
    "    \n",
    "    return np.mean(returns)\n",
    "\n",
    "\n",
    "# Create environment and compute values\n",
    "env = GridWorld()\n",
    "\n",
    "print(\"COMPUTING STATE VALUES V(s)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nUsing Monte Carlo simulation with optimal policy...\")\n",
    "print(\"(Running 500 episodes per state)\")\n",
    "\n",
    "V = np.zeros((4, 4))\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        state = (row, col)\n",
    "        if state == env.goal:\n",
    "            V[row, col] = 0  # Terminal state has no future rewards\n",
    "        else:\n",
    "            V[row, col] = estimate_value_monte_carlo(\n",
    "                env, deterministic_policy, state, n_episodes=500\n",
    "            )\n",
    "\n",
    "print(\"\\nState Values V(s):\")\n",
    "print(\"-\"*60)\n",
    "print(\"         Col 0    Col 1    Col 2    Col 3\")\n",
    "for row in range(4):\n",
    "    values = \" \".join([f\"{V[row, col]:8.2f}\" for col in range(4)])\n",
    "    print(f\"Row {row}:  {values}\")\n",
    "print(\"-\"*60)\n",
    "print(\"(Goal at (3,3) has value 0 - it's the terminal state)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the value function as a heatmap\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Heatmap\n",
    "ax1 = axes[0]\n",
    "im = ax1.imshow(V, cmap='RdYlGn', origin='upper')\n",
    "\n",
    "# Add value annotations\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        value = V[i, j]\n",
    "        color = 'white' if value < np.mean(V) else 'black'\n",
    "        if (i, j) == env.goal:\n",
    "            ax1.text(j, i, 'GOAL\\n0', ha='center', va='center', \n",
    "                    fontsize=11, fontweight='bold', color='white')\n",
    "        else:\n",
    "            ax1.text(j, i, f'{value:.1f}', ha='center', va='center', \n",
    "                    fontsize=12, fontweight='bold', color=color)\n",
    "\n",
    "ax1.set_xticks(range(4))\n",
    "ax1.set_yticks(range(4))\n",
    "ax1.set_xlabel('Column', fontsize=12)\n",
    "ax1.set_ylabel('Row', fontsize=12)\n",
    "ax1.set_title('State Values V(s)\\n(Higher = Better State)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax1, label='Value')\n",
    "\n",
    "# Right: Interpretation\n",
    "ax2 = axes[1]\n",
    "ax2.axis('off')\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "\n",
    "interpretation = \"\"\"\n",
    "INTERPRETATION:\n",
    "\n",
    "• States CLOSER to goal have HIGHER values\n",
    "  (Less distance = less negative reward)\n",
    "\n",
    "• State (3,2) has highest value (~3.1)\n",
    "  (One step from goal: -1 + 10 × 0.9 = 8.0)\n",
    "\n",
    "• State (0,0) has lowest value (~-0.9)\n",
    "  (Farthest from goal, most steps needed)\n",
    "\n",
    "• Values tell us: \"How good is it to BE here?\"\n",
    "\n",
    "ANALOGY:\n",
    "• Goal = Beach\n",
    "• Value = \"How nice is this location?\"\n",
    "• High value = Close to beach (desirable)\n",
    "• Low value = Far from beach (less desirable)\n",
    "\"\"\"\n",
    "\n",
    "ax2.text(0.1, 0.95, interpretation, transform=ax2.transAxes,\n",
    "         fontsize=11, verticalalignment='top', family='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Action-Value Function Q(s, a): \"How Good Is This Action?\"\n",
    "\n",
    "The **action-value function** Q^π(s, a) tells us the expected return of taking action a in state s, then following policy π.\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │            ACTION-VALUE FUNCTION Q(s, a)                │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  Q^π(s, a) = Expected Return starting from (s, a)      │\n",
    "    │            = E[G_t | s_t = s, a_t = a, then follow π]  │\n",
    "    │                                                         │\n",
    "    │  In plain English:                                     │\n",
    "    │    \"If I'm in state s and take action a,               │\n",
    "    │     how much reward will I get in total?\"              │\n",
    "    │                                                         │\n",
    "    │  ANALOGY: Restaurant Ratings                           │\n",
    "    │    Q(downtown, pizza) = 8.5  \"Pizza downtown is great!\"│\n",
    "    │    Q(downtown, sushi) = 7.2  \"Sushi downtown is okay\"  │\n",
    "    │    Q(downtown, tacos) = 9.1  \"Tacos downtown are best!\"│\n",
    "    │                                                         │\n",
    "    │    Best action = argmax Q(s, a) = TACOS!               │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Why Q is More Useful Than V\n",
    "\n",
    "- **V(s)** tells you how good a state is\n",
    "- **Q(s, a)** tells you how good each action is\n",
    "\n",
    "With Q-values, choosing the best action is easy:\n",
    "```\n",
    "Best action = argmax_a Q(s, a)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_q_value(env, policy, state, action, n_episodes=500, max_steps=50):\n",
    "    \"\"\"\n",
    "    Estimate Q(s, a) using Monte Carlo simulation.\n",
    "    \n",
    "    Method: Start with action a in state s, then follow policy.\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # First step: take the specified action\n",
    "        current_state = state\n",
    "        next_state = env.get_next_state(current_state, action)\n",
    "        episode_return = env.get_reward(current_state, next_state)\n",
    "        discount = env.gamma\n",
    "        current_state = next_state\n",
    "        \n",
    "        # Then follow the policy\n",
    "        for step in range(max_steps - 1):\n",
    "            if current_state == env.goal:\n",
    "                break\n",
    "            \n",
    "            action_t = policy(current_state)\n",
    "            next_state = env.get_next_state(current_state, action_t)\n",
    "            reward = env.get_reward(current_state, next_state)\n",
    "            \n",
    "            episode_return += discount * reward\n",
    "            discount *= env.gamma\n",
    "            current_state = next_state\n",
    "        \n",
    "        returns.append(episode_return)\n",
    "    \n",
    "    return np.mean(returns)\n",
    "\n",
    "\n",
    "# Compute Q-values for a specific state\n",
    "state = (1, 1)\n",
    "\n",
    "print(f\"Q-VALUES AT STATE {state}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n'How good is each action from this state?'\\n\")\n",
    "\n",
    "q_values = []\n",
    "for action in range(4):\n",
    "    q = estimate_q_value(env, deterministic_policy, state, action, n_episodes=500)\n",
    "    q_values.append(q)\n",
    "    print(f\"  Q({state}, {env.action_names[action]:5s}) = {q:6.2f}\")\n",
    "\n",
    "best_action = np.argmax(q_values)\n",
    "print(f\"\\n  Best action: {env.action_names[best_action]} (highest Q-value!)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Q-values for a state\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(5, 9.5, f'Q-Values at State {state}', \n",
    "        ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Central state box\n",
    "state_box = FancyBboxPatch((3.5, 4), 3, 2, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#bbdefb', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(state_box)\n",
    "ax.text(5, 5.2, f'State', ha='center', fontsize=14, fontweight='bold', color='#1976d2')\n",
    "ax.text(5, 4.5, f'{state}', ha='center', fontsize=12, color='#1976d2')\n",
    "\n",
    "# Action Q-values around the state\n",
    "actions_info = [\n",
    "    {'name': 'UP', 'pos': (5, 7.5), 'q': q_values[0], 'color': '#f44336'},\n",
    "    {'name': 'RIGHT', 'pos': (8, 5), 'q': q_values[1], 'color': '#4caf50'},\n",
    "    {'name': 'DOWN', 'pos': (5, 2), 'q': q_values[2], 'color': '#ff9800'},\n",
    "    {'name': 'LEFT', 'pos': (2, 5), 'q': q_values[3], 'color': '#9c27b0'},\n",
    "]\n",
    "\n",
    "best_q = max(q_values)\n",
    "\n",
    "for info in actions_info:\n",
    "    x, y = info['pos']\n",
    "    \n",
    "    # Highlight best action\n",
    "    if info['q'] == best_q:\n",
    "        face_color = '#c8e6c9'\n",
    "        edge_color = '#388e3c'\n",
    "        text = f\"{info['name']}\\nQ = {info['q']:.2f}\\n★ BEST\"\n",
    "    else:\n",
    "        face_color = '#fff3e0'\n",
    "        edge_color = info['color']\n",
    "        text = f\"{info['name']}\\nQ = {info['q']:.2f}\"\n",
    "    \n",
    "    box = FancyBboxPatch((x-1, y-0.8), 2, 1.6, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=face_color, edgecolor=edge_color, linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Draw arrow from state to action\n",
    "    dx = (x - 5) * 0.4\n",
    "    dy = (y - 5) * 0.4\n",
    "    ax.annotate('', xy=(5 + dx * 2, 5 + dy * 2), xytext=(5 + dx * 0.8, 5 + dy * 0.8),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color=edge_color))\n",
    "\n",
    "ax.text(5, 0.5, 'Optimal policy: Always pick the action with highest Q-value!',\n",
    "        ha='center', fontsize=12, style='italic', color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Relationship Between V and Q\n",
    "\n",
    "V and Q are related by:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │              V(s) AND Q(s,a) RELATIONSHIP               │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  V^π(s) = Σ_a π(a|s) × Q^π(s, a)                       │\n",
    "    │                                                         │\n",
    "    │  In plain English:                                     │\n",
    "    │    \"The value of a state = weighted average of         │\n",
    "    │     Q-values for all actions\"                          │\n",
    "    │                                                         │\n",
    "    │  Example (ε-greedy with ε=0.1):                        │\n",
    "    │    V(s) = 0.925×Q(s,best) + 0.025×Q(s,a₁)             │\n",
    "    │         + 0.025×Q(s,a₂) + 0.025×Q(s,a₃)               │\n",
    "    │                                                         │\n",
    "    │  For deterministic policy:                             │\n",
    "    │    V(s) = Q(s, π(s))   (just the Q-value of chosen    │\n",
    "    │                         action)                        │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the V-Q relationship\n",
    "\n",
    "state = (1, 1)\n",
    "\n",
    "print(\"V(s) AND Q(s,a) RELATIONSHIP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nState: {state}\")\n",
    "\n",
    "# Q-values we computed earlier\n",
    "print(f\"\\nQ-values:\")\n",
    "for i, name in enumerate(env.action_names):\n",
    "    print(f\"  Q(s, {name:5s}) = {q_values[i]:6.2f}\")\n",
    "\n",
    "# For deterministic policy: V(s) = Q(s, π(s))\n",
    "best_action = deterministic_policy(state)\n",
    "v_deterministic = q_values[best_action]\n",
    "print(f\"\\nFor DETERMINISTIC policy (always RIGHT):\")\n",
    "print(f\"  V(s) = Q(s, RIGHT) = {v_deterministic:.2f}\")\n",
    "\n",
    "# For epsilon-greedy policy: V(s) = sum of π(a|s) * Q(s,a)\n",
    "epsilon = 0.1\n",
    "probs = get_policy_distribution(state, epsilon)\n",
    "v_stochastic = sum(p * q for p, q in zip(probs, q_values))\n",
    "\n",
    "print(f\"\\nFor STOCHASTIC policy (ε-greedy, ε={epsilon}):\")\n",
    "print(f\"  V(s) = Σ π(a|s) × Q(s,a)\")\n",
    "print(f\"       = 0.925×{q_values[1]:.2f} + 0.025×{q_values[0]:.2f} + 0.025×{q_values[2]:.2f} + 0.025×{q_values[3]:.2f}\")\n",
    "print(f\"       = {v_stochastic:.2f}\")\n",
    "\n",
    "print(f\"\\nNote: Stochastic V < Deterministic V because exploration sometimes picks worse actions!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Optimal Value Functions: The Best Possible\n",
    "\n",
    "The **optimal value functions** are the best values achievable by ANY policy:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │              OPTIMAL VALUE FUNCTIONS                    │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  V*(s) = max over all policies π of V^π(s)             │\n",
    "    │        = \"Best possible value from state s\"            │\n",
    "    │                                                         │\n",
    "    │  Q*(s, a) = max over all policies π of Q^π(s, a)       │\n",
    "    │           = \"Best possible value for action a in s\"    │\n",
    "    │                                                         │\n",
    "    │  The OPTIMAL POLICY π* is the one that achieves V*     │\n",
    "    │                                                         │\n",
    "    │  Key insight: Once you have Q*, the optimal policy is: │\n",
    "    │                                                         │\n",
    "    │       π*(s) = argmax_a Q*(s, a)                        │\n",
    "    │                                                         │\n",
    "    │  \"Just pick the action with the highest Q-value!\"      │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### The Goal of RL\n",
    "\n",
    "The goal of reinforcement learning is to find:\n",
    "1. The optimal value functions (V* and Q*), OR\n",
    "2. The optimal policy π* directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimal vs random policy values\n",
    "\n",
    "def random_policy(state):\n",
    "    \"\"\"Random policy: pick any action with equal probability.\"\"\"\n",
    "    return np.random.randint(0, 4)\n",
    "\n",
    "print(\"OPTIMAL vs RANDOM POLICY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nComparing values for the same states under different policies...\\n\")\n",
    "\n",
    "# Compute values for a few states\n",
    "test_states = [(0, 0), (1, 1), (2, 2), (0, 3)]\n",
    "\n",
    "print(f\"{'State':<10} {'V(optimal)':<15} {'V(random)':<15} {'Difference':<15}\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "for state in test_states:\n",
    "    v_optimal = estimate_value_monte_carlo(env, deterministic_policy, state, n_episodes=500)\n",
    "    v_random = estimate_value_monte_carlo(env, random_policy, state, n_episodes=500)\n",
    "    diff = v_optimal - v_random\n",
    "    print(f\"{str(state):<10} {v_optimal:<15.2f} {v_random:<15.2f} {diff:<15.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"  Optimal policy gives MUCH higher values!\")\n",
    "print(\"  This is why finding the optimal policy matters!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the optimal policy as arrows on the grid\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Draw grid\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        if (row, col) == env.goal:\n",
    "            color = '#c8e6c9'\n",
    "        elif (row, col) == (0, 0):\n",
    "            color = '#bbdefb'\n",
    "        else:\n",
    "            color = 'white'\n",
    "        \n",
    "        y = 3 - row\n",
    "        rect = Rectangle((col, y), 1, 1, facecolor=color, \n",
    "                           edgecolor='black', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add policy arrows (except at goal)\n",
    "        if (row, col) != env.goal:\n",
    "            action = deterministic_policy((row, col))\n",
    "            cx, cy = col + 0.5, y + 0.5\n",
    "            \n",
    "            # Arrow direction based on action\n",
    "            arrows = [(0, 0.3), (0.3, 0), (0, -0.3), (-0.3, 0)]  # UP, RIGHT, DOWN, LEFT\n",
    "            dx, dy = arrows[action]\n",
    "            \n",
    "            ax.arrow(cx - dx/2, cy - dy/2, dx, dy,\n",
    "                    head_width=0.15, head_length=0.1,\n",
    "                    fc='#f44336', ec='#f44336', linewidth=2)\n",
    "\n",
    "# Labels\n",
    "ax.text(0.5, 3.5, 'START', ha='center', va='center', fontsize=9, \n",
    "        fontweight='bold', color='#1976d2')\n",
    "ax.text(3.5, 0.5, 'GOAL', ha='center', va='center', fontsize=10, \n",
    "        fontweight='bold', color='#388e3c')\n",
    "\n",
    "ax.set_xlim(0, 4)\n",
    "ax.set_ylim(0, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Optimal Policy π*(s)\\n(Arrows show best action in each state)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The optimal policy always moves toward the goal!\")\n",
    "print(\"• When not at goal column: go RIGHT\")\n",
    "print(\"• When at goal column: go DOWN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### What is a Policy?\n",
    "\n",
    "| Type | Symbol | Description | Example |\n",
    "|------|--------|-------------|----------|\n",
    "| Deterministic | π(s) = a | Same action every time | GPS: \"Turn right\" |\n",
    "| Stochastic | π(a\\|s) | Probability of each action | ε-greedy exploration |\n",
    "\n",
    "### Value Functions\n",
    "\n",
    "| Function | Symbol | Question It Answers |\n",
    "|----------|--------|---------------------|\n",
    "| State-Value | V(s) | \"How good is being in state s?\" |\n",
    "| Action-Value | Q(s,a) | \"How good is action a in state s?\" |\n",
    "\n",
    "### Optimal = Best Possible\n",
    "\n",
    "- **V*(s)**: Best value achievable from state s\n",
    "- **Q*(s,a)**: Best value achievable starting with action a in state s\n",
    "- **π*(s)**: The policy that achieves V* and Q*\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "Once you have Q*, the optimal policy is simple:\n",
    "```\n",
    "π*(s) = argmax_a Q*(s, a)\n",
    "```\n",
    "\"Just pick the action with the highest Q-value!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What is a policy?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "A policy is a mapping from states to actions - it tells the agent what to do in each situation. It can be deterministic (always the same action) or stochastic (probability distribution over actions).\n",
    "</details>\n",
    "\n",
    "**2. What does V(s) represent?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "V(s) is the state-value function. It represents the expected total future reward (return) starting from state s and following a particular policy. It answers: \"How good is it to be in this state?\"\n",
    "</details>\n",
    "\n",
    "**3. What's the difference between V(s) and Q(s,a)?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "V(s) tells you how good a state is overall (averaging over all actions). Q(s,a) tells you how good a specific action is in that state. Q is more directly useful because you can pick the best action by finding argmax_a Q(s,a).\n",
    "</details>\n",
    "\n",
    "**4. How do you get the optimal policy from Q*?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "π*(s) = argmax_a Q*(s, a). In other words, in each state, just pick the action with the highest Q-value!\n",
    "</details>\n",
    "\n",
    "**5. Why might you use a stochastic policy?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Three main reasons: (1) Exploration - random actions help discover better strategies. (2) Game theory - unpredictable behavior is harder for opponents to exploit. (3) Uncertainty - when the state is partially observable, hedging bets can be optimal.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Excellent work! Now you understand policies and value functions.\n",
    "\n",
    "In the next notebook, we'll learn about **Bellman Equations** - the mathematical foundation that allows us to COMPUTE value functions efficiently:\n",
    "- The Bellman expectation equation\n",
    "- The Bellman optimality equation\n",
    "- How these equations connect current and future values\n",
    "\n",
    "**Continue to:** [Notebook 5: Bellman Equations](05_bellman_equations.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Policies tell us what to do, value functions tell us how good it is. Together, they form the core of reinforcement learning!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
