{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDPs)\n",
    "\n",
    "Welcome back! In the previous notebook, we learned what reinforcement learning is through fun analogies. Now, let's learn the **mathematical language** that lets us describe RL problems precisely.\n",
    "\n",
    "Don't worry - we'll still use lots of analogies and visualizations!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- What an MDP is (with a board game analogy!)\n",
    "- The five components: States, Actions, Transitions, Rewards, Discount\n",
    "- The Markov property (why \"history doesn't matter\")\n",
    "- How to model any decision problem as an MDP\n",
    "- You'll build and visualize your own MDP!\n",
    "\n",
    "**Prerequisites:** Notebook 1 (What is RL?)\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: A Board Game Analogy\n",
    "\n",
    "Imagine you're playing a board game like **Snakes and Ladders**:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  ğŸ² SNAKES AND LADDERS ğŸ²                      â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                â”‚\n",
    "    â”‚  STATES: Which square you're on (1-100)        â”‚\n",
    "    â”‚                                                â”‚\n",
    "    â”‚  ACTIONS: Roll the dice                        â”‚\n",
    "    â”‚                                                â”‚\n",
    "    â”‚  TRANSITIONS: Where you land                   â”‚\n",
    "    â”‚    - Normal squares: move forward              â”‚\n",
    "    â”‚    - Ladders: jump up! ğŸªœ                       â”‚\n",
    "    â”‚    - Snakes: slide down! ğŸ                    â”‚\n",
    "    â”‚                                                â”‚\n",
    "    â”‚  REWARDS: +100 for reaching square 100!        â”‚\n",
    "    â”‚                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "This simple game has all the components of an **MDP**!\n",
    "\n",
    "An MDP (Markov Decision Process) is just a formal way to describe:\n",
    "- **Where you are** (states)\n",
    "- **What you can do** (actions)\n",
    "- **What happens** when you act (transitions)\n",
    "- **How good/bad** the outcome is (rewards)\n",
    "- **How much you care** about the future (discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Five Components of an MDP\n",
    "\n",
    "Every MDP has exactly **five components**. Let's learn them with a simple example: **a robot navigating a warehouse**.\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                      WAREHOUSE ROBOT MDP                        â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                 â”‚\n",
    "    â”‚  S (States)       = Robot's location in the warehouse          â”‚\n",
    "    â”‚                     {(0,0), (0,1), (1,0), (1,1), ...}          â”‚\n",
    "    â”‚                                                                 â”‚\n",
    "    â”‚  A (Actions)      = Movement options                            â”‚\n",
    "    â”‚                     {UP, DOWN, LEFT, RIGHT}                     â”‚\n",
    "    â”‚                                                                 â”‚\n",
    "    â”‚  P (Transitions)  = Where robot ends up after moving            â”‚\n",
    "    â”‚                     P(new_location | current_location, action)  â”‚\n",
    "    â”‚                                                                 â”‚\n",
    "    â”‚  R (Rewards)      = Feedback for each action                    â”‚\n",
    "    â”‚                     +10 deliver package, -1 per step            â”‚\n",
    "    â”‚                                                                 â”‚\n",
    "    â”‚  Î³ (Gamma)        = How much robot cares about future           â”‚\n",
    "    â”‚                     0.99 = cares a lot about future             â”‚\n",
    "    â”‚                                                                 â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Let's visualize this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, FancyArrowPatch\n",
    "import numpy as np\n",
    "\n",
    "# Create a beautiful visualization of MDP components\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 11)\n",
    "ax.axis('off')\n",
    "ax.set_facecolor('#f8f9fa')\n",
    "\n",
    "# Title\n",
    "ax.text(7, 10.5, 'The 5 Components of an MDP', ha='center', \n",
    "        fontsize=22, fontweight='bold', color='#333')\n",
    "ax.text(7, 9.8, 'MDP = (S, A, P, R, Î³)', ha='center', \n",
    "        fontsize=16, color='#666', style='italic')\n",
    "\n",
    "# Component boxes with descriptions\n",
    "components = [\n",
    "    {'name': 'S', 'full': 'States', 'color': '#e3f2fd', 'edge': '#1976d2',\n",
    "     'desc': 'All possible situations', 'example': '\"Where am I?\"'},\n",
    "    {'name': 'A', 'full': 'Actions', 'color': '#e8f5e9', 'edge': '#388e3c',\n",
    "     'desc': 'What agent can do', 'example': '\"What can I do?\"'},\n",
    "    {'name': 'P', 'full': 'Transitions', 'color': '#fff3e0', 'edge': '#f57c00',\n",
    "     'desc': 'What happens next', 'example': '\"Where will I end up?\"'},\n",
    "    {'name': 'R', 'full': 'Rewards', 'color': '#fce4ec', 'edge': '#c2185b',\n",
    "     'desc': 'Feedback signal', 'example': '\"Was that good or bad?\"'},\n",
    "    {'name': 'Î³', 'full': 'Discount', 'color': '#f3e5f5', 'edge': '#7b1fa2',\n",
    "     'desc': 'Future importance', 'example': '\"How patient am I?\"'},\n",
    "]\n",
    "\n",
    "# Draw component boxes in a row\n",
    "start_x = 0.5\n",
    "box_width = 2.4\n",
    "box_height = 3\n",
    "spacing = 0.35\n",
    "\n",
    "for i, comp in enumerate(components):\n",
    "    x = start_x + i * (box_width + spacing)\n",
    "    y = 5.5\n",
    "    \n",
    "    # Draw box\n",
    "    box = FancyBboxPatch((x, y), box_width, box_height, \n",
    "                          boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=comp['color'], \n",
    "                          edgecolor=comp['edge'], linewidth=3)\n",
    "    ax.add_patch(box)\n",
    "    \n",
    "    # Symbol\n",
    "    ax.text(x + box_width/2, y + 2.5, comp['name'], ha='center', va='center',\n",
    "            fontsize=28, fontweight='bold', color=comp['edge'])\n",
    "    \n",
    "    # Full name\n",
    "    ax.text(x + box_width/2, y + 1.8, comp['full'], ha='center', va='center',\n",
    "            fontsize=12, fontweight='bold', color=comp['edge'])\n",
    "    \n",
    "    # Description\n",
    "    ax.text(x + box_width/2, y + 1.2, comp['desc'], ha='center', va='center',\n",
    "            fontsize=9, color='#555')\n",
    "    \n",
    "    # Example question\n",
    "    ax.text(x + box_width/2, y + 0.5, comp['example'], ha='center', va='center',\n",
    "            fontsize=9, color='#888', style='italic')\n",
    "\n",
    "# Add a concrete example at the bottom\n",
    "example_box = FancyBboxPatch((1, 1), 12, 3.5, boxstyle=\"round,pad=0.1\",\n",
    "                              facecolor='white', edgecolor='#333', linewidth=2)\n",
    "ax.add_patch(example_box)\n",
    "\n",
    "ax.text(7, 4, 'Example: Pac-Man Game', ha='center', fontsize=14, fontweight='bold', color='#333')\n",
    "ax.text(2.5, 3.2, 'S: Pac-Man & ghost positions', fontsize=10, color='#1976d2')\n",
    "ax.text(2.5, 2.6, 'A: UP, DOWN, LEFT, RIGHT', fontsize=10, color='#388e3c')\n",
    "ax.text(7.5, 3.2, 'P: Move in chosen direction', fontsize=10, color='#f57c00')\n",
    "ax.text(7.5, 2.6, 'R: +10 dot, +200 ghost, -500 die', fontsize=10, color='#c2185b')\n",
    "ax.text(11, 2.9, 'Î³ = 0.99', fontsize=10, color='#7b1fa2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MEMORY TRICK: Think 'SAPRGAMMA'\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nS - States:      WHERE am I? (all possible situations)\")\n",
    "print(\"A - Actions:     WHAT can I do? (all possible choices)\")\n",
    "print(\"P - Probability: WHERE will I end up? (transition function)\")\n",
    "print(\"R - Rewards:     HOW GOOD was that? (feedback signal)\")\n",
    "print(\"Î³ - Gamma:       HOW PATIENT am I? (discount factor)\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Markov Property: \"History Doesn't Matter\"\n",
    "\n",
    "The most important assumption in MDPs is the **Markov property**.\n",
    "\n",
    "### The GPS Analogy\n",
    "\n",
    "Imagine you're using GPS navigation:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                    GPS NAVIGATION                       â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Your GPS only needs to know:                          â”‚\n",
    "    â”‚    âœ“ Where you ARE (current location)                  â”‚\n",
    "    â”‚    âœ“ Where you want to GO (destination)                â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Your GPS does NOT need to know:                       â”‚\n",
    "    â”‚    âœ— Where you started from                            â”‚\n",
    "    â”‚    âœ— What route you took to get here                   â”‚\n",
    "    â”‚    âœ— How many wrong turns you made                     â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  The current location has ALL the information          â”‚\n",
    "    â”‚  needed to find the best route forward!                â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "This is exactly what the **Markov property** says:\n",
    "\n",
    "> **The future depends only on where you ARE, not how you GOT there.**\n",
    "\n",
    "### Chess Example\n",
    "\n",
    "In chess, the optimal move depends only on the current board position:\n",
    "- It doesn't matter if you opened with the Sicilian Defense or Queen's Gambit\n",
    "- It doesn't matter if you made mistakes earlier\n",
    "- All that matters is: where are the pieces RIGHT NOW?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Markov Property\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: NON-Markov (history matters)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 6)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('WITHOUT Markov Property\\n(History matters - complicated!)', \n",
    "              fontsize=14, color='#d32f2f', fontweight='bold')\n",
    "\n",
    "# Draw states with history\n",
    "states_left = [(2, 4.5, 'State A'), (5, 4.5, 'State B'), (8, 4.5, 'State C')]\n",
    "for x, y, label in states_left:\n",
    "    circle = Circle((x, y), 0.6, facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "    ax1.add_patch(circle)\n",
    "    ax1.text(x, y, label.split()[1], ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Arrows with history labels\n",
    "ax1.annotate('', xy=(4.3, 4.5), xytext=(2.7, 4.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f'))\n",
    "ax1.annotate('', xy=(7.3, 4.5), xytext=(5.7, 4.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f'))\n",
    "\n",
    "# Decision at C depends on path\n",
    "ax1.text(5, 2.5, 'To decide at C, need to know:', fontsize=11, ha='center')\n",
    "ax1.text(5, 2.0, 'â€¢ Did I come from Aâ†’Bâ†’C?', fontsize=10, ha='center', color='#666')\n",
    "ax1.text(5, 1.5, 'â€¢ Or from Dâ†’Bâ†’C?', fontsize=10, ha='center', color='#666')\n",
    "ax1.text(5, 1.0, 'â€¢ Or from Aâ†’Eâ†’Bâ†’C?', fontsize=10, ha='center', color='#666')\n",
    "ax1.text(5, 0.4, 'ğŸ˜° Must remember ENTIRE history!', fontsize=10, ha='center', color='#d32f2f')\n",
    "\n",
    "# Right: Markov (only current state matters)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 6)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('WITH Markov Property\\n(Only current state matters - simple!)', \n",
    "              fontsize=14, color='#388e3c', fontweight='bold')\n",
    "\n",
    "# Draw current state prominently\n",
    "circle = Circle((5, 4), 1.2, facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax2.add_patch(circle)\n",
    "ax2.text(5, 4, 'C', ha='center', va='center', fontsize=24, fontweight='bold')\n",
    "ax2.text(5, 2.5, 'Current\\nState', ha='center', fontsize=12, color='#388e3c')\n",
    "\n",
    "# Faded past\n",
    "for x, y, label, alpha in [(2, 5, 'A', 0.3), (2, 3, 'D', 0.3), (3.5, 4.5, 'B', 0.3)]:\n",
    "    circle = Circle((x, y), 0.5, facecolor='#e0e0e0', edgecolor='#9e9e9e', \n",
    "                     linewidth=1, alpha=alpha)\n",
    "    ax2.add_patch(circle)\n",
    "    ax2.text(x, y, label, ha='center', va='center', fontsize=10, alpha=alpha)\n",
    "\n",
    "ax2.text(2.5, 1.5, '(Past doesn\\'t matter)', fontsize=10, ha='center', color='#9e9e9e')\n",
    "\n",
    "# Decision only depends on C\n",
    "ax2.text(7.5, 4, 'Decision only\\ndepends on C!', fontsize=12, ha='center', color='#388e3c')\n",
    "ax2.text(7.5, 2.5, 'ğŸ˜Š Simple!', fontsize=14, ha='center', color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"THE MARKOV PROPERTY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n'The future is independent of the past, given the present.'\")\n",
    "print(\"\\nIn math: P(future | present, past) = P(future | present)\")\n",
    "print(\"\\nIn plain English: If I know where I AM, I don't need to know\")\n",
    "print(\"                  how I GOT here to make the best decision!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Let's Build a Grid World MDP!\n",
    "\n",
    "Now let's create a real MDP and explore its components. We'll build the same grid world from Notebook 1, but this time we'll look at it through the lens of an MDP.\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”\n",
    "    â”‚ R â”‚   â”‚   â”‚   â”‚   R = Robot starts here (0,0)\n",
    "    â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "    â”‚   â”‚   â”‚   â”‚   â”‚\n",
    "    â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "    â”‚   â”‚   â”‚   â”‚   â”‚\n",
    "    â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "    â”‚   â”‚   â”‚   â”‚ G â”‚   G = Goal (3,3)\n",
    "    â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\n",
    "    \n",
    "    S = {(0,0), (0,1), ..., (3,3)}  (16 states)\n",
    "    A = {UP, RIGHT, DOWN, LEFT}     (4 actions)\n",
    "    P = Deterministic movement       (action always succeeds)\n",
    "    R = -1 per step, +10 at goal\n",
    "    Î³ = 0.99\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "class GridWorldMDP:\n",
    "    \"\"\"\n",
    "    A 4x4 Grid World as an MDP.\n",
    "    \n",
    "    This is a complete MDP with all 5 components:\n",
    "    - S: 16 states (grid positions)\n",
    "    - A: 4 actions (UP, RIGHT, DOWN, LEFT)\n",
    "    - P: Deterministic transitions\n",
    "    - R: -1 per step, +10 at goal\n",
    "    - Î³: 0.99 (cares a lot about future)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # ========================================\n",
    "        # Component 1: STATE SPACE (S)\n",
    "        # ========================================\n",
    "        self.grid_size = 4\n",
    "        self.n_states = self.grid_size ** 2  # 16 states\n",
    "        self.states = [(r, c) for r in range(self.grid_size) \n",
    "                              for c in range(self.grid_size)]\n",
    "        \n",
    "        # ========================================\n",
    "        # Component 2: ACTION SPACE (A)\n",
    "        # ========================================\n",
    "        self.n_actions = 4\n",
    "        self.actions = [0, 1, 2, 3]  # UP, RIGHT, DOWN, LEFT\n",
    "        self.action_names = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
    "        self.action_symbols = ['â†‘', 'â†’', 'â†“', 'â†']\n",
    "        \n",
    "        # ========================================\n",
    "        # Component 5: DISCOUNT FACTOR (Î³)\n",
    "        # ========================================\n",
    "        self.gamma = 0.99  # Agent cares about future rewards\n",
    "        \n",
    "        # Special states\n",
    "        self.goal = (3, 3)  # Bottom-right corner\n",
    "        \n",
    "        # Build transition and reward functions\n",
    "        self._build_dynamics()\n",
    "    \n",
    "    def _build_dynamics(self):\n",
    "        \"\"\"\n",
    "        Build Components 3 and 4: Transitions (P) and Rewards (R)\n",
    "        \"\"\"\n",
    "        # ========================================\n",
    "        # Component 3: TRANSITION FUNCTION (P)\n",
    "        # P[state][action] = [(probability, next_state), ...]\n",
    "        # ========================================\n",
    "        self.P = {}\n",
    "        \n",
    "        # ========================================\n",
    "        # Component 4: REWARD FUNCTION (R)\n",
    "        # R[state][action] = reward\n",
    "        # ========================================\n",
    "        self.R = {}\n",
    "        \n",
    "        for state in self.states:\n",
    "            self.P[state] = {}\n",
    "            self.R[state] = {}\n",
    "            \n",
    "            for action in self.actions:\n",
    "                next_state = self._get_next_state(state, action)\n",
    "                reward = self._get_reward(state, action, next_state)\n",
    "                \n",
    "                # Deterministic: probability 1.0 for the next state\n",
    "                self.P[state][action] = [(1.0, next_state)]\n",
    "                self.R[state][action] = reward\n",
    "    \n",
    "    def _get_next_state(self, state, action):\n",
    "        \"\"\"Determine next state given current state and action.\"\"\"\n",
    "        row, col = state\n",
    "        \n",
    "        if action == 0:    # UP\n",
    "            row = max(0, row - 1)\n",
    "        elif action == 1:  # RIGHT\n",
    "            col = min(self.grid_size - 1, col + 1)\n",
    "        elif action == 2:  # DOWN\n",
    "            row = min(self.grid_size - 1, row + 1)\n",
    "        elif action == 3:  # LEFT\n",
    "            col = max(0, col - 1)\n",
    "        \n",
    "        return (row, col)\n",
    "    \n",
    "    def _get_reward(self, state, action, next_state):\n",
    "        \"\"\"Determine reward for a transition.\"\"\"\n",
    "        if next_state == self.goal:\n",
    "            return 10   # Big reward for reaching goal!\n",
    "        return -1       # Small penalty for each step\n",
    "    \n",
    "    def visualize_mdp(self):\n",
    "        \"\"\"Create a visual representation of the MDP.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # 1. State Space\n",
    "        ax1 = axes[0]\n",
    "        ax1.set_title('STATE SPACE (S)\\n16 grid positions', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for r in range(self.grid_size):\n",
    "            for c in range(self.grid_size):\n",
    "                color = '#c8e6c9' if (r, c) == self.goal else '#e3f2fd'\n",
    "                rect = plt.Rectangle((c, 3-r), 1, 1, facecolor=color, \n",
    "                                       edgecolor='black', linewidth=2)\n",
    "                ax1.add_patch(rect)\n",
    "                ax1.text(c+0.5, 3-r+0.5, f'({r},{c})', ha='center', va='center', fontsize=10)\n",
    "        \n",
    "        ax1.text(3.5, 0.5, 'GOAL', ha='center', va='center', fontsize=9, fontweight='bold', color='green')\n",
    "        ax1.set_xlim(0, 4)\n",
    "        ax1.set_ylim(0, 4)\n",
    "        ax1.set_aspect('equal')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # 2. Action Space\n",
    "        ax2 = axes[1]\n",
    "        ax2.set_title('ACTION SPACE (A)\\n4 movement directions', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlim(0, 4)\n",
    "        ax2.set_ylim(0, 4)\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        # Draw action arrows from center\n",
    "        center = (2, 2)\n",
    "        circle = plt.Circle(center, 0.3, facecolor='#bbdefb', edgecolor='#1976d2', linewidth=2)\n",
    "        ax2.add_patch(circle)\n",
    "        ax2.text(center[0], center[1], 'S', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        arrows = [\n",
    "            ((2, 2.4), (2, 3.2), 'â†‘ UP', '#4caf50'),\n",
    "            ((2.4, 2), (3.2, 2), 'â†’ RIGHT', '#2196f3'),\n",
    "            ((2, 1.6), (2, 0.8), 'â†“ DOWN', '#ff9800'),\n",
    "            ((1.6, 2), (0.8, 2), 'â† LEFT', '#9c27b0')\n",
    "        ]\n",
    "        \n",
    "        for (x1, y1), (x2, y2), label, color in arrows:\n",
    "            ax2.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                        arrowprops=dict(arrowstyle='->', lw=3, color=color))\n",
    "        \n",
    "        ax2.text(2, 3.6, 'UP (0)', ha='center', fontsize=10, color='#4caf50')\n",
    "        ax2.text(3.6, 2, 'RIGHT (1)', ha='center', fontsize=10, color='#2196f3')\n",
    "        ax2.text(2, 0.4, 'DOWN (2)', ha='center', fontsize=10, color='#ff9800')\n",
    "        ax2.text(0.4, 2, 'LEFT (3)', ha='center', fontsize=10, color='#9c27b0')\n",
    "        \n",
    "        # 3. Rewards\n",
    "        ax3 = axes[2]\n",
    "        ax3.set_title('REWARD FUNCTION (R)\\nFeedback for actions', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlim(0, 4)\n",
    "        ax3.set_ylim(0, 4)\n",
    "        ax3.axis('off')\n",
    "        \n",
    "        # Draw reward structure\n",
    "        ax3.text(2, 3.2, 'Each step: -1', fontsize=14, ha='center', color='#d32f2f',\n",
    "                bbox=dict(boxstyle='round', facecolor='#ffebee', edgecolor='#d32f2f'))\n",
    "        ax3.text(2, 2.2, '(Encourages efficiency!)', fontsize=10, ha='center', color='#666')\n",
    "        \n",
    "        ax3.text(2, 1.2, 'Reach goal: +10', fontsize=14, ha='center', color='#388e3c',\n",
    "                bbox=dict(boxstyle='round', facecolor='#e8f5e9', edgecolor='#388e3c'))\n",
    "        ax3.text(2, 0.5, f'Discount Î³ = {self.gamma}', fontsize=12, ha='center', color='#7b1fa2')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and visualize our MDP\n",
    "mdp = GridWorldMDP()\n",
    "\n",
    "print(\"ğŸ® GRID WORLD MDP\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nS (States):      {mdp.n_states} grid positions\")\n",
    "print(f\"A (Actions):     {mdp.n_actions} directions: {mdp.action_names}\")\n",
    "print(f\"P (Transitions): Deterministic (actions always succeed)\")\n",
    "print(f\"R (Rewards):     -1 per step, +10 at goal\")\n",
    "print(f\"Î³ (Gamma):       {mdp.gamma} (cares about future)\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "mdp.visualize_mdp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploring Transitions: What Happens When We Act?\n",
    "\n",
    "Let's look at the transition function in detail. We'll pick a state and see what happens for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_transitions_from_state(mdp, state):\n",
    "    \"\"\"Visualize all transitions from a given state.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.set_xlim(-0.5, 4.5)\n",
    "    ax.set_ylim(-0.5, 4.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Draw grid\n",
    "    for r in range(mdp.grid_size):\n",
    "        for c in range(mdp.grid_size):\n",
    "            if (r, c) == mdp.goal:\n",
    "                color = '#c8e6c9'\n",
    "            elif (r, c) == state:\n",
    "                color = '#bbdefb'\n",
    "            else:\n",
    "                color = 'white'\n",
    "            rect = plt.Rectangle((c, 3-r), 1, 1, facecolor=color, \n",
    "                                   edgecolor='black', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    # Mark goal\n",
    "    ax.text(3.5, 0.5, 'GOAL', ha='center', va='center', fontsize=10, \n",
    "            fontweight='bold', color='green')\n",
    "    \n",
    "    # Draw transitions from current state\n",
    "    row, col = state\n",
    "    center_x, center_y = col + 0.5, 3 - row + 0.5\n",
    "    \n",
    "    # Agent marker\n",
    "    ax.text(center_x, center_y, 'ğŸ¤–', ha='center', va='center', fontsize=24)\n",
    "    \n",
    "    # Draw arrows for each action\n",
    "    colors = ['#4caf50', '#2196f3', '#ff9800', '#9c27b0']\n",
    "    offsets = [(-0.3, 0), (0, 0.3), (0.3, 0), (0, -0.3)]  # For labeling\n",
    "    \n",
    "    for action in mdp.actions:\n",
    "        transitions = mdp.P[state][action]\n",
    "        reward = mdp.R[state][action]\n",
    "        \n",
    "        for prob, next_state in transitions:\n",
    "            next_row, next_col = next_state\n",
    "            next_x, next_y = next_col + 0.5, 3 - next_row + 0.5\n",
    "            \n",
    "            # Draw arrow\n",
    "            if next_state != state:  # Only draw if we actually move\n",
    "                ax.annotate('', xy=(next_x, next_y), xytext=(center_x, center_y),\n",
    "                           arrowprops=dict(arrowstyle='->', lw=3, color=colors[action]))\n",
    "    \n",
    "    # Title with transition info\n",
    "    ax.set_title(f'Transitions from State {state}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Legend\n",
    "    legend_y = -0.3\n",
    "    for action in mdp.actions:\n",
    "        next_state = mdp.P[state][action][0][1]\n",
    "        reward = mdp.R[state][action]\n",
    "        ax.text(action + 0.5, legend_y, \n",
    "                f\"{mdp.action_names[action]}\\nâ†’ {next_state}\\nR={reward:+d}\", \n",
    "                ha='center', va='top', fontsize=9, color=colors[action])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize transitions from state (1, 1)\n",
    "print(\"Let's see what happens when the agent is at position (1, 1):\")\n",
    "print(\"=\"*50)\n",
    "visualize_transitions_from_state(mdp, (1, 1))\n",
    "\n",
    "# Print detailed transition info\n",
    "state = (1, 1)\n",
    "print(f\"\\nDetailed transitions from state {state}:\")\n",
    "print(\"-\" * 50)\n",
    "for action in mdp.actions:\n",
    "    transitions = mdp.P[state][action]\n",
    "    reward = mdp.R[state][action]\n",
    "    for prob, next_state in transitions:\n",
    "        print(f\"  {mdp.action_names[action]:6s} â†’ {next_state}  \"\n",
    "              f\"(prob={prob:.0%}, reward={reward:+d})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stochastic Transitions: When Actions Don't Always Work\n",
    "\n",
    "In the real world, actions don't always succeed. Imagine a robot on a **slippery floor**:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                  SLIPPERY FLOOR MDP                    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Robot tries to go RIGHT:                              â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚    80% â†’ Actually goes RIGHT (success!)                â”‚\n",
    "    â”‚    10% â†’ Slips and goes UP (oops!)                     â”‚\n",
    "    â”‚    10% â†’ Slips and goes DOWN (oops!)                   â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  This is a STOCHASTIC transition!                      â”‚\n",
    "    â”‚  The outcome is partly random.                         â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Let's create a slippery grid world!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlipperyGridWorldMDP(GridWorldMDP):\n",
    "    \"\"\"\n",
    "    A slippery grid world where actions succeed 80% of the time.\n",
    "    With 10% probability each, the agent slips perpendicular.\n",
    "    \n",
    "    This demonstrates STOCHASTIC transitions!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, slip_prob=0.1):\n",
    "        self.slip_prob = slip_prob\n",
    "        self.success_prob = 1 - 2 * slip_prob\n",
    "        super().__init__()\n",
    "    \n",
    "    def _build_dynamics(self):\n",
    "        \"\"\"Build STOCHASTIC transition function.\"\"\"\n",
    "        self.P = {}\n",
    "        self.R = {}\n",
    "        \n",
    "        for state in self.states:\n",
    "            self.P[state] = {}\n",
    "            self.R[state] = {}\n",
    "            \n",
    "            for action in self.actions:\n",
    "                transitions = []\n",
    "                expected_reward = 0\n",
    "                \n",
    "                # Main action succeeds with probability (1 - 2*slip_prob)\n",
    "                main_next = self._get_next_state(state, action)\n",
    "                main_reward = self._get_reward(state, action, main_next)\n",
    "                transitions.append((self.success_prob, main_next))\n",
    "                expected_reward += self.success_prob * main_reward\n",
    "                \n",
    "                # Slip left (perpendicular)\n",
    "                left_action = (action - 1) % 4\n",
    "                left_next = self._get_next_state(state, left_action)\n",
    "                left_reward = self._get_reward(state, left_action, left_next)\n",
    "                transitions.append((self.slip_prob, left_next))\n",
    "                expected_reward += self.slip_prob * left_reward\n",
    "                \n",
    "                # Slip right (perpendicular)\n",
    "                right_action = (action + 1) % 4\n",
    "                right_next = self._get_next_state(state, right_action)\n",
    "                right_reward = self._get_reward(state, right_action, right_next)\n",
    "                transitions.append((self.slip_prob, right_next))\n",
    "                expected_reward += self.slip_prob * right_reward\n",
    "                \n",
    "                self.P[state][action] = transitions\n",
    "                self.R[state][action] = expected_reward\n",
    "\n",
    "# Create slippery MDP\n",
    "slippery_mdp = SlipperyGridWorldMDP(slip_prob=0.1)\n",
    "\n",
    "print(\"ğŸ§Š SLIPPERY GRID WORLD MDP\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nSlip probability: {slippery_mdp.slip_prob:.0%}\")\n",
    "print(f\"Success probability: {slippery_mdp.success_prob:.0%}\")\n",
    "\n",
    "# Compare deterministic vs stochastic\n",
    "state = (1, 1)\n",
    "action = 1  # RIGHT\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"Comparison: Taking action RIGHT from state {state}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nğŸ“¦ DETERMINISTIC (normal grid):\")\n",
    "for prob, next_state in mdp.P[state][action]:\n",
    "    print(f\"   â†’ {next_state} with {prob:.0%} probability\")\n",
    "\n",
    "print(f\"\\nğŸ§Š STOCHASTIC (slippery grid):\")\n",
    "for prob, next_state in slippery_mdp.P[state][action]:\n",
    "    print(f\"   â†’ {next_state} with {prob:.0%} probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "def draw_grid(ax, mdp, state, title, is_stochastic=False):\n",
    "    ax.set_xlim(-0.5, 4.5)\n",
    "    ax.set_ylim(-0.5, 4.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Draw grid\n",
    "    for r in range(mdp.grid_size):\n",
    "        for c in range(mdp.grid_size):\n",
    "            color = 'white'\n",
    "            if (r, c) == mdp.goal:\n",
    "                color = '#c8e6c9'\n",
    "            elif (r, c) == state:\n",
    "                color = '#bbdefb'\n",
    "            rect = plt.Rectangle((c, 3-r), 1, 1, facecolor=color, \n",
    "                                   edgecolor='black', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    row, col = state\n",
    "    cx, cy = col + 0.5, 3 - row + 0.5\n",
    "    ax.text(cx, cy, 'ğŸ¤–', ha='center', va='center', fontsize=20)\n",
    "    \n",
    "    # Draw transition for RIGHT action\n",
    "    action = 1  # RIGHT\n",
    "    for prob, next_state in mdp.P[state][action]:\n",
    "        next_row, next_col = next_state\n",
    "        nx, ny = next_col + 0.5, 3 - next_row + 0.5\n",
    "        \n",
    "        if next_state != state:\n",
    "            # Arrow thickness based on probability\n",
    "            lw = 2 + prob * 4\n",
    "            alpha = 0.5 + prob * 0.5\n",
    "            ax.annotate('', xy=(nx, ny), xytext=(cx, cy),\n",
    "                       arrowprops=dict(arrowstyle='->', lw=lw, \n",
    "                                      color='#2196f3', alpha=alpha))\n",
    "            ax.text(nx, ny - 0.3, f'{prob:.0%}', ha='center', fontsize=10, color='#2196f3')\n",
    "    \n",
    "    ax.text(2, 4.2, f'Action: RIGHT (â†’)', ha='center', fontsize=12, color='#2196f3')\n",
    "\n",
    "# Deterministic\n",
    "draw_grid(axes[0], mdp, (1, 1), 'ğŸ“¦ Deterministic: 100% success')\n",
    "\n",
    "# Stochastic\n",
    "draw_grid(axes[1], slippery_mdp, (1, 1), 'ğŸ§Š Stochastic: May slip!')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHT: Stochastic MDPs are harder to solve!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nIn deterministic MDPs:\")\n",
    "print(\"  - You know exactly what will happen\")\n",
    "print(\"  - Planning is easier\")\n",
    "print(\"\\nIn stochastic MDPs:\")\n",
    "print(\"  - Outcomes are uncertain\")\n",
    "print(\"  - Must plan for multiple possibilities\")\n",
    "print(\"  - More like the real world!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Modeling Real Problems as MDPs\n",
    "\n",
    "Any sequential decision-making problem can be modeled as an MDP. Let's practice with some examples!\n",
    "\n",
    "### The 5-Step MDP Design Process\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚               5 STEPS TO CREATE AN MDP                      â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                             â”‚\n",
    "    â”‚  1. STATES:  What information does the agent need?          â”‚\n",
    "    â”‚              (What defines the \"situation\"?)                â”‚\n",
    "    â”‚                                                             â”‚\n",
    "    â”‚  2. ACTIONS: What can the agent do?                         â”‚\n",
    "    â”‚              (What are the choices?)                        â”‚\n",
    "    â”‚                                                             â”‚\n",
    "    â”‚  3. TRANSITIONS: What happens after each action?            â”‚\n",
    "    â”‚              (Is it deterministic or random?)               â”‚\n",
    "    â”‚                                                             â”‚\n",
    "    â”‚  4. REWARDS: What behavior do you want?                     â”‚\n",
    "    â”‚              (How do you measure success?)                  â”‚\n",
    "    â”‚                                                             â”‚\n",
    "    â”‚  5. DISCOUNT: How patient should the agent be?              â”‚\n",
    "    â”‚              (Î³ close to 1 = patient, close to 0 = greedy)  â”‚\n",
    "    â”‚                                                             â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's design some real-world MDPs!\n",
    "\n",
    "real_world_mdps = {\n",
    "    \"ğŸ® Video Game (Pac-Man)\": {\n",
    "        \"States\": \"Position of Pac-Man, ghosts, remaining dots, power-up status\",\n",
    "        \"Actions\": \"UP, DOWN, LEFT, RIGHT\",\n",
    "        \"Transitions\": \"Mostly deterministic; ghosts move unpredictably\",\n",
    "        \"Rewards\": \"+10 eat dot, +200 eat ghost, -500 die, +1000 win\",\n",
    "        \"Discount\": \"0.99 (survive and score high)\"\n",
    "    },\n",
    "    \n",
    "    \"ğŸš— Self-Driving Car\": {\n",
    "        \"States\": \"Position, speed, camera images, sensor readings, traffic\",\n",
    "        \"Actions\": \"Accelerate, brake, steer left/right, change lane\",\n",
    "        \"Transitions\": \"Stochastic (traffic, weather, other drivers)\",\n",
    "        \"Rewards\": \"+1/sec safe driving, +100 reach destination, -1000 crash\",\n",
    "        \"Discount\": \"0.99 (care about reaching destination safely)\"\n",
    "    },\n",
    "    \n",
    "    \"ğŸ¤– Robot Arm\": {\n",
    "        \"States\": \"Joint angles, gripper state, object positions\",\n",
    "        \"Actions\": \"Move each joint, open/close gripper\",\n",
    "        \"Transitions\": \"Mostly deterministic (physics simulation)\",\n",
    "        \"Rewards\": \"+10 pick up object, +100 place correctly, -1 time\",\n",
    "        \"Discount\": \"0.95 (efficiency matters)\"\n",
    "    },\n",
    "    \n",
    "    \"ğŸ’¹ Stock Trading\": {\n",
    "        \"States\": \"Portfolio, cash, prices, market indicators\",\n",
    "        \"Actions\": \"Buy/sell/hold each stock, amount\",\n",
    "        \"Transitions\": \"Highly stochastic (market moves)\",\n",
    "        \"Rewards\": \"Portfolio value change (daily P&L)\",\n",
    "        \"Discount\": \"0.99 (long-term wealth building)\"\n",
    "    },\n",
    "    \n",
    "    \"ğŸ©º Medical Treatment\": {\n",
    "        \"States\": \"Patient vitals, test results, treatment history\",\n",
    "        \"Actions\": \"Prescribe medication, dosage, procedures\",\n",
    "        \"Transitions\": \"Stochastic (patient response varies)\",\n",
    "        \"Rewards\": \"+100 recovery, -10 side effects, -1000 adverse event\",\n",
    "        \"Discount\": \"0.95 (patient health is priority)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REAL-WORLD MDP EXAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, components in real_world_mdps.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * 60)\n",
    "    for comp, desc in components.items():\n",
    "        print(f\"  {comp:12s}: {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Interactive: Design Your Own MDP!\n",
    "\n",
    "Let's create a simple **thermostat MDP** together. A thermostat must decide when to turn heating on/off to maintain a comfortable temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermostatMDP:\n",
    "    \"\"\"\n",
    "    A simple thermostat MDP.\n",
    "    \n",
    "    The thermostat must keep the room at a comfortable temperature (20Â°C)\n",
    "    while minimizing energy use.\n",
    "    \n",
    "    States: Temperature in the room (15Â°C to 25Â°C)\n",
    "    Actions: HEAT_ON, HEAT_OFF\n",
    "    Transitions: Temperature changes based on action and outside temp\n",
    "    Rewards: Penalty for being far from 20Â°C, penalty for heating\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # State space: Temperatures from 15 to 25 (discretized)\n",
    "        self.states = list(range(15, 26))  # 15, 16, ..., 25\n",
    "        self.n_states = len(self.states)\n",
    "        \n",
    "        # Action space\n",
    "        self.actions = [0, 1]  # 0 = HEAT_OFF, 1 = HEAT_ON\n",
    "        self.action_names = ['HEAT_OFF', 'HEAT_ON']\n",
    "        \n",
    "        # Target temperature\n",
    "        self.target_temp = 20\n",
    "        self.outside_temp = 10  # Cold outside!\n",
    "        \n",
    "        # Discount factor\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        # Build MDP\n",
    "        self._build_dynamics()\n",
    "    \n",
    "    def _build_dynamics(self):\n",
    "        self.P = {}\n",
    "        self.R = {}\n",
    "        \n",
    "        for temp in self.states:\n",
    "            self.P[temp] = {}\n",
    "            self.R[temp] = {}\n",
    "            \n",
    "            for action in self.actions:\n",
    "                # Temperature change based on action\n",
    "                if action == 1:  # HEAT_ON\n",
    "                    # Room heats up\n",
    "                    change = 2 if temp < 25 else 0\n",
    "                else:  # HEAT_OFF\n",
    "                    # Room cools toward outside temp\n",
    "                    change = -1 if temp > self.outside_temp else 0\n",
    "                \n",
    "                next_temp = max(15, min(25, temp + change))\n",
    "                \n",
    "                # Stochastic: 80% intended, 20% stay same\n",
    "                self.P[temp][action] = [(0.8, next_temp), (0.2, temp)]\n",
    "                \n",
    "                # Reward: Negative of distance from target, minus heating cost\n",
    "                comfort_penalty = -abs(next_temp - self.target_temp)\n",
    "                heating_cost = -1 if action == 1 else 0\n",
    "                self.R[temp][action] = comfort_penalty + heating_cost\n",
    "    \n",
    "    def visualize(self):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Left: State-action transitions\n",
    "        ax1 = axes[0]\n",
    "        ax1.set_title('Thermostat State Space\\n(Temperature)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        temps = self.states\n",
    "        y_positions = range(len(temps))\n",
    "        colors = ['#ffcdd2' if t < 18 else '#c8e6c9' if 19 <= t <= 21 else '#fff9c4' \n",
    "                  for t in temps]\n",
    "        \n",
    "        bars = ax1.barh(y_positions, [1]*len(temps), color=colors, edgecolor='black')\n",
    "        ax1.set_yticks(y_positions)\n",
    "        ax1.set_yticklabels([f'{t}Â°C' for t in temps])\n",
    "        ax1.set_xlim(0, 1.5)\n",
    "        ax1.set_xlabel('')\n",
    "        ax1.axhline(5, color='green', linestyle='--', linewidth=2, label='Target: 20Â°C')\n",
    "        \n",
    "        # Add comfort zone annotation\n",
    "        ax1.text(1.1, 5, 'â† Comfortable!', fontsize=10, color='green', va='center')\n",
    "        ax1.text(1.1, 1, 'â† Too cold!', fontsize=10, color='red', va='center')\n",
    "        ax1.text(1.1, 9, 'â† Too hot!', fontsize=10, color='orange', va='center')\n",
    "        \n",
    "        ax1.set_xticks([])\n",
    "        \n",
    "        # Right: Reward structure\n",
    "        ax2 = axes[1]\n",
    "        ax2.set_title('Reward Structure', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Calculate rewards for each temperature\n",
    "        heat_off_rewards = [self.R[t][0] for t in temps]\n",
    "        heat_on_rewards = [self.R[t][1] for t in temps]\n",
    "        \n",
    "        x = np.arange(len(temps))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax2.bar(x - width/2, heat_off_rewards, width, label='HEAT_OFF', color='#90caf9')\n",
    "        ax2.bar(x + width/2, heat_on_rewards, width, label='HEAT_ON', color='#ffab91')\n",
    "        \n",
    "        ax2.set_xlabel('Temperature')\n",
    "        ax2.set_ylabel('Reward')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels([f'{t}Â°' for t in temps])\n",
    "        ax2.legend()\n",
    "        ax2.axvline(5, color='green', linestyle='--', linewidth=2, alpha=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and visualize\n",
    "thermostat = ThermostatMDP()\n",
    "\n",
    "print(\"ğŸŒ¡ï¸ THERMOSTAT MDP\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nStates: Room temperature ({min(thermostat.states)}Â°C to {max(thermostat.states)}Â°C)\")\n",
    "print(f\"Actions: {thermostat.action_names}\")\n",
    "print(f\"Target: {thermostat.target_temp}Â°C\")\n",
    "print(f\"Outside: {thermostat.outside_temp}Â°C (it's cold!)\")\n",
    "print(f\"Discount: {thermostat.gamma}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "thermostat.visualize()\n",
    "\n",
    "# Show example transitions\n",
    "print(\"\\nExample transitions from 18Â°C:\")\n",
    "for action in thermostat.actions:\n",
    "    print(f\"\\n  {thermostat.action_names[action]}:\")\n",
    "    for prob, next_temp in thermostat.P[18][action]:\n",
    "        reward = thermostat.R[18][action]\n",
    "        print(f\"    â†’ {next_temp}Â°C with {prob:.0%} probability, reward = {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "Let's recap what we learned about MDPs!\n",
    "\n",
    "### The 5 Components (S, A, P, R, Î³)\n",
    "\n",
    "| Component | Symbol | Question It Answers | Example (Pac-Man) |\n",
    "|-----------|--------|---------------------|-------------------|\n",
    "| **States** | S | Where am I? | Position on the maze |\n",
    "| **Actions** | A | What can I do? | UP, DOWN, LEFT, RIGHT |\n",
    "| **Transitions** | P | What happens? | Move in chosen direction |\n",
    "| **Rewards** | R | Was it good? | +10 for eating a dot |\n",
    "| **Discount** | Î³ | How patient? | 0.99 (care about future) |\n",
    "\n",
    "### The Markov Property\n",
    "\n",
    "- **\"The future depends only on the present, not the past\"**\n",
    "- Like GPS: only needs current location, not your travel history\n",
    "- Makes problems tractable!\n",
    "\n",
    "### Deterministic vs Stochastic\n",
    "\n",
    "- **Deterministic:** Actions always succeed (easier)\n",
    "- **Stochastic:** Actions might fail (more realistic)\n",
    "\n",
    "### Any Decision Problem â†’ MDP\n",
    "\n",
    "Follow the 5-step process to model any problem as an MDP!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What are the 5 components of an MDP?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "States (S), Actions (A), Transitions (P), Rewards (R), and Discount factor (Î³).\n",
    "</details>\n",
    "\n",
    "**2. What does the Markov property mean?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The future depends only on the current state, not on how we got there. All the information needed for optimal decisions is contained in the current state.\n",
    "</details>\n",
    "\n",
    "**3. What's the difference between deterministic and stochastic transitions?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Deterministic: The outcome of an action is certain (probability 1.0 for one next state).\n",
    "Stochastic: The outcome is uncertain (probability spread across multiple possible next states).\n",
    "</details>\n",
    "\n",
    "**4. In the slippery grid world, if slip probability is 10%, what's the success probability?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "80%. The agent can slip left (10%) or right (10%), so the intended action succeeds with 100% - 10% - 10% = 80%.\n",
    "</details>\n",
    "\n",
    "**5. Why is modeling a problem as an MDP useful?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Once a problem is formulated as an MDP, we can apply standard RL algorithms to find optimal policies. MDPs provide a mathematical framework that makes problems precise and solvable.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Great job! Now you understand the mathematical framework behind RL.\n",
    "\n",
    "In the next notebook, we'll dive into **Rewards and Returns** - how to measure the total success of a policy over time. We'll learn about:\n",
    "- Cumulative rewards\n",
    "- Discounting and why it matters\n",
    "- Expected returns\n",
    "\n",
    "**Continue to:** [Notebook 3: Rewards and Returns](03_rewards_and_returns.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*You now speak the language of MDPs! Every RL problem you encounter can be described using these 5 components.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
