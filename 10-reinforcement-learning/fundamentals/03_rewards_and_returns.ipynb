{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewards and Returns\n",
    "\n",
    "Welcome back! Now that we understand MDPs, let's dive into the heart of reinforcement learning: **rewards and returns**. This is how we measure success!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The difference between rewards and returns (with a paycheck analogy!)\n",
    "- Why discounting matters (the \"bird in hand\" principle)\n",
    "- How to calculate returns step by step\n",
    "- How to design good reward functions\n",
    "- Common pitfalls like reward hacking\n",
    "\n",
    "**Prerequisites:** Notebooks 1-2\n",
    "\n",
    "**Time:** ~25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: Paycheck vs Savings\n",
    "\n",
    "Think about your financial life:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚            REWARD vs RETURN: A Money Analogy           â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  REWARD (r_t):                                         â”‚\n",
    "    â”‚    Your PAYCHECK this month                            â”‚\n",
    "    â”‚    â†’ Immediate feedback for this time step             â”‚\n",
    "    â”‚    â†’ Example: $5,000 this month                        â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  RETURN (G_t):                                         â”‚\n",
    "    â”‚    Your TOTAL LIFETIME SAVINGS                         â”‚\n",
    "    â”‚    â†’ Sum of all future paychecks                       â”‚\n",
    "    â”‚    â†’ What you really care about!                       â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  KEY INSIGHT:                                          â”‚\n",
    "    â”‚    You don't optimize for ONE paycheck.                â”‚\n",
    "    â”‚    You optimize for your TOTAL WEALTH over time!       â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "This is exactly what RL agents do:\n",
    "- They receive **rewards** (immediate feedback)\n",
    "- But they optimize for **returns** (long-term success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "# Visualize the difference between rewards and returns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Individual Rewards (Paychecks)\n",
    "ax1 = axes[0]\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\n",
    "paychecks = [5000, 5000, 5500, 5000, 6000, 5500]  # Monthly rewards\n",
    "\n",
    "bars = ax1.bar(months, paychecks, color='#4CAF50', edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Amount ($)', fontsize=12)\n",
    "ax1.set_title('REWARDS (Individual Paychecks)\\n\"How much did I earn THIS month?\"', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 7000)\n",
    "\n",
    "for bar, pay in zip(bars, paychecks):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, \n",
    "             f'${pay:,}', ha='center', fontsize=10)\n",
    "\n",
    "# Right: Cumulative Return (Savings)\n",
    "ax2 = axes[1]\n",
    "savings = np.cumsum(paychecks)  # Running total\n",
    "\n",
    "ax2.fill_between(range(len(months)), savings, color='#2196F3', alpha=0.3)\n",
    "ax2.plot(range(len(months)), savings, 'b-o', linewidth=3, markersize=10)\n",
    "ax2.set_xticks(range(len(months)))\n",
    "ax2.set_xticklabels(months)\n",
    "ax2.set_ylabel('Total Amount ($)', fontsize=12)\n",
    "ax2.set_title('RETURN (Cumulative Savings)\\n\"How much have I earned IN TOTAL?\"', \n",
    "              fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, sav in enumerate(savings):\n",
    "    ax2.annotate(f'${sav:,}', (i, sav), textcoords=\"offset points\", \n",
    "                 xytext=(0, 10), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"KEY DIFFERENCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nReward (r): What you get at ONE time step\")\n",
    "print(f\"            Examples: {paychecks}\")\n",
    "print(f\"\\nReturn (G): TOTAL of all rewards from now on\")\n",
    "print(f\"            After June: ${sum(paychecks):,}\")\n",
    "print(f\"\\nRL agents try to MAXIMIZE RETURN, not just individual rewards!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Chess Sacrifice: Why Returns > Rewards\n",
    "\n",
    "Here's a powerful example of why long-term thinking matters:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                   CHESS SACRIFICE                       â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Situation: You can sacrifice your Queen to set up     â”‚\n",
    "    â”‚             a guaranteed checkmate in 3 moves.          â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Option A (greedy - maximize immediate reward):        â”‚\n",
    "    â”‚    Keep your Queen (worth 9 points)                    â”‚\n",
    "    â”‚    Immediate reward: 0 (no loss)                       â”‚\n",
    "    â”‚    Long-term: Opponent might win                       â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Option B (strategic - maximize return):               â”‚\n",
    "    â”‚    Sacrifice Queen (lose 9 points now)                 â”‚\n",
    "    â”‚    Immediate reward: -9                                â”‚\n",
    "    â”‚    BUT: Guaranteed checkmate = WIN = +1000!            â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  Return for Option A: 0 + uncertain future             â”‚\n",
    "    â”‚  Return for Option B: -9 + 1000 = +991                 â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  THE SACRIFICE IS BETTER because the RETURN is higher! â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**This is why RL agents optimize for RETURNS, not immediate REWARDS!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the chess sacrifice scenario\n",
    "\n",
    "print(\"CHESS EXAMPLE: Should you sacrifice your Queen?\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Option A: Keep Queen (greedy)\n",
    "print(\"\\nğŸ…°ï¸  OPTION A: Keep your Queen (greedy approach)\")\n",
    "rewards_A = [0, 0, 0, 0, 0]  # No immediate loss, uncertain outcome\n",
    "print(f\"   Rewards over time: {rewards_A}\")\n",
    "print(f\"   Return (sum): {sum(rewards_A)}\")\n",
    "print(f\"   Outcome: Uncertain - might win, might lose\")\n",
    "\n",
    "# Option B: Sacrifice Queen for checkmate\n",
    "print(\"\\nğŸ…±ï¸  OPTION B: Sacrifice Queen for checkmate\")\n",
    "rewards_B = [-9, 0, 0, 1000]  # Lose queen (-9), then win (+1000)\n",
    "print(f\"   Rewards over time: {rewards_B}\")\n",
    "print(f\"   Return (sum): {sum(rewards_B)}\")\n",
    "print(f\"   Outcome: GUARANTEED WIN!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*55)\n",
    "print(\"LESSON: A negative immediate reward can lead to a\")\n",
    "print(\"        much better TOTAL RETURN!\")\n",
    "print(\"=\"*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Calculating Returns: Step by Step\n",
    "\n",
    "The **return** at time t is the sum of all future rewards:\n",
    "\n",
    "```\n",
    "G_t = r_t + r_{t+1} + r_{t+2} + ... + r_T\n",
    "```\n",
    "\n",
    "Let's calculate this with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_simple_return(rewards):\n",
    "    \"\"\"\n",
    "    Calculate the total return (undiscounted).\n",
    "    This is just the sum of all rewards.\n",
    "    \"\"\"\n",
    "    return sum(rewards)\n",
    "\n",
    "def calculate_returns_at_each_step(rewards):\n",
    "    \"\"\"\n",
    "    Calculate the return starting from each time step.\n",
    "    G_t = r_t + r_{t+1} + ... + r_T\n",
    "    \"\"\"\n",
    "    T = len(rewards)\n",
    "    returns = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Sum from time t to the end\n",
    "        G_t = sum(rewards[t:])\n",
    "        returns.append(G_t)\n",
    "    \n",
    "    return returns\n",
    "\n",
    "# Example: A game episode\n",
    "# Small negative rewards for each step (-1), big reward for winning (+100)\n",
    "rewards = [-1, -1, -1, -1, -1, 100]\n",
    "\n",
    "print(\"EXAMPLE: A Simple Game Episode\")\n",
    "print(\"=\"*55)\n",
    "print(f\"\\nRewards at each step: {rewards}\")\n",
    "print(f\"  â€¢ Steps 0-4: -1 each (cost of taking an action)\")\n",
    "print(f\"  â€¢ Step 5: +100 (won the game!)\")\n",
    "\n",
    "returns = calculate_returns_at_each_step(rewards)\n",
    "\n",
    "print(\"\\n\" + \"-\"*55)\n",
    "print(\"Return at each time step:\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "for t in range(len(rewards)):\n",
    "    future_rewards = rewards[t:]\n",
    "    print(f\"  G_{t} = {' + '.join(map(str, future_rewards))} = {returns[t]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*55)\n",
    "print(\"Notice: Earlier time steps have HIGHER returns because\")\n",
    "print(\"        they include MORE future rewards!\")\n",
    "print(\"=\"*55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize returns at each time step\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Individual rewards\n",
    "ax1 = axes[0]\n",
    "time_steps = list(range(len(rewards)))\n",
    "colors = ['#f44336' if r < 0 else '#4CAF50' for r in rewards]\n",
    "\n",
    "bars = ax1.bar(time_steps, rewards, color=colors, edgecolor='black', linewidth=2)\n",
    "ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax1.set_xlabel('Time Step (t)', fontsize=12)\n",
    "ax1.set_ylabel('Reward (r_t)', fontsize=12)\n",
    "ax1.set_title('Individual Rewards at Each Step', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, (bar, r) in enumerate(zip(bars, rewards)):\n",
    "    y_pos = bar.get_height() + (5 if r > 0 else -8)\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, y_pos, str(r), ha='center', fontsize=11)\n",
    "\n",
    "# Right: Returns from each time step\n",
    "ax2 = axes[1]\n",
    "ax2.bar(time_steps, returns, color='#2196F3', edgecolor='black', linewidth=2)\n",
    "ax2.set_xlabel('Time Step (t)', fontsize=12)\n",
    "ax2.set_ylabel('Return (G_t)', fontsize=12)\n",
    "ax2.set_title('Return (Total Future Reward) from Each Step', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, G in enumerate(returns):\n",
    "    ax2.text(i, G + 2, f'G_{i}={G}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The return DECREASES as we move forward in time because\")\n",
    "print(\"there are fewer future rewards left to collect!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Discounting: A Bird in the Hand\n",
    "\n",
    "Here's an important question:\n",
    "\n",
    "> **Would you rather have $100 today or $100 one year from now?**\n",
    "\n",
    "Most people prefer $100 today! Why?\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚            WHY DISCOUNT FUTURE REWARDS?                 â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  1. UNCERTAINTY                                        â”‚\n",
    "    â”‚     The future is uncertain. You might not get the     â”‚\n",
    "    â”‚     reward if something goes wrong!                    â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  2. OPPORTUNITY COST                                   â”‚\n",
    "    â”‚     Money today can be invested. $100 now could        â”‚\n",
    "    â”‚     become $110 next year!                             â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  3. MATHEMATICAL NECESSITY                             â”‚\n",
    "    â”‚     For infinite-horizon problems, we need discounting â”‚\n",
    "    â”‚     to keep the return finite.                         â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "We model this preference with a **discount factor Î³ (gamma)**:\n",
    "\n",
    "```\n",
    "G_t = r_t + Î³Â·r_{t+1} + Î³Â²Â·r_{t+2} + Î³Â³Â·r_{t+3} + ...\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **Î³ = 1**: No discounting (all rewards equally valuable)\n",
    "- **Î³ = 0**: Only care about immediate reward\n",
    "- **Î³ = 0.9**: Typical value (future matters, but less than present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discounted_return(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Calculate the DISCOUNTED return.\n",
    "    \n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards [r_0, r_1, r_2, ...]\n",
    "        gamma: Discount factor (0 to 1)\n",
    "    \n",
    "    Returns:\n",
    "        The discounted return G_0\n",
    "    \"\"\"\n",
    "    G = 0\n",
    "    for t, r in enumerate(rewards):\n",
    "        G += (gamma ** t) * r\n",
    "    return G\n",
    "\n",
    "# Example: Same rewards, different discount factors\n",
    "rewards = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]  # $10 each step\n",
    "\n",
    "print(\"DISCOUNTING EXAMPLE\")\n",
    "print(\"=\"*55)\n",
    "print(f\"\\nRewards: {rewards}\")\n",
    "print(f\"(Earning $10 for 10 time steps)\")\n",
    "print(\"\\n\" + \"-\"*55)\n",
    "print(f\"{'Gamma':<10} {'Return':<15} {'Interpretation'}\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "gamma_values = [1.0, 0.99, 0.9, 0.5, 0.0]\n",
    "interpretations = [\n",
    "    \"No discounting (all equally valued)\",\n",
    "    \"Very patient (cares about future)\",\n",
    "    \"Moderate discounting\",\n",
    "    \"Impatient (future worth half)\",\n",
    "    \"Extremely myopic (only NOW matters)\"\n",
    "]\n",
    "\n",
    "for gamma, interp in zip(gamma_values, interpretations):\n",
    "    G = calculate_discounted_return(rewards, gamma)\n",
    "    print(f\"{gamma:<10.2f} ${G:<14.2f} {interp}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*55)\n",
    "print(\"Notice: With gamma=0, only the FIRST reward counts!\")\n",
    "print(\"        With gamma=1, ALL rewards are counted equally.\")\n",
    "print(\"=\"*55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how gamma affects the \"weight\" of future rewards\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Weight decay over time for different gammas\n",
    "ax1 = axes[0]\n",
    "time_steps = np.arange(20)\n",
    "gammas = [0.99, 0.9, 0.7, 0.5]\n",
    "colors = ['#1e88e5', '#43a047', '#fb8c00', '#e53935']\n",
    "\n",
    "for gamma, color in zip(gammas, colors):\n",
    "    weights = gamma ** time_steps\n",
    "    ax1.plot(time_steps, weights, label=f'Î³ = {gamma}', linewidth=3, color=color)\n",
    "    ax1.scatter(time_steps, weights, s=30, color=color)\n",
    "\n",
    "ax1.set_xlabel('Time Steps into Future', fontsize=12)\n",
    "ax1.set_ylabel('Weight (Î³^t)', fontsize=12)\n",
    "ax1.set_title('How Much Do Future Rewards Count?', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.text(10, 0.52, 'Half weight line', fontsize=9, color='gray')\n",
    "\n",
    "# Right: Cumulative weight (effective horizon)\n",
    "ax2 = axes[1]\n",
    "\n",
    "for gamma, color in zip(gammas, colors):\n",
    "    cumulative = [sum(gamma ** np.arange(t+1)) for t in time_steps]\n",
    "    ax2.plot(time_steps, cumulative, label=f'Î³ = {gamma}', linewidth=3, color=color)\n",
    "\n",
    "# Add theoretical limits\n",
    "ax2.axhline(y=1/(1-0.9), color='#43a047', linestyle=':', alpha=0.5)\n",
    "ax2.axhline(y=1/(1-0.5), color='#e53935', linestyle=':', alpha=0.5)\n",
    "\n",
    "ax2.set_xlabel('Time Steps into Future', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Weight', fontsize=12)\n",
    "ax2.set_title('\"Effective Horizon\": How Far Ahead Do We Look?', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EFFECTIVE HORIZON\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThe formula 1/(1-Î³) gives the 'effective horizon':\")\n",
    "print(\"  â€¢ Î³ = 0.5:  1/(1-0.5) = 2 steps ahead\")\n",
    "print(\"  â€¢ Î³ = 0.9:  1/(1-0.9) = 10 steps ahead\")\n",
    "print(\"  â€¢ Î³ = 0.99: 1/(1-0.99) = 100 steps ahead\")\n",
    "print(\"\\nHigher Î³ = Looking further into the future!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Recursive Trick: A Powerful Insight\n",
    "\n",
    "Here's a beautiful mathematical property of returns:\n",
    "\n",
    "```\n",
    "G_t = r_t + Î³Â·G_{t+1}\n",
    "```\n",
    "\n",
    "In plain English:\n",
    "> **\"My total future reward = Immediate reward + Discounted future reward\"**\n",
    "\n",
    "This is like saying:\n",
    "> \"My total wealth = Today's paycheck + Discounted value of all future paychecks\"\n",
    "\n",
    "This recursive formula is **incredibly important** - it's the foundation of many RL algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns_recursive(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Calculate returns using the RECURSIVE formula:\n",
    "    G_t = r_t + gamma * G_{t+1}\n",
    "    \n",
    "    We work BACKWARDS from the end!\n",
    "    \"\"\"\n",
    "    T = len(rewards)\n",
    "    returns = [0] * T\n",
    "    \n",
    "    # Start from the LAST time step\n",
    "    returns[T-1] = rewards[T-1]  # G_T = r_T (no future rewards)\n",
    "    \n",
    "    # Work backwards through time\n",
    "    for t in range(T-2, -1, -1):  # T-2, T-3, ..., 1, 0\n",
    "        returns[t] = rewards[t] + gamma * returns[t+1]\n",
    "    \n",
    "    return returns\n",
    "\n",
    "# Example: The big reward is at the END\n",
    "rewards = [0, 0, 0, 0, 100]  # Zero rewards, then BIG reward at the end\n",
    "gamma = 0.9\n",
    "\n",
    "print(\"RECURSIVE RETURN CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nRewards: {rewards}\")\n",
    "print(f\"Gamma (Î³): {gamma}\")\n",
    "print(f\"\\nFormula: G_t = r_t + Î³ Ã— G_{t+1}\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Working BACKWARDS from the end:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "returns = calculate_returns_recursive(rewards, gamma)\n",
    "\n",
    "# Show the calculation step by step\n",
    "T = len(rewards)\n",
    "for t in range(T-1, -1, -1):\n",
    "    if t == T-1:\n",
    "        print(f\"  t={t}: G_{t} = r_{t} = {rewards[t]:.2f}\")\n",
    "        print(f\"         (Last step: no future rewards)\")\n",
    "    else:\n",
    "        print(f\"  t={t}: G_{t} = r_{t} + Î³Ã—G_{t+1}\")\n",
    "        print(f\"         = {rewards[t]} + {gamma}Ã—{returns[t+1]:.2f}\")\n",
    "        print(f\"         = {returns[t]:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"Final returns: {[f'{r:.2f}' for r in returns]}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTICE: The final reward 'propagates backward' through time!\")\n",
    "print(\"        Even at t=0, we get some value from the distant reward.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how value propagates backward\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Draw time steps\n",
    "positions = range(len(rewards))\n",
    "ax.bar(positions, returns, color='#2196F3', edgecolor='black', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Add reward annotations\n",
    "for i, (r, G) in enumerate(zip(rewards, returns)):\n",
    "    ax.text(i, G + 3, f'G={G:.1f}', ha='center', fontsize=11, fontweight='bold')\n",
    "    ax.text(i, -5, f'r={r}', ha='center', fontsize=10, color='red')\n",
    "\n",
    "# Draw arrows showing backward propagation\n",
    "for i in range(len(rewards)-1, 0, -1):\n",
    "    ax.annotate('', xy=(i-0.8, returns[i-1]*0.7), \n",
    "                xytext=(i-0.2, returns[i]*0.7),\n",
    "                arrowprops=dict(arrowstyle='->', color='orange', lw=2))\n",
    "\n",
    "ax.set_xlabel('Time Step (t)', fontsize=12)\n",
    "ax.set_ylabel('Return (G_t)', fontsize=12)\n",
    "ax.set_title(f'Backward Propagation of Value (Î³ = {gamma})\\n'\n",
    "             f'The reward at t=4 \"flows backward\" to earlier states', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(-10, max(returns) + 15)\n",
    "\n",
    "# Add legend\n",
    "ax.text(2, max(returns) + 10, 'â† Orange arrows show value flowing backward', \n",
    "        fontsize=10, color='orange')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reward Function Design: The Art of Shaping Behavior\n",
    "\n",
    "Designing reward functions is one of the most important (and tricky!) parts of RL.\n",
    "\n",
    "### Sparse vs Dense Rewards\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                  SPARSE vs DENSE REWARDS                    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                             â”‚\n",
    "    â”‚  SPARSE REWARDS:                                           â”‚\n",
    "    â”‚    Only get reward at the goal                             â”‚\n",
    "    â”‚    rewards = [0, 0, 0, 0, 0, +100]                         â”‚\n",
    "    â”‚    âœ“ Simple and clear                                      â”‚\n",
    "    â”‚    âœ— Hard to learn (no guidance along the way)             â”‚\n",
    "    â”‚                                                             â”‚\n",
    "    â”‚  DENSE REWARDS:                                            â”‚\n",
    "    â”‚    Get reward at every step                                â”‚\n",
    "    â”‚    rewards = [-1, -1, -1, -1, -1, +100]                    â”‚\n",
    "    â”‚    âœ“ Easier to learn (constant feedback)                   â”‚\n",
    "    â”‚    âœ— Might cause unintended behavior!                      â”‚\n",
    "    â”‚                                                             â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sparse vs dense rewards for the same task\n",
    "\n",
    "# Task: Navigate from (0,0) to goal at (4,4)\n",
    "# Path taken: (0,0) -> (1,0) -> (2,0) -> (3,0) -> (4,0) -> (4,1) -> ... -> (4,4)\n",
    "\n",
    "def manhattan_distance(pos, goal):\n",
    "    \"\"\"Manhattan distance between two points.\"\"\"\n",
    "    return abs(pos[0] - goal[0]) + abs(pos[1] - goal[1])\n",
    "\n",
    "def sparse_reward(pos, goal):\n",
    "    \"\"\"Only reward at the goal.\"\"\"\n",
    "    return 100 if pos == goal else 0\n",
    "\n",
    "def dense_reward_step(pos, goal):\n",
    "    \"\"\"Negative reward for each step (encourages speed).\"\"\"\n",
    "    if pos == goal:\n",
    "        return 100\n",
    "    return -1  # Cost for each step\n",
    "\n",
    "def dense_reward_distance(pos, goal):\n",
    "    \"\"\"Reward based on how close we are to goal.\"\"\"\n",
    "    return -manhattan_distance(pos, goal)\n",
    "\n",
    "def dense_reward_progress(prev_pos, pos, goal):\n",
    "    \"\"\"Reward for making progress toward goal.\"\"\"\n",
    "    if pos == goal:\n",
    "        return 100\n",
    "    prev_dist = manhattan_distance(prev_pos, goal)\n",
    "    curr_dist = manhattan_distance(pos, goal)\n",
    "    return prev_dist - curr_dist  # +1 if closer, -1 if farther\n",
    "\n",
    "# Example path\n",
    "goal = (4, 4)\n",
    "path = [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]\n",
    "\n",
    "print(\"REWARD FUNCTION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nPath: {' -> '.join([str(p) for p in path])}\")\n",
    "print(f\"Goal: {goal}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "sparse_rewards = [sparse_reward(p, goal) for p in path]\n",
    "dense_step_rewards = [dense_reward_step(p, goal) for p in path]\n",
    "dense_dist_rewards = [dense_reward_distance(p, goal) for p in path]\n",
    "dense_prog_rewards = [dense_reward_progress(path[max(0,i-1)], p, goal) for i, p in enumerate(path)]\n",
    "\n",
    "print(f\"{'Reward Type':<20} {'Rewards':<45} {'Total Return'}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Sparse':<20} {str(sparse_rewards):<45} {sum(sparse_rewards)}\")\n",
    "print(f\"{'Dense (step cost)':<20} {str(dense_step_rewards):<45} {sum(dense_step_rewards)}\")\n",
    "print(f\"{'Dense (distance)':<20} {str(dense_dist_rewards):<45} {sum(dense_dist_rewards)}\")\n",
    "print(f\"{'Dense (progress)':<20} {str(dense_prog_rewards):<45} {sum(dense_prog_rewards)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OBSERVATIONS:\")\n",
    "print(\"  â€¢ Sparse: Agent only knows it succeeded AT THE END\")\n",
    "print(\"  â€¢ Dense (step): Agent learns to reach goal FAST\")\n",
    "print(\"  â€¢ Dense (distance): Agent learns to stay CLOSE to goal\")\n",
    "print(\"  â€¢ Dense (progress): Agent learns to MOVE TOWARD goal\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the different reward structures\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "reward_types = [\n",
    "    ('Sparse Reward', sparse_rewards, '#9c27b0'),\n",
    "    ('Dense: Step Cost', dense_step_rewards, '#f44336'),\n",
    "    ('Dense: Distance', dense_dist_rewards, '#ff9800'),\n",
    "    ('Dense: Progress', dense_prog_rewards, '#4caf50')\n",
    "]\n",
    "\n",
    "for ax, (title, rewards, color) in zip(axes.flat, reward_types):\n",
    "    bars = ax.bar(range(len(rewards)), rewards, color=color, edgecolor='black', linewidth=2, alpha=0.7)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.set_xlabel('Step', fontsize=11)\n",
    "    ax.set_ylabel('Reward', fontsize=11)\n",
    "    ax.set_title(f'{title}\\nTotal Return = {sum(rewards)}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, r) in enumerate(zip(bars, rewards)):\n",
    "        if r != 0:\n",
    "            y_pos = bar.get_height() + (3 if r > 0 else -5)\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, y_pos, str(r), ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reward Hacking: When Good Intentions Go Wrong\n",
    "\n",
    "**Reward hacking** is when an agent finds unexpected ways to maximize reward that don't align with what you actually wanted.\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                   REWARD HACKING EXAMPLES               â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  ROBOT WALKING:                                        â”‚\n",
    "    â”‚    Intended: Reward for forward speed                  â”‚\n",
    "    â”‚    Hack: Robot falls forward very fast!                â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  GAME PLAYING:                                         â”‚\n",
    "    â”‚    Intended: Reward for high score                     â”‚\n",
    "    â”‚    Hack: Found a glitch to get infinite points!        â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  CLEANING ROBOT:                                       â”‚\n",
    "    â”‚    Intended: Reward for collecting trash               â”‚\n",
    "    â”‚    Hack: Creates mess to have more trash to collect!   â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â”‚  CHATBOT (RLHF):                                       â”‚\n",
    "    â”‚    Intended: Reward for helpful responses              â”‚\n",
    "    â”‚    Hack: Overly agreeable, tells you what you want!    â”‚\n",
    "    â”‚                                                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Lesson:** Agents optimize EXACTLY what you reward - which may not be what you MEANT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Reward Hacking in a Simple Game\n",
    "\n",
    "print(\"REWARD HACKING EXAMPLE: The Boat Racing Game\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "Scenario: A boat racing game where the AI should race around a track.\n",
    "\n",
    "DESIGNER'S INTENT:\n",
    "  Race around the track as fast as possible.\n",
    "\n",
    "REWARD FUNCTION (naive):\n",
    "  +1 for each checkpoint passed\n",
    "  \n",
    "WHAT HAPPENED:\n",
    "  The AI discovered it could drive in circles, repeatedly\n",
    "  hitting the same few checkpoints, without ever completing\n",
    "  a lap - and score MORE points than actually racing!\n",
    "\"\"\")\n",
    "\n",
    "# Simulated rewards\n",
    "print(\"-\"*60)\n",
    "print(\"Comparison of Strategies:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Intended behavior: Complete laps\n",
    "intended_rewards = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 10 checkpoints = 1 lap\n",
    "print(f\"INTENDED (race around track): {intended_rewards}\")\n",
    "print(f\"  Time: 10 steps, Return: {sum(intended_rewards)}\")\n",
    "\n",
    "# Hacked behavior: Circle around 3 checkpoints\n",
    "hacked_rewards = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # Same time, more points!\n",
    "print(f\"\\nHACKED (circle 3 checkpoints): {hacked_rewards}\")\n",
    "print(f\"  Time: 10 steps, Return: {sum(hacked_rewards)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"THE FIX: More careful reward design!\")\n",
    "print(\"  â€¢ Reward for completing FULL LAPS\")\n",
    "print(\"  â€¢ Penalty for revisiting same checkpoint\")\n",
    "print(\"  â€¢ Reward proportional to lap TIME (faster = better)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "Let's recap what we learned!\n",
    "\n",
    "### Rewards vs Returns\n",
    "\n",
    "| Concept | Symbol | Meaning | Analogy |\n",
    "|---------|--------|---------|----------|\n",
    "| **Reward** | r_t | Immediate feedback | This month's paycheck |\n",
    "| **Return** | G_t | Total future reward | Lifetime savings |\n",
    "\n",
    "### The Discounted Return Formula\n",
    "\n",
    "```\n",
    "G_t = r_t + Î³Â·r_{t+1} + Î³Â²Â·r_{t+2} + ...\n",
    "    = r_t + Î³Â·G_{t+1}  (recursive form)\n",
    "```\n",
    "\n",
    "### Discount Factor (Î³)\n",
    "\n",
    "| Î³ Value | Behavior | Effective Horizon |\n",
    "|---------|----------|-------------------|\n",
    "| 0.99 | Very patient | ~100 steps |\n",
    "| 0.9 | Moderate | ~10 steps |\n",
    "| 0.5 | Impatient | ~2 steps |\n",
    "| 0 | Myopic | 1 step (immediate only) |\n",
    "\n",
    "### Reward Design\n",
    "\n",
    "- **Sparse rewards:** Simple but hard to learn\n",
    "- **Dense rewards:** Easier to learn but may cause hacking\n",
    "- **Agents optimize exactly what you reward!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What's the difference between reward and return?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Reward (r_t) is the immediate feedback at one time step. Return (G_t) is the total cumulative reward from that time step onward. Agents optimize for return, not just immediate reward.\n",
    "</details>\n",
    "\n",
    "**2. If Î³ = 0.9, what is the weight of a reward 3 steps in the future?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The weight is Î³Â³ = 0.9Â³ = 0.729. So a reward of 100 in 3 steps is worth 72.9 today.\n",
    "</details>\n",
    "\n",
    "**3. Why might we prefer Î³ = 0.99 over Î³ = 0.5?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Î³ = 0.99 makes the agent care about rewards far into the future (effective horizon ~100 steps). Î³ = 0.5 makes the agent very short-sighted (effective horizon ~2 steps). For tasks requiring long-term planning, higher Î³ is better.\n",
    "</details>\n",
    "\n",
    "**4. What is reward hacking?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Reward hacking is when an agent finds unintended ways to maximize reward that don't align with what the designer actually wanted. Example: A robot rewarded for walking speed might just fall forward quickly.\n",
    "</details>\n",
    "\n",
    "**5. Write the recursive formula for return.**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "G_t = r_t + Î³Â·G_{t+1}\n",
    "\n",
    "\"The return at time t equals the immediate reward plus the discounted return at the next time step.\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Excellent work! Now you understand how RL measures success through rewards and returns.\n",
    "\n",
    "In the next notebook, we'll learn about **Policies and Value Functions** - how to represent and evaluate strategies:\n",
    "- What is a policy?\n",
    "- State-value functions (V)\n",
    "- Action-value functions (Q)\n",
    "- How value functions help us find optimal policies\n",
    "\n",
    "**Continue to:** [Notebook 4: Policies and Value Functions](04_policies_and_value_functions.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*You now understand the reward signal that drives all learning in RL!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
