{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equations: The Foundation of All RL Algorithms\n",
    "\n",
    "Welcome to the most important mathematical concept in RL! The Bellman equations are the foundation upon which ALL reinforcement learning algorithms are built.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The recursive nature of value (with a treasure map analogy!)\n",
    "- Bellman expectation equations for V and Q\n",
    "- Bellman optimality equations (finding the best!)\n",
    "- Policy evaluation: computing values for a given policy\n",
    "- Value iteration: finding optimal values directly\n",
    "- Why these equations matter for RL\n",
    "\n",
    "**Prerequisites:** Notebooks 1-4 (especially policies and value functions)\n",
    "\n",
    "**Time:** ~35 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Treasure Map Analogy\n",
    "\n",
    "Imagine you're a pirate with a treasure map:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE BELLMAN EQUATION: A TREASURE MAP ANALOGY          │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  You're at Island A, trying to reach the treasure at Island X │\n",
    "    │                                                                │\n",
    "    │       (A) ──$10──> (B) ──$20──> (C) ──$100──> (X) TREASURE!   │\n",
    "    │                                                                │\n",
    "    │  Question: What's the value of being at Island A?             │\n",
    "    │                                                                │\n",
    "    │  NAIVE APPROACH:                                               │\n",
    "    │    Calculate the entire path: $10 + $20 + $100 = $130         │\n",
    "    │    But this requires knowing the ENTIRE path!                 │\n",
    "    │                                                                │\n",
    "    │  BELLMAN'S INSIGHT:                                           │\n",
    "    │    V(A) = Reward(A→B) + V(B)                                  │\n",
    "    │         = $10 + (value of being at B)                         │\n",
    "    │                                                                │\n",
    "    │    \"My value = What I get now + Value of where I end up\"      │\n",
    "    │                                                                │\n",
    "    │  This is RECURSIVE! We break a big problem into smaller ones. │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**The Bellman equation says: The value of a state equals the immediate reward plus the (discounted) value of the next state.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, FancyArrowPatch, Rectangle\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Visualize the treasure map analogy\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 7.5, 'The Bellman Equation: Recursive Value Calculation', \n",
    "        ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Islands (states)\n",
    "islands = [\n",
    "    {'name': 'A', 'x': 1.5, 'color': '#bbdefb', 'value': 117},\n",
    "    {'name': 'B', 'x': 4.5, 'color': '#c8e6c9', 'value': 119},\n",
    "    {'name': 'C', 'x': 7.5, 'color': '#fff3e0', 'value': 110},\n",
    "    {'name': 'X', 'x': 10.5, 'color': '#ffeb3b', 'value': 0},  # Treasure!\n",
    "]\n",
    "\n",
    "# Draw islands\n",
    "for island in islands:\n",
    "    circle = Circle((island['x'], 4), 0.8, facecolor=island['color'], \n",
    "                     edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(island['x'], 4.2, island['name'], ha='center', va='center', \n",
    "            fontsize=16, fontweight='bold')\n",
    "    if island['name'] == 'X':\n",
    "        ax.text(island['x'], 3.5, 'TREASURE!', ha='center', fontsize=8, color='#795548')\n",
    "    else:\n",
    "        ax.text(island['x'], 3.5, f'V={island[\"value\"]}', ha='center', fontsize=9)\n",
    "\n",
    "# Draw arrows with rewards\n",
    "rewards = [('A', 'B', 10), ('B', 'C', 20), ('C', 'X', 100)]\n",
    "for i, (start, end, reward) in enumerate(rewards):\n",
    "    x1, x2 = islands[i]['x'] + 0.8, islands[i+1]['x'] - 0.8\n",
    "    ax.annotate('', xy=(x2, 4), xytext=(x1, 4),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "    ax.text((x1 + x2) / 2, 4.6, f'+${reward}', ha='center', fontsize=11, \n",
    "            color='#388e3c', fontweight='bold')\n",
    "\n",
    "# Bellman equation explanation\n",
    "ax.text(7, 1.8, 'BELLMAN EQUATION:', ha='center', fontsize=13, fontweight='bold')\n",
    "ax.text(7, 1.2, 'V(A) = Reward(A→B) + γ × V(B)', ha='center', fontsize=12)\n",
    "ax.text(7, 0.6, '117 = 10 + 0.9 × 119', ha='center', fontsize=11, color='#666')\n",
    "ax.text(7, 0.1, '\"My value = Immediate reward + Discounted future value\"', \n",
    "        ha='center', fontsize=10, style='italic', color='#888')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THE KEY INSIGHT: RECURSION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Instead of calculating the ENTIRE path's value:\n",
    "  V(A) = r₀ + γr₁ + γ²r₂ + γ³r₃ + ...\n",
    "\n",
    "We use RECURSION:\n",
    "  V(A) = r + γ × V(next_state)\n",
    "       = \"what I get now\" + \"discounted value of where I end up\"\n",
    "\n",
    "This breaks a complex problem into simpler sub-problems!\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Bellman Expectation Equation\n",
    "\n",
    "The Bellman equation we saw was simplified. The full version handles:\n",
    "- **Stochastic policies** (probability of actions)\n",
    "- **Stochastic transitions** (probability of next states)\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │           BELLMAN EXPECTATION EQUATION FOR V^π                 │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  V^π(s) = Σ_a π(a|s) × Σ_s' P(s'|s,a) × [R(s,a,s') + γ×V^π(s')]│\n",
    "    │           ─────────   ──────────────   ─────────────────────── │\n",
    "    │           sum over    sum over all     immediate   discounted  │\n",
    "    │           actions     next states      reward    + future value│\n",
    "    │                                                                │\n",
    "    │  In plain English:                                            │\n",
    "    │    \"The value of state s under policy π equals the            │\n",
    "    │     EXPECTED immediate reward plus discounted future value,   │\n",
    "    │     averaged over all actions and possible next states.\"      │\n",
    "    │                                                                │\n",
    "    │  Simplified (deterministic case):                             │\n",
    "    │    V(s) = R(s,a) + γ × V(s')                                  │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "There's also a Bellman equation for Q:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │           BELLMAN EXPECTATION EQUATION FOR Q^π                 │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Q^π(s,a) = Σ_s' P(s'|s,a) × [R + γ × Σ_a' π(a'|s') × Q^π(s',a')]│\n",
    "    │                                                                │\n",
    "    │  In plain English:                                            │\n",
    "    │    \"The value of action a in state s equals the expected      │\n",
    "    │     immediate reward plus the discounted expected future Q.\"  │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Bellman equation structure\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 9.5, 'Bellman Equation: Breaking Down V(s)', \n",
    "        ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Current state\n",
    "state_circle = Circle((2, 5), 0.8, facecolor='#bbdefb', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(state_circle)\n",
    "ax.text(2, 5, 's', ha='center', va='center', fontsize=18, fontweight='bold')\n",
    "ax.text(2, 3.8, 'V(s) = ?', ha='center', fontsize=11, color='#1976d2', fontweight='bold')\n",
    "\n",
    "# Actions\n",
    "actions = [\n",
    "    {'name': 'a₁', 'y': 7, 'prob': 'π(a₁|s)'},\n",
    "    {'name': 'a₂', 'y': 5, 'prob': 'π(a₂|s)'},\n",
    "    {'name': 'a₃', 'y': 3, 'prob': 'π(a₃|s)'}\n",
    "]\n",
    "\n",
    "for action in actions:\n",
    "    # Action box\n",
    "    box = FancyBboxPatch((4.5, action['y']-0.4), 1.5, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(5.25, action['y'], action['name'], ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Arrow from state to action\n",
    "    ax.annotate('', xy=(4.4, action['y']), xytext=(2.85, 5),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5, color='#666'))\n",
    "    ax.text(3.5, (action['y'] + 5)/2 + 0.3, action['prob'], fontsize=9, color='#f57c00')\n",
    "\n",
    "# Next states (for action a₂)\n",
    "next_states = [\n",
    "    {'name': \"s'₁\", 'y': 6.5, 'prob': \"P(s'₁|s,a₂)\", 'value': \"V(s'₁)\"},\n",
    "    {'name': \"s'₂\", 'y': 5, 'prob': \"P(s'₂|s,a₂)\", 'value': \"V(s'₂)\"},\n",
    "    {'name': \"s'₃\", 'y': 3.5, 'prob': \"P(s'₃|s,a₂)\", 'value': \"V(s'₃)\"}\n",
    "]\n",
    "\n",
    "for ns in next_states:\n",
    "    # Next state circle\n",
    "    circle = Circle((9, ns['y']), 0.6, facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(9, ns['y'], ns['name'], ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "    ax.text(10, ns['y'], ns['value'], ha='left', fontsize=10, color='#388e3c')\n",
    "    \n",
    "    # Arrow from action to next state\n",
    "    ax.annotate('', xy=(8.35, ns['y']), xytext=(6.1, 5),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5, color='#666'))\n",
    "    ax.text(7.2, ns['y'] + 0.3, ns['prob'], fontsize=8, color='#388e3c')\n",
    "\n",
    "# Rewards\n",
    "ax.text(7, 7.5, 'R', fontsize=11, color='#d32f2f', fontweight='bold')\n",
    "ax.text(7, 5.5, 'R', fontsize=11, color='#d32f2f', fontweight='bold')\n",
    "ax.text(7, 4, 'R', fontsize=11, color='#d32f2f', fontweight='bold')\n",
    "\n",
    "# Equation at bottom\n",
    "ax.text(7, 1.5, 'V(s) = Σₐ π(a|s) × Σₛ\\' P(s\\'|s,a) × [R + γ × V(s\\')]', \n",
    "        ha='center', fontsize=14, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='#e3f2fd', edgecolor='#1976d2'))\n",
    "\n",
    "ax.text(7, 0.5, '\"Average over actions, then average over next states\"',\n",
    "        ha='center', fontsize=11, style='italic', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building a Grid World MDP\n",
    "\n",
    "Let's create a concrete example to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldMDP:\n",
    "    \"\"\"\n",
    "    4x4 Grid World MDP for demonstrating Bellman equations.\n",
    "    \n",
    "    Layout:\n",
    "        ┌───┬───┬───┬───┐\n",
    "        │ S │   │   │   │   S = Start (0,0)\n",
    "        ├───┼───┼───┼───┤\n",
    "        │   │   │   │   │\n",
    "        ├───┼───┼───┼───┤\n",
    "        │   │   │   │   │\n",
    "        ├───┼───┼───┼───┤\n",
    "        │   │   │   │ G │   G = Goal (3,3)\n",
    "        └───┴───┴───┴───┘\n",
    "    \n",
    "    Actions: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "    Rewards: -1 per step, +10 at goal\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.n_states = 16\n",
    "        self.n_actions = 4\n",
    "        self.goal = (3, 3)\n",
    "        self.gamma = 0.9  # Discount factor\n",
    "        \n",
    "        # All possible states and actions\n",
    "        self.states = [(r, c) for r in range(4) for c in range(4)]\n",
    "        self.actions = [0, 1, 2, 3]  # up, right, down, left\n",
    "        self.action_names = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
    "        self.action_symbols = ['↑', '→', '↓', '←']\n",
    "        \n",
    "        # Build transition model\n",
    "        self.P = self._build_transitions()\n",
    "    \n",
    "    def _next_state(self, state, action):\n",
    "        \"\"\"Get next state given current state and action.\"\"\"\n",
    "        row, col = state\n",
    "        \n",
    "        if action == 0:    # UP\n",
    "            row = max(0, row - 1)\n",
    "        elif action == 1:  # RIGHT\n",
    "            col = min(3, col + 1)\n",
    "        elif action == 2:  # DOWN\n",
    "            row = min(3, row + 1)\n",
    "        elif action == 3:  # LEFT\n",
    "            col = max(0, col - 1)\n",
    "        \n",
    "        return (row, col)\n",
    "    \n",
    "    def _build_transitions(self):\n",
    "        \"\"\"\n",
    "        Build the transition probability dictionary.\n",
    "        \n",
    "        P[s][a] = list of (probability, next_state, reward) tuples\n",
    "        \"\"\"\n",
    "        P = {}\n",
    "        \n",
    "        for s in self.states:\n",
    "            P[s] = {}\n",
    "            \n",
    "            for a in self.actions:\n",
    "                next_s = self._next_state(s, a)\n",
    "                reward = 10 if next_s == self.goal else -1\n",
    "                \n",
    "                # Deterministic: probability 1.0 of reaching next_s\n",
    "                P[s][a] = [(1.0, next_s, reward)]\n",
    "        \n",
    "        return P\n",
    "\n",
    "\n",
    "# Create MDP\n",
    "mdp = GridWorldMDP()\n",
    "\n",
    "print(\"GRID WORLD MDP\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Grid size: {mdp.size}x{mdp.size}\")\n",
    "print(f\"Number of states: {mdp.n_states}\")\n",
    "print(f\"Number of actions: {mdp.n_actions}\")\n",
    "print(f\"Goal state: {mdp.goal}\")\n",
    "print(f\"Discount factor (γ): {mdp.gamma}\")\n",
    "print(f\"\\nRewards:\")\n",
    "print(f\"  -1 for each step\")\n",
    "print(f\"  +10 for reaching the goal\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Policy Evaluation: Computing V^π\n",
    "\n",
    "**Goal:** Given a policy π, compute V^π(s) for all states.\n",
    "\n",
    "**Method:** Iteratively apply the Bellman equation until values converge!\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                   POLICY EVALUATION                            │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  1. Initialize V(s) = 0 for all states                        │\n",
    "    │                                                                │\n",
    "    │  2. Repeat until convergence:                                 │\n",
    "    │                                                                │\n",
    "    │     For each state s:                                         │\n",
    "    │       V_new(s) = Σ_a π(a|s) × Σ_s' P(s'|s,a) × [R + γ×V(s')] │\n",
    "    │                                                                │\n",
    "    │  3. Return V                                                  │\n",
    "    │                                                                │\n",
    "    │  Convergence: When max|V_new(s) - V_old(s)| < threshold       │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Analogy:** Like a ripple spreading through water - values propagate from the goal backward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(mdp, policy, threshold=1e-6, max_iterations=1000, verbose=False):\n",
    "    \"\"\"\n",
    "    Compute V^π by iteratively applying the Bellman expectation equation.\n",
    "    \n",
    "    Args:\n",
    "        mdp: The MDP\n",
    "        policy: dict mapping state -> dict of action -> probability\n",
    "                e.g., policy[s][a] = probability of action a in state s\n",
    "        threshold: Stop when max value change < threshold\n",
    "        max_iterations: Maximum iterations\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        V: dict mapping state -> value\n",
    "        history: list of V at each iteration (for visualization)\n",
    "    \"\"\"\n",
    "    # ========================================\n",
    "    # STEP 1: Initialize all values to zero\n",
    "    # ========================================\n",
    "    V = {s: 0.0 for s in mdp.states}\n",
    "    history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0  # Track maximum change\n",
    "        \n",
    "        # ========================================\n",
    "        # STEP 2: Update each state's value\n",
    "        # ========================================\n",
    "        for s in mdp.states:\n",
    "            if s == mdp.goal:\n",
    "                continue  # Terminal state has value 0\n",
    "            \n",
    "            v_old = V[s]\n",
    "            \n",
    "            # BELLMAN EXPECTATION EQUATION:\n",
    "            # V(s) = Σ_a π(a|s) × Σ_s' P(s'|s,a) × [R + γ×V(s')]\n",
    "            v_new = 0\n",
    "            for a in mdp.actions:\n",
    "                action_prob = policy[s][a]  # π(a|s)\n",
    "                \n",
    "                for (trans_prob, next_s, reward) in mdp.P[s][a]:\n",
    "                    # P(s'|s,a) × [R + γ×V(s')]\n",
    "                    v_new += action_prob * trans_prob * (reward + mdp.gamma * V[next_s])\n",
    "            \n",
    "            V[s] = v_new\n",
    "            delta = max(delta, abs(v_old - v_new))\n",
    "        \n",
    "        # Save history for visualization\n",
    "        history.append({s: V[s] for s in mdp.states})\n",
    "        \n",
    "        # ========================================\n",
    "        # STEP 3: Check for convergence\n",
    "        # ========================================\n",
    "        if delta < threshold:\n",
    "            if verbose:\n",
    "                print(f\"Converged after {iteration + 1} iterations (Δ < {threshold})\")\n",
    "            break\n",
    "    \n",
    "    return V, history\n",
    "\n",
    "\n",
    "# Create a RANDOM policy (equal probability for all actions)\n",
    "random_policy = {s: {a: 0.25 for a in mdp.actions} for s in mdp.states}\n",
    "\n",
    "print(\"POLICY EVALUATION: Random Policy\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPolicy: π(a|s) = 0.25 for all actions (random)\")\n",
    "print(\"\\nRunning policy evaluation...\")\n",
    "\n",
    "V_random, history_random = policy_evaluation(mdp, random_policy, verbose=True)\n",
    "\n",
    "print(\"\\nValue Function V^π(s):\")\n",
    "print(\"-\"*35)\n",
    "for row in range(4):\n",
    "    values = [V_random[(row, col)] for col in range(4)]\n",
    "    print(\" \".join([f\"{v:8.2f}\" for v in values]))\n",
    "print(\"-\"*35)\n",
    "print(\"(Goal state at bottom-right has value 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how values propagate over iterations\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "iterations_to_show = [0, 2, 5, 10, 20, 50, 100, len(history_random)-1]\n",
    "iterations_to_show = [i for i in iterations_to_show if i < len(history_random)]\n",
    "\n",
    "# Fill remaining with last iteration\n",
    "while len(iterations_to_show) < 8:\n",
    "    iterations_to_show.append(len(history_random)-1)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    it = iterations_to_show[idx]\n",
    "    V_it = history_random[it]\n",
    "    \n",
    "    # Create grid\n",
    "    V_grid = np.array([[V_it[(r, c)] for c in range(4)] for r in range(4)])\n",
    "    \n",
    "    im = ax.imshow(V_grid, cmap='RdYlGn', vmin=-10, vmax=5)\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            color = 'white' if V_grid[i, j] < -2 else 'black'\n",
    "            ax.text(j, i, f'{V_grid[i, j]:.1f}', ha='center', va='center', \n",
    "                    fontsize=10, fontweight='bold', color=color)\n",
    "    \n",
    "    ax.set_title(f'Iteration {it + 1}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('Policy Evaluation: Values Propagating Over Iterations\\n(Random Policy)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Watch how the values 'ripple' backward from the goal!\")\n",
    "print(\"States closer to the goal get higher values first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare random policy vs optimal policy\n",
    "\n",
    "def create_optimal_policy(mdp):\n",
    "    \"\"\"\n",
    "    Create an optimal policy that always moves toward the goal.\n",
    "    \"\"\"\n",
    "    policy = {}\n",
    "    \n",
    "    for s in mdp.states:\n",
    "        row, col = s\n",
    "        policy[s] = {a: 0.0 for a in mdp.actions}\n",
    "        \n",
    "        # Determine best action to move toward goal (3, 3)\n",
    "        if col < 3:  # Not at goal column\n",
    "            best_action = 1  # RIGHT\n",
    "        elif row < 3:  # At goal column, not at goal row\n",
    "            best_action = 2  # DOWN\n",
    "        else:\n",
    "            best_action = 0  # At goal, any action\n",
    "        \n",
    "        policy[s][best_action] = 1.0\n",
    "    \n",
    "    return policy\n",
    "\n",
    "\n",
    "# Create and evaluate optimal policy\n",
    "optimal_policy = create_optimal_policy(mdp)\n",
    "\n",
    "print(\"COMPARING POLICIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. RANDOM POLICY (equal probability for all actions):\")\n",
    "print(\"-\"*60)\n",
    "for row in range(4):\n",
    "    values = [V_random[(row, col)] for col in range(4)]\n",
    "    print(\" \".join([f\"{v:8.2f}\" for v in values]))\n",
    "\n",
    "print(\"\\n2. OPTIMAL POLICY (always move toward goal):\")\n",
    "V_optimal, _ = policy_evaluation(mdp, optimal_policy, verbose=True)\n",
    "print(\"-\"*60)\n",
    "for row in range(4):\n",
    "    values = [V_optimal[(row, col)] for col in range(4)]\n",
    "    print(\" \".join([f\"{v:8.2f}\" for v in values]))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OBSERVATION:\")\n",
    "print(f\"  Random policy: V(start) = {V_random[(0,0)]:.2f}\")\n",
    "print(f\"  Optimal policy: V(start) = {V_optimal[(0,0)]:.2f}\")\n",
    "print(f\"  Difference: {V_optimal[(0,0)] - V_random[(0,0)]:.2f}\")\n",
    "print(\"\\nThe optimal policy gives MUCH higher values!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Bellman Optimality Equation\n",
    "\n",
    "What if we want to find the **best possible** values, not just for a given policy?\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │             BELLMAN OPTIMALITY EQUATION                        │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  EXPECTATION (for policy π):                                  │\n",
    "    │    V^π(s) = Σ_a π(a|s) × [R + γ×V^π(s')]                      │\n",
    "    │             ───────────                                        │\n",
    "    │             AVERAGE over actions (weighted by policy)          │\n",
    "    │                                                                │\n",
    "    │  OPTIMALITY (for optimal policy π*):                          │\n",
    "    │    V*(s) = MAX_a [R + γ×V*(s')]                               │\n",
    "    │            ─────                                               │\n",
    "    │            MAXIMUM over actions (pick the best!)              │\n",
    "    │                                                                │\n",
    "    │  The key difference:                                          │\n",
    "    │    - Expectation: Average over what π DOES                    │\n",
    "    │    - Optimality: Maximum over what's POSSIBLE                 │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**For Q*:**\n",
    "```\n",
    "Q*(s,a) = R + γ × max_a' Q*(s', a')\n",
    "```\n",
    "\n",
    "This is the equation that Q-learning uses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference between expectation and optimality\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Bellman Expectation (averaging)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Bellman EXPECTATION\\n(Average over policy)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# State\n",
    "circle = Circle((2, 5), 0.6, facecolor='#bbdefb', edgecolor='#1976d2', linewidth=2)\n",
    "ax1.add_patch(circle)\n",
    "ax1.text(2, 5, 's', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Actions with values\n",
    "action_values = [\n",
    "    {'y': 7, 'q': 5.0, 'prob': 0.25},\n",
    "    {'y': 5.5, 'q': 8.0, 'prob': 0.25},\n",
    "    {'y': 4, 'q': 3.0, 'prob': 0.25},\n",
    "    {'y': 2.5, 'q': 6.0, 'prob': 0.25},\n",
    "]\n",
    "\n",
    "for av in action_values:\n",
    "    box = FancyBboxPatch((5, av['y']-0.3), 2, 0.6, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#fff3e0', edgecolor='#f57c00', linewidth=1)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(6, av['y'], f'Q={av[\"q\"]}', ha='center', va='center', fontsize=10)\n",
    "    ax1.annotate('', xy=(4.9, av['y']), xytext=(2.7, 5),\n",
    "                 arrowprops=dict(arrowstyle='->', lw=1, color='#666'))\n",
    "    ax1.text(3.8, av['y'] + 0.3, f'{av[\"prob\"]}', fontsize=9, color='#666')\n",
    "\n",
    "# Result\n",
    "ax1.text(5, 1.5, 'V(s) = 0.25×5 + 0.25×8 + 0.25×3 + 0.25×6', ha='center', fontsize=10)\n",
    "ax1.text(5, 0.8, '= 5.5 (average)', ha='center', fontsize=12, fontweight='bold', color='#1976d2')\n",
    "\n",
    "# Right: Bellman Optimality (maximum)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Bellman OPTIMALITY\\n(Maximum over actions)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# State\n",
    "circle = Circle((2, 5), 0.6, facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax2.add_patch(circle)\n",
    "ax2.text(2, 5, 's', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Actions with values (highlight max)\n",
    "for i, av in enumerate(action_values):\n",
    "    is_max = av['q'] == 8.0\n",
    "    color = '#c8e6c9' if is_max else '#f5f5f5'\n",
    "    edge = '#388e3c' if is_max else '#999'\n",
    "    lw = 3 if is_max else 1\n",
    "    \n",
    "    box = FancyBboxPatch((5, av['y']-0.3), 2, 0.6, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor=color, edgecolor=edge, linewidth=lw)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(6, av['y'], f'Q={av[\"q\"]}' + (' ★' if is_max else ''), \n",
    "             ha='center', va='center', fontsize=10, fontweight='bold' if is_max else 'normal')\n",
    "    ax2.annotate('', xy=(4.9, av['y']), xytext=(2.7, 5),\n",
    "                 arrowprops=dict(arrowstyle='->', lw=2 if is_max else 1, \n",
    "                                color='#388e3c' if is_max else '#ccc'))\n",
    "\n",
    "# Result\n",
    "ax2.text(5, 1.5, 'V*(s) = max(5, 8, 3, 6)', ha='center', fontsize=10)\n",
    "ax2.text(5, 0.8, '= 8.0 (maximum!)', ha='center', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe OPTIMALITY equation finds the BEST possible value!\")\n",
    "print(\"It assumes we'll always pick the best action.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Value Iteration: Finding V* Directly\n",
    "\n",
    "**Idea:** Iteratively apply the Bellman optimality equation to find V*.\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                   VALUE ITERATION                              │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  1. Initialize V(s) = 0 for all states                        │\n",
    "    │                                                                │\n",
    "    │  2. Repeat until convergence:                                 │\n",
    "    │                                                                │\n",
    "    │     For each state s:                                         │\n",
    "    │       V(s) = MAX_a Σ_s' P(s'|s,a) × [R + γ×V(s')]             │\n",
    "    │              ─────                                             │\n",
    "    │              Take the BEST action's value!                    │\n",
    "    │                                                                │\n",
    "    │  3. Extract optimal policy:                                   │\n",
    "    │       π*(s) = argmax_a Σ_s' P(s'|s,a) × [R + γ×V*(s')]        │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, threshold=1e-6, max_iterations=1000, verbose=False):\n",
    "    \"\"\"\n",
    "    Find V* using value iteration.\n",
    "    \n",
    "    Returns:\n",
    "        V: Optimal value function\n",
    "        policy: Optimal policy\n",
    "        history: Value function at each iteration\n",
    "    \"\"\"\n",
    "    # ========================================\n",
    "    # STEP 1: Initialize values to zero\n",
    "    # ========================================\n",
    "    V = {s: 0.0 for s in mdp.states}\n",
    "    history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        \n",
    "        # ========================================\n",
    "        # STEP 2: Bellman OPTIMALITY update\n",
    "        # ========================================\n",
    "        for s in mdp.states:\n",
    "            if s == mdp.goal:\n",
    "                continue\n",
    "            \n",
    "            v_old = V[s]\n",
    "            \n",
    "            # Compute Q(s, a) for each action\n",
    "            action_values = []\n",
    "            for a in mdp.actions:\n",
    "                q_value = 0\n",
    "                for (prob, next_s, reward) in mdp.P[s][a]:\n",
    "                    q_value += prob * (reward + mdp.gamma * V[next_s])\n",
    "                action_values.append(q_value)\n",
    "            \n",
    "            # V(s) = MAX over actions (Bellman optimality!)\n",
    "            V[s] = max(action_values)\n",
    "            delta = max(delta, abs(v_old - V[s]))\n",
    "        \n",
    "        history.append({s: V[s] for s in mdp.states})\n",
    "        \n",
    "        if delta < threshold:\n",
    "            if verbose:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 3: Extract optimal policy\n",
    "    # ========================================\n",
    "    policy = {}\n",
    "    for s in mdp.states:\n",
    "        if s == mdp.goal:\n",
    "            policy[s] = 0  # Doesn't matter at goal\n",
    "            continue\n",
    "        \n",
    "        # Find action with highest Q-value\n",
    "        action_values = []\n",
    "        for a in mdp.actions:\n",
    "            q_value = 0\n",
    "            for (prob, next_s, reward) in mdp.P[s][a]:\n",
    "                q_value += prob * (reward + mdp.gamma * V[next_s])\n",
    "            action_values.append(q_value)\n",
    "        \n",
    "        policy[s] = np.argmax(action_values)\n",
    "    \n",
    "    return V, policy, history\n",
    "\n",
    "\n",
    "# Run value iteration\n",
    "print(\"VALUE ITERATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nFinding optimal value function V* and policy π*...\")\n",
    "\n",
    "V_star, pi_star, history_vi = value_iteration(mdp, verbose=True)\n",
    "\n",
    "print(\"\\nOptimal Value Function V*:\")\n",
    "print(\"-\"*35)\n",
    "for row in range(4):\n",
    "    values = [V_star[(row, col)] for col in range(4)]\n",
    "    print(\" \".join([f\"{v:8.2f}\" for v in values]))\n",
    "\n",
    "print(\"\\nOptimal Policy π*:\")\n",
    "print(\"-\"*35)\n",
    "for row in range(4):\n",
    "    actions = [mdp.action_symbols[pi_star[(row, col)]] for col in range(4)]\n",
    "    print(\"    \".join(actions))\n",
    "print(\"-\"*35)\n",
    "print(\"(Arrows show the best action in each state)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize V* and π* together\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Value function heatmap\n",
    "ax1 = axes[0]\n",
    "V_grid = np.array([[V_star[(r, c)] for c in range(4)] for r in range(4)])\n",
    "\n",
    "im = ax1.imshow(V_grid, cmap='RdYlGn')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        color = 'white' if V_grid[i, j] < np.mean(V_grid) else 'black'\n",
    "        ax1.text(j, i, f'{V_grid[i, j]:.1f}', ha='center', va='center', \n",
    "                 fontsize=12, fontweight='bold', color=color)\n",
    "\n",
    "ax1.set_title('Optimal Value Function V*', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(range(4))\n",
    "ax1.set_yticks(range(4))\n",
    "plt.colorbar(im, ax=ax1, label='Value')\n",
    "\n",
    "# Right: Policy visualization\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Draw grid\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        color = '#c8e6c9' if (row, col) == mdp.goal else '#e3f2fd' if (row, col) == (0, 0) else 'white'\n",
    "        rect = Rectangle((col, 3 - row), 1, 1, facecolor=color, edgecolor='black', linewidth=2)\n",
    "        ax2.add_patch(rect)\n",
    "\n",
    "# Draw arrows for policy\n",
    "arrow_dx = [0, 0.35, 0, -0.35]  # up, right, down, left\n",
    "arrow_dy = [0.35, 0, -0.35, 0]\n",
    "\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        if (row, col) == mdp.goal:\n",
    "            ax2.text(col + 0.5, 3 - row + 0.5, 'GOAL', ha='center', va='center', \n",
    "                     fontsize=10, fontweight='bold', color='#388e3c')\n",
    "        else:\n",
    "            a = pi_star[(row, col)]\n",
    "            cx, cy = col + 0.5, 3 - row + 0.5\n",
    "            ax2.arrow(cx - arrow_dx[a]/2, cy - arrow_dy[a]/2, \n",
    "                      arrow_dx[a], arrow_dy[a],\n",
    "                      head_width=0.15, head_length=0.1, \n",
    "                      fc='#f44336', ec='#f44336', linewidth=2)\n",
    "\n",
    "ax2.set_xlim(0, 4)\n",
    "ax2.set_ylim(0, 4)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Optimal Policy π*\\n(Arrows = Best Actions)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe optimal policy always takes the shortest path to the goal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why Bellman Equations Matter\n",
    "\n",
    "The Bellman equations are the foundation of virtually ALL RL algorithms:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          BELLMAN EQUATIONS → RL ALGORITHMS                     │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  ALGORITHM                 USES WHICH BELLMAN EQUATION?       │\n",
    "    │  ─────────────────────────────────────────────────────        │\n",
    "    │                                                                │\n",
    "    │  Policy Evaluation         Bellman Expectation for V^π        │\n",
    "    │  Policy Iteration          Bellman Expectation + greedy       │\n",
    "    │  Value Iteration           Bellman Optimality for V*          │\n",
    "    │                                                                │\n",
    "    │  TD Learning               Bellman Expectation (sampled)      │\n",
    "    │  Q-Learning                Bellman Optimality for Q*          │\n",
    "    │  SARSA                     Bellman Expectation for Q^π        │\n",
    "    │                                                                │\n",
    "    │  DQN                       Bellman Optimality for Q*          │\n",
    "    │  Actor-Critic              Bellman Expectation (critic)       │\n",
    "    │                                                                │\n",
    "    │  The Bellman equations give us a way to break down the        │\n",
    "    │  value of a state into immediate + future components.         │\n",
    "    │  This RECURSIVE structure makes RL tractable!                 │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison: Random vs Optimal policies\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Random policy values\n",
    "ax1 = axes[0]\n",
    "V_rand_grid = np.array([[V_random[(r, c)] for c in range(4)] for r in range(4)])\n",
    "im1 = ax1.imshow(V_rand_grid, cmap='RdYlGn', vmin=-5, vmax=10)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax1.text(j, i, f'{V_rand_grid[i, j]:.1f}', ha='center', va='center', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "ax1.set_title('V^π (Random Policy)\\nAverage value', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "# Optimal policy values\n",
    "ax2 = axes[1]\n",
    "V_opt_grid = np.array([[V_optimal[(r, c)] for c in range(4)] for r in range(4)])\n",
    "im2 = ax2.imshow(V_opt_grid, cmap='RdYlGn', vmin=-5, vmax=10)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax2.text(j, i, f'{V_opt_grid[i, j]:.1f}', ha='center', va='center', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "ax2.set_title('V^π* (Optimal Policy)\\nBetter values!', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "# Difference\n",
    "ax3 = axes[2]\n",
    "diff_grid = V_opt_grid - V_rand_grid\n",
    "im3 = ax3.imshow(diff_grid, cmap='Blues')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax3.text(j, i, f'+{diff_grid[i, j]:.1f}', ha='center', va='center', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Improvement\\n(Optimal - Random)', fontsize=12, fontweight='bold')\n",
    "ax3.set_xticks([])\n",
    "ax3.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe optimal policy provides MUCH higher values everywhere!\")\n",
    "print(f\"At start (0,0): Random = {V_random[(0,0)]:.2f}, Optimal = {V_optimal[(0,0)]:.2f}\")\n",
    "print(f\"Improvement: +{V_optimal[(0,0)] - V_random[(0,0)]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### The Core Insight\n",
    "\n",
    "**Value is recursive:** V(s) = Immediate reward + Discounted future value\n",
    "\n",
    "### Two Types of Bellman Equations\n",
    "\n",
    "| Type | Equation | Use |\n",
    "|------|----------|-----|\n",
    "| **Expectation** | V^π(s) = Σ_a π(a\\|s) × [R + γV(s')] | Evaluate a given policy |\n",
    "| **Optimality** | V*(s) = max_a [R + γV*(s')] | Find the best policy |\n",
    "\n",
    "### Two Fundamental Algorithms\n",
    "\n",
    "| Algorithm | What it does | Uses |\n",
    "|-----------|--------------|------|\n",
    "| **Policy Evaluation** | Compute V^π for policy π | Bellman Expectation |\n",
    "| **Value Iteration** | Find V* directly | Bellman Optimality |\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- ALL RL algorithms are built on Bellman equations\n",
    "- The recursive structure makes RL tractable\n",
    "- Q-learning uses the Bellman optimality equation for Q*\n",
    "- TD learning uses sampled Bellman equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What is the key insight of the Bellman equation?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The value of a state can be decomposed into immediate reward plus discounted future value: V(s) = R + γV(s'). This recursive structure breaks a complex problem (total future reward) into simpler sub-problems.\n",
    "</details>\n",
    "\n",
    "**2. What's the difference between Bellman expectation and optimality equations?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "- Expectation: V^π(s) = Σ_a π(a|s) × [R + γV(s')] - AVERAGES over actions according to policy π\n",
    "- Optimality: V*(s) = max_a [R + γV*(s')] - Takes the MAXIMUM over actions\n",
    "\n",
    "Expectation evaluates a given policy; optimality finds the best possible value.\n",
    "</details>\n",
    "\n",
    "**3. How does policy evaluation work?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "1. Initialize V(s) = 0 for all states\n",
    "2. Repeatedly apply Bellman expectation: V(s) = Σ_a π(a|s) × [R + γV(s')]\n",
    "3. Stop when values converge (change < threshold)\n",
    "\n",
    "It finds V^π by iteratively \"propagating\" values backward from terminal states.\n",
    "</details>\n",
    "\n",
    "**4. How does value iteration find V*?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "1. Initialize V(s) = 0 for all states\n",
    "2. Repeatedly apply Bellman optimality: V(s) = max_a [R + γV(s')]\n",
    "3. Stop when values converge\n",
    "4. Extract policy: π*(s) = argmax_a [R + γV*(s')]\n",
    "\n",
    "It finds V* directly without needing a policy, then derives the optimal policy from V*.\n",
    "</details>\n",
    "\n",
    "**5. Which RL algorithms use Bellman equations?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Virtually ALL of them!\n",
    "- Q-learning: Bellman optimality for Q*\n",
    "- SARSA: Bellman expectation for Q^π\n",
    "- TD learning: Sampled Bellman expectation\n",
    "- DQN: Bellman optimality for Q* (with neural networks)\n",
    "- Actor-Critic: Critic uses Bellman expectation\n",
    "\n",
    "The Bellman equations are the mathematical foundation of RL.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the **RL Fundamentals** section! You now understand:\n",
    "\n",
    "- The RL paradigm and agent-environment interaction\n",
    "- MDPs as the mathematical framework\n",
    "- Rewards, returns, and discounting\n",
    "- Policies and value functions (V and Q)\n",
    "- **Bellman equations** - the foundation of all RL\n",
    "\n",
    "**Next:** Move on to [Classic Algorithms](../classic-algorithms/) to learn Q-learning, SARSA, and Monte Carlo methods!\n",
    "\n",
    "---\n",
    "\n",
    "*You now have the mathematical foundation to understand ANY RL algorithm!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
