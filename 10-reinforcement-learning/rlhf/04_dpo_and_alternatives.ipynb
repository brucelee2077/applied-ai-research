{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO and Alternatives: Simpler Paths to Alignment\n",
    "\n",
    "What if we could skip the reward model and train directly on preferences?\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The shortcut analogy: why DPO is revolutionary\n",
    "- The math behind DPO (it's elegant!)\n",
    "- Implementing DPO loss from scratch\n",
    "- When to use DPO vs RLHF\n",
    "- Other alternatives: RLAIF, Constitutional AI, ORPO\n",
    "- Using TRL's DPOTrainer\n",
    "\n",
    "**Prerequisites:** Notebooks 1-3 (RLHF basics, Reward Modeling, PPO for LLMs)\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Shortcut Analogy\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          THE SHORTCUT ANALOGY                                  â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Imagine you want to get from A to C...                       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  RLHF ROUTE (PPO):                                            â”‚\n",
    "    â”‚    A â†’ B â†’ C                                                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    A: Preference data (human labels)                         â”‚\n",
    "    â”‚    B: Train reward model (intermediate step)                  â”‚\n",
    "    â”‚    C: Train policy with PPO (final destination)              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    Problems:                                                  â”‚\n",
    "    â”‚    - Need to train 2 models                                  â”‚\n",
    "    â”‚    - PPO is unstable                                         â”‚\n",
    "    â”‚    - Complex pipeline                                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  DPO SHORTCUT:                                                â”‚\n",
    "    â”‚    A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ C                                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    Skip the reward model entirely!                           â”‚\n",
    "    â”‚    Go directly from preferences to aligned model!            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE KEY INSIGHT:                                             â”‚\n",
    "    â”‚    The optimal policy under RLHF has a closed-form!          â”‚\n",
    "    â”‚    We can derive what RL would learn... analytically!        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, FancyArrowPatch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualize RLHF vs DPO\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "# Left: RLHF Pipeline\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('RLHF Pipeline\\n(3 Stages)', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Stage boxes\n",
    "stages_rlhf = [\n",
    "    (1.5, 7.5, 'Stage 1:\\nPreferences', '#e3f2fd', '#1976d2'),\n",
    "    (1.5, 4.5, 'Stage 2:\\nReward Model', '#fff3e0', '#f57c00'),\n",
    "    (1.5, 1.5, 'Stage 3:\\nPPO Training', '#ffcdd2', '#d32f2f'),\n",
    "]\n",
    "\n",
    "for x, y, text, fcolor, ecolor in stages_rlhf:\n",
    "    box = FancyBboxPatch((x, y), 7, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=fcolor, edgecolor=ecolor, linewidth=2)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(x + 3.5, y + 1, text, ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Arrows\n",
    "ax1.annotate('', xy=(5, 6.5), xytext=(5, 7.4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax1.annotate('', xy=(5, 3.5), xytext=(5, 4.4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Complexity indicators\n",
    "ax1.text(9, 8.5, 'ğŸ˜“', fontsize=16)\n",
    "ax1.text(9, 5.5, 'ğŸ˜°', fontsize=16)\n",
    "ax1.text(9, 2.5, 'ğŸ˜µ', fontsize=16)\n",
    "\n",
    "# Right: DPO Pipeline\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('DPO Pipeline\\n(1 Stage!)', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Just two boxes\n",
    "box1 = FancyBboxPatch((1.5, 7), 7, 2, boxstyle=\"round,pad=0.1\",\n",
    "                       facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "ax2.add_patch(box1)\n",
    "ax2.text(5, 8, 'Preference Data', ha='center', va='center', fontsize=10)\n",
    "\n",
    "box2 = FancyBboxPatch((1.5, 2), 7, 2, boxstyle=\"round,pad=0.1\",\n",
    "                       facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax2.add_patch(box2)\n",
    "ax2.text(5, 3, 'Direct Training!\\n(One supervised loss)', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Big arrow\n",
    "ax2.annotate('', xy=(5, 4.1), xytext=(5, 6.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=4, color='#388e3c'))\n",
    "\n",
    "ax2.text(6.5, 5.5, 'SKIP\\nSTAGES!', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Happy face\n",
    "ax2.text(9, 5, 'ğŸ˜', fontsize=24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDPO vs RLHF:\")\n",
    "print(\"  RLHF: 3 stages, 2 models, unstable PPO\")\n",
    "print(\"  DPO:  1 stage, 1 model, stable supervised learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Math: From RLHF to DPO\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              THE MATHEMATICAL INSIGHT                          â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  STEP 1: RLHF Objective                                       â”‚\n",
    "    â”‚    max E[RM(x,y)] - Î² Ã— KL(Ï€ || Ï€_ref)                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  STEP 2: The Optimal Policy (closed form!)                    â”‚\n",
    "    â”‚    The solution to this optimization is:                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    Ï€*(y|x) = Ï€_ref(y|x) Ã— exp(r(x,y) / Î²) / Z(x)             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    where Z(x) is the normalization constant                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  STEP 3: Rearranging for Reward                               â”‚\n",
    "    â”‚    If we know the optimal policy, we can recover the reward:  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    r(x,y) = Î² Ã— log(Ï€*(y|x) / Ï€_ref(y|x)) + Î² Ã— log Z(x)     â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  STEP 4: The DPO Trick                                        â”‚\n",
    "    â”‚    For preference pairs, the Z(x) terms CANCEL OUT!           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    P(y_w > y_l) = Ïƒ(Î² Ã— [log Ï€(y_w|x)/Ï€_ref(y_w|x)           â”‚\n",
    "    â”‚                        - log Ï€(y_l|x)/Ï€_ref(y_l|x)])         â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  RESULT: We can train directly on preferences!                â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the mathematical derivation\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('DPO Derivation: The Key Insight', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Step boxes\n",
    "steps = [\n",
    "    (0.5, 9.5, 'Step 1: RLHF Objective', \n",
    "     'max E[r(x,y)] - Î² Ã— KL(Ï€ || Ï€_ref)', '#e3f2fd', '#1976d2'),\n",
    "    (0.5, 7, 'Step 2: Optimal Policy',\n",
    "     'Ï€*(y|x) âˆ Ï€_ref(y|x) Ã— exp(r(x,y)/Î²)', '#fff3e0', '#f57c00'),\n",
    "    (0.5, 4.5, 'Step 3: Reward from Policy',\n",
    "     'r(x,y) = Î² Ã— log(Ï€(y|x)/Ï€_ref(y|x)) + C', '#e1bee7', '#7b1fa2'),\n",
    "    (0.5, 2, 'Step 4: DPO Loss',\n",
    "     'L = -log Ïƒ(Î² Ã— [log Ï€(y_w)/Ï€_ref(y_w) - log Ï€(y_l)/Ï€_ref(y_l)])', '#c8e6c9', '#388e3c'),\n",
    "]\n",
    "\n",
    "for x, y, title, formula, fcolor, ecolor in steps:\n",
    "    box = FancyBboxPatch((x, y), 10, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=fcolor, edgecolor=ecolor, linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 5, y + 1.5, title, ha='center', fontsize=11, fontweight='bold', color=ecolor)\n",
    "    ax.text(x + 5, y + 0.6, formula, ha='center', fontsize=10, family='monospace')\n",
    "\n",
    "# Arrows\n",
    "for y in [9.4, 6.9, 4.4]:\n",
    "    ax.annotate('', xy=(5.5, y-0.3), xytext=(5.5, y),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Key insight\n",
    "insight_box = FancyBboxPatch((11, 5), 2.8, 3, boxstyle=\"round,pad=0.1\",\n",
    "                              facecolor='#ffeb3b', edgecolor='#f57f17', linewidth=2)\n",
    "ax.add_patch(insight_box)\n",
    "ax.text(12.4, 7.3, 'KEY', ha='center', fontsize=10, fontweight='bold', color='#f57f17')\n",
    "ax.text(12.4, 6.6, 'INSIGHT', ha='center', fontsize=10, fontweight='bold', color='#f57f17')\n",
    "ax.text(12.4, 5.8, 'Z(x) terms', ha='center', fontsize=9)\n",
    "ax.text(12.4, 5.3, 'cancel out!', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTHE GENIUS OF DPO:\")\n",
    "print(\"  The normalization constant Z(x) is intractable...\")\n",
    "print(\"  BUT when comparing pairs, it cancels out!\")\n",
    "print(\"  No reward model needed - just supervised learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The DPO Loss Function\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              DPO LOSS FUNCTION                                  â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  LOSS:                                                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    L_DPO = -E[ log Ïƒ( Î² Ã— (                                   â”‚\n",
    "    â”‚                   log Ï€_Î¸(y_w|x)/Ï€_ref(y_w|x)                 â”‚\n",
    "    â”‚                 - log Ï€_Î¸(y_l|x)/Ï€_ref(y_l|x)                 â”‚\n",
    "    â”‚               ) ) ]                                           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WHERE:                                                       â”‚\n",
    "    â”‚    y_w = winning (preferred) response                        â”‚\n",
    "    â”‚    y_l = losing (rejected) response                          â”‚\n",
    "    â”‚    Ï€_Î¸ = policy being trained                                â”‚\n",
    "    â”‚    Ï€_ref = reference policy (frozen)                         â”‚\n",
    "    â”‚    Î² = temperature parameter                                 â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  INTUITION:                                                   â”‚\n",
    "    â”‚    \"Increase probability of y_w relative to reference\"       â”‚\n",
    "    â”‚    \"Decrease probability of y_l relative to reference\"       â”‚\n",
    "    â”‚    Both constrained to not drift too far from reference!     â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_loss(policy_chosen_logps, policy_rejected_logps,\n",
    "             reference_chosen_logps, reference_rejected_logps,\n",
    "             beta=0.1):\n",
    "    \"\"\"\n",
    "    Compute DPO loss.\n",
    "    \n",
    "    This is THE key function - it's remarkably simple!\n",
    "    \n",
    "    Args:\n",
    "        policy_chosen_logps: Log probs of chosen response under policy\n",
    "        policy_rejected_logps: Log probs of rejected response under policy\n",
    "        reference_chosen_logps: Log probs of chosen under reference\n",
    "        reference_rejected_logps: Log probs of rejected under reference\n",
    "        beta: Temperature parameter (controls strength)\n",
    "    \n",
    "    Returns:\n",
    "        loss: DPO loss value\n",
    "        chosen_rewards: Implicit rewards for chosen\n",
    "        rejected_rewards: Implicit rewards for rejected\n",
    "    \"\"\"\n",
    "    # Compute \"implicit rewards\" \n",
    "    # These are what RLHF would learn, derived analytically!\n",
    "    chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps)\n",
    "    rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps)\n",
    "    \n",
    "    # DPO loss: maximize margin between chosen and rejected rewards\n",
    "    loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()\n",
    "    \n",
    "    return loss, chosen_rewards, rejected_rewards\n",
    "\n",
    "\n",
    "# Demonstrate DPO loss\n",
    "print(\"DPO LOSS DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simulated log probabilities (batch of 4 examples)\n",
    "torch.manual_seed(42)\n",
    "batch_size = 4\n",
    "\n",
    "# Reference model (frozen) - same for both\n",
    "reference_chosen_logps = torch.randn(batch_size) - 5  # Around -5\n",
    "reference_rejected_logps = torch.randn(batch_size) - 5\n",
    "\n",
    "print(\"\\nScenario 1: Policy increases chosen, decreases rejected (GOOD!)\")\n",
    "policy_chosen_logps = reference_chosen_logps + 0.5  # Increased\n",
    "policy_rejected_logps = reference_rejected_logps - 0.3  # Decreased\n",
    "loss1, cr1, rr1 = dpo_loss(policy_chosen_logps, policy_rejected_logps,\n",
    "                           reference_chosen_logps, reference_rejected_logps)\n",
    "print(f\"  Loss: {loss1.item():.4f} (lower is better)\")\n",
    "print(f\"  Chosen rewards: {cr1.mean().item():.4f}\")\n",
    "print(f\"  Rejected rewards: {rr1.mean().item():.4f}\")\n",
    "\n",
    "print(\"\\nScenario 2: Policy increases both equally (BAD!)\")\n",
    "policy_chosen_logps = reference_chosen_logps + 0.5\n",
    "policy_rejected_logps = reference_rejected_logps + 0.5\n",
    "loss2, cr2, rr2 = dpo_loss(policy_chosen_logps, policy_rejected_logps,\n",
    "                           reference_chosen_logps, reference_rejected_logps)\n",
    "print(f\"  Loss: {loss2.item():.4f}\")\n",
    "print(f\"  Chosen rewards: {cr2.mean().item():.4f}\")\n",
    "print(f\"  Rejected rewards: {rr2.mean().item():.4f}\")\n",
    "\n",
    "print(\"\\nScenario 3: Policy prefers rejected over chosen (VERY BAD!)\")\n",
    "policy_chosen_logps = reference_chosen_logps - 0.5  # Decreased\n",
    "policy_rejected_logps = reference_rejected_logps + 0.5  # Increased\n",
    "loss3, cr3, rr3 = dpo_loss(policy_chosen_logps, policy_rejected_logps,\n",
    "                           reference_chosen_logps, reference_rejected_logps)\n",
    "print(f\"  Loss: {loss3.item():.4f} (very high!)\")\n",
    "print(f\"  Chosen rewards: {cr3.mean().item():.4f}\")\n",
    "print(f\"  Rejected rewards: {rr3.mean().item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DPO loss landscape\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Loss vs reward margin\n",
    "ax1 = axes[0]\n",
    "margin = np.linspace(-3, 3, 100)  # chosen_reward - rejected_reward\n",
    "loss = -np.log(1 / (1 + np.exp(-margin)) + 1e-10)  # -log sigmoid\n",
    "\n",
    "ax1.plot(margin, loss, 'b-', linewidth=3)\n",
    "ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax1.fill_between(margin, 0, loss, where=(margin < 0), alpha=0.2, color='red', \n",
    "                 label='Rejected preferred (bad)')\n",
    "ax1.fill_between(margin, 0, loss, where=(margin > 0), alpha=0.2, color='green',\n",
    "                 label='Chosen preferred (good)')\n",
    "\n",
    "ax1.set_xlabel('Reward Margin (chosen - rejected)', fontsize=11)\n",
    "ax1.set_ylabel('DPO Loss', fontsize=11)\n",
    "ax1.set_title('DPO Loss vs Reward Margin', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 4)\n",
    "\n",
    "# Right: Effect of beta\n",
    "ax2 = axes[1]\n",
    "log_ratio = np.linspace(-2, 2, 100)  # log(Ï€/Ï€_ref) difference\n",
    "\n",
    "for beta in [0.05, 0.1, 0.2, 0.5]:\n",
    "    margin = beta * log_ratio\n",
    "    loss = -np.log(1 / (1 + np.exp(-margin)) + 1e-10)\n",
    "    ax2.plot(log_ratio, loss, linewidth=2, label=f'Î² = {beta}')\n",
    "\n",
    "ax2.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Log Ratio Difference', fontsize=11)\n",
    "ax2.set_ylabel('DPO Loss', fontsize=11)\n",
    "ax2.set_title('Effect of Î² (Temperature)', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBETA (Î²) CONTROLS:\")\n",
    "print(\"  â€¢ Higher Î²: More sensitive to preference differences\")\n",
    "print(\"  â€¢ Lower Î²: More forgiving, smoother gradients\")\n",
    "print(\"  â€¢ Typical values: 0.1 - 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training with DPO: Step by Step\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              DPO TRAINING WORKFLOW                             â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  INPUT DATA:                                                  â”‚\n",
    "    â”‚    Preference pairs: (prompt, chosen, rejected)              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  MODELS:                                                      â”‚\n",
    "    â”‚    Ï€_Î¸ (policy): Being trained                               â”‚\n",
    "    â”‚    Ï€_ref (reference): Frozen copy of initial Ï€_Î¸             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TRAINING LOOP:                                               â”‚\n",
    "    â”‚    for batch in dataloader:                                  â”‚\n",
    "    â”‚        # 1. Get log probs from both models                   â”‚\n",
    "    â”‚        Ï€_chosen = policy(chosen_response)                    â”‚\n",
    "    â”‚        Ï€_rejected = policy(rejected_response)                â”‚\n",
    "    â”‚        ref_chosen = reference(chosen_response)               â”‚\n",
    "    â”‚        ref_rejected = reference(rejected_response)           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚        # 2. Compute DPO loss                                 â”‚\n",
    "    â”‚        loss = dpo_loss(Ï€_chosen, Ï€_rejected,                 â”‚\n",
    "    â”‚                        ref_chosen, ref_rejected)             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚        # 3. Standard gradient descent                        â”‚\n",
    "    â”‚        loss.backward()                                       â”‚\n",
    "    â”‚        optimizer.step()                                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  That's it! Just supervised learning with a special loss!    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDPOTrainer:\n",
    "    \"\"\"\n",
    "    A simplified DPO trainer for demonstration.\n",
    "    \n",
    "    In practice, use TRL's DPOTrainer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_model, reference_model, beta=0.1, lr=1e-5):\n",
    "        self.policy = policy_model\n",
    "        self.reference = reference_model\n",
    "        self.beta = beta\n",
    "        \n",
    "        # Freeze reference model\n",
    "        for param in self.reference.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.optimizer = optim.AdamW(self.policy.parameters(), lr=lr)\n",
    "    \n",
    "    def compute_log_probs(self, model, input_ids):\n",
    "        \"\"\"\n",
    "        Compute log probabilities for a sequence.\n",
    "        \n",
    "        In practice, this would use the actual tokens.\n",
    "        Here we simulate with embeddings.\n",
    "        \"\"\"\n",
    "        with torch.set_grad_enabled(model.training):\n",
    "            logits = model(input_ids)\n",
    "            # Simplified: just sum of logits as \"log prob\"\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            return log_probs.sum(dim=-1).mean(dim=-1)\n",
    "    \n",
    "    def train_step(self, chosen_ids, rejected_ids):\n",
    "        \"\"\"\n",
    "        Perform one DPO training step.\n",
    "        \n",
    "        Args:\n",
    "            chosen_ids: Token ids for chosen responses\n",
    "            rejected_ids: Token ids for rejected responses\n",
    "        \n",
    "        Returns:\n",
    "            loss: DPO loss value\n",
    "            metrics: Dictionary of metrics\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Get log probs from policy\n",
    "        self.policy.train()\n",
    "        policy_chosen_logps = self.compute_log_probs(self.policy, chosen_ids)\n",
    "        policy_rejected_logps = self.compute_log_probs(self.policy, rejected_ids)\n",
    "        \n",
    "        # Get log probs from reference (no grad)\n",
    "        self.reference.eval()\n",
    "        with torch.no_grad():\n",
    "            ref_chosen_logps = self.compute_log_probs(self.reference, chosen_ids)\n",
    "            ref_rejected_logps = self.compute_log_probs(self.reference, rejected_ids)\n",
    "        \n",
    "        # Compute DPO loss\n",
    "        loss, chosen_rewards, rejected_rewards = dpo_loss(\n",
    "            policy_chosen_logps, policy_rejected_logps,\n",
    "            ref_chosen_logps, ref_rejected_logps,\n",
    "            beta=self.beta\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Compute metrics\n",
    "        with torch.no_grad():\n",
    "            reward_margin = (chosen_rewards - rejected_rewards).mean().item()\n",
    "            accuracy = (chosen_rewards > rejected_rewards).float().mean().item()\n",
    "        \n",
    "        return loss.item(), {\n",
    "            'reward_margin': reward_margin,\n",
    "            'accuracy': accuracy,\n",
    "            'chosen_reward': chosen_rewards.mean().item(),\n",
    "            'rejected_reward': rejected_rewards.mean().item(),\n",
    "        }\n",
    "\n",
    "\n",
    "# Create simple model for demonstration\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size=1000, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(self.embedding(x))\n",
    "\n",
    "\n",
    "# Demonstrate training\n",
    "print(\"DPO TRAINING DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create models\n",
    "import copy\n",
    "policy = SimpleModel()\n",
    "reference = copy.deepcopy(policy)  # Frozen copy\n",
    "\n",
    "# Create trainer\n",
    "trainer = SimpleDPOTrainer(policy, reference, beta=0.1)\n",
    "\n",
    "# Generate synthetic data\n",
    "batch_size = 8\n",
    "seq_len = 20\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "print(\"\\nTraining for 100 steps...\")\n",
    "for step in range(100):\n",
    "    # Random preference pairs (in reality, from dataset)\n",
    "    chosen_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "    rejected_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "    \n",
    "    loss, metrics = trainer.train_step(chosen_ids, rejected_ids)\n",
    "    losses.append(loss)\n",
    "    accuracies.append(metrics['accuracy'])\n",
    "    \n",
    "    if (step + 1) % 20 == 0:\n",
    "        print(f\"  Step {step+1}: Loss={loss:.4f}, Accuracy={metrics['accuracy']:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Loss curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(losses, color='#f57c00', linewidth=2)\n",
    "ax1.set_xlabel('Training Step', fontsize=11)\n",
    "ax1.set_ylabel('DPO Loss', fontsize=11)\n",
    "ax1.set_title('DPO Training Loss', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Accuracy curve\n",
    "ax2 = axes[1]\n",
    "ax2.plot(accuracies, color='#4caf50', linewidth=2)\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', label='Random (50%)')\n",
    "ax2.set_xlabel('Training Step', fontsize=11)\n",
    "ax2.set_ylabel('Preference Accuracy', fontsize=11)\n",
    "ax2.set_title('DPO Preference Prediction Accuracy', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylim(0.3, 1.0)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDPO TRAINING CHARACTERISTICS:\")\n",
    "print(\"  â€¢ Stable loss decrease (no PPO instability!)\")\n",
    "print(\"  â€¢ Simple supervised learning dynamics\")\n",
    "print(\"  â€¢ Accuracy improves as model learns preferences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DPO vs RLHF: When to Use Which?\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              DPO vs RLHF COMPARISON                            â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  USE DPO WHEN:                                                â”‚\n",
    "    â”‚    âœ“ You have good preference data                           â”‚\n",
    "    â”‚    âœ“ Simplicity is important                                 â”‚\n",
    "    â”‚    âœ“ You want stable training                                â”‚\n",
    "    â”‚    âœ“ Compute is limited                                      â”‚\n",
    "    â”‚    âœ“ One-shot alignment (no iteration)                       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  USE RLHF (PPO) WHEN:                                         â”‚\n",
    "    â”‚    âœ“ You need online learning (iterative improvement)        â”‚\n",
    "    â”‚    âœ“ Reward model needs to score new generations             â”‚\n",
    "    â”‚    âœ“ More control over reward function                       â”‚\n",
    "    â”‚    âœ“ Complex reward shaping is needed                        â”‚\n",
    "    â”‚    âœ“ Multi-objective optimization                            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  CURRENT TREND (2024):                                        â”‚\n",
    "    â”‚    Many practitioners now START with DPO!                    â”‚\n",
    "    â”‚    Simpler, often as effective as RLHF.                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('DPO vs RLHF: Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# RLHF column\n",
    "rlhf_box = FancyBboxPatch((0.5, 1), 6, 7.5, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#fff3e0', edgecolor='#f57c00', linewidth=3)\n",
    "ax.add_patch(rlhf_box)\n",
    "ax.text(3.5, 8, 'RLHF (PPO)', ha='center', fontsize=14, fontweight='bold', color='#f57c00')\n",
    "\n",
    "rlhf_items = [\n",
    "    ('Stages', '3 (SFTâ†’RMâ†’PPO)'),\n",
    "    ('Complexity', 'High'),\n",
    "    ('Stability', 'Can be unstable'),\n",
    "    ('Online Learning', 'âœ“ Yes'),\n",
    "    ('Memory', 'Higher (4 models)'),\n",
    "    ('Flexibility', 'Very flexible'),\n",
    "]\n",
    "\n",
    "for i, (label, value) in enumerate(rlhf_items):\n",
    "    y = 7 - i * 0.9\n",
    "    ax.text(1, y, f'{label}:', fontsize=10, fontweight='bold')\n",
    "    ax.text(3.2, y, value, fontsize=10)\n",
    "\n",
    "# DPO column\n",
    "dpo_box = FancyBboxPatch((7.5, 1), 6, 7.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(dpo_box)\n",
    "ax.text(10.5, 8, 'DPO', ha='center', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "dpo_items = [\n",
    "    ('Stages', '1 (Direct!)'),\n",
    "    ('Complexity', 'Low'),\n",
    "    ('Stability', 'Very stable'),\n",
    "    ('Online Learning', 'âœ— Offline only'),\n",
    "    ('Memory', 'Lower (2 models)'),\n",
    "    ('Flexibility', 'Less flexible'),\n",
    "]\n",
    "\n",
    "for i, (label, value) in enumerate(dpo_items):\n",
    "    y = 7 - i * 0.9\n",
    "    ax.text(8, y, f'{label}:', fontsize=10, fontweight='bold')\n",
    "    ax.text(10.2, y, value, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\nDETAILED COMPARISON TABLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Aspect':<20} {'RLHF (PPO)':<20} {'DPO':<20}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "comparisons = [\n",
    "    ('Training Stages', '3 stages', '1 stage'),\n",
    "    ('Reward Model', 'Required', 'Not needed'),\n",
    "    ('Stability', 'Can be unstable', 'Very stable'),\n",
    "    ('Online Data', 'Supported', 'Not supported'),\n",
    "    ('Memory (GPUs)', '4 models', '2 models'),\n",
    "    ('Implementation', 'Complex', 'Simple'),\n",
    "    ('Flexibility', 'High', 'Medium'),\n",
    "    ('Performance', 'Excellent', 'Very good'),\n",
    "]\n",
    "\n",
    "for aspect, rlhf, dpo in comparisons:\n",
    "    print(f\"{aspect:<20} {rlhf:<20} {dpo:<20}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Other Alternatives to RLHF\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              OTHER ALIGNMENT METHODS                           â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  RLAIF (RL from AI Feedback):                                 â”‚\n",
    "    â”‚    Use AI to generate preferences instead of humans           â”‚\n",
    "    â”‚    â†’ Cheaper, faster, more scalable                          â”‚\n",
    "    â”‚    â†’ But may inherit AI biases                               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Constitutional AI (CAI):                                     â”‚\n",
    "    â”‚    Model critiques and revises its own outputs               â”‚\n",
    "    â”‚    Based on a set of principles/rules                        â”‚\n",
    "    â”‚    â†’ Self-improvement without human labels                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  ORPO (Odds Ratio Preference Optimization):                   â”‚\n",
    "    â”‚    Combines SFT and preference learning in one step          â”‚\n",
    "    â”‚    â†’ Even simpler than DPO                                   â”‚\n",
    "    â”‚    â†’ No need for reference model during training             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  IPO (Identity Preference Optimization):                      â”‚\n",
    "    â”‚    Fixes some theoretical issues with DPO                    â”‚\n",
    "    â”‚    â†’ Better calibration                                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  KTO (Kahneman-Tversky Optimization):                        â”‚\n",
    "    â”‚    Based on prospect theory from economics                   â”‚\n",
    "    â”‚    â†’ Can work with just thumbs up/down (no pairs!)          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the landscape of alignment methods\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('Landscape of LLM Alignment Methods', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Methods as boxes\n",
    "methods = [\n",
    "    (1, 7, 'RLHF\\n(PPO)', 'The OG', '#fff3e0', '#f57c00'),\n",
    "    (4, 7, 'DPO', 'Simple & Effective', '#c8e6c9', '#388e3c'),\n",
    "    (7, 7, 'ORPO', 'Combined SFT+DPO', '#bbdefb', '#1976d2'),\n",
    "    (10, 7, 'KTO', 'No pairs needed', '#e1bee7', '#7b1fa2'),\n",
    "    (2.5, 4, 'RLAIF', 'AI as annotator', '#ffcdd2', '#d32f2f'),\n",
    "    (5.5, 4, 'Constitutional\\nAI', 'Self-critique', '#b2dfdb', '#00796b'),\n",
    "    (8.5, 4, 'IPO', 'Better calibration', '#fff9c4', '#fbc02d'),\n",
    "]\n",
    "\n",
    "for x, y, name, desc, fcolor, ecolor in methods:\n",
    "    box = FancyBboxPatch((x, y), 2.5, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=fcolor, edgecolor=ecolor, linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 1.25, y + 1.4, name, ha='center', fontsize=9, fontweight='bold')\n",
    "    ax.text(x + 1.25, y + 0.4, desc, ha='center', fontsize=8, color='#666')\n",
    "\n",
    "# Timeline arrow\n",
    "ax.annotate('', xy=(12, 1.5), xytext=(1, 1.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#333'))\n",
    "ax.text(6.5, 0.8, 'Trend: Simpler â†’ More efficient', ha='center', fontsize=11, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY TRENDS IN LLM ALIGNMENT:\")\n",
    "print(\"  1. Moving away from complex RL (PPO)\")\n",
    "print(\"  2. Preference learning without reward models\")\n",
    "print(\"  3. Using AI to generate training signal\")\n",
    "print(\"  4. Simpler methods that work just as well\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using TRL's DPOTrainer\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              TRL DPOTrainer                                    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TRL provides a production-ready DPOTrainer!                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  KEY FEATURES:                                                â”‚\n",
    "    â”‚    â€¢ Handles reference model automatically                   â”‚\n",
    "    â”‚    â€¢ Supports LoRA/QLoRA for efficient training              â”‚\n",
    "    â”‚    â€¢ Distributed training support                            â”‚\n",
    "    â”‚    â€¢ Logging and evaluation built-in                         â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  DATA FORMAT:                                                 â”‚\n",
    "    â”‚    {                                                          â”‚\n",
    "    â”‚      \"prompt\": \"Question here\",                              â”‚\n",
    "    â”‚      \"chosen\": \"Good answer\",                                â”‚\n",
    "    â”‚      \"rejected\": \"Bad answer\"                                â”‚\n",
    "    â”‚    }                                                          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TRL availability\n",
    "try:\n",
    "    from trl import DPOTrainer, DPOConfig\n",
    "    TRL_AVAILABLE = True\n",
    "    print(\"âœ“ TRL is installed!\")\n",
    "except ImportError:\n",
    "    TRL_AVAILABLE = False\n",
    "    print(\"âœ— TRL not installed.\")\n",
    "    print(\"  Install with: pip install trl transformers\")\n",
    "\n",
    "# Show example code\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRL DPOTrainer EXAMPLE CODE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "example_code = '''\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load model and tokenizer\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Load preference dataset\n",
    "# Format: {\"prompt\": ..., \"chosen\": ..., \"rejected\": ...}\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:1000]\")\n",
    "\n",
    "# 3. Configure LoRA for efficient training\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "# 4. Configure DPO training\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"dpo_model\",\n",
    "    beta=0.1,                        # KL penalty coefficient\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    max_length=512,\n",
    "    max_prompt_length=256,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# 5. Create trainer (reference model created automatically!)\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,  # Optional: use LoRA\n",
    ")\n",
    "\n",
    "# 6. Train!\n",
    "trainer.train()\n",
    "\n",
    "# 7. Save\n",
    "trainer.save_model(\"dpo_model_final\")\n",
    "'''\n",
    "\n",
    "print(example_code)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### DPO Key Formula\n",
    "\n",
    "```\n",
    "L_DPO = -log Ïƒ(Î² Ã— [log Ï€(y_w)/Ï€_ref(y_w) - log Ï€(y_l)/Ï€_ref(y_l)])\n",
    "```\n",
    "\n",
    "### DPO vs RLHF\n",
    "\n",
    "| Aspect | DPO | RLHF (PPO) |\n",
    "|--------|-----|------------|\n",
    "| Stages | 1 | 3 |\n",
    "| Reward Model | Not needed | Required |\n",
    "| Training | Supervised | Reinforcement |\n",
    "| Stability | High | Can be unstable |\n",
    "| Online Learning | No | Yes |\n",
    "\n",
    "### Other Methods\n",
    "\n",
    "| Method | Key Feature |\n",
    "|--------|-------------|\n",
    "| RLAIF | AI generates preferences |\n",
    "| Constitutional AI | Self-critique |\n",
    "| ORPO | Combined SFT + DPO |\n",
    "| KTO | No pairs needed |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What is the key mathematical insight behind DPO?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The optimal policy under the RLHF objective has a closed-form solution:\n",
    "Ï€*(y|x) âˆ Ï€_ref(y|x) Ã— exp(r(x,y)/Î²)\n",
    "\n",
    "By rearranging this, we can express the reward in terms of log probabilities. When comparing pairs, the intractable normalization constant Z(x) cancels out, allowing direct training on preferences without learning a reward model!\n",
    "</details>\n",
    "\n",
    "**2. Why doesn't DPO need a reward model?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "DPO learns the \"implicit reward\" directly from the log probability ratios:\n",
    "r(x,y) = Î² Ã— log(Ï€(y|x)/Ï€_ref(y|x))\n",
    "\n",
    "Instead of training a separate model to predict rewards, DPO learns a policy where the probability ratios encode the reward signal. The preference data directly guides policy optimization.\n",
    "</details>\n",
    "\n",
    "**3. When should you choose RLHF over DPO?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Choose RLHF when:\n",
    "- You need online learning (generate, collect feedback, iterate)\n",
    "- You want to use the reward model for other purposes\n",
    "- You need complex reward shaping or multi-objective optimization\n",
    "- You want maximum flexibility in the reward function\n",
    "\n",
    "DPO is offline-only - it can't improve based on new generations.\n",
    "</details>\n",
    "\n",
    "**4. What does Î² control in DPO?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Î² is the temperature/KL penalty coefficient:\n",
    "- Higher Î²: Stronger signal from preferences, more aggressive changes\n",
    "- Lower Î²: Weaker signal, stay closer to reference model\n",
    "\n",
    "It's the same Î² as in RLHF! It controls how much the model can deviate from the reference policy.\n",
    "</details>\n",
    "\n",
    "**5. What is RLAIF and why is it useful?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "RLAIF (RL from AI Feedback) uses an AI model to generate preferences instead of humans:\n",
    "- Much cheaper and faster than human annotation\n",
    "- Can generate millions of preference pairs\n",
    "- Scales easily\n",
    "\n",
    "Drawback: May inherit biases from the AI labeler. Works best when combined with some human oversight.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Now you understand DPO and alternatives! In the next notebook, we'll explore the **TRL Library** in depth for practical RLHF/DPO training.\n",
    "\n",
    "**Continue to:** [Notebook 5: TRL Library Tutorial](05_trl_library_tutorial.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*DPO: \"Skip the middleman, go straight to alignment!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
