{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Modeling: Teaching AI to Understand Human Preferences\n",
    "\n",
    "The reward model is the \"judge\" in RLHF - it learns to score responses based on human preferences.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The essay grader analogy: how reward models learn preferences\n",
    "- The Bradley-Terry model: math behind preference learning\n",
    "- Building a reward model from scratch\n",
    "- Common pitfalls and how to avoid them\n",
    "- Using TRL for reward model training\n",
    "\n",
    "**Prerequisites:** Notebook 1 (What is RLHF)\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Essay Grader Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE ESSAY GRADER ANALOGY                              │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Imagine training a teaching assistant to grade essays...     │\n",
    "    │                                                                │\n",
    "    │  THE PROBLEM:                                                 │\n",
    "    │    Professor can't grade 10,000 essays personally.            │\n",
    "    │    But they CAN compare pairs of essays:                      │\n",
    "    │      \"Essay A is better than Essay B\"                        │\n",
    "    │                                                                │\n",
    "    │  THE SOLUTION (Reward Model):                                 │\n",
    "    │    Train a TA to grade like the professor would!              │\n",
    "    │                                                                │\n",
    "    │    Step 1: Professor compares 1000 essay pairs               │\n",
    "    │    Step 2: TA learns from these comparisons                   │\n",
    "    │    Step 3: TA can now grade 10,000 essays automatically!      │\n",
    "    │                                                                │\n",
    "    │  IN RLHF:                                                     │\n",
    "    │    Professor = Human annotators                               │\n",
    "    │    Essays = LLM responses                                     │\n",
    "    │    TA = Reward Model                                         │\n",
    "    │                                                                │\n",
    "    │  The reward model learns: RM(prompt, response) → score       │\n",
    "    │  Higher score = More human-preferred response                 │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle\n",
    "\n",
    "# Visualize the reward model concept\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('Reward Model: Learning to Score Responses', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Input: Prompt + Response\n",
    "input_box = FancyBboxPatch((1, 5.5), 4, 3, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(input_box)\n",
    "ax.text(3, 7.8, 'INPUT', ha='center', fontsize=12, fontweight='bold', color='#1976d2')\n",
    "ax.text(3, 7, 'Prompt + Response', ha='center', fontsize=10)\n",
    "ax.text(3, 6.2, '\"How do I learn Python?\"', ha='center', fontsize=9, style='italic')\n",
    "ax.text(3, 5.8, '\"Start with basics...\"', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# Reward Model\n",
    "rm_box = FancyBboxPatch((6, 5), 3, 4, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#fff3e0', edgecolor='#f57c00', linewidth=3)\n",
    "ax.add_patch(rm_box)\n",
    "ax.text(7.5, 8.2, 'REWARD', ha='center', fontsize=12, fontweight='bold', color='#f57c00')\n",
    "ax.text(7.5, 7.6, 'MODEL', ha='center', fontsize=12, fontweight='bold', color='#f57c00')\n",
    "ax.text(7.5, 6.5, 'Transformer\\n+ Reward Head', ha='center', fontsize=10)\n",
    "ax.text(7.5, 5.5, 'Trained on human\\npreferences', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# Output: Score\n",
    "output_box = FancyBboxPatch((10.5, 6), 2.5, 2, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(output_box)\n",
    "ax.text(11.75, 7.5, 'OUTPUT', ha='center', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax.text(11.75, 6.8, 'Score: 7.3', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(11.75, 6.3, '(Scalar)', ha='center', fontsize=9)\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(5.9, 7), xytext=(5.1, 7),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#666'))\n",
    "ax.annotate('', xy=(10.4, 7), xytext=(9.1, 7),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#666'))\n",
    "\n",
    "# Scale explanation\n",
    "ax.text(7, 2.5, 'Score Interpretation:', ha='center', fontsize=11, fontweight='bold')\n",
    "scale_items = [\n",
    "    ('High (8-10)', '#4caf50', 'Excellent, preferred by humans'),\n",
    "    ('Medium (5-7)', '#ff9800', 'Acceptable, but could be better'),\n",
    "    ('Low (1-4)', '#f44336', 'Poor, humans would reject'),\n",
    "]\n",
    "for i, (label, color, desc) in enumerate(scale_items):\n",
    "    ax.text(4 + i*3.5, 1.5, label, ha='center', fontsize=10, fontweight='bold', color=color)\n",
    "    ax.text(4 + i*3.5, 1, desc, ha='center', fontsize=8, color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nREWARD MODEL SUMMARY:\")\n",
    "print(\"  Input: (prompt, response) pair\")\n",
    "print(\"  Output: Single scalar score\")\n",
    "print(\"  Training: Learn from human preference comparisons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Bradley-Terry Model: Learning from Comparisons\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              THE BRADLEY-TERRY MODEL                           │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  THE KEY INSIGHT:                                             │\n",
    "    │    We don't need ABSOLUTE scores, just RELATIVE preferences!  │\n",
    "    │                                                                │\n",
    "    │  PROBABILITY MODEL:                                           │\n",
    "    │    Given responses A and B, probability A is preferred:       │\n",
    "    │                                                                │\n",
    "    │    P(A > B) = exp(RM(A)) / (exp(RM(A)) + exp(RM(B)))          │\n",
    "    │             = σ(RM(A) - RM(B))                                 │\n",
    "    │                                                                │\n",
    "    │    where σ is the sigmoid function                            │\n",
    "    │                                                                │\n",
    "    │  TRAINING LOSS:                                               │\n",
    "    │    We have pairs where humans chose A over B                  │\n",
    "    │    We want to maximize P(A > B)                               │\n",
    "    │                                                                │\n",
    "    │    Loss = -log P(chosen > rejected)                           │\n",
    "    │         = -log σ(RM(chosen) - RM(rejected))                   │\n",
    "    │                                                                │\n",
    "    │  INTUITION:                                                   │\n",
    "    │    If RM(chosen) >> RM(rejected) → Loss is small             │\n",
    "    │    If RM(chosen) ≈ RM(rejected) → Loss is large              │\n",
    "    │    If RM(chosen) << RM(rejected) → Loss is very large!       │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the Bradley-Terry model\n",
    "\n",
    "def bradley_terry_prob(score_a, score_b):\n",
    "    \"\"\"\n",
    "    Probability that A is preferred over B.\n",
    "    P(A > B) = σ(score_A - score_B)\n",
    "    \"\"\"\n",
    "    return torch.sigmoid(score_a - score_b)\n",
    "\n",
    "\n",
    "def reward_model_loss(score_chosen, score_rejected):\n",
    "    \"\"\"\n",
    "    Bradley-Terry ranking loss.\n",
    "    \n",
    "    Loss = -log σ(score_chosen - score_rejected)\n",
    "    \n",
    "    Args:\n",
    "        score_chosen: RM score for human-preferred response\n",
    "        score_rejected: RM score for rejected response\n",
    "    \n",
    "    Returns:\n",
    "        Mean loss over the batch\n",
    "    \"\"\"\n",
    "    return -F.logsigmoid(score_chosen - score_rejected).mean()\n",
    "\n",
    "\n",
    "# Demonstrate with examples\n",
    "print(\"BRADLEY-TERRY MODEL DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scenarios = [\n",
    "    (\"RM clearly prefers chosen\", torch.tensor(8.0), torch.tensor(2.0)),\n",
    "    (\"RM slightly prefers chosen\", torch.tensor(5.5), torch.tensor(5.0)),\n",
    "    (\"RM is uncertain\", torch.tensor(5.0), torch.tensor(5.0)),\n",
    "    (\"RM prefers rejected (wrong!)\", torch.tensor(3.0), torch.tensor(7.0)),\n",
    "]\n",
    "\n",
    "print(\"\\nScenario Analysis:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Scenario':<35} {'Chosen':>8} {'Rejected':>8} {'P(C>R)':>8} {'Loss':>8}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for name, chosen, rejected in scenarios:\n",
    "    prob = bradley_terry_prob(chosen, rejected).item()\n",
    "    loss = reward_model_loss(chosen, rejected).item()\n",
    "    print(f\"{name:<35} {chosen.item():>8.1f} {rejected.item():>8.1f} {prob:>8.1%} {loss:>8.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHT: The loss is low when RM agrees with human preference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss function\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Probability curve\n",
    "ax1 = axes[0]\n",
    "score_diff = np.linspace(-6, 6, 100)\n",
    "prob = 1 / (1 + np.exp(-score_diff))  # Sigmoid\n",
    "\n",
    "ax1.plot(score_diff, prob, 'b-', linewidth=3)\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax1.fill_between(score_diff, prob, 0.5, where=(score_diff > 0), alpha=0.3, color='green', label='RM prefers chosen')\n",
    "ax1.fill_between(score_diff, prob, 0.5, where=(score_diff < 0), alpha=0.3, color='red', label='RM prefers rejected')\n",
    "\n",
    "ax1.set_xlabel('RM(chosen) - RM(rejected)', fontsize=11)\n",
    "ax1.set_ylabel('P(chosen > rejected)', fontsize=11)\n",
    "ax1.set_title('Bradley-Terry Probability', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Loss curve\n",
    "ax2 = axes[1]\n",
    "loss = -np.log(prob + 1e-10)  # -log(sigmoid(diff))\n",
    "\n",
    "ax2.plot(score_diff, loss, 'r-', linewidth=3)\n",
    "ax2.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Annotate regions\n",
    "ax2.annotate('Low loss\\n(RM correct)', xy=(3, 0.1), fontsize=10, ha='center', color='green')\n",
    "ax2.annotate('High loss\\n(RM wrong!)', xy=(-3, 3), fontsize=10, ha='center', color='red')\n",
    "\n",
    "ax2.set_xlabel('RM(chosen) - RM(rejected)', fontsize=11)\n",
    "ax2.set_ylabel('Loss', fontsize=11)\n",
    "ax2.set_title('Bradley-Terry Loss', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim(0, 6)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTRAINING OBJECTIVE:\")\n",
    "print(\"  The RM learns to make score(chosen) > score(rejected)\")\n",
    "print(\"  Larger margin = lower loss = more confident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building a Reward Model from Scratch\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              REWARD MODEL ARCHITECTURE                         │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  STRUCTURE:                                                    │\n",
    "    │    RM = Transformer Backbone + Reward Head                    │\n",
    "    │                                                                │\n",
    "    │    ┌───────────────────────────┐                              │\n",
    "    │    │  [CLS] token embedding    │                              │\n",
    "    │    └───────────┬───────────────┘                              │\n",
    "    │                │                                               │\n",
    "    │    ┌───────────▼───────────────┐                              │\n",
    "    │    │     Linear Layer          │  (hidden_dim → 1)            │\n",
    "    │    └───────────┬───────────────┘                              │\n",
    "    │                │                                               │\n",
    "    │    ┌───────────▼───────────────┐                              │\n",
    "    │    │     Scalar Output         │  (no activation!)            │\n",
    "    │    └───────────────────────────┘                              │\n",
    "    │                                                                │\n",
    "    │  OPTIONS FOR BASE MODEL:                                      │\n",
    "    │    • Same as SFT model (common)                               │\n",
    "    │    • Smaller model (faster)                                   │\n",
    "    │    • Initialized from SFT checkpoint (better!)                │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified Reward Model for demonstration.\n",
    "    \n",
    "    In practice, this would be a transformer model.\n",
    "    Here we use a simple MLP to show the concept.\n",
    "    \n",
    "    Architecture:\n",
    "        Input embedding → Hidden layers → Scalar output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # \"Backbone\" - in practice, this is a transformer\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Reward head - outputs scalar\n",
    "        self.reward_head = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: embedding → scalar reward.\n",
    "        \n",
    "        Args:\n",
    "            x: Input embedding (batch_size, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Scalar reward for each input (batch_size, 1)\n",
    "        \"\"\"\n",
    "        features = self.backbone(x)\n",
    "        reward = self.reward_head(features)\n",
    "        return reward\n",
    "\n",
    "\n",
    "# Create and inspect the model\n",
    "print(\"SIMPLE REWARD MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rm = SimpleRewardModel(input_dim=64, hidden_dim=128)\n",
    "print(rm)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in rm.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(4, 64)  # Batch of 4 embeddings\n",
    "rewards = rm(test_input)\n",
    "print(f\"\\nInput shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {rewards.shape}\")\n",
    "print(f\"Sample rewards: {rewards.squeeze().tolist()}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reward_model(rm, preference_data, epochs=100, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train the reward model on preference data.\n",
    "    \n",
    "    Args:\n",
    "        rm: Reward model\n",
    "        preference_data: Dict with 'chosen' and 'rejected' embeddings\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "        Training history (losses, accuracies)\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(rm.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    chosen = preference_data['chosen']\n",
    "    rejected = preference_data['rejected']\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get rewards for both\n",
    "        reward_chosen = rm(chosen)\n",
    "        reward_rejected = rm(rejected)\n",
    "        \n",
    "        # Bradley-Terry loss\n",
    "        loss = -F.logsigmoid(reward_chosen - reward_rejected).mean()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        accuracy = (reward_chosen > reward_rejected).float().mean().item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    return {'losses': losses, 'accuracies': accuracies}\n",
    "\n",
    "\n",
    "# Create synthetic preference data\n",
    "print(\"TRAINING REWARD MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate data where \"chosen\" has a clear pattern\n",
    "n_samples = 500\n",
    "np.random.seed(42)\n",
    "\n",
    "# Chosen responses have higher values in first few dimensions\n",
    "chosen_embeddings = torch.randn(n_samples, 64)\n",
    "chosen_embeddings[:, :5] += 2  # Add \"quality signal\"\n",
    "\n",
    "# Rejected responses have lower values\n",
    "rejected_embeddings = torch.randn(n_samples, 64)\n",
    "rejected_embeddings[:, :5] -= 1  # Lower quality\n",
    "\n",
    "preference_data = {\n",
    "    'chosen': chosen_embeddings,\n",
    "    'rejected': rejected_embeddings\n",
    "}\n",
    "\n",
    "print(f\"Training samples: {n_samples}\")\n",
    "print(f\"Embedding dimension: 64\")\n",
    "print(\"\\nTraining...\")\n",
    "\n",
    "# Train\n",
    "rm = SimpleRewardModel(input_dim=64, hidden_dim=128)\n",
    "history = train_reward_model(rm, preference_data, epochs=200, lr=0.01)\n",
    "\n",
    "print(f\"\\nFinal loss: {history['losses'][-1]:.4f}\")\n",
    "print(f\"Final accuracy: {history['accuracies'][-1]:.1%}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Loss curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(history['losses'], color='#f57c00', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=11)\n",
    "ax1.set_ylabel('Loss', fontsize=11)\n",
    "ax1.set_title('Reward Model Training Loss', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Accuracy curve\n",
    "ax2 = axes[1]\n",
    "ax2.plot(history['accuracies'], color='#4caf50', linewidth=2)\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', label='Random (50%)')\n",
    "ax2.set_xlabel('Epoch', fontsize=11)\n",
    "ax2.set_ylabel('Accuracy', fontsize=11)\n",
    "ax2.set_title('Preference Prediction Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim(0.4, 1.0)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe reward model learned to predict human preferences!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned reward distribution\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "with torch.no_grad():\n",
    "    chosen_rewards = rm(chosen_embeddings).numpy().flatten()\n",
    "    rejected_rewards = rm(rejected_embeddings).numpy().flatten()\n",
    "\n",
    "ax.hist(chosen_rewards, bins=30, alpha=0.7, label='Chosen (Preferred)', color='#4caf50', density=True)\n",
    "ax.hist(rejected_rewards, bins=30, alpha=0.7, label='Rejected', color='#f44336', density=True)\n",
    "\n",
    "ax.axvline(x=np.mean(chosen_rewards), color='#388e3c', linestyle='--', linewidth=2, \n",
    "           label=f'Chosen mean: {np.mean(chosen_rewards):.2f}')\n",
    "ax.axvline(x=np.mean(rejected_rewards), color='#d32f2f', linestyle='--', linewidth=2,\n",
    "           label=f'Rejected mean: {np.mean(rejected_rewards):.2f}')\n",
    "\n",
    "ax.set_xlabel('Reward Score', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Learned Reward Score Distribution', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe distributions are well separated!\")\n",
    "print(f\"Mean difference: {np.mean(chosen_rewards) - np.mean(rejected_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Common Pitfalls in Reward Modeling\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              REWARD MODEL PITFALLS                             │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  1. REWARD HACKING                                            │\n",
    "    │     RM learns spurious correlations instead of true quality   │\n",
    "    │     Example: Longer responses get higher scores               │\n",
    "    │     Solution: Length normalization, diverse training data     │\n",
    "    │                                                                │\n",
    "    │  2. DISTRIBUTION SHIFT                                        │\n",
    "    │     RM trained on SFT outputs, but PPO produces different    │\n",
    "    │     Example: RM never saw adversarial outputs                │\n",
    "    │     Solution: Iterative training, online data collection     │\n",
    "    │                                                                │\n",
    "    │  3. ANNOTATOR DISAGREEMENT                                    │\n",
    "    │     Humans don't always agree on preferences                 │\n",
    "    │     Example: Humor is subjective                             │\n",
    "    │     Solution: Multiple annotators, uncertainty modeling      │\n",
    "    │                                                                │\n",
    "    │  4. OVERCONFIDENCE                                           │\n",
    "    │     RM gives high scores to out-of-distribution inputs       │\n",
    "    │     Solution: Ensemble RMs, calibration                      │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate length bias (common pitfall)\n",
    "\n",
    "print(\"PITFALL: LENGTH BIAS DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simulate: longer responses get higher scores (BAD!)\n",
    "def biased_scoring(responses):\n",
    "    \"\"\"A biased scoring function that prefers longer responses.\"\"\"\n",
    "    return [len(r) * 0.1 for r in responses]\n",
    "\n",
    "responses = [\n",
    "    \"Paris.\",  # Correct but short\n",
    "    \"The capital of France is Paris.\",  # Good response\n",
    "    \"The capital of France is Paris. Paris is known as the City of Light and has many famous landmarks including the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\",  # Verbose\n",
    "    \"I'm not entirely sure but I think maybe possibly it could be Paris or perhaps some other city I don't really know for certain...\",  # Verbose but BAD\n",
    "]\n",
    "\n",
    "biased_scores = biased_scoring(responses)\n",
    "\n",
    "print(\"\\nResponses scored by length-biased RM:\")\n",
    "print(\"-\"*60)\n",
    "for i, (resp, score) in enumerate(zip(responses, biased_scores)):\n",
    "    print(f\"Response {i+1} (len={len(resp):3d}): Score = {score:.1f}\")\n",
    "    print(f\"  '{resp[:60]}...'\" if len(resp) > 60 else f\"  '{resp}'\")\n",
    "    print()\n",
    "\n",
    "print(\"PROBLEM: Response 4 is BAD but gets highest score!\")\n",
    "print(\"SOLUTION: Normalize by length, train with diverse data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using TRL for Reward Model Training\n",
    "\n",
    "In practice, use the TRL library from Hugging Face!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TRL availability and show example\n",
    "try:\n",
    "    from trl import RewardTrainer, RewardConfig\n",
    "    TRL_AVAILABLE = True\n",
    "    print(\"✓ TRL is installed!\")\n",
    "except ImportError:\n",
    "    TRL_AVAILABLE = False\n",
    "    print(\"✗ TRL not installed.\")\n",
    "    print(\"  Install with: pip install trl transformers\")\n",
    "\n",
    "# Show example code\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRL REWARD MODEL TRAINING EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "example_code = '''\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load base model (often same architecture as SFT model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    num_labels=1  # Single scalar output\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load preference dataset\n",
    "# Format: {\"chosen\": \"good response\", \"rejected\": \"bad response\"}\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:1000]\")\n",
    "\n",
    "# Configure training\n",
    "training_args = RewardConfig(\n",
    "    output_dir=\"reward_model\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-5,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "# Save\n",
    "trainer.save_model(\"reward_model_final\")\n",
    "'''\n",
    "\n",
    "print(example_code)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### Reward Model Concept\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Input** | (prompt, response) pair |\n",
    "| **Output** | Scalar reward score |\n",
    "| **Training** | Bradley-Terry loss on preferences |\n",
    "\n",
    "### Bradley-Terry Model\n",
    "\n",
    "```\n",
    "P(A > B) = σ(RM(A) - RM(B))\n",
    "\n",
    "Loss = -log σ(RM(chosen) - RM(rejected))\n",
    "```\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "| Pitfall | Solution |\n",
    "|---------|----------|\n",
    "| Length bias | Normalize, diverse data |\n",
    "| Distribution shift | Iterative training |\n",
    "| Annotator disagreement | Multiple annotators |\n",
    "| Overconfidence | Ensemble models |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What is the Bradley-Terry model?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Bradley-Terry is a probability model for pairwise comparisons. It models the probability that response A is preferred over B as:\n",
    "\n",
    "P(A > B) = σ(RM(A) - RM(B))\n",
    "\n",
    "where σ is the sigmoid function. The reward model is trained to maximize this probability for human-chosen responses.\n",
    "</details>\n",
    "\n",
    "**2. Why do we train on comparisons instead of absolute scores?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Comparisons are easier for humans to provide! Saying \"A is better than B\" is more natural than assigning a score like \"7.3\". Also, comparison data is more consistent across annotators and less prone to calibration issues.\n",
    "</details>\n",
    "\n",
    "**3. What is reward hacking and how do we prevent it?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Reward hacking is when the RL policy exploits weaknesses in the reward model to get high scores without actually being helpful. For example, if the RM prefers longer responses, the policy might generate verbose nonsense.\n",
    "\n",
    "Prevention: Length normalization, diverse training data, KL penalty during PPO, iterative reward model updates.\n",
    "</details>\n",
    "\n",
    "**4. What architecture is typically used for reward models?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Reward models typically use the same transformer architecture as the language model being aligned, but with a reward head (linear layer) that outputs a scalar instead of vocabulary logits. Often initialized from the SFT checkpoint for better starting point.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Now that you understand reward modeling, let's see how to use it with PPO!\n",
    "\n",
    "**Continue to:** [Notebook 3: PPO for Language Models](03_ppo_for_language_models.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*The reward model is the judge - it learns to distinguish good from bad responses!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
