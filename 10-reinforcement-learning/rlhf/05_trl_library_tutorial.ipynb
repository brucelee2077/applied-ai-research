{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRL Library Tutorial: Your Complete Toolkit for LLM Alignment\n",
    "\n",
    "TRL is like a Swiss Army knife for RLHF - everything you need in one library!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The toolbox analogy: what TRL provides and why\n",
    "- SFTTrainer: Supervised fine-tuning made easy\n",
    "- RewardTrainer: Training reward models\n",
    "- PPOTrainer: Full RLHF training\n",
    "- DPOTrainer: The simpler alternative\n",
    "- Practical tips for each trainer\n",
    "\n",
    "**Prerequisites:** Notebooks 1-4 (RLHF basics through DPO)\n",
    "\n",
    "**Time:** ~35 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Toolbox Analogy\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          THE TRL TOOLBOX ANALOGY                               â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Imagine building a house...                                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WITHOUT TRL (manual construction):                           â”‚\n",
    "    â”‚    - Cut each board by hand                                   â”‚\n",
    "    â”‚    - Make your own nails                                      â”‚\n",
    "    â”‚    - Design each joint from scratch                           â”‚\n",
    "    â”‚    â†’ Takes months, easy to make mistakes!                     â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WITH TRL (professional toolkit):                             â”‚\n",
    "    â”‚    ðŸ”§ SFTTrainer = Foundation (instruction tuning)            â”‚\n",
    "    â”‚    ðŸ”§ RewardTrainer = Measuring stick (what's good?)          â”‚\n",
    "    â”‚    ðŸ”§ PPOTrainer = Power tools (optimize with RL)             â”‚\n",
    "    â”‚    ðŸ”§ DPOTrainer = Quick assembly (direct training)           â”‚\n",
    "    â”‚    â†’ Professional results in days!                            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TRL PHILOSOPHY:                                              â”‚\n",
    "    â”‚    \"Don't reinvent the wheel - use proven tools!\"            â”‚\n",
    "    â”‚    \"Same patterns that trained ChatGPT, Claude, etc.\"        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, FancyArrowPatch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check TRL availability\n",
    "TRL_AVAILABLE = False\n",
    "try:\n",
    "    import trl\n",
    "    from trl import SFTTrainer, SFTConfig, DPOTrainer, DPOConfig\n",
    "    TRL_AVAILABLE = True\n",
    "    print(f\"âœ“ TRL version {trl.__version__} is installed!\")\n",
    "except ImportError:\n",
    "    print(\"âœ— TRL not installed.\")\n",
    "    print(\"  Install with: pip install trl transformers peft accelerate\")\n",
    "\n",
    "# Check transformers\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"âœ“ Transformers version {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âœ— Transformers not installed.\")\n",
    "\n",
    "# Check PEFT\n",
    "try:\n",
    "    import peft\n",
    "    print(f\"âœ“ PEFT version {peft.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âœ— PEFT not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TRL components\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('TRL Library: Your Complete Alignment Toolkit', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Main TRL box\n",
    "trl_box = FancyBboxPatch((1, 1), 12, 9.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#f5f5f5', edgecolor='#333', linewidth=3)\n",
    "ax.add_patch(trl_box)\n",
    "ax.text(7, 10, 'TRL (Transformer Reinforcement Learning)', ha='center', \n",
    "        fontsize=14, fontweight='bold')\n",
    "\n",
    "# Component boxes\n",
    "components = [\n",
    "    (1.5, 7, 'SFTTrainer', 'Supervised\\nFine-Tuning', '#c8e6c9', '#388e3c',\n",
    "     'Foundation'),\n",
    "    (6, 7, 'RewardTrainer', 'Reward Model\\nTraining', '#fff3e0', '#f57c00',\n",
    "     'Scoring'),\n",
    "    (10.5, 7, 'PPOTrainer', 'PPO for\\nLanguage Models', '#e1bee7', '#7b1fa2',\n",
    "     'RLHF'),\n",
    "    (3.75, 3, 'DPOTrainer', 'Direct Preference\\nOptimization', '#bbdefb', '#1976d2',\n",
    "     'Simpler'),\n",
    "    (8.25, 3, 'ORPOTrainer', 'Odds Ratio\\nPreference', '#ffcdd2', '#d32f2f',\n",
    "     'Combined'),\n",
    "]\n",
    "\n",
    "for x, y, name, desc, fcolor, ecolor, label in components:\n",
    "    box = FancyBboxPatch((x, y), 3, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=fcolor, edgecolor=ecolor, linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 1.5, y + 2, name, ha='center', fontsize=10, fontweight='bold', color=ecolor)\n",
    "    ax.text(x + 1.5, y + 1, desc, ha='center', fontsize=9)\n",
    "    ax.text(x + 1.5, y + 0.3, f'({label})', ha='center', fontsize=8, style='italic', color='#666')\n",
    "\n",
    "# Flow arrows showing typical pipeline\n",
    "ax.annotate('', xy=(5.9, 8.25), xytext=(4.6, 8.25),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(10.4, 8.25), xytext=(9.1, 8.25),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# OR arrow to DPO\n",
    "ax.annotate('', xy=(5.25, 5.6), xytext=(3, 6.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#1976d2', linestyle='--'))\n",
    "ax.text(3.5, 6.5, 'OR', fontsize=10, color='#1976d2', fontweight='bold')\n",
    "\n",
    "# Pipeline labels\n",
    "ax.text(7.5, 9.2, 'Traditional RLHF Pipeline â†’', ha='center', fontsize=10, color='#666')\n",
    "ax.text(5.25, 6.3, 'Direct Path (skip RM!)', ha='center', fontsize=9, color='#1976d2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTRL COMPONENT SUMMARY:\")\n",
    "print(\"  â€¢ SFTTrainer: Turn base model into instruction-following model\")\n",
    "print(\"  â€¢ RewardTrainer: Train a model to score response quality\")\n",
    "print(\"  â€¢ PPOTrainer: Optimize with reinforcement learning\")\n",
    "print(\"  â€¢ DPOTrainer: Direct training on preferences (simpler!)\")\n",
    "print(\"  â€¢ ORPOTrainer: Combined SFT + preference in one step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TRL Data Formats: Understanding the Input\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              TRL DATA FORMATS                                  â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  SFTTrainer (instruction data):                               â”‚\n",
    "    â”‚    {                                                          â”‚\n",
    "    â”‚      \"text\": \"User: How do I...\\nAssistant: Here's how...\"   â”‚\n",
    "    â”‚    }                                                          â”‚\n",
    "    â”‚    OR with chat template:                                     â”‚\n",
    "    â”‚    {                                                          â”‚\n",
    "    â”‚      \"messages\": [                                           â”‚\n",
    "    â”‚        {\"role\": \"user\", \"content\": \"How do I...\"},          â”‚\n",
    "    â”‚        {\"role\": \"assistant\", \"content\": \"Here's how...\"}    â”‚\n",
    "    â”‚      ]                                                        â”‚\n",
    "    â”‚    }                                                          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  RewardTrainer (preference pairs):                            â”‚\n",
    "    â”‚    {                                                          â”‚\n",
    "    â”‚      \"chosen\": \"Good response text\",                        â”‚\n",
    "    â”‚      \"rejected\": \"Bad response text\"                        â”‚\n",
    "    â”‚    }                                                          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  DPOTrainer (preference pairs with prompt):                   â”‚\n",
    "    â”‚    {                                                          â”‚\n",
    "    â”‚      \"prompt\": \"User question\",                             â”‚\n",
    "    â”‚      \"chosen\": \"Preferred response\",                        â”‚\n",
    "    â”‚      \"rejected\": \"Non-preferred response\"                   â”‚\n",
    "    â”‚    }                                                          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate data formats\n",
    "\n",
    "print(\"TRL DATA FORMAT EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# SFT format (text)\n",
    "sft_text_example = {\n",
    "    \"text\": \"\"\"### Human: What is machine learning?\n",
    "\n",
    "### Assistant: Machine learning is a subset of artificial intelligence where computers learn patterns from data rather than being explicitly programmed. Instead of writing rules, we show the algorithm examples and it learns to make predictions or decisions.\"\"\"\n",
    "}\n",
    "\n",
    "print(\"\\n1. SFT FORMAT (text-based):\")\n",
    "print(\"-\"*40)\n",
    "print(sft_text_example[\"text\"][:200] + \"...\")\n",
    "\n",
    "# SFT format (messages)\n",
    "sft_messages_example = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of AI...\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\n2. SFT FORMAT (messages):\")\n",
    "print(\"-\"*40)\n",
    "for msg in sft_messages_example[\"messages\"]:\n",
    "    print(f\"  {msg['role']}: {msg['content'][:50]}...\")\n",
    "\n",
    "# Preference format\n",
    "preference_example = {\n",
    "    \"prompt\": \"Explain quantum computing in simple terms.\",\n",
    "    \"chosen\": \"Quantum computing uses quantum bits (qubits) that can be 0, 1, or both at once. This lets quantum computers solve certain problems much faster than regular computers.\",\n",
    "    \"rejected\": \"Quantum computing is very complicated and uses advanced physics. It's probably too technical for you to understand.\"\n",
    "}\n",
    "\n",
    "print(\"\\n3. PREFERENCE FORMAT (DPO/Reward):\")\n",
    "print(\"-\"*40)\n",
    "print(f\"  Prompt: {preference_example['prompt']}\")\n",
    "print(f\"  Chosen: {preference_example['chosen'][:60]}...\")\n",
    "print(f\"  Rejected: {preference_example['rejected'][:60]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Component 1: SFTTrainer - Building the Foundation\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              SFTTrainer: SUPERVISED FINE-TUNING                â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  PURPOSE:                                                     â”‚\n",
    "    â”‚    Transform base LLM â†’ Instruction-following model           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  ANALOGY: Teaching someone a new job                          â”‚\n",
    "    â”‚    Base model = Smart person who reads a lot                 â”‚\n",
    "    â”‚    SFT = Showing them examples of good work                  â”‚\n",
    "    â”‚    Result = Employee who knows how to do the job             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  KEY FEATURES:                                                â”‚\n",
    "    â”‚    â€¢ Automatic dataset formatting                            â”‚\n",
    "    â”‚    â€¢ LoRA/QLoRA support (efficient training)                 â”‚\n",
    "    â”‚    â€¢ Multi-GPU support                                       â”‚\n",
    "    â”‚    â€¢ Packing for efficient training                          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WHEN TO USE:                                                 â”‚\n",
    "    â”‚    â€¢ First step in any LLM training pipeline                 â”‚\n",
    "    â”‚    â€¢ Converting base model to chat model                     â”‚\n",
    "    â”‚    â€¢ Domain adaptation                                       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTTrainer example code\n",
    "\n",
    "print(\"SFTTrainer COMPLETE EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sft_code = '''\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load base model and tokenizer\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"  # Or any base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Load instruction dataset\n",
    "# Format: {\"text\": \"Human: ...\\\\nAssistant: ...\"}\n",
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")\n",
    "\n",
    "# 3. Configure LoRA for efficient training\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                         # Rank of update matrices\n",
    "    lora_alpha=32,                # Scaling factor\n",
    "    lora_dropout=0.05,            # Dropout for regularization\n",
    "    target_modules=[              # Which layers to adapt\n",
    "        \"q_proj\", \"v_proj\",       # Attention projections\n",
    "        \"k_proj\", \"o_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 4. Configure SFT training\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    # Sequence settings\n",
    "    max_seq_length=512,\n",
    "    packing=True,                  # Pack short examples together\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=100,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# 5. Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# 6. Train!\n",
    "trainer.train()\n",
    "\n",
    "# 7. Save the model\n",
    "trainer.save_model(\"./sft_final\")\n",
    "'''\n",
    "\n",
    "print(sft_code)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SFT process\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title('SFTTrainer: From Base Model to Instruction Model', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Base model\n",
    "base_box = FancyBboxPatch((0.5, 2.5), 3, 3, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "ax.add_patch(base_box)\n",
    "ax.text(2, 5, 'Base LLM', ha='center', fontsize=11, fontweight='bold', color='#1976d2')\n",
    "ax.text(2, 4, '(GPT-2, Llama, etc.)', ha='center', fontsize=9)\n",
    "ax.text(2, 3.3, 'Knows language,', ha='center', fontsize=9, color='#666')\n",
    "ax.text(2, 2.8, 'but not instructions', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Instruction data\n",
    "data_box = FancyBboxPatch((5, 5.5), 4, 2, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax.add_patch(data_box)\n",
    "ax.text(7, 6.8, 'Instruction Data', ha='center', fontsize=10, fontweight='bold', color='#f57c00')\n",
    "ax.text(7, 6, '(Question, Answer pairs)', ha='center', fontsize=9)\n",
    "\n",
    "# SFTTrainer\n",
    "sft_box = FancyBboxPatch((5, 2.5), 4, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(sft_box)\n",
    "ax.text(7, 4.5, 'SFTTrainer', ha='center', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax.text(7, 3.7, '+ LoRA', ha='center', fontsize=10)\n",
    "ax.text(7, 3, 'Supervised Learning', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Aligned model\n",
    "aligned_box = FancyBboxPatch((10.5, 2.5), 3, 3, boxstyle=\"round,pad=0.1\",\n",
    "                              facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax.add_patch(aligned_box)\n",
    "ax.text(12, 5, 'Instruction', ha='center', fontsize=11, fontweight='bold', color='#388e3c')\n",
    "ax.text(12, 4.5, 'Model', ha='center', fontsize=11, fontweight='bold', color='#388e3c')\n",
    "ax.text(12, 3.7, 'Follows instructions!', ha='center', fontsize=9, color='#666')\n",
    "ax.text(12, 3.2, 'Ready for RLHF', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(4.9, 4), xytext=(3.6, 4),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(7, 5.4), xytext=(7, 5.1),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#f57c00'))\n",
    "ax.annotate('', xy=(10.4, 4), xytext=(9.1, 4),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#388e3c'))\n",
    "\n",
    "# Key points\n",
    "ax.text(7, 0.8, 'Key: SFT teaches the model HOW to respond to instructions', \n",
    "        ha='center', fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Component 2: RewardTrainer - Training the Judge\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              RewardTrainer: REWARD MODEL TRAINING              â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  PURPOSE:                                                     â”‚\n",
    "    â”‚    Train a model to score response quality                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  ANALOGY: Training a judge for a contest                      â”‚\n",
    "    â”‚    Show pairs: \"This response is better than that one\"       â”‚\n",
    "    â”‚    Judge learns: What makes a response good?                 â”‚\n",
    "    â”‚    Result: Can score ANY new response                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  KEY FEATURES:                                                â”‚\n",
    "    â”‚    â€¢ Bradley-Terry loss (pairwise comparisons)               â”‚\n",
    "    â”‚    â€¢ Works with any transformer model                        â”‚\n",
    "    â”‚    â€¢ Outputs scalar reward per (prompt, response)            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WHEN TO USE:                                                 â”‚\n",
    "    â”‚    â€¢ RLHF with PPO (need reward model)                       â”‚\n",
    "    â”‚    â€¢ When you have preference data                           â”‚\n",
    "    â”‚    â€¢ NOT needed for DPO!                                     â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RewardTrainer example code\n",
    "\n",
    "print(\"RewardTrainer COMPLETE EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "reward_code = '''\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load model FOR SEQUENCE CLASSIFICATION (not causal LM!)\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=1,              # Single scalar output!\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Load preference dataset\n",
    "# Format: {\"chosen\": \"good response\", \"rejected\": \"bad response\"}\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:10000]\")\n",
    "\n",
    "# 3. Configure reward training\n",
    "training_args = RewardConfig(\n",
    "    output_dir=\"./reward_model\",\n",
    "    \n",
    "    # Training settings\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    # Sequence settings\n",
    "    max_length=512,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=1e-5,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# 4. Create trainer\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 5. Train!\n",
    "trainer.train()\n",
    "\n",
    "# 6. Save\n",
    "trainer.save_model(\"./reward_model_final\")\n",
    "\n",
    "# 7. Use the reward model\n",
    "def get_reward(prompt, response):\n",
    "    text = f\"{prompt}\\\\n{response}\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.logits.item()  # Scalar reward\n",
    "'''\n",
    "\n",
    "print(reward_code)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Component 3: PPOTrainer - Full RLHF\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              PPOTrainer: REINFORCEMENT LEARNING                â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  PURPOSE:                                                     â”‚\n",
    "    â”‚    Optimize language model using reward model feedback       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE LOOP:                                                    â”‚\n",
    "    â”‚    1. Generate responses with current policy                 â”‚\n",
    "    â”‚    2. Score responses with reward model                      â”‚\n",
    "    â”‚    3. Update policy with PPO to maximize reward              â”‚\n",
    "    â”‚    4. KL penalty keeps model from going crazy                â”‚\n",
    "    â”‚    5. Repeat!                                                â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  KEY FEATURES:                                                â”‚\n",
    "    â”‚    â€¢ Automatic value head management                         â”‚\n",
    "    â”‚    â€¢ KL penalty computation                                  â”‚\n",
    "    â”‚    â€¢ Advantage estimation                                    â”‚\n",
    "    â”‚    â€¢ Response generation                                     â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WHEN TO USE:                                                 â”‚\n",
    "    â”‚    â€¢ Online learning (generate â†’ feedback â†’ improve)         â”‚\n",
    "    â”‚    â€¢ When you need flexible reward shaping                   â”‚\n",
    "    â”‚    â€¢ When you have a trained reward model                    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPOTrainer example code\n",
    "\n",
    "print(\"PPOTrainer COMPLETE EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ppo_code = '''\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load model WITH VALUE HEAD (TRL special wrapper)\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"./sft_model\")\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"./sft_model\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./sft_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Load pre-trained reward model\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"./reward_model\")\n",
    "\n",
    "# 3. Configure PPO\n",
    "ppo_config = PPOConfig(\n",
    "    # Core PPO settings\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=64,\n",
    "    mini_batch_size=4,\n",
    "    ppo_epochs=4,\n",
    "    \n",
    "    # KL penalty settings\n",
    "    kl_penalty=\"kl\",\n",
    "    init_kl_coef=0.2,     # Î² coefficient\n",
    "    target_kl=6.0,        # Target KL divergence\n",
    "    \n",
    "    # Generation settings\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# 4. Create PPO trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model,\n",
    "    ref_model=ref_model,     # For KL penalty\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 5. Training loop\n",
    "for epoch in range(3):\n",
    "    for batch in tqdm(dataloader):\n",
    "        # Get prompts\n",
    "        query_tensors = [tokenizer.encode(q, return_tensors=\"pt\").squeeze() \n",
    "                        for q in batch[\"prompt\"]]\n",
    "        \n",
    "        # Generate responses with current policy\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            query_tensors,\n",
    "            return_prompt=False,\n",
    "        )\n",
    "        \n",
    "        # Decode responses\n",
    "        responses = [tokenizer.decode(r, skip_special_tokens=True) \n",
    "                    for r in response_tensors]\n",
    "        \n",
    "        # Get rewards from reward model\n",
    "        rewards = []\n",
    "        for prompt, response in zip(batch[\"prompt\"], responses):\n",
    "            text = f\"{prompt}\\\\n{response}\"\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "            with torch.no_grad():\n",
    "                score = reward_model(**inputs).logits.item()\n",
    "            rewards.append(torch.tensor(score))\n",
    "        \n",
    "        # PPO step - this does the magic!\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        \n",
    "        # Log\n",
    "        print(f\"Mean reward: {stats[\\'ppo/mean_scores\\']:.3f}\")\n",
    "        print(f\"KL divergence: {stats[\\'objective/kl\\']:.3f}\")\n",
    "\n",
    "# 6. Save final model\n",
    "model.save_pretrained(\"./ppo_aligned_model\")\n",
    "'''\n",
    "\n",
    "print(ppo_code)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PPO training loop\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('PPOTrainer: The RLHF Training Loop', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Central loop\n",
    "center_x, center_y = 7, 5\n",
    "\n",
    "# Steps around the loop\n",
    "steps = [\n",
    "    (2, 7, 'Generate', 'Create responses\\nwith current policy', '#c8e6c9', '#388e3c'),\n",
    "    (9, 7, 'Score', 'Reward model\\nrates responses', '#e1bee7', '#7b1fa2'),\n",
    "    (9, 2.5, 'Update', 'PPO optimizes\\npolicy weights', '#fff3e0', '#f57c00'),\n",
    "    (2, 2.5, 'Constrain', 'KL penalty\\nkeeps stability', '#ffcdd2', '#d32f2f'),\n",
    "]\n",
    "\n",
    "for x, y, title, desc, fcolor, ecolor in steps:\n",
    "    box = FancyBboxPatch((x, y), 3, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=fcolor, edgecolor=ecolor, linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 1.5, y + 1.5, title, ha='center', fontsize=11, fontweight='bold', color=ecolor)\n",
    "    ax.text(x + 1.5, y + 0.6, desc, ha='center', fontsize=9)\n",
    "\n",
    "# Arrows forming the loop\n",
    "ax.annotate('', xy=(8.9, 8), xytext=(5.1, 8),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(10.5, 7), xytext=(10.5, 4.6),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(5.1, 3.5), xytext=(8.9, 3.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(3.5, 4.6), xytext=(3.5, 7),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Central \"REPEAT\" text\n",
    "circle = Circle((7, 5), 1, facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "ax.add_patch(circle)\n",
    "ax.text(7, 5.2, 'REPEAT', ha='center', fontsize=10, fontweight='bold', color='#1976d2')\n",
    "ax.text(7, 4.7, '1000s of', ha='center', fontsize=8)\n",
    "ax.text(7, 4.3, 'iterations', ha='center', fontsize=8)\n",
    "\n",
    "# Models involved\n",
    "ax.text(3.5, 0.8, 'Models: Policy (being trained) + Reference (frozen) + Reward Model', \n",
    "        ha='center', fontsize=10, color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Component 4: DPOTrainer - The Simpler Path\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              DPOTrainer: DIRECT PREFERENCE OPTIMIZATION        â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  PURPOSE:                                                     â”‚\n",
    "    â”‚    Train on preferences WITHOUT a reward model!              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WHY DPO?                                                     â”‚\n",
    "    â”‚    â€¢ No reward model training needed                         â”‚\n",
    "    â”‚    â€¢ No complex PPO loop                                     â”‚\n",
    "    â”‚    â€¢ Just supervised learning with special loss              â”‚\n",
    "    â”‚    â€¢ Often works just as well as PPO!                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE LOSS:                                                    â”‚\n",
    "    â”‚    L = -log Ïƒ(Î² Ã— (log Ï€(y_w)/Ï€_ref(y_w)                     â”‚\n",
    "    â”‚                  - log Ï€(y_l)/Ï€_ref(y_l)))                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    \"Increase prob of chosen, decrease prob of rejected,      â”‚\n",
    "    â”‚     but don't drift too far from reference\"                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WHEN TO USE:                                                 â”‚\n",
    "    â”‚    â€¢ You have preference pairs                               â”‚\n",
    "    â”‚    â€¢ You want simplicity                                     â”‚\n",
    "    â”‚    â€¢ No need for online learning                             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPOTrainer example code\n",
    "\n",
    "print(\"DPOTrainer COMPLETE EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dpo_code = '''\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load SFT model (starting point)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./sft_model\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./sft_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Load preference dataset\n",
    "# Format: {\"prompt\": \"...\", \"chosen\": \"...\", \"rejected\": \"...\"}\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:5000]\")\n",
    "\n",
    "# 3. Optional: LoRA for efficient training\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 4. Configure DPO training\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"./dpo_output\",\n",
    "    \n",
    "    # DPO-specific settings\n",
    "    beta=0.1,                      # KL penalty coefficient\n",
    "    loss_type=\"sigmoid\",           # Standard DPO loss\n",
    "    \n",
    "    # Training settings\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    # Sequence settings  \n",
    "    max_length=512,\n",
    "    max_prompt_length=256,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=5e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# 5. Create DPO trainer\n",
    "# Reference model is created automatically from initial weights!\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,  # Optional: use LoRA\n",
    ")\n",
    "\n",
    "# 6. Train!\n",
    "trainer.train()\n",
    "\n",
    "# 7. Save\n",
    "trainer.save_model(\"./dpo_aligned_model\")\n",
    "'''\n",
    "\n",
    "print(dpo_code)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PPO vs DPO comparison\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "# Left: PPO pipeline\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 12)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('PPOTrainer Pipeline\\n(3 stages, complex)', fontsize=12, fontweight='bold', color='#f57c00')\n",
    "\n",
    "ppo_stages = [\n",
    "    (3.5, 9.5, 'SFT', 'SFTTrainer', '#c8e6c9', '#388e3c'),\n",
    "    (3.5, 6.5, 'Reward\\nModel', 'RewardTrainer', '#e1bee7', '#7b1fa2'),\n",
    "    (3.5, 3.5, 'PPO', 'PPOTrainer', '#fff3e0', '#f57c00'),\n",
    "    (3.5, 0.5, 'Aligned\\nModel', 'âœ“', '#bbdefb', '#1976d2'),\n",
    "]\n",
    "\n",
    "for x, y, title, subtitle, fcolor, ecolor in ppo_stages:\n",
    "    box = FancyBboxPatch((x, y), 3, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=fcolor, edgecolor=ecolor, linewidth=2)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(x + 1.5, y + 1.3, title, ha='center', fontsize=10, fontweight='bold')\n",
    "    ax1.text(x + 1.5, y + 0.5, subtitle, ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Arrows\n",
    "for y in [9.4, 6.4, 3.4]:\n",
    "    ax1.annotate('', xy=(5, y-0.5), xytext=(5, y),\n",
    "                 arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Complexity indicators\n",
    "ax1.text(8, 10.3, 'ðŸ˜Š', fontsize=16)\n",
    "ax1.text(8, 7.3, 'ðŸ˜“', fontsize=16)\n",
    "ax1.text(8, 4.3, 'ðŸ˜µ', fontsize=16)\n",
    "\n",
    "# Right: DPO pipeline\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 12)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('DPOTrainer Pipeline\\n(2 stages, simple!)', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "\n",
    "dpo_stages = [\n",
    "    (3.5, 8, 'SFT', 'SFTTrainer', '#c8e6c9', '#388e3c'),\n",
    "    (3.5, 4, 'DPO', 'DPOTrainer', '#bbdefb', '#1976d2'),\n",
    "    (3.5, 0.5, 'Aligned\\nModel', 'âœ“', '#c8e6c9', '#388e3c'),\n",
    "]\n",
    "\n",
    "for x, y, title, subtitle, fcolor, ecolor in dpo_stages:\n",
    "    box = FancyBboxPatch((x, y), 3, 2.5 if 'DPO' in title or 'SFT' in title else 2, \n",
    "                          boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=fcolor, edgecolor=ecolor, linewidth=2)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(x + 1.5, y + 1.5 if 'DPO' in title or 'SFT' in title else y + 1.3, \n",
    "             title, ha='center', fontsize=10, fontweight='bold')\n",
    "    ax2.text(x + 1.5, y + 0.7 if 'DPO' in title or 'SFT' in title else y + 0.5, \n",
    "             subtitle, ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Arrows\n",
    "ax2.annotate('', xy=(5, 6.9), xytext=(5, 7.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax2.annotate('', xy=(5, 3), xytext=(5, 3.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Happy faces\n",
    "ax2.text(8, 9, 'ðŸ˜Š', fontsize=16)\n",
    "ax2.text(8, 5, 'ðŸ˜Ž', fontsize=20)\n",
    "\n",
    "# Skip arrow\n",
    "ax2.text(8.5, 7, 'Skip\\nReward\\nModel!', fontsize=9, ha='center', color='#d32f2f', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDPO ADVANTAGES:\")\n",
    "print(\"  â€¢ No reward model training (saves time!)\")\n",
    "print(\"  â€¢ No complex PPO loop (easier to debug)\")\n",
    "print(\"  â€¢ Often matches PPO performance\")\n",
    "print(\"  â€¢ Recommended starting point for most projects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practical Tips for Each Trainer\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              PRACTICAL TIPS                                    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  SFTTrainer Tips:                                             â”‚\n",
    "    â”‚    â€¢ Use packing=True for efficiency                         â”‚\n",
    "    â”‚    â€¢ Start with small max_seq_length (256-512)               â”‚\n",
    "    â”‚    â€¢ Use LoRA unless you have lots of GPUs                   â”‚\n",
    "    â”‚    â€¢ Quality of data >> quantity of data                     â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  RewardTrainer Tips:                                          â”‚\n",
    "    â”‚    â€¢ Use same base model as your SFT model                   â”‚\n",
    "    â”‚    â€¢ 5K-10K preference pairs is often enough                 â”‚\n",
    "    â”‚    â€¢ Watch for length bias in rewards                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  PPOTrainer Tips:                                             â”‚\n",
    "    â”‚    â€¢ Start with default hyperparameters                      â”‚\n",
    "    â”‚    â€¢ Monitor KL divergence (should stay < 10)                â”‚\n",
    "    â”‚    â€¢ Use small batch sizes to start                          â”‚\n",
    "    â”‚    â€¢ Train for fewer steps than you think                    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  DPOTrainer Tips:                                             â”‚\n",
    "    â”‚    â€¢ beta=0.1 is a good default                              â”‚\n",
    "    â”‚    â€¢ Use LoRA for memory efficiency                          â”‚\n",
    "    â”‚    â€¢ Reference model created automatically                    â”‚\n",
    "    â”‚    â€¢ Often better to start here instead of PPO               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference: hyperparameters\n",
    "\n",
    "print(\"HYPERPARAMETER QUICK REFERENCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hyperparams = {\n",
    "    'SFTTrainer': {\n",
    "        'learning_rate': '2e-4 (with LoRA) / 2e-5 (full)',\n",
    "        'max_seq_length': '512-1024',\n",
    "        'batch_size': '4-8 per GPU',\n",
    "        'epochs': '1-3',\n",
    "        'packing': 'True (recommended)',\n",
    "    },\n",
    "    'RewardTrainer': {\n",
    "        'learning_rate': '1e-5',\n",
    "        'max_length': '512',\n",
    "        'batch_size': '4 per GPU',\n",
    "        'epochs': '1',\n",
    "    },\n",
    "    'PPOTrainer': {\n",
    "        'learning_rate': '1.41e-5',\n",
    "        'batch_size': '64-128',\n",
    "        'mini_batch_size': '4-8',\n",
    "        'ppo_epochs': '4',\n",
    "        'init_kl_coef': '0.2',\n",
    "        'target_kl': '6.0',\n",
    "    },\n",
    "    'DPOTrainer': {\n",
    "        'learning_rate': '5e-5 (with LoRA) / 5e-7 (full)',\n",
    "        'beta': '0.1',\n",
    "        'max_length': '512',\n",
    "        'max_prompt_length': '256',\n",
    "        'epochs': '1-3',\n",
    "    },\n",
    "}\n",
    "\n",
    "for trainer, params in hyperparams.items():\n",
    "    print(f\"\\n{trainer}:\")\n",
    "    for param, value in params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Common Data Sources\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              DATASETS FOR EACH STAGE                           â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  SFT (Instruction Data):                                      â”‚\n",
    "    â”‚    â€¢ timdettmers/openassistant-guanaco                       â”‚\n",
    "    â”‚    â€¢ databricks/databricks-dolly-15k                         â”‚\n",
    "    â”‚    â€¢ HuggingFaceH4/ultrachat_200k                            â”‚\n",
    "    â”‚    â€¢ tatsu-lab/alpaca                                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Preference Data (DPO/Reward):                                â”‚\n",
    "    â”‚    â€¢ Anthropic/hh-rlhf (human preferences)                   â”‚\n",
    "    â”‚    â€¢ HuggingFaceH4/ultrafeedback_binarized                   â”‚\n",
    "    â”‚    â€¢ argilla/ultrafeedback-binarized-preferences             â”‚\n",
    "    â”‚    â€¢ stanfordnlp/SHP (Stack Exchange preferences)            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TIP: Start small (1K-5K examples) to validate pipeline!     â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example of loading datasets\n",
    "\n",
    "print(\"LOADING DATASETS FOR TRL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "loading_code = '''\n",
    "from datasets import load_dataset\n",
    "\n",
    "# For SFT: Instruction-following data\n",
    "sft_dataset = load_dataset(\n",
    "    \"timdettmers/openassistant-guanaco\", \n",
    "    split=\"train\"\n",
    ")\n",
    "print(f\"SFT dataset: {len(sft_dataset)} examples\")\n",
    "print(f\"Columns: {sft_dataset.column_names}\")\n",
    "# Column: \"text\" with Human/Assistant conversation\n",
    "\n",
    "# For DPO/Reward: Preference data\n",
    "preference_dataset = load_dataset(\n",
    "    \"Anthropic/hh-rlhf\",\n",
    "    split=\"train[:5000]\"  # Start small!\n",
    ")\n",
    "print(f\"\\\\nPreference dataset: {len(preference_dataset)} examples\")\n",
    "print(f\"Columns: {preference_dataset.column_names}\")\n",
    "# Columns: \"chosen\", \"rejected\"\n",
    "\n",
    "# Convert to DPO format if needed (add \"prompt\" column)\n",
    "def extract_prompt(example):\n",
    "    \"\"\"Extract prompt from chosen response.\"\"\"\n",
    "    # hh-rlhf has full conversation in chosen/rejected\n",
    "    # Extract just the human turn as prompt\n",
    "    chosen = example[\"chosen\"]\n",
    "    # Find the assistant response start\n",
    "    if \"\\\\nAssistant:\" in chosen:\n",
    "        parts = chosen.split(\"\\\\nAssistant:\")\n",
    "        prompt = parts[0].replace(\"Human: \", \"\")\n",
    "        chosen_response = parts[1]\n",
    "    else:\n",
    "        prompt = chosen[:100]\n",
    "        chosen_response = chosen\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": chosen_response,\n",
    "        \"rejected\": example[\"rejected\"].split(\"\\\\nAssistant:\")[-1] if \"\\\\nAssistant:\" in example[\"rejected\"] else example[\"rejected\"]\n",
    "    }\n",
    "\n",
    "dpo_dataset = preference_dataset.map(extract_prompt)\n",
    "'''\n",
    "\n",
    "print(loading_code)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: TRL Quick Reference\n",
    "\n",
    "### Which Trainer to Use?\n",
    "\n",
    "| Trainer | Use When | Data Format |\n",
    "|---------|----------|-------------|\n",
    "| **SFTTrainer** | Starting with base model | `{\"text\": \"...\"}` or `{\"messages\": [...]}` |\n",
    "| **RewardTrainer** | Need explicit reward model | `{\"chosen\": \"...\", \"rejected\": \"...\"}` |\n",
    "| **PPOTrainer** | Online RLHF training | Prompts + Reward Model |\n",
    "| **DPOTrainer** | Simple preference learning | `{\"prompt\": \"...\", \"chosen\": \"...\", \"rejected\": \"...\"}` |\n",
    "\n",
    "### Recommended Pipeline\n",
    "\n",
    "```\n",
    "For most projects:  SFTTrainer â†’ DPOTrainer â†’ Done! âœ“\n",
    "\n",
    "For advanced use:   SFTTrainer â†’ RewardTrainer â†’ PPOTrainer\n",
    "```\n",
    "\n",
    "### Key Imports\n",
    "\n",
    "```python\n",
    "# Core trainers\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "# For PPO\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# Supporting libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What is the main difference between PPOTrainer and DPOTrainer?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "PPOTrainer requires a separate reward model and performs online RL training (generate â†’ score â†’ update). DPOTrainer skips the reward model entirely and directly trains on preference pairs using a special loss function. DPO is simpler and often just as effective!\n",
    "</details>\n",
    "\n",
    "**2. What data format does DPOTrainer expect?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "DPOTrainer expects preference pairs with three fields:\n",
    "- \"prompt\": The user question/input\n",
    "- \"chosen\": The preferred/better response\n",
    "- \"rejected\": The non-preferred/worse response\n",
    "</details>\n",
    "\n",
    "**3. Why is LoRA important for TRL training?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "LoRA (Low-Rank Adaptation) makes training much more efficient:\n",
    "- Uses ~10x less memory\n",
    "- Trains only ~0.1% of parameters\n",
    "- Enables training 7B+ models on consumer GPUs\n",
    "- Preserves most of the base model's knowledge\n",
    "</details>\n",
    "\n",
    "**4. What does the beta parameter control in DPO?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Beta (Î²) controls how strongly the model is penalized for deviating from the reference model:\n",
    "- Higher Î²: Stronger constraint, stays closer to reference\n",
    "- Lower Î²: Weaker constraint, more aggressive preference learning\n",
    "- Default 0.1 is a good starting point\n",
    "</details>\n",
    "\n",
    "**5. When would you use PPOTrainer instead of DPOTrainer?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Use PPOTrainer when:\n",
    "- You need online learning (continually generating and improving)\n",
    "- You want more control over the reward function\n",
    "- You have a pre-trained reward model you want to use\n",
    "- You need complex reward shaping (combining multiple signals)\n",
    "\n",
    "For most projects, DPOTrainer is simpler and sufficient!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Now you understand TRL's toolkit! In the next notebook, we'll put it all together with a complete end-to-end RLHF pipeline.\n",
    "\n",
    "**Continue to:** [Notebook 6: Full RLHF Pipeline](06_fine_tuning_with_rlhf.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*TRL: \"Don't reinvent the wheel - use the same tools that built ChatGPT!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
