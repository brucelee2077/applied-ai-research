{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is RLHF? The Secret Behind ChatGPT\n",
    "\n",
    "Welcome to RLHF - the breakthrough technique that transformed language models into helpful AI assistants like ChatGPT!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- Why raw language models need alignment (with a talented-but-clueless student analogy!)\n",
    "- The three-stage RLHF pipeline, step by step\n",
    "- How reward models learn human preferences\n",
    "- Why PPO is used for fine-tuning\n",
    "- The history from InstructGPT to ChatGPT\n",
    "- Code demonstrations of each concept!\n",
    "\n",
    "**Prerequisites:** Notebooks in `advanced-algorithms/` (PPO)\n",
    "\n",
    "**Time:** ~45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Brilliant But Clueless Student\n",
    "\n",
    "Imagine a student who has read EVERY book ever written:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │           THE ALIGNMENT PROBLEM: A STUDENT ANALOGY             │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  PRETRAINED LLM = Brilliant Student Who Read Everything       │\n",
    "    │                                                                │\n",
    "    │  KNOWS:                        DOESN'T KNOW:                  │\n",
    "    │  • All of Wikipedia            • What answer YOU want         │\n",
    "    │  • Every textbook              • How to be helpful            │\n",
    "    │  • All of Reddit               • When to refuse requests      │\n",
    "    │  • Code from GitHub            • How to be safe               │\n",
    "    │                                                                │\n",
    "    │  You ask: \"What's the capital of France?\"                     │\n",
    "    │                                                                │\n",
    "    │  HELPFUL response:   \"The capital of France is Paris.\"        │\n",
    "    │                                                                │\n",
    "    │  UNHELPFUL (but valid text completion):                       │\n",
    "    │    • \"What's the capital of Germany? Berlin...\" (off-topic)   │\n",
    "    │    • \"I'll tell you a story about France...\" (rambling)       │\n",
    "    │    • \"The capital of France is Paris. But did you know...\"    │\n",
    "    │      (overly verbose)                                         │\n",
    "    │                                                                │\n",
    "    │  THE PROBLEM: Knowing everything ≠ Knowing how to help        │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**RLHF is like hiring a tutor to teach this brilliant student HOW to use their knowledge helpfully!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch, Circle, Rectangle\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Visualize the alignment problem\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Before RLHF\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('BEFORE RLHF\\n(Pretrained LLM)', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# LLM box\n",
    "llm_box = FancyBboxPatch((2, 3), 6, 4, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=3)\n",
    "ax1.add_patch(llm_box)\n",
    "ax1.text(5, 5.5, 'Pretrained LLM', ha='center', fontsize=14, fontweight='bold')\n",
    "ax1.text(5, 4.5, '\"I know EVERYTHING!\"', ha='center', fontsize=11, style='italic')\n",
    "ax1.text(5, 3.5, '\"...but what do you want?\"', ha='center', fontsize=10, color='#666')\n",
    "\n",
    "# Outputs\n",
    "outputs = [\n",
    "    ('Helpful', '#c8e6c9', 0.2),\n",
    "    ('Harmful', '#ffcdd2', 0.3),\n",
    "    ('Off-topic', '#fff3e0', 0.3),\n",
    "    ('Rambling', '#e3f2fd', 0.2)\n",
    "]\n",
    "\n",
    "ax1.text(5, 8.5, 'Possible Outputs:', ha='center', fontsize=11)\n",
    "start_x = 1\n",
    "for label, color, width in outputs:\n",
    "    box = FancyBboxPatch((start_x, 7.5), width*10, 0.7, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor=color, edgecolor='gray', linewidth=1)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(start_x + width*5, 7.85, label, ha='center', fontsize=8)\n",
    "    start_x += width * 10 + 0.2\n",
    "\n",
    "# Right: After RLHF\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('AFTER RLHF\\n(Aligned LLM)', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# LLM box\n",
    "llm_box2 = FancyBboxPatch((2, 3), 6, 4, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax2.add_patch(llm_box2)\n",
    "ax2.text(5, 5.5, 'RLHF-Trained LLM', ha='center', fontsize=14, fontweight='bold')\n",
    "ax2.text(5, 4.5, '\"I know EVERYTHING!\"', ha='center', fontsize=11, style='italic')\n",
    "ax2.text(5, 3.5, '\"AND I know how to help!\"', ha='center', fontsize=10, color='#388e3c', fontweight='bold')\n",
    "\n",
    "# Outputs - mostly helpful now!\n",
    "outputs2 = [\n",
    "    ('Helpful', '#c8e6c9', 0.85),\n",
    "    ('Other', '#e0e0e0', 0.15)\n",
    "]\n",
    "\n",
    "ax2.text(5, 8.5, 'Possible Outputs:', ha='center', fontsize=11)\n",
    "start_x = 1\n",
    "for label, color, width in outputs2:\n",
    "    box = FancyBboxPatch((start_x, 7.5), width*10, 0.7, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor=color, edgecolor='gray', linewidth=1)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(start_x + width*5, 7.85, label, ha='center', fontsize=9, fontweight='bold')\n",
    "    start_x += width * 10 + 0.2\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THE ALIGNMENT PROBLEM\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Pretrained LLMs are trained on ONE objective:\n",
    "  → Predict the next token (word)\n",
    "\n",
    "But we ACTUALLY want:\n",
    "  → Helpful responses\n",
    "  → Honest information\n",
    "  → Safe behavior\n",
    "  → Follow instructions\n",
    "\n",
    "RLHF bridges this gap by teaching the model human preferences!\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The RLHF Pipeline: Three Stages\n",
    "\n",
    "RLHF transforms a pretrained LLM into a helpful assistant through three stages:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                    THE RLHF PIPELINE                           │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │   ┌─────────────────┐                                         │\n",
    "    │   │  PRETRAINED LLM │  GPT, LLaMA, etc.                       │\n",
    "    │   └────────┬────────┘                                         │\n",
    "    │            │                                                   │\n",
    "    │            ▼                                                   │\n",
    "    │   ┌─────────────────┐                                         │\n",
    "    │   │  STAGE 1: SFT   │  Supervised Fine-Tuning                 │\n",
    "    │   │                 │  \"Learn from example conversations\"     │\n",
    "    │   └────────┬────────┘                                         │\n",
    "    │            │                                                   │\n",
    "    │            ▼                                                   │\n",
    "    │   ┌─────────────────┐                                         │\n",
    "    │   │  STAGE 2: RM    │  Reward Model Training                  │\n",
    "    │   │                 │  \"Learn what humans prefer\"             │\n",
    "    │   └────────┬────────┘                                         │\n",
    "    │            │                                                   │\n",
    "    │            ▼                                                   │\n",
    "    │   ┌─────────────────┐                                         │\n",
    "    │   │  STAGE 3: PPO   │  Reinforcement Learning                 │\n",
    "    │   │                 │  \"Optimize for human preferences\"       │\n",
    "    │   └────────┬────────┘                                         │\n",
    "    │            │                                                   │\n",
    "    │            ▼                                                   │\n",
    "    │   ┌─────────────────┐                                         │\n",
    "    │   │  ALIGNED LLM    │  ChatGPT, Claude, etc.                  │\n",
    "    │   └─────────────────┘                                         │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Let's understand each stage in detail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the three stages\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(7, 11.5, 'The RLHF Training Pipeline', ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "stages = [\n",
    "    {\n",
    "        'y': 9,\n",
    "        'title': 'Stage 1: Supervised Fine-Tuning (SFT)',\n",
    "        'color': '#bbdefb',\n",
    "        'edge': '#1976d2',\n",
    "        'icon': '1',\n",
    "        'description': 'Train on high-quality demonstrations',\n",
    "        'analogy': 'Like a student learning from example answers'\n",
    "    },\n",
    "    {\n",
    "        'y': 6,\n",
    "        'title': 'Stage 2: Reward Model (RM)',\n",
    "        'color': '#fff3e0',\n",
    "        'edge': '#f57c00',\n",
    "        'icon': '2',\n",
    "        'description': 'Train a model to predict human preferences',\n",
    "        'analogy': 'Like training a judge to grade essays'\n",
    "    },\n",
    "    {\n",
    "        'y': 3,\n",
    "        'title': 'Stage 3: PPO Fine-Tuning',\n",
    "        'color': '#c8e6c9',\n",
    "        'edge': '#388e3c',\n",
    "        'icon': '3',\n",
    "        'description': 'Use RL to maximize reward model scores',\n",
    "        'analogy': 'Like a student improving based on grades'\n",
    "    }\n",
    "]\n",
    "\n",
    "for stage in stages:\n",
    "    y = stage['y']\n",
    "    \n",
    "    # Stage box\n",
    "    box = FancyBboxPatch((1, y-1), 12, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=stage['color'], edgecolor=stage['edge'], linewidth=3)\n",
    "    ax.add_patch(box)\n",
    "    \n",
    "    # Stage number\n",
    "    circle = Circle((2, y), 0.4, facecolor=stage['edge'], edgecolor='white', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(2, y, stage['icon'], ha='center', va='center', fontsize=14, \n",
    "            fontweight='bold', color='white')\n",
    "    \n",
    "    # Title and description\n",
    "    ax.text(3, y+0.4, stage['title'], fontsize=13, fontweight='bold', color=stage['edge'])\n",
    "    ax.text(3, y-0.1, stage['description'], fontsize=11)\n",
    "    ax.text(3, y-0.6, stage['analogy'], fontsize=10, style='italic', color='#666')\n",
    "\n",
    "# Arrows between stages\n",
    "for i in range(len(stages) - 1):\n",
    "    y1 = stages[i]['y'] - 1\n",
    "    y2 = stages[i+1]['y'] + 1\n",
    "    ax.annotate('', xy=(7, y2), xytext=(7, y1),\n",
    "                arrowprops=dict(arrowstyle='->', lw=3, color='#666'))\n",
    "\n",
    "# Result\n",
    "result_box = FancyBboxPatch((4, 0.2), 6, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#f3e5f5', edgecolor='#7b1fa2', linewidth=3)\n",
    "ax.add_patch(result_box)\n",
    "ax.text(7, 0.8, 'Aligned, Helpful AI Assistant!', ha='center', fontsize=13, \n",
    "        fontweight='bold', color='#7b1fa2')\n",
    "\n",
    "ax.annotate('', xy=(7, 1.4), xytext=(7, 2),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#7b1fa2'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1: Supervised Fine-Tuning (SFT)\n",
    "\n",
    "The first stage teaches the model what good responses look like:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │            STAGE 1: SUPERVISED FINE-TUNING (SFT)               │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  WHAT: Train the LLM on high-quality demonstration data       │\n",
    "    │                                                                │\n",
    "    │  HOW:                                                          │\n",
    "    │    1. Collect prompts from users                              │\n",
    "    │    2. Human experts write ideal responses                     │\n",
    "    │    3. Fine-tune the LLM on these (prompt, response) pairs     │\n",
    "    │                                                                │\n",
    "    │  EXAMPLE:                                                      │\n",
    "    │    Prompt: \"Explain quantum computing to a 5-year-old\"        │\n",
    "    │    Response: \"Imagine you have a magic coin that can be       │\n",
    "    │              both heads AND tails at the same time...\"        │\n",
    "    │                                                                │\n",
    "    │  ANALOGY: Learning by Imitation                               │\n",
    "    │    Like a student reading model essay answers before          │\n",
    "    │    writing their own. They learn the format, tone, and        │\n",
    "    │    style that's expected.                                     │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate SFT data format\n",
    "\n",
    "print(\"STAGE 1: SUPERVISED FINE-TUNING (SFT)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Example SFT training data\n",
    "sft_examples = [\n",
    "    {\n",
    "        \"prompt\": \"What is the capital of France?\",\n",
    "        \"response\": \"The capital of France is Paris. It's known as the 'City of Light' and is home to famous landmarks like the Eiffel Tower and the Louvre Museum.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a haiku about programming.\",\n",
    "        \"response\": \"Lines of code flow free\\nDebugging through the night sky\\nProgram comes alive\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How do I make a peanut butter sandwich?\",\n",
    "        \"response\": \"Here's how to make a peanut butter sandwich:\\n1. Get two slices of bread\\n2. Spread peanut butter on one slice\\n3. Optionally add jelly on the other slice\\n4. Press the slices together\\n5. Enjoy!\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nExample SFT Training Data:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for i, example in enumerate(sft_examples, 1):\n",
    "    print(f\"\\n[Example {i}]\")\n",
    "    print(f\"PROMPT: {example['prompt']}\")\n",
    "    print(f\"RESPONSE: {example['response'][:100]}...\" if len(example['response']) > 100 else f\"RESPONSE: {example['response']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SFT TRAINING OBJECTIVE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "The model learns to:\n",
    "  P(response | prompt) → Maximize likelihood of human-written responses\n",
    "\n",
    "This is standard language model fine-tuning!\n",
    "  Loss = -log P(response | prompt)\n",
    "\n",
    "After SFT, the model:\n",
    "  ✓ Knows the format of helpful responses\n",
    "  ✓ Can follow basic instructions\n",
    "  ✗ Still doesn't know WHICH response is BEST\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple simulation of SFT\n",
    "\n",
    "class SimpleSFTModel:\n",
    "    \"\"\"\n",
    "    A simplified demonstration of SFT.\n",
    "    \n",
    "    In reality, this would be a transformer LLM.\n",
    "    Here we just simulate the training process.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_data = []\n",
    "        self.trained = False\n",
    "    \n",
    "    def add_demonstration(self, prompt, response):\n",
    "        \"\"\"Add a human demonstration to training data.\"\"\"\n",
    "        self.training_data.append({\n",
    "            'prompt': prompt,\n",
    "            'response': response\n",
    "        })\n",
    "    \n",
    "    def train(self, epochs=3):\n",
    "        \"\"\"Simulate training on demonstrations.\"\"\"\n",
    "        print(f\"Training on {len(self.training_data)} demonstrations...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Simulate loss decreasing\n",
    "            loss = 2.5 - (epoch * 0.7)\n",
    "            print(f\"  Epoch {epoch+1}: Loss = {loss:.2f}\")\n",
    "        \n",
    "        self.trained = True\n",
    "        print(\"Training complete!\")\n",
    "    \n",
    "    def generate(self, prompt):\n",
    "        \"\"\"Generate a response (simplified).\"\"\"\n",
    "        if not self.trained:\n",
    "            return \"[Model not trained yet]\"\n",
    "        \n",
    "        # In reality, this would be transformer generation\n",
    "        # Here we just show the model learned the pattern\n",
    "        return f\"[SFT Model Response to: '{prompt[:30]}...']\"\n",
    "\n",
    "\n",
    "# Demonstrate SFT\n",
    "print(\"\\nSIMULATED SFT TRAINING\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "sft_model = SimpleSFTModel()\n",
    "\n",
    "# Add demonstrations\n",
    "for example in sft_examples:\n",
    "    sft_model.add_demonstration(example['prompt'], example['response'])\n",
    "\n",
    "# Train\n",
    "sft_model.train(epochs=3)\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"After SFT, the model can generate helpful-LOOKING responses.\")\n",
    "print(\"But it doesn't know which response is BEST!\")\n",
    "print(\"That's where Stage 2 comes in...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: Reward Model Training\n",
    "\n",
    "The reward model learns to predict which responses humans prefer:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │               STAGE 2: REWARD MODEL (RM)                       │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  WHAT: Train a model to score responses based on human prefs  │\n",
    "    │                                                                │\n",
    "    │  HOW:                                                          │\n",
    "    │    1. Generate multiple responses to the same prompt          │\n",
    "    │    2. Humans rank them (e.g., \"A is better than B\")           │\n",
    "    │    3. Train RM to predict these rankings                      │\n",
    "    │                                                                │\n",
    "    │  EXAMPLE:                                                      │\n",
    "    │    Prompt: \"Tell me a joke\"                                   │\n",
    "    │    Response A: \"Why did the chicken cross the road? To get    │\n",
    "    │                 to the other side!\" → Score: 7.2              │\n",
    "    │    Response B: \"Knock knock... [unfunny joke]\" → Score: 3.1   │\n",
    "    │                                                                │\n",
    "    │    Human ranked: A > B                                        │\n",
    "    │    RM learns: score(A) > score(B)                             │\n",
    "    │                                                                │\n",
    "    │  ANALOGY: The Essay Grader                                    │\n",
    "    │    Like training a teaching assistant to grade essays the     │\n",
    "    │    same way the professor would. Once trained, they can       │\n",
    "    │    grade thousands of essays automatically!                   │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the reward model concept\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Comparison data collection\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Step 1: Collect Human Preferences', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Prompt\n",
    "prompt_box = FancyBboxPatch((1, 7), 8, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "ax1.add_patch(prompt_box)\n",
    "ax1.text(5, 7.9, 'Prompt: \"How do I learn Python?\"', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Response A\n",
    "resp_a = FancyBboxPatch((0.5, 3.5), 4, 3, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax1.add_patch(resp_a)\n",
    "ax1.text(2.5, 5.8, 'Response A', ha='center', fontsize=11, fontweight='bold')\n",
    "ax1.text(2.5, 5.0, '\"Start with basics like\\nvariables and loops.\\nTry Codecademy...\"', \n",
    "         ha='center', fontsize=9)\n",
    "ax1.text(2.5, 3.8, 'WINNER', ha='center', fontsize=10, color='#388e3c', fontweight='bold')\n",
    "\n",
    "# Response B\n",
    "resp_b = FancyBboxPatch((5.5, 3.5), 4, 3, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "ax1.add_patch(resp_b)\n",
    "ax1.text(7.5, 5.8, 'Response B', ha='center', fontsize=11, fontweight='bold')\n",
    "ax1.text(7.5, 5.0, '\"Python is a\\nprogramming language\\nused for...\"', \n",
    "         ha='center', fontsize=9)\n",
    "\n",
    "# Human annotator\n",
    "ax1.text(5, 1.5, 'Human: \"Response A is better!\"', ha='center', fontsize=12, \n",
    "         style='italic', color='#666')\n",
    "ax1.text(5, 0.7, '(More helpful, actionable advice)', ha='center', fontsize=10, color='#888')\n",
    "\n",
    "# Arrows\n",
    "ax1.annotate('', xy=(2.5, 6.5), xytext=(3.5, 7),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax1.annotate('', xy=(7.5, 6.5), xytext=(6.5, 7),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Right: Reward Model Training\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Step 2: Train Reward Model', fontsize=14, fontweight='bold')\n",
    "\n",
    "# RM box\n",
    "rm_box = FancyBboxPatch((2, 4), 6, 3, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#fff3e0', edgecolor='#f57c00', linewidth=3)\n",
    "ax2.add_patch(rm_box)\n",
    "ax2.text(5, 6.2, 'REWARD MODEL', ha='center', fontsize=14, fontweight='bold', color='#f57c00')\n",
    "ax2.text(5, 5.2, 'RM(prompt, response) → score', ha='center', fontsize=11)\n",
    "ax2.text(5, 4.5, '(Higher score = Better response)', ha='center', fontsize=10, color='#666')\n",
    "\n",
    "# Input/Output\n",
    "ax2.text(5, 8.5, 'Input: (prompt, response) pair', ha='center', fontsize=11)\n",
    "ax2.annotate('', xy=(5, 7), xytext=(5, 8),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "ax2.text(5, 2, 'Output: Score (e.g., 7.2)', ha='center', fontsize=11)\n",
    "ax2.annotate('', xy=(5, 2.5), xytext=(5, 4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#f57c00'))\n",
    "\n",
    "# Training objective\n",
    "ax2.text(5, 0.8, 'Training: Learn that score(A) > score(B)', ha='center', \n",
    "         fontsize=11, style='italic', color='#f57c00')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a simple Reward Model\n",
    "\n",
    "class SimpleRewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified Reward Model.\n",
    "    \n",
    "    In reality, this would be a transformer that takes\n",
    "    (prompt, response) and outputs a scalar score.\n",
    "    \n",
    "    Here we simulate with a simple network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simple network that outputs a score\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)  # Output: scalar score\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Return a scalar reward score.\"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "def reward_model_loss(rm, chosen, rejected):\n",
    "    \"\"\"\n",
    "    Reward Model Loss: Bradley-Terry model.\n",
    "    \n",
    "    We want: score(chosen) > score(rejected)\n",
    "    \n",
    "    Loss = -log(sigmoid(score_chosen - score_rejected))\n",
    "    \n",
    "    This is the key insight: we train on PREFERENCES, not absolute scores!\n",
    "    \"\"\"\n",
    "    score_chosen = rm(chosen)\n",
    "    score_rejected = rm(rejected)\n",
    "    \n",
    "    # We want chosen to have higher score\n",
    "    loss = -F.logsigmoid(score_chosen - score_rejected).mean()\n",
    "    \n",
    "    return loss, score_chosen.mean().item(), score_rejected.mean().item()\n",
    "\n",
    "\n",
    "# Demonstrate RM training\n",
    "print(\"REWARD MODEL TRAINING DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create RM\n",
    "rm = SimpleRewardModel(input_dim=10)\n",
    "optimizer = torch.optim.Adam(rm.parameters(), lr=0.01)\n",
    "\n",
    "# Simulate preference data\n",
    "# In reality, this would be encoded (prompt, response) pairs\n",
    "# Here we use random vectors where \"chosen\" has a specific pattern\n",
    "n_pairs = 100\n",
    "\n",
    "# Chosen responses have slightly higher \"quality signal\" in dimension 0\n",
    "chosen_data = torch.randn(n_pairs, 10)\n",
    "chosen_data[:, 0] += 1  # Add quality signal\n",
    "\n",
    "rejected_data = torch.randn(n_pairs, 10)\n",
    "rejected_data[:, 0] -= 1  # Lower quality signal\n",
    "\n",
    "print(\"\\nTraining RM on preference pairs...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss, score_c, score_r = reward_model_loss(rm, chosen_data, rejected_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        accuracy = (rm(chosen_data) > rm(rejected_data)).float().mean().item()\n",
    "        print(f\"Epoch {epoch+1:3d}: Loss = {loss.item():.4f}, \"\n",
    "              f\"Chosen: {score_c:.2f}, Rejected: {score_r:.2f}, \"\n",
    "              f\"Accuracy: {accuracy:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"The RM learned to give HIGHER scores to CHOSEN responses!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RM training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Training loss\n",
    "ax1 = axes[0]\n",
    "ax1.plot(losses, color='#f57c00', linewidth=2)\n",
    "ax1.set_xlabel('Training Step', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Reward Model Training Loss', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Score distribution\n",
    "ax2 = axes[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    chosen_scores = rm(chosen_data).numpy().flatten()\n",
    "    rejected_scores = rm(rejected_data).numpy().flatten()\n",
    "\n",
    "ax2.hist(chosen_scores, bins=20, alpha=0.7, label='Chosen (Preferred)', color='#4caf50')\n",
    "ax2.hist(rejected_scores, bins=20, alpha=0.7, label='Rejected', color='#f44336')\n",
    "ax2.axvline(x=np.mean(chosen_scores), color='#388e3c', linestyle='--', linewidth=2, label=f'Chosen mean: {np.mean(chosen_scores):.2f}')\n",
    "ax2.axvline(x=np.mean(rejected_scores), color='#d32f2f', linestyle='--', linewidth=2, label=f'Rejected mean: {np.mean(rejected_scores):.2f}')\n",
    "ax2.set_xlabel('Reward Score', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Learned Score Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe Reward Model has learned to distinguish good from bad responses!\")\n",
    "print(\"Chosen responses get higher scores (green) than rejected ones (red).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3: PPO Fine-Tuning\n",
    "\n",
    "Now we use reinforcement learning to optimize the LLM!\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │               STAGE 3: PPO FINE-TUNING                         │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  WHAT: Use RL to make the LLM generate high-reward responses  │\n",
    "    │                                                                │\n",
    "    │  THE RL SETUP:                                                │\n",
    "    │    • State: The prompt                                        │\n",
    "    │    • Action: Each token the LLM generates                     │\n",
    "    │    • Reward: Score from the Reward Model                      │\n",
    "    │    • Policy: The LLM itself!                                  │\n",
    "    │                                                                │\n",
    "    │  PROCESS:                                                      │\n",
    "    │    1. LLM generates a response                                │\n",
    "    │    2. Reward Model scores the response                        │\n",
    "    │    3. PPO updates LLM to generate better responses            │\n",
    "    │    4. Repeat!                                                 │\n",
    "    │                                                                │\n",
    "    │  THE KL PENALTY (Critical!):                                  │\n",
    "    │    Reward = RM_score - β × KL(policy || reference)            │\n",
    "    │                                                                │\n",
    "    │    Without KL penalty: LLM might \"hack\" the reward model     │\n",
    "    │    With KL penalty: LLM stays close to the SFT model         │\n",
    "    │                                                                │\n",
    "    │  ANALOGY: Studying with a Tutor                               │\n",
    "    │    - LLM writes essay (generates response)                    │\n",
    "    │    - Tutor grades it (reward model)                          │\n",
    "    │    - LLM learns from feedback (PPO update)                   │\n",
    "    │    - KL penalty: \"Stay true to what you learned in class!\"   │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the PPO training loop for LLMs\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 11.5, 'PPO Fine-Tuning Loop for LLMs', ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "# 1. Prompt\n",
    "prompt_box = FancyBboxPatch((0.5, 8.5), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "ax.add_patch(prompt_box)\n",
    "ax.text(2, 9.5, '1. PROMPT', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(2, 9, '\"How do I...\"', ha='center', fontsize=10, style='italic')\n",
    "\n",
    "# 2. LLM Policy\n",
    "llm_box = FancyBboxPatch((5, 8.5), 4, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax.add_patch(llm_box)\n",
    "ax.text(7, 9.5, '2. LLM (Policy)', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(7, 9, 'Generates response', ha='center', fontsize=10)\n",
    "\n",
    "# 3. Response\n",
    "resp_box = FancyBboxPatch((10.5, 8.5), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax.add_patch(resp_box)\n",
    "ax.text(12, 9.5, '3. RESPONSE', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(12, 9, '\"You should...\"', ha='center', fontsize=10, style='italic')\n",
    "\n",
    "# 4. Reward Model\n",
    "rm_box = FancyBboxPatch((8.5, 5.5), 4, 2, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#f3e5f5', edgecolor='#7b1fa2', linewidth=2)\n",
    "ax.add_patch(rm_box)\n",
    "ax.text(10.5, 6.8, '4. REWARD MODEL', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(10.5, 6.2, 'Scores response', ha='center', fontsize=10)\n",
    "ax.text(10.5, 5.8, 'Score: 7.3', ha='center', fontsize=10, color='#7b1fa2', fontweight='bold')\n",
    "\n",
    "# 5. KL Penalty\n",
    "kl_box = FancyBboxPatch((3.5, 5.5), 4, 2, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "ax.add_patch(kl_box)\n",
    "ax.text(5.5, 6.8, '5. KL PENALTY', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(5.5, 6.2, 'Stay close to SFT', ha='center', fontsize=10)\n",
    "ax.text(5.5, 5.8, 'Penalty: -0.5', ha='center', fontsize=10, color='#d32f2f', fontweight='bold')\n",
    "\n",
    "# 6. Total Reward\n",
    "reward_box = FancyBboxPatch((5.5, 3), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(reward_box)\n",
    "ax.text(7, 4, '6. TOTAL REWARD', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(7, 3.4, '7.3 - 0.5 = 6.8', ha='center', fontsize=10, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# 7. PPO Update\n",
    "ppo_box = FancyBboxPatch((5.5, 0.5), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#bbdefb', edgecolor='#1976d2', linewidth=2)\n",
    "ax.add_patch(ppo_box)\n",
    "ax.text(7, 1.5, '7. PPO UPDATE', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(7, 1, 'Improve policy', ha='center', fontsize=10)\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(4.9, 9.25), xytext=(3.6, 9.25),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(10.4, 9.25), xytext=(9.1, 9.25),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(10.5, 7.5), xytext=(10.5, 8.4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(5.5, 7.5), xytext=(7, 8.4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(7, 4.5), xytext=(5.5, 5.4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(7, 4.5), xytext=(8.5, 5.4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(7, 2), xytext=(7, 2.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Loop back arrow\n",
    "ax.annotate('', xy=(5.4, 8.5), xytext=(5.4, 1.25),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#1976d2',\n",
    "                            connectionstyle='arc3,rad=-0.3'))\n",
    "ax.text(3.5, 5, 'REPEAT', fontsize=10, color='#1976d2', fontweight='bold', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the RLHF reward calculation\n",
    "\n",
    "print(\"RLHF REWARD CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def calculate_rlhf_reward(rm_score, kl_divergence, beta=0.1):\n",
    "    \"\"\"\n",
    "    Calculate the RLHF reward.\n",
    "    \n",
    "    Reward = RM_score - beta * KL(policy || reference)\n",
    "    \n",
    "    Args:\n",
    "        rm_score: Score from reward model\n",
    "        kl_divergence: KL divergence from reference (SFT) model\n",
    "        beta: Weight of KL penalty\n",
    "    \n",
    "    Returns:\n",
    "        Total RLHF reward\n",
    "    \"\"\"\n",
    "    kl_penalty = beta * kl_divergence\n",
    "    total_reward = rm_score - kl_penalty\n",
    "    return total_reward, kl_penalty\n",
    "\n",
    "# Example scenarios\n",
    "scenarios = [\n",
    "    {\"name\": \"Good response, normal\", \"rm_score\": 8.0, \"kl\": 0.5},\n",
    "    {\"name\": \"Great response, normal\", \"rm_score\": 9.5, \"kl\": 0.8},\n",
    "    {\"name\": \"Hacked response (high RM, high KL)\", \"rm_score\": 10.0, \"kl\": 5.0},\n",
    "    {\"name\": \"Safe response (moderate RM, low KL)\", \"rm_score\": 7.0, \"kl\": 0.2},\n",
    "]\n",
    "\n",
    "print(\"\\nScenarios with beta = 0.1:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Scenario':<35} {'RM':<6} {'KL':<6} {'Penalty':<8} {'Total':<8}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for s in scenarios:\n",
    "    total, penalty = calculate_rlhf_reward(s[\"rm_score\"], s[\"kl\"], beta=0.1)\n",
    "    print(f\"{s['name']:<35} {s['rm_score']:<6.1f} {s['kl']:<6.1f} {penalty:<8.1f} {total:<8.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHT: The KL penalty\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "The 'Hacked response' gets the HIGHEST RM score (10.0),\n",
    "but the KL penalty (-0.5) brings it down.\n",
    "\n",
    "This prevents the LLM from:\n",
    "  • Exploiting loopholes in the reward model\n",
    "  • Generating nonsensical but high-scoring text\n",
    "  • Drifting too far from coherent language\n",
    "\n",
    "The KL penalty says: \"Stay close to what you learned!\"\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why RLHF Works: The Key Insights\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                    WHY RLHF WORKS                              │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  1. PREFERENCES ARE EASIER THAN DEMONSTRATIONS                 │\n",
    "    │     Telling which response is BETTER is easier than            │\n",
    "    │     writing the PERFECT response yourself.                     │\n",
    "    │                                                                │\n",
    "    │  2. PREFERENCES SCALE BETTER                                   │\n",
    "    │     Non-experts can compare responses.                         │\n",
    "    │     Writing expert demonstrations is expensive!                │\n",
    "    │                                                                │\n",
    "    │  3. PREFERENCES CAPTURE SUBTLE QUALITIES                       │\n",
    "    │     \"Helpful\", \"harmless\", \"honest\" are hard to define,      │\n",
    "    │     but humans can recognize them when they see them!          │\n",
    "    │                                                                │\n",
    "    │  4. RL OPTIMIZES FOR WHAT HUMANS ACTUALLY WANT                 │\n",
    "    │     Instead of just imitating (SFT), RL actively seeks         │\n",
    "    │     to maximize the qualities humans prefer.                   │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison: Demonstrations vs Preferences\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Demonstrations (SFT)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Demonstrations (for SFT)\\n\"Write the perfect answer\"', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Expert writing\n",
    "expert_box = FancyBboxPatch((2, 5), 6, 3, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "ax1.add_patch(expert_box)\n",
    "ax1.text(5, 7.2, 'Expert writes response', ha='center', fontsize=12, fontweight='bold')\n",
    "ax1.text(5, 6.5, 'Time: 5-10 minutes per response', ha='center', fontsize=10, color='#d32f2f')\n",
    "ax1.text(5, 5.8, 'Cost: $$$ (expert time)', ha='center', fontsize=10, color='#d32f2f')\n",
    "ax1.text(5, 5.2, 'Scale: 10,000 examples', ha='center', fontsize=10)\n",
    "\n",
    "ax1.text(5, 3, 'Requires domain expertise', ha='center', fontsize=11, style='italic', color='#666')\n",
    "ax1.text(5, 2, 'Hard to scale', ha='center', fontsize=11, style='italic', color='#666')\n",
    "\n",
    "# Right: Preferences (RM)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Preferences (for RM)\\n\"Which response is better?\"', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Human comparing\n",
    "pref_box = FancyBboxPatch((2, 5), 6, 3, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax2.add_patch(pref_box)\n",
    "ax2.text(5, 7.2, 'Human compares responses', ha='center', fontsize=12, fontweight='bold')\n",
    "ax2.text(5, 6.5, 'Time: 30 seconds per comparison', ha='center', fontsize=10, color='#388e3c')\n",
    "ax2.text(5, 5.8, 'Cost: $ (crowdsourcing)', ha='center', fontsize=10, color='#388e3c')\n",
    "ax2.text(5, 5.2, 'Scale: 100,000+ comparisons', ha='center', fontsize=10)\n",
    "\n",
    "ax2.text(5, 3, 'Anyone can judge \"which is better\"', ha='center', fontsize=11, style='italic', color='#388e3c')\n",
    "ax2.text(5, 2, 'Scales easily!', ha='center', fontsize=11, style='italic', color='#388e3c', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHT:\")\n",
    "print(\"=\"*60)\n",
    "print(\"It's MUCH easier to say 'A is better than B' than to write A!\")\n",
    "print(\"This is why preference data scales 10x better than demonstrations.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The History of RLHF\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                    RLHF TIMELINE                               │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  2017  Deep RL from Human Preferences                         │\n",
    "    │        ├── First RLHF paper (Christiano et al.)               │\n",
    "    │        └── Applied to Atari games, not language               │\n",
    "    │                                                                │\n",
    "    │  2020  Learning to Summarize with Human Feedback              │\n",
    "    │        ├── OpenAI applies RLHF to text summarization          │\n",
    "    │        └── Shows RLHF > SFT for summarization quality         │\n",
    "    │                                                                │\n",
    "    │  2022  InstructGPT Paper                                      │\n",
    "    │        ├── Full RLHF pipeline for general instruction         │\n",
    "    │        ├── Establishes the 3-stage process                    │\n",
    "    │        └── Foundation for ChatGPT                             │\n",
    "    │                                                                │\n",
    "    │  Nov   ChatGPT Released                                       │\n",
    "    │  2022  ├── RLHF at scale                                      │\n",
    "    │        ├── 100M+ users in months                              │\n",
    "    │        └── Proves RLHF works in production                    │\n",
    "    │                                                                │\n",
    "    │  2023  Alternatives Emerge                                    │\n",
    "    │        ├── DPO (Direct Preference Optimization)               │\n",
    "    │        ├── RLAIF (RL from AI Feedback)                        │\n",
    "    │        └── Constitutional AI                                  │\n",
    "    │                                                                │\n",
    "    │  2024  RLHF becomes standard for LLM alignment                │\n",
    "    │        └── Used by OpenAI, Anthropic, Google, Meta, etc.      │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the timeline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(2016, 2025)\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "# Timeline\n",
    "ax.axhline(y=5, color='#333', linewidth=3)\n",
    "\n",
    "# Events\n",
    "events = [\n",
    "    {'year': 2017, 'y': 7, 'title': 'First RLHF Paper', 'desc': 'Applied to Atari', 'color': '#1976d2'},\n",
    "    {'year': 2020, 'y': 3, 'title': 'Summarization', 'desc': 'RLHF for text', 'color': '#388e3c'},\n",
    "    {'year': 2022, 'y': 7.5, 'title': 'InstructGPT', 'desc': '3-stage pipeline', 'color': '#f57c00'},\n",
    "    {'year': 2022.8, 'y': 2.5, 'title': 'ChatGPT', 'desc': 'RLHF at scale', 'color': '#d32f2f'},\n",
    "    {'year': 2023.5, 'y': 7, 'title': 'DPO & Alternatives', 'desc': 'Simpler methods', 'color': '#7b1fa2'},\n",
    "    {'year': 2024, 'y': 3, 'title': 'Industry Standard', 'desc': 'Used everywhere', 'color': '#00838f'},\n",
    "]\n",
    "\n",
    "for event in events:\n",
    "    # Dot on timeline\n",
    "    ax.scatter(event['year'], 5, s=200, c=event['color'], zorder=5, edgecolors='white', linewidths=2)\n",
    "    \n",
    "    # Line to label\n",
    "    ax.plot([event['year'], event['year']], [5, event['y']], color=event['color'], linewidth=2, linestyle='--')\n",
    "    \n",
    "    # Label box\n",
    "    box = FancyBboxPatch((event['year']-0.4, event['y']-0.5 if event['y'] > 5 else event['y']-0.8), \n",
    "                          1.5, 1.3, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor=event['color'], edgecolor='white', linewidth=2, alpha=0.9)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(event['year']+0.35, event['y']+0.3 if event['y'] > 5 else event['y'], \n",
    "            event['title'], ha='center', fontsize=9, fontweight='bold', color='white')\n",
    "    ax.text(event['year']+0.35, event['y']-0.2 if event['y'] > 5 else event['y']-0.5, \n",
    "            event['desc'], ha='center', fontsize=8, color='white')\n",
    "\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_title('The Rise of RLHF', fontsize=16, fontweight='bold')\n",
    "ax.set_yticks([])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### What is RLHF?\n",
    "\n",
    "RLHF = Teaching LLMs human preferences through reinforcement learning\n",
    "\n",
    "### The Three Stages\n",
    "\n",
    "| Stage | Name | What It Does | Analogy |\n",
    "|-------|------|--------------|----------|\n",
    "| 1 | SFT | Learn from example responses | Student reading model essays |\n",
    "| 2 | RM | Learn what humans prefer | Training a grading assistant |\n",
    "| 3 | PPO | Optimize for preferences | Student improving from grades |\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Reward Model** | Predicts human preference scores |\n",
    "| **KL Penalty** | Prevents model from drifting too far from SFT |\n",
    "| **Preference Data** | Pairs of (chosen, rejected) responses |\n",
    "\n",
    "### Why RLHF Works\n",
    "\n",
    "1. Preferences are easier to provide than demonstrations\n",
    "2. Scales better (crowdsourcing)\n",
    "3. Captures subtle qualities\n",
    "4. Optimizes for actual human wants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What problem does RLHF solve?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "RLHF solves the alignment problem: pretrained LLMs are good at predicting text but don't know what responses humans actually want. They might be unhelpful, harmful, or off-topic. RLHF teaches them human preferences.\n",
    "</details>\n",
    "\n",
    "**2. What are the three stages of RLHF?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "1. Supervised Fine-Tuning (SFT): Train on human-written demonstrations\n",
    "2. Reward Model Training: Train a model to predict human preferences\n",
    "3. PPO Fine-Tuning: Use RL to maximize reward model scores\n",
    "</details>\n",
    "\n",
    "**3. Why is preference data easier to collect than demonstration data?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "It's much easier to say \"A is better than B\" than to write the perfect answer yourself. Anyone can compare responses (30 seconds), but writing expert demonstrations requires domain knowledge and time (5-10 minutes). This makes preference data 10x more scalable.\n",
    "</details>\n",
    "\n",
    "**4. What is the KL penalty and why is it important?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The KL penalty = β × KL(policy || reference) penalizes the model for straying too far from the SFT model. Without it, the LLM might \"hack\" the reward model by generating nonsensical but high-scoring text. It keeps the model coherent and prevents reward hacking.\n",
    "</details>\n",
    "\n",
    "**5. How does the reward model learn?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The reward model learns from comparison data: pairs of (chosen, rejected) responses where humans indicated which is better. It's trained so that score(chosen) > score(rejected). This is called the Bradley-Terry model.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Excellent work! You now understand the RLHF pipeline that powers ChatGPT!\n",
    "\n",
    "In the next notebooks, we'll dive deeper into each component:\n",
    "- **Reward Modeling** in detail\n",
    "- **PPO for Language Models**\n",
    "- **DPO** (a simpler alternative to PPO)\n",
    "- **Practical RLHF** with the TRL library\n",
    "\n",
    "**Continue to:** [Notebook 2: Reward Modeling](02_reward_modeling.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*You now understand the secret sauce behind ChatGPT and other AI assistants!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
