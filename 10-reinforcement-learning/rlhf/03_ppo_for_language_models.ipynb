{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO for Language Models: Training AI to Follow Instructions\n",
    "\n",
    "PPO is the workhorse of RLHF - it's how we actually optimize the language model!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The dog training analogy: how PPO guides LLM behavior\n",
    "- Why standard PPO needs modifications for LLMs\n",
    "- The KL penalty: keeping the model grounded\n",
    "- The value head: predicting future rewards\n",
    "- Implementing PPO for text generation from scratch\n",
    "- Using TRL's PPOTrainer\n",
    "\n",
    "**Prerequisites:** Notebooks 1-2 (RLHF intro, Reward Modeling)\n",
    "\n",
    "**Time:** ~35 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Dog Training Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE DOG TRAINING ANALOGY                              │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Imagine training a dog to perform tricks...                  │\n",
    "    │                                                                │\n",
    "    │  THE DOG (Language Model):                                    │\n",
    "    │    Already knows how to move, bark, sit (pre-trained)         │\n",
    "    │    But doesn't know WHAT behaviors you want                   │\n",
    "    │                                                                │\n",
    "    │  THE TRAINER'S CLICKER (Reward Model):                        │\n",
    "    │    Click! = Good behavior                                     │\n",
    "    │    Silence = Not quite right                                  │\n",
    "    │                                                                │\n",
    "    │  THE TREAT (PPO Gradient):                                    │\n",
    "    │    Higher reward → More of that behavior                      │\n",
    "    │    Lower reward → Less of that behavior                       │\n",
    "    │                                                                │\n",
    "    │  THE LEASH (KL Penalty):                                      │\n",
    "    │    Don't stray TOO far from original behavior!                │\n",
    "    │    A dog that only does tricks isn't a dog anymore...        │\n",
    "    │    We want to REFINE behavior, not replace it!                │\n",
    "    │                                                                │\n",
    "    │  RLHF with PPO = Training with clicker + treats + leash      │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, FancyArrowPatch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualize the PPO for LLMs concept\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('PPO for Language Models: The Training Loop', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Input: Prompt\n",
    "prompt_box = FancyBboxPatch((0.5, 9), 2.5, 2, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(prompt_box)\n",
    "ax.text(1.75, 10.3, 'PROMPT', ha='center', fontsize=10, fontweight='bold', color='#1976d2')\n",
    "ax.text(1.75, 9.5, '\"How do I\\nlearn Python?\"', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# Policy (LLM)\n",
    "llm_box = FancyBboxPatch((4, 8.5), 3, 3, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(llm_box)\n",
    "ax.text(5.5, 10.8, 'POLICY', ha='center', fontsize=11, fontweight='bold', color='#388e3c')\n",
    "ax.text(5.5, 10.1, '(Language Model)', ha='center', fontsize=9)\n",
    "ax.text(5.5, 9.4, 'π_θ(token|context)', ha='center', fontsize=10)\n",
    "ax.text(5.5, 8.8, '+Value Head', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# Response\n",
    "response_box = FancyBboxPatch((8, 9), 3, 2, boxstyle=\"round,pad=0.1\",\n",
    "                               facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax.add_patch(response_box)\n",
    "ax.text(9.5, 10.3, 'RESPONSE', ha='center', fontsize=10, fontweight='bold', color='#f57c00')\n",
    "ax.text(9.5, 9.5, '\"Start with the\\nbasics...\"', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# Reward Model\n",
    "rm_box = FancyBboxPatch((9, 5.5), 3.5, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#e1bee7', edgecolor='#7b1fa2', linewidth=3)\n",
    "ax.add_patch(rm_box)\n",
    "ax.text(10.75, 7.3, 'REWARD MODEL', ha='center', fontsize=10, fontweight='bold', color='#7b1fa2')\n",
    "ax.text(10.75, 6.5, 'Score: 7.3', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(10.75, 5.9, '(The \"Clicker\")', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# KL Penalty\n",
    "kl_box = FancyBboxPatch((5, 5.5), 3, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "ax.add_patch(kl_box)\n",
    "ax.text(6.5, 7.3, 'KL PENALTY', ha='center', fontsize=10, fontweight='bold', color='#d32f2f')\n",
    "ax.text(6.5, 6.5, 'β × KL(π || π_ref)', ha='center', fontsize=10)\n",
    "ax.text(6.5, 5.9, '(The \"Leash\")', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# Combined Reward\n",
    "combined_box = FancyBboxPatch((6.5, 2.5), 4, 2, boxstyle=\"round,pad=0.1\",\n",
    "                               facecolor='#b2dfdb', edgecolor='#00796b', linewidth=3)\n",
    "ax.add_patch(combined_box)\n",
    "ax.text(8.5, 3.8, 'TOTAL REWARD', ha='center', fontsize=10, fontweight='bold', color='#00796b')\n",
    "ax.text(8.5, 3, 'R = RM(x,y) - β×KL', ha='center', fontsize=10)\n",
    "\n",
    "# PPO Update\n",
    "ppo_box = FancyBboxPatch((1.5, 2.5), 3.5, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#fff9c4', edgecolor='#fbc02d', linewidth=3)\n",
    "ax.add_patch(ppo_box)\n",
    "ax.text(3.25, 3.8, 'PPO UPDATE', ha='center', fontsize=10, fontweight='bold', color='#f57f17')\n",
    "ax.text(3.25, 3, '(The \"Treat\")', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(3.9, 10), xytext=(3.1, 10),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(7.9, 10), xytext=(7.1, 10),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(10.5, 8), xytext=(10.5, 8.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(6.5, 8), xytext=(5.8, 8.4),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(8.5, 4.6), xytext=(8.5, 5.4),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(5, 3.5), xytext=(6.4, 3.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(3.25, 4.6), xytext=(3.25, 8.4),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#fbc02d', connectionstyle='arc3,rad=-0.3'))\n",
    "\n",
    "ax.text(2, 6.5, 'Update\\nPolicy', ha='center', fontsize=9, color='#f57f17')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPPO FOR LANGUAGE MODELS:\")\n",
    "print(\"  1. Generate response with current policy\")\n",
    "print(\"  2. Score with reward model\")\n",
    "print(\"  3. Compute KL penalty (don't drift too far!)\")\n",
    "print(\"  4. PPO update to maximize reward - KL penalty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why Standard PPO Needs Modifications\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              LLM-SPECIFIC CHALLENGES                           │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  1. HUGE ACTION SPACE                                         │\n",
    "    │     Standard RL: Maybe 4-10 actions (up, down, left, right)  │\n",
    "    │     LLM: 50,000+ actions (every token in vocabulary!)        │\n",
    "    │     → Need efficient probability computation                 │\n",
    "    │                                                                │\n",
    "    │  2. SEQUENTIAL DECISIONS                                      │\n",
    "    │     Each token depends on all previous tokens                │\n",
    "    │     Response = [token1, token2, ..., tokenN]                 │\n",
    "    │     → Credit assignment is tricky                            │\n",
    "    │                                                                │\n",
    "    │  3. REWARD AT THE END                                         │\n",
    "    │     Reward model scores COMPLETE response                    │\n",
    "    │     Individual tokens don't get direct feedback              │\n",
    "    │     → Need value function to distribute credit               │\n",
    "    │                                                                │\n",
    "    │  4. MUST STAY COHERENT                                        │\n",
    "    │     Can't just maximize reward at any cost                   │\n",
    "    │     Model might produce gibberish that fools RM              │\n",
    "    │     → KL penalty keeps language quality                      │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the action space difference\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Standard RL action space\n",
    "ax1 = axes[0]\n",
    "actions = ['Up', 'Down', 'Left', 'Right']\n",
    "probs = [0.3, 0.1, 0.2, 0.4]\n",
    "colors = ['#64b5f6', '#81c784', '#ffb74d', '#ef5350']\n",
    "\n",
    "bars = ax1.bar(actions, probs, color=colors, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Probability', fontsize=11)\n",
    "ax1.set_title('Standard RL: 4 Actions', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 0.6)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, prob in zip(bars, probs):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{prob:.0%}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Right: LLM action space (tokens)\n",
    "ax2 = axes[1]\n",
    "# Show top tokens and \"...50,000 more\"\n",
    "tokens = ['the', 'a', 'to', 'of', 'and', '...', '50K+\\nmore']\n",
    "probs2 = [0.15, 0.08, 0.06, 0.05, 0.04, 0, 0]\n",
    "colors2 = ['#64b5f6']*5 + ['white', '#e0e0e0']\n",
    "\n",
    "bars = ax2.bar(tokens, probs2, color=colors2, edgecolor='black', linewidth=2)\n",
    "bars[-1].set_edgecolor('#999')\n",
    "bars[-1].set_linewidth(1)\n",
    "bars[-1].set_linestyle('--')\n",
    "\n",
    "ax2.set_ylabel('Probability', fontsize=11)\n",
    "ax2.set_title('LLM: 50,000+ Actions (Tokens)!', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "ax2.set_ylim(0, 0.25)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add long tail annotation\n",
    "ax2.annotate('Long tail of\\nrare tokens', xy=(5.5, 0.02), fontsize=9, ha='center', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nACTION SPACE COMPARISON:\")\n",
    "print(\"  Standard RL: ~4-10 discrete actions\")\n",
    "print(\"  Language Models: ~50,000 tokens in vocabulary!\")\n",
    "print(\"  This is a 10,000x increase in complexity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The RLHF Objective: Reward Minus KL Penalty\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              THE RLHF OBJECTIVE                                │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  NAIVE OBJECTIVE (doesn't work!):                             │\n",
    "    │    maximize E[RM(prompt, response)]                           │\n",
    "    │                                                                │\n",
    "    │    Problem: Model finds exploits!                             │\n",
    "    │    - Repeats phrases that fool RM                            │\n",
    "    │    - Produces gibberish with high RM scores                  │\n",
    "    │    - Loses ability to be a general language model            │\n",
    "    │                                                                │\n",
    "    │  RLHF OBJECTIVE (with KL penalty):                            │\n",
    "    │                                                                │\n",
    "    │    maximize E[RM(x,y) - β × KL(π_θ(y|x) || π_ref(y|x))]      │\n",
    "    │                         └───────────────────────────┘         │\n",
    "    │                           Don't drift too far!                │\n",
    "    │                                                                │\n",
    "    │  WHERE:                                                       │\n",
    "    │    π_θ    = Current policy (being trained)                   │\n",
    "    │    π_ref  = Reference policy (SFT model, frozen)             │\n",
    "    │    β      = KL penalty coefficient (0.01 - 0.2)              │\n",
    "    │                                                                │\n",
    "    │  INTUITION:                                                   │\n",
    "    │    \"Be helpful (high RM) but stay sane (low KL)\"             │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_penalty(log_probs_policy, log_probs_ref):\n",
    "    \"\"\"\n",
    "    Compute KL divergence penalty between policy and reference.\n",
    "    \n",
    "    KL(π || π_ref) = E_π[log π(a|s) - log π_ref(a|s)]\n",
    "    \n",
    "    For autoregressive models, this is sum over tokens.\n",
    "    \"\"\"\n",
    "    kl = log_probs_policy - log_probs_ref\n",
    "    return kl\n",
    "\n",
    "\n",
    "def compute_rlhf_reward(rm_score, log_probs_policy, log_probs_ref, beta=0.1):\n",
    "    \"\"\"\n",
    "    Compute the RLHF reward with KL penalty.\n",
    "    \n",
    "    R = RM(x, y) - β × KL(π || π_ref)\n",
    "    \n",
    "    Args:\n",
    "        rm_score: Reward model score for the response\n",
    "        log_probs_policy: Log probs from current policy\n",
    "        log_probs_ref: Log probs from reference model\n",
    "        beta: KL penalty coefficient\n",
    "    \n",
    "    Returns:\n",
    "        Total reward (scalar)\n",
    "    \"\"\"\n",
    "    kl = compute_kl_penalty(log_probs_policy, log_probs_ref)\n",
    "    total_reward = rm_score - beta * kl.sum()\n",
    "    return total_reward, kl\n",
    "\n",
    "\n",
    "# Demonstrate the tradeoff\n",
    "print(\"RLHF REWARD CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example: 5 tokens in response\n",
    "torch.manual_seed(42)\n",
    "rm_score = 7.5  # Good response\n",
    "log_probs_policy = torch.tensor([-2.0, -1.5, -3.0, -2.5, -1.8])  # Current policy\n",
    "log_probs_ref = torch.tensor([-2.1, -1.6, -2.8, -2.6, -1.9])     # Reference (similar)\n",
    "\n",
    "print(f\"\\nReward Model Score: {rm_score}\")\n",
    "print(f\"Policy log probs: {log_probs_policy.numpy()}\")\n",
    "print(f\"Reference log probs: {log_probs_ref.numpy()}\")\n",
    "\n",
    "betas = [0.0, 0.05, 0.1, 0.2, 0.5]\n",
    "print(f\"\\n{'β':<8} {'KL Penalty':<15} {'Total Reward':<15}\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "for beta in betas:\n",
    "    total_reward, kl = compute_rlhf_reward(rm_score, log_probs_policy, log_probs_ref, beta)\n",
    "    kl_sum = kl.sum().item()\n",
    "    print(f\"{beta:<8} {beta * kl_sum:<15.3f} {total_reward.item():<15.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Higher β → More penalty for diverging from reference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the KL penalty effect\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Reward vs KL tradeoff\n",
    "ax1 = axes[0]\n",
    "kl_values = np.linspace(0, 10, 100)\n",
    "rm_score = 8.0\n",
    "\n",
    "for beta in [0.05, 0.1, 0.2, 0.5]:\n",
    "    total_reward = rm_score - beta * kl_values\n",
    "    ax1.plot(kl_values, total_reward, linewidth=2, label=f'β = {beta}')\n",
    "\n",
    "ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax1.fill_between(kl_values, -5, 0, alpha=0.1, color='red', label='Negative reward zone')\n",
    "\n",
    "ax1.set_xlabel('KL Divergence from Reference', fontsize=11)\n",
    "ax1.set_ylabel('Total RLHF Reward', fontsize=11)\n",
    "ax1.set_title('KL Penalty Effect\\n(Higher β = Stronger Leash)', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(-5, 10)\n",
    "\n",
    "# Right: What happens without KL penalty\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Why KL Penalty Matters', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Without KL\n",
    "ax2.text(2, 8.5, 'Without KL Penalty:', fontsize=11, fontweight='bold', color='#d32f2f')\n",
    "problems = [\n",
    "    '❌ Model finds reward hacks',\n",
    "    '❌ Outputs become repetitive',\n",
    "    '❌ Loses general language ability',\n",
    "    '❌ Gibberish that fools RM',\n",
    "]\n",
    "for i, problem in enumerate(problems):\n",
    "    ax2.text(2, 7.5 - i*0.8, problem, fontsize=10, color='#d32f2f')\n",
    "\n",
    "# With KL\n",
    "ax2.text(2, 4, 'With KL Penalty:', fontsize=11, fontweight='bold', color='#388e3c')\n",
    "benefits = [\n",
    "    '✓ Stays close to SFT model',\n",
    "    '✓ Maintains language quality',\n",
    "    '✓ Learns genuine improvements',\n",
    "    '✓ Can still be used as LLM',\n",
    "]\n",
    "for i, benefit in enumerate(benefits):\n",
    "    ax2.text(2, 3 - i*0.8, benefit, fontsize=10, color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Value Head: Predicting Future Rewards\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              THE VALUE HEAD                                    │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  PROBLEM: Reward comes at the END                             │\n",
    "    │    Response: [t1, t2, t3, ..., t100] → RM Score = 7.5        │\n",
    "    │    How do we know which tokens were good?                     │\n",
    "    │                                                                │\n",
    "    │  SOLUTION: Add a VALUE HEAD to the LLM                        │\n",
    "    │                                                                │\n",
    "    │    ┌─────────────────────┐                                    │\n",
    "    │    │  Transformer LLM    │                                    │\n",
    "    │    │  (frozen or fine-   │                                    │\n",
    "    │    │   tuned backbone)   │                                    │\n",
    "    │    └─────────┬───────────┘                                    │\n",
    "    │              │ hidden_state                                   │\n",
    "    │        ┌─────┴─────┐                                          │\n",
    "    │        │           │                                          │\n",
    "    │        ▼           ▼                                          │\n",
    "    │    ┌──────┐    ┌──────┐                                       │\n",
    "    │    │ LM   │    │Value │                                       │\n",
    "    │    │ Head │    │ Head │  ← NEW!                               │\n",
    "    │    └──┬───┘    └──┬───┘                                       │\n",
    "    │       │           │                                           │\n",
    "    │       ▼           ▼                                           │\n",
    "    │    Vocab        Scalar                                        │\n",
    "    │    Probs        Value                                         │\n",
    "    │                                                                │\n",
    "    │  THE VALUE HEAD predicts: \"How much reward will we get       │\n",
    "    │  from this point onwards?\"                                   │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerWithValueHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified model showing the value head architecture.\n",
    "    \n",
    "    In practice, this would be a full transformer.\n",
    "    Here we show the key concept: LM head + Value head.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=1000, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simplified \"backbone\" (in reality: full transformer)\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "        \n",
    "        # LM Head: Predicts next token probabilities\n",
    "        self.lm_head = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Value Head: Predicts expected future reward (NEW!)\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Forward pass returning both token probs and values.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token indices (batch_size, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            logits: Token logits (batch_size, seq_len, vocab_size)\n",
    "            values: Value predictions (batch_size, seq_len, 1)\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        # Pass through backbone\n",
    "        hidden_states = self.backbone(x)\n",
    "        \n",
    "        # Two outputs!\n",
    "        logits = self.lm_head(hidden_states)  # For next token\n",
    "        values = self.value_head(hidden_states)  # For PPO\n",
    "        \n",
    "        return logits, values\n",
    "    \n",
    "    def generate(self, input_ids, max_length=20):\n",
    "        \"\"\"Simple greedy generation (for demonstration).\"\"\"\n",
    "        for _ in range(max_length):\n",
    "            logits, _ = self.forward(input_ids)\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        return input_ids\n",
    "\n",
    "\n",
    "# Create and inspect model\n",
    "print(\"MODEL WITH VALUE HEAD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = SimpleTransformerWithValueHead(vocab_size=1000, hidden_dim=256)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "lm_head_params = sum(p.numel() for p in model.lm_head.parameters())\n",
    "value_head_params = sum(p.numel() for p in model.value_head.parameters())\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"LM Head parameters: {lm_head_params:,}\")\n",
    "print(f\"Value Head parameters: {value_head_params:,}\")\n",
    "print(f\"Value Head adds only {value_head_params/total_params*100:.1f}% extra parameters!\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randint(0, 1000, (2, 10))  # Batch of 2, length 10\n",
    "logits, values = model(test_input)\n",
    "\n",
    "print(f\"\\nInput shape: {test_input.shape}\")\n",
    "print(f\"Logits shape: {logits.shape} (for next token prediction)\")\n",
    "print(f\"Values shape: {values.shape} (for PPO)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize value head architecture\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('LLM Architecture with Value Head', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Input tokens\n",
    "input_box = FancyBboxPatch((4, 10), 4, 1, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "ax.add_patch(input_box)\n",
    "ax.text(6, 10.5, 'Input Tokens', ha='center', fontsize=10)\n",
    "\n",
    "# Embedding\n",
    "emb_box = FancyBboxPatch((4, 8.2), 4, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax.add_patch(emb_box)\n",
    "ax.text(6, 8.8, 'Embedding Layer', ha='center', fontsize=10)\n",
    "ax.annotate('', xy=(6, 9.5), xytext=(6, 9.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Transformer backbone\n",
    "trans_box = FancyBboxPatch((3.5, 5), 5, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(trans_box)\n",
    "ax.text(6, 6.7, 'Transformer Backbone', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(6, 6, '(Self-Attention + FFN)', ha='center', fontsize=9)\n",
    "ax.text(6, 5.4, 'Hidden States', ha='center', fontsize=9, style='italic')\n",
    "ax.annotate('', xy=(6, 7.6), xytext=(6, 8.1),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Split into two heads\n",
    "ax.annotate('', xy=(3.5, 4), xytext=(5.5, 4.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(8.5, 4), xytext=(6.5, 4.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# LM Head (left)\n",
    "lm_box = FancyBboxPatch((1.5, 2), 3.5, 1.8, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#e1bee7', edgecolor='#7b1fa2', linewidth=2)\n",
    "ax.add_patch(lm_box)\n",
    "ax.text(3.25, 3.3, 'LM Head', ha='center', fontsize=10, fontweight='bold', color='#7b1fa2')\n",
    "ax.text(3.25, 2.6, 'Linear(hidden → vocab)', ha='center', fontsize=9)\n",
    "\n",
    "# LM output\n",
    "ax.text(3.25, 1.2, 'Token Probabilities', ha='center', fontsize=10, fontweight='bold')\n",
    "ax.text(3.25, 0.6, 'P(next_token | context)', ha='center', fontsize=9, style='italic')\n",
    "ax.annotate('', xy=(3.25, 1.5), xytext=(3.25, 1.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#7b1fa2'))\n",
    "\n",
    "# Value Head (right)\n",
    "value_box = FancyBboxPatch((7, 2), 3.5, 1.8, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "ax.add_patch(value_box)\n",
    "ax.text(8.75, 3.3, 'Value Head', ha='center', fontsize=10, fontweight='bold', color='#d32f2f')\n",
    "ax.text(8.75, 2.6, 'Linear(hidden → 1)', ha='center', fontsize=9)\n",
    "ax.text(9.5, 2.1, 'NEW!', fontsize=8, color='#d32f2f', fontweight='bold')\n",
    "\n",
    "# Value output\n",
    "ax.text(8.75, 1.2, 'Expected Reward', ha='center', fontsize=10, fontweight='bold')\n",
    "ax.text(8.75, 0.6, 'V(state) = future rewards', ha='center', fontsize=9, style='italic')\n",
    "ax.annotate('', xy=(8.75, 1.5), xytext=(8.75, 1.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PPO Update for Language Models\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              PPO FOR LANGUAGE MODELS                           │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  STANDARD PPO OBJECTIVE (from advanced-algorithms):           │\n",
    "    │                                                                │\n",
    "    │    L_PPO = E[ min(r(θ)×Â, clip(r(θ), 1-ε, 1+ε)×Â) ]         │\n",
    "    │                                                                │\n",
    "    │    where r(θ) = π_θ(a|s) / π_old(a|s)                        │\n",
    "    │                                                                │\n",
    "    │  FOR LANGUAGE MODELS:                                         │\n",
    "    │                                                                │\n",
    "    │    • State s = (prompt, tokens_so_far)                       │\n",
    "    │    • Action a = next_token                                   │\n",
    "    │    • π_θ(a|s) = softmax(logits)[token_id]                   │\n",
    "    │                                                                │\n",
    "    │  ADVANTAGE ESTIMATION:                                        │\n",
    "    │                                                                │\n",
    "    │    For each token position t:                                │\n",
    "    │    Â_t = R_total - V(state_t)                               │\n",
    "    │                                                                │\n",
    "    │    Or using GAE for better variance reduction:               │\n",
    "    │    Â_t = Σ (γλ)^l × δ_{t+l}                                 │\n",
    "    │                                                                │\n",
    "    │  VALUE LOSS:                                                  │\n",
    "    │    L_V = (V(s) - R_target)²                                  │\n",
    "    │                                                                │\n",
    "    │  TOTAL LOSS:                                                  │\n",
    "    │    L = -L_PPO + c₁×L_V - c₂×Entropy                         │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(rewards, values, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation (GAE).\n",
    "    \n",
    "    For language models, this is computed per-token.\n",
    "    \n",
    "    Args:\n",
    "        rewards: Per-token rewards (mostly 0, final token has RM score)\n",
    "        values: Value predictions at each position\n",
    "        gamma: Discount factor\n",
    "        lam: GAE lambda\n",
    "    \n",
    "    Returns:\n",
    "        advantages: GAE advantages for each position\n",
    "        returns: Discounted returns for value loss\n",
    "    \"\"\"\n",
    "    seq_len = len(rewards)\n",
    "    advantages = torch.zeros(seq_len)\n",
    "    returns = torch.zeros(seq_len)\n",
    "    \n",
    "    # Work backwards\n",
    "    last_gae = 0\n",
    "    last_return = 0\n",
    "    \n",
    "    for t in reversed(range(seq_len)):\n",
    "        if t == seq_len - 1:\n",
    "            next_value = 0  # Terminal state\n",
    "        else:\n",
    "            next_value = values[t + 1]\n",
    "        \n",
    "        # TD error\n",
    "        delta = rewards[t] + gamma * next_value - values[t]\n",
    "        \n",
    "        # GAE\n",
    "        advantages[t] = delta + gamma * lam * last_gae\n",
    "        last_gae = advantages[t]\n",
    "        \n",
    "        # Returns for value loss\n",
    "        returns[t] = rewards[t] + gamma * last_return\n",
    "        last_return = returns[t]\n",
    "    \n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "def ppo_loss_for_lm(log_probs_new, log_probs_old, advantages, epsilon=0.2):\n",
    "    \"\"\"\n",
    "    Compute PPO clipped objective for language model.\n",
    "    \n",
    "    Args:\n",
    "        log_probs_new: Log probs under current policy\n",
    "        log_probs_old: Log probs under old policy (from generation)\n",
    "        advantages: GAE advantages\n",
    "        epsilon: Clipping range\n",
    "    \n",
    "    Returns:\n",
    "        PPO loss (to maximize)\n",
    "    \"\"\"\n",
    "    # Probability ratio\n",
    "    ratio = torch.exp(log_probs_new - log_probs_old)\n",
    "    \n",
    "    # Clipped and unclipped objectives\n",
    "    obj_unclipped = ratio * advantages\n",
    "    obj_clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    \n",
    "    # PPO objective: take minimum (pessimistic)\n",
    "    ppo_obj = torch.min(obj_unclipped, obj_clipped)\n",
    "    \n",
    "    return ppo_obj.mean()\n",
    "\n",
    "\n",
    "# Demonstrate\n",
    "print(\"PPO FOR LANGUAGE MODELS: EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example: 10 token response\n",
    "seq_len = 10\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Rewards: 0 for all but last token (which has RM score minus KL)\n",
    "rewards = torch.zeros(seq_len)\n",
    "rewards[-1] = 5.0  # Final reward from RM - KL penalty\n",
    "\n",
    "# Value predictions\n",
    "values = torch.tensor([2.0, 2.5, 3.0, 3.5, 4.0, 4.2, 4.5, 4.8, 5.0, 5.0])\n",
    "\n",
    "# Compute advantages\n",
    "advantages, returns = compute_advantages(rewards, values)\n",
    "\n",
    "print(f\"\\nPer-token rewards: {rewards.numpy()}\")\n",
    "print(f\"Value predictions: {values.numpy()}\")\n",
    "print(f\"GAE advantages: {advantages.numpy().round(3)}\")\n",
    "print(f\"Returns: {returns.numpy().round(3)}\")\n",
    "\n",
    "# Compute PPO loss\n",
    "log_probs_old = torch.randn(seq_len) * 0.5 - 2  # Simulated old log probs\n",
    "log_probs_new = log_probs_old + torch.randn(seq_len) * 0.1  # Slight change\n",
    "\n",
    "ppo_obj = ppo_loss_for_lm(log_probs_new, log_probs_old, advantages)\n",
    "print(f\"\\nPPO Objective: {ppo_obj.item():.4f}\")\n",
    "print(\"(Positive = policy improving, Negative = policy worsening)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the PPO update for a single response\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top left: Rewards per token\n",
    "ax1 = axes[0, 0]\n",
    "positions = np.arange(seq_len)\n",
    "ax1.bar(positions, rewards.numpy(), color='#64b5f6', edgecolor='black')\n",
    "ax1.set_xlabel('Token Position', fontsize=11)\n",
    "ax1.set_ylabel('Reward', fontsize=11)\n",
    "ax1.set_title('Per-Token Rewards\\n(Only final token has RM score)', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.annotate('RM Score!', xy=(seq_len-1, rewards[-1].item()), xytext=(seq_len-2, rewards[-1].item()+1),\n",
    "             arrowprops=dict(arrowstyle='->', color='red'), fontsize=10, color='red')\n",
    "\n",
    "# Top right: Value predictions vs returns\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(positions, values.numpy(), 'go-', linewidth=2, label='V(s) predictions')\n",
    "ax2.plot(positions, returns.numpy(), 'rs-', linewidth=2, label='Actual returns')\n",
    "ax2.set_xlabel('Token Position', fontsize=11)\n",
    "ax2.set_ylabel('Value', fontsize=11)\n",
    "ax2.set_title('Value Predictions vs Actual Returns', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom left: Advantages\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['#4caf50' if a > 0 else '#f44336' for a in advantages.numpy()]\n",
    "ax3.bar(positions, advantages.numpy(), color=colors, edgecolor='black')\n",
    "ax3.axhline(y=0, color='black', linewidth=1)\n",
    "ax3.set_xlabel('Token Position', fontsize=11)\n",
    "ax3.set_ylabel('Advantage', fontsize=11)\n",
    "ax3.set_title('GAE Advantages\\n(Green = good tokens, Red = bad tokens)', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Bottom right: Probability ratio effect\n",
    "ax4 = axes[1, 1]\n",
    "ratio = torch.exp(log_probs_new - log_probs_old).numpy()\n",
    "ax4.bar(positions, ratio, color='#9575cd', edgecolor='black')\n",
    "ax4.axhline(y=1, color='black', linewidth=1, linestyle='--')\n",
    "ax4.axhline(y=1.2, color='red', linewidth=1, linestyle=':', label='Clip bounds (1±0.2)')\n",
    "ax4.axhline(y=0.8, color='red', linewidth=1, linestyle=':')\n",
    "ax4.set_xlabel('Token Position', fontsize=11)\n",
    "ax4.set_ylabel('Probability Ratio', fontsize=11)\n",
    "ax4.set_title('Probability Ratio r(θ) = π_new/π_old', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using TRL's PPOTrainer\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              TRL PPOTrainer                                    │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  TRL (Transformer Reinforcement Learning) from Hugging Face   │\n",
    "    │  provides production-ready RLHF components.                   │\n",
    "    │                                                                │\n",
    "    │  KEY COMPONENTS:                                              │\n",
    "    │                                                                │\n",
    "    │  1. AutoModelForCausalLMWithValueHead                        │\n",
    "    │     • Wraps any HF model with value head                     │\n",
    "    │     • Automatically handles architecture                     │\n",
    "    │                                                                │\n",
    "    │  2. PPOConfig                                                 │\n",
    "    │     • learning_rate, batch_size, mini_batch_size             │\n",
    "    │     • ppo_epochs, kl_penalty                                 │\n",
    "    │                                                                │\n",
    "    │  3. PPOTrainer                                                │\n",
    "    │     • .generate() - Generate responses                       │\n",
    "    │     • .step() - PPO update step                              │\n",
    "    │     • Handles KL penalty automatically                       │\n",
    "    │                                                                │\n",
    "    │  WORKFLOW:                                                    │\n",
    "    │    for batch in dataloader:                                  │\n",
    "    │        responses = trainer.generate(prompts)                 │\n",
    "    │        rewards = reward_model(prompts, responses)            │\n",
    "    │        trainer.step(prompts, responses, rewards)             │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TRL availability\n",
    "try:\n",
    "    from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "    from transformers import AutoTokenizer\n",
    "    TRL_AVAILABLE = True\n",
    "    print(\"✓ TRL is installed!\")\n",
    "except ImportError:\n",
    "    TRL_AVAILABLE = False\n",
    "    print(\"✗ TRL not installed.\")\n",
    "    print(\"  Install with: pip install trl transformers\")\n",
    "\n",
    "# Show example code\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRL PPOTrainer EXAMPLE CODE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "example_code = '''\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Load model WITH value head\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Configure PPO\n",
    "config = PPOConfig(\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=16,\n",
    "    mini_batch_size=4,\n",
    "    ppo_epochs=4,\n",
    "    kl_penalty=\"kl\",     # Use KL divergence penalty\n",
    "    init_kl_coef=0.2,    # Initial KL coefficient (β)\n",
    "    target_kl=6.0,       # Target KL divergence\n",
    ")\n",
    "\n",
    "# 3. Create trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    ref_model=ref_model,  # For KL penalty\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 4. Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        # Get prompts\n",
    "        query_tensors = [tokenizer.encode(q) for q in batch[\"prompts\"]]\n",
    "        \n",
    "        # Generate responses\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            query_tensors,\n",
    "            max_length=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        \n",
    "        # Get rewards from reward model\n",
    "        texts = [tokenizer.decode(r) for r in response_tensors]\n",
    "        rewards = [reward_model.score(text) for text in texts]\n",
    "        rewards = [torch.tensor(r) for r in rewards]\n",
    "        \n",
    "        # PPO step - this does everything!\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        \n",
    "        # Log stats\n",
    "        print(f\"Mean reward: {stats['ppo/mean_scores']:.3f}\")\n",
    "        print(f\"KL divergence: {stats['objective/kl']:.3f}\")\n",
    "'''\n",
    "\n",
    "print(example_code)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the TRL workflow\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('TRL PPOTrainer Workflow', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Steps\n",
    "steps = [\n",
    "    (1, 7.5, 'Load Models', 'AutoModelFor...WithValueHead\\n+ Reference model', '#e3f2fd', '#1976d2'),\n",
    "    (5, 7.5, 'Configure', 'PPOConfig(...)\\nbatch_size, kl_coef, etc.', '#fff3e0', '#f57c00'),\n",
    "    (9, 7.5, 'Create Trainer', 'PPOTrainer(\\n  model, ref_model,\\n  tokenizer)', '#c8e6c9', '#388e3c'),\n",
    "    (3, 4.5, 'Generate', 'trainer.generate(\\n  prompts)', '#e1bee7', '#7b1fa2'),\n",
    "    (7, 4.5, 'Get Rewards', 'reward_model(\\n  prompts, responses)', '#ffcdd2', '#d32f2f'),\n",
    "    (11, 4.5, 'PPO Step', 'trainer.step(\\n  queries, responses,\\n  rewards)', '#b2dfdb', '#00796b'),\n",
    "]\n",
    "\n",
    "for x, y, title, desc, fcolor, ecolor in steps:\n",
    "    box = FancyBboxPatch((x, y), 3.5, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=fcolor, edgecolor=ecolor, linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 1.75, y + 1.6, title, ha='center', fontsize=10, fontweight='bold', color=ecolor)\n",
    "    ax.text(x + 1.75, y + 0.7, desc, ha='center', fontsize=8)\n",
    "\n",
    "# Arrows for top row\n",
    "ax.annotate('', xy=(4.9, 8.5), xytext=(4.6, 8.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(8.9, 8.5), xytext=(8.6, 8.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Arrow down to loop\n",
    "ax.annotate('', xy=(10.75, 6.5), xytext=(10.75, 7.4),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Arrows for bottom row\n",
    "ax.annotate('', xy=(6.9, 5.5), xytext=(6.6, 5.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(10.9, 5.5), xytext=(10.6, 5.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Loop back arrow\n",
    "ax.annotate('', xy=(3, 6.5), xytext=(12, 4.3),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666',\n",
    "                            connectionstyle='arc3,rad=0.3'))\n",
    "ax.text(7, 2.5, 'Repeat for many batches...', ha='center', fontsize=10, style='italic')\n",
    "\n",
    "# Labels\n",
    "ax.text(7, 9.5, 'Setup (once)', ha='center', fontsize=11, fontweight='bold', color='#666')\n",
    "ax.text(7, 6.8, 'Training Loop (repeat)', ha='center', fontsize=11, fontweight='bold', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### PPO for LLMs: Key Modifications\n",
    "\n",
    "| Standard PPO | PPO for LLMs |\n",
    "|--------------|-------------|\n",
    "| Small action space | 50K+ tokens |\n",
    "| Dense rewards | Sparse (end only) |\n",
    "| Simple states | Sequential context |\n",
    "| No reference | KL penalty |\n",
    "\n",
    "### The RLHF Objective\n",
    "\n",
    "```\n",
    "R = RM(prompt, response) - β × KL(π_θ || π_ref)\n",
    "```\n",
    "\n",
    "### TRL Components\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| `AutoModelForCausalLMWithValueHead` | LLM + Value head |\n",
    "| `PPOConfig` | Hyperparameters |\n",
    "| `PPOTrainer.generate()` | Sample responses |\n",
    "| `PPOTrainer.step()` | PPO update |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why do we need a KL penalty in RLHF?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Without KL penalty, the model can \"reward hack\" - find outputs that score high with the reward model but are actually gibberish or repetitive. The KL penalty keeps the model close to the original SFT model, preserving language quality and preventing degenerate solutions.\n",
    "</details>\n",
    "\n",
    "**2. What is the value head and why is it needed?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The value head is a neural network layer added to the LLM that predicts expected future rewards from any position. It's needed because:\n",
    "- Rewards only come at the END of generation\n",
    "- We need to estimate which tokens contributed to the final reward\n",
    "- It enables computing advantages for each token position\n",
    "</details>\n",
    "\n",
    "**3. Why is the action space so much larger for LLMs?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "In standard RL, actions might be discrete choices like up/down/left/right (4 actions). For LLMs, each action is choosing the next token from the entire vocabulary - typically 32K to 100K+ tokens! This makes the policy network (softmax over vocab) much larger and requires efficient implementations.\n",
    "</details>\n",
    "\n",
    "**4. What does β control in the RLHF objective?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "β is the KL penalty coefficient:\n",
    "- β = 0: No penalty, maximize RM score only (dangerous!)\n",
    "- β small (0.01-0.05): Light constraint, more reward optimization\n",
    "- β large (0.2-0.5): Strong constraint, stay very close to reference\n",
    "\n",
    "Typical values: 0.05-0.2. Often tuned based on observed KL divergence.\n",
    "</details>\n",
    "\n",
    "**5. How does TRL's PPOTrainer simplify RLHF?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "TRL's PPOTrainer handles:\n",
    "- Automatic value head wrapping\n",
    "- KL penalty computation vs reference model\n",
    "- Advantage estimation (GAE)\n",
    "- PPO clipped objective\n",
    "- Batching and optimization\n",
    "\n",
    "You just need to provide: prompts, generated responses, and rewards. The trainer does the rest!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "PPO is powerful but complex. In the next notebook, we'll explore **DPO (Direct Preference Optimization)** - a simpler alternative that skips the reward model entirely!\n",
    "\n",
    "**Continue to:** [Notebook 4: DPO and Alternatives](04_dpo_and_alternatives.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*PPO for LLMs: Like training a dog - reward good behavior, keep the leash tight!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
