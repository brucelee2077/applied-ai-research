{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full RLHF Pipeline: From Base Model to Aligned Assistant\n",
    "\n",
    "Let's put everything together and build a complete LLM alignment pipeline!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The factory assembly line analogy: complete RLHF workflow\n",
    "- End-to-end pipeline: SFT → Reward Model → PPO (or DPO)\n",
    "- Practical implementation with real code\n",
    "- Evaluation metrics: how to measure success\n",
    "- Common pitfalls and how to avoid them\n",
    "- Tips for production deployment\n",
    "\n",
    "**Prerequisites:** Notebooks 1-5 (all previous RLHF notebooks)\n",
    "\n",
    "**Time:** ~40 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Factory Assembly Line\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE FACTORY ASSEMBLY LINE ANALOGY                     │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Building an aligned AI assistant is like a factory...        │\n",
    "    │                                                                │\n",
    "    │  RAW MATERIALS:                                               │\n",
    "    │    ┌─────────────────────────────────────────┐               │\n",
    "    │    │  Base LLM (pre-trained on internet)     │               │\n",
    "    │    │  Instruction Data (Q&A pairs)           │               │\n",
    "    │    │  Preference Data (good vs bad)          │               │\n",
    "    │    └─────────────────────────────────────────┘               │\n",
    "    │                      ↓                                        │\n",
    "    │  ASSEMBLY LINE:                                               │\n",
    "    │    ┌─────────┐    ┌─────────┐    ┌─────────┐                │\n",
    "    │    │ Station │ → │ Station │ → │ Station │                │\n",
    "    │    │   1     │    │   2     │    │   3     │                │\n",
    "    │    │  SFT    │    │ Reward  │    │  PPO    │                │\n",
    "    │    └─────────┘    └─────────┘    └─────────┘                │\n",
    "    │                      ↓                                        │\n",
    "    │  QUALITY CONTROL:                                            │\n",
    "    │    ┌─────────────────────────────────────────┐               │\n",
    "    │    │  Evaluation: Win rate, Human eval, KL   │               │\n",
    "    │    └─────────────────────────────────────────┘               │\n",
    "    │                      ↓                                        │\n",
    "    │  FINISHED PRODUCT:                                           │\n",
    "    │    ┌─────────────────────────────────────────┐               │\n",
    "    │    │  Helpful, Harmless, Honest AI Assistant │               │\n",
    "    │    └─────────────────────────────────────────┘               │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, FancyArrowPatch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check available libraries\n",
    "print(\"LIBRARY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "libraries = {\n",
    "    'torch': 'PyTorch',\n",
    "    'transformers': 'Transformers',\n",
    "    'trl': 'TRL',\n",
    "    'peft': 'PEFT',\n",
    "    'datasets': 'Datasets',\n",
    "}\n",
    "\n",
    "available = {}\n",
    "for module, name in libraries.items():\n",
    "    try:\n",
    "        lib = __import__(module)\n",
    "        version = getattr(lib, '__version__', 'unknown')\n",
    "        available[module] = True\n",
    "        print(f\"✓ {name}: {version}\")\n",
    "    except ImportError:\n",
    "        available[module] = False\n",
    "        print(f\"✗ {name}: Not installed\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the complete RLHF pipeline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.set_xlim(0, 16)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('Complete RLHF Pipeline: From Base Model to Aligned Assistant', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Stage 0: Input\n",
    "input_box = FancyBboxPatch((0.5, 8.5), 3, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "ax.add_patch(input_box)\n",
    "ax.text(2, 10.5, 'INPUT', ha='center', fontsize=10, fontweight='bold', color='#1976d2')\n",
    "ax.text(2, 9.8, 'Base LLM', ha='center', fontsize=9)\n",
    "ax.text(2, 9.2, '(Llama, GPT-2, etc.)', ha='center', fontsize=8, color='#666')\n",
    "\n",
    "# Stage 1: SFT\n",
    "sft_box = FancyBboxPatch((4.5, 8.5), 3, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(sft_box)\n",
    "ax.text(6, 10.5, 'STAGE 1: SFT', ha='center', fontsize=10, fontweight='bold', color='#388e3c')\n",
    "ax.text(6, 9.6, 'Supervised', ha='center', fontsize=9)\n",
    "ax.text(6, 9.1, 'Fine-Tuning', ha='center', fontsize=9)\n",
    "\n",
    "# Data for SFT\n",
    "sft_data = FancyBboxPatch((4.5, 6.5), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#fff3e0', edgecolor='#f57c00', linewidth=1)\n",
    "ax.add_patch(sft_data)\n",
    "ax.text(6, 7.5, 'Instruction Data', ha='center', fontsize=8, fontweight='bold')\n",
    "ax.text(6, 6.9, '(Q&A pairs)', ha='center', fontsize=7, color='#666')\n",
    "\n",
    "# Stage 2a: Reward Model (for PPO path)\n",
    "rm_box = FancyBboxPatch((8.5, 9), 3, 2, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#e1bee7', edgecolor='#7b1fa2', linewidth=2)\n",
    "ax.add_patch(rm_box)\n",
    "ax.text(10, 10.5, 'STAGE 2a', ha='center', fontsize=9, fontweight='bold', color='#7b1fa2')\n",
    "ax.text(10, 9.9, 'Reward Model', ha='center', fontsize=9)\n",
    "ax.text(10, 9.3, '(for PPO)', ha='center', fontsize=8, color='#666')\n",
    "\n",
    "# Stage 3a: PPO\n",
    "ppo_box = FancyBboxPatch((12.5, 9), 3, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax.add_patch(ppo_box)\n",
    "ax.text(14, 10.5, 'STAGE 3a', ha='center', fontsize=9, fontweight='bold', color='#f57c00')\n",
    "ax.text(14, 9.9, 'PPO Training', ha='center', fontsize=9)\n",
    "ax.text(14, 9.3, '(RL optimization)', ha='center', fontsize=8, color='#666')\n",
    "\n",
    "# Stage 2b: DPO (simpler path)\n",
    "dpo_box = FancyBboxPatch((10.5, 5.5), 3, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#bbdefb', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(dpo_box)\n",
    "ax.text(12, 7, 'STAGE 2b', ha='center', fontsize=9, fontweight='bold', color='#1976d2')\n",
    "ax.text(12, 6.4, 'DPO Training', ha='center', fontsize=9)\n",
    "ax.text(12, 5.8, '(Simpler!)', ha='center', fontsize=8, color='#666')\n",
    "\n",
    "# Preference data (shared)\n",
    "pref_data = FancyBboxPatch((8.5, 3.5), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=1)\n",
    "ax.add_patch(pref_data)\n",
    "ax.text(10, 4.5, 'Preference Data', ha='center', fontsize=8, fontweight='bold')\n",
    "ax.text(10, 3.9, '(chosen vs rejected)', ha='center', fontsize=7, color='#666')\n",
    "\n",
    "# Output: Aligned model\n",
    "output_box = FancyBboxPatch((12.5, 1), 3, 2, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(output_box)\n",
    "ax.text(14, 2.5, 'OUTPUT', ha='center', fontsize=10, fontweight='bold', color='#388e3c')\n",
    "ax.text(14, 1.8, 'Aligned Model', ha='center', fontsize=9)\n",
    "ax.text(14, 1.3, '✓', ha='center', fontsize=14, color='#388e3c')\n",
    "\n",
    "# Arrows\n",
    "# Input → SFT\n",
    "ax.annotate('', xy=(4.4, 9.75), xytext=(3.6, 9.75),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# SFT → RM\n",
    "ax.annotate('', xy=(8.4, 10), xytext=(7.6, 9.75),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# RM → PPO\n",
    "ax.annotate('', xy=(12.4, 10), xytext=(11.6, 10),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# SFT → DPO\n",
    "ax.annotate('', xy=(10.4, 6.5), xytext=(7.6, 8.4),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#1976d2'))\n",
    "\n",
    "# PPO → Output\n",
    "ax.annotate('', xy=(14, 3.1), xytext=(14, 8.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# DPO → Output\n",
    "ax.annotate('', xy=(13, 3.1), xytext=(12, 5.4),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#1976d2'))\n",
    "\n",
    "# Data arrows\n",
    "ax.annotate('', xy=(6, 8.4), xytext=(6, 8.1),\n",
    "            arrowprops=dict(arrowstyle='->', lw=1, color='#f57c00'))\n",
    "ax.annotate('', xy=(10, 8.9), xytext=(10, 5.1),\n",
    "            arrowprops=dict(arrowstyle='->', lw=1, color='#d32f2f'))\n",
    "ax.annotate('', xy=(12, 5.4), xytext=(11, 5.1),\n",
    "            arrowprops=dict(arrowstyle='->', lw=1, color='#d32f2f'))\n",
    "\n",
    "# Labels\n",
    "ax.text(9, 7.5, 'PPO Path\\n(Complex)', ha='center', fontsize=8, color='#f57c00')\n",
    "ax.text(9, 6, 'DPO Path\\n(Simple)', ha='center', fontsize=8, color='#1976d2', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTWO PATHS TO ALIGNMENT:\")\n",
    "print(\"  1. PPO Path: SFT → Reward Model → PPO (more complex, more control)\")\n",
    "print(\"  2. DPO Path: SFT → DPO (simpler, often just as good!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete Implementation: The DPO Path (Recommended)\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              RECOMMENDED PIPELINE: SFT + DPO                   │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  For most projects, we recommend the DPO path:                │\n",
    "    │                                                                │\n",
    "    │    Step 1: SFT (Supervised Fine-Tuning)                       │\n",
    "    │      • Teaches model to follow instructions                  │\n",
    "    │      • Input: Base model + instruction dataset               │\n",
    "    │      • Output: Instruction-following model                   │\n",
    "    │                                                                │\n",
    "    │    Step 2: DPO (Direct Preference Optimization)               │\n",
    "    │      • Aligns model with human preferences                   │\n",
    "    │      • Input: SFT model + preference dataset                 │\n",
    "    │      • Output: Aligned model                                 │\n",
    "    │                                                                │\n",
    "    │  WHY DPO?                                                     │\n",
    "    │    • Simpler (no reward model training)                      │\n",
    "    │    • Faster (2 stages instead of 3)                          │\n",
    "    │    • Often just as effective as PPO                          │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete RLHF Pipeline Code\n",
    "\n",
    "print(\"COMPLETE RLHF PIPELINE (DPO PATH)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "complete_code = '''\n",
    "# ============================================================\n",
    "# COMPLETE RLHF PIPELINE: SFT + DPO\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig, DPOTrainer, DPOConfig\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Llama-2-7b-hf\"  # Or any base model\n",
    "SFT_OUTPUT = \"./sft_model\"\n",
    "DPO_OUTPUT = \"./dpo_aligned_model\"\n",
    "\n",
    "# LoRA configuration (for memory efficiency)\n",
    "LORA_CONFIG = LoraConfig(\n",
    "    r=16,                         # Rank\n",
    "    lora_alpha=32,                # Scaling\n",
    "    lora_dropout=0.05,            # Regularization\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: SUPERVISED FINE-TUNING (SFT)\n",
    "# ============================================================\n",
    "\n",
    "print(\"Step 1: Supervised Fine-Tuning\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load instruction dataset\n",
    "# Popular choices: openassistant-guanaco, dolly-15k, alpaca\n",
    "sft_dataset = load_dataset(\n",
    "    \"timdettmers/openassistant-guanaco\",\n",
    "    split=\"train\"\n",
    ")\n",
    "print(f\"SFT dataset size: {len(sft_dataset)} examples\")\n",
    "\n",
    "# Configure SFT training\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=SFT_OUTPUT,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_seq_length=512,\n",
    "    packing=True,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# Create SFT trainer\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=sft_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=LORA_CONFIG,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "print(\"Training SFT model...\")\n",
    "sft_trainer.train()\n",
    "sft_trainer.save_model(SFT_OUTPUT)\n",
    "print(f\"SFT model saved to {SFT_OUTPUT}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: DIRECT PREFERENCE OPTIMIZATION (DPO)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nStep 2: Direct Preference Optimization\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load SFT model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_OUTPUT,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(SFT_OUTPUT)\n",
    "\n",
    "# Load preference dataset\n",
    "# Popular choices: hh-rlhf, ultrafeedback\n",
    "preference_dataset = load_dataset(\n",
    "    \"Anthropic/hh-rlhf\",\n",
    "    split=\"train[:5000]\"  # Start small!\n",
    ")\n",
    "print(f\"Preference dataset size: {len(preference_dataset)} examples\")\n",
    "\n",
    "# Process preference data to DPO format\n",
    "def format_for_dpo(example):\n",
    "    \"\"\"Convert hh-rlhf format to DPO format.\"\"\"\n",
    "    chosen = example[\"chosen\"]\n",
    "    rejected = example[\"rejected\"]\n",
    "    \n",
    "    # Extract prompt (Human turn)\n",
    "    if \"\\\\nAssistant:\" in chosen:\n",
    "        prompt = chosen.split(\"\\\\nAssistant:\")[0].replace(\"Human: \", \"\")\n",
    "        chosen_response = chosen.split(\"\\\\nAssistant:\")[-1]\n",
    "        rejected_response = rejected.split(\"\\\\nAssistant:\")[-1] if \"\\\\nAssistant:\" in rejected else rejected\n",
    "    else:\n",
    "        prompt = chosen[:100]\n",
    "        chosen_response = chosen\n",
    "        rejected_response = rejected\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt.strip(),\n",
    "        \"chosen\": chosen_response.strip(),\n",
    "        \"rejected\": rejected_response.strip(),\n",
    "    }\n",
    "\n",
    "dpo_dataset = preference_dataset.map(format_for_dpo)\n",
    "\n",
    "# Configure DPO training\n",
    "dpo_config = DPOConfig(\n",
    "    output_dir=DPO_OUTPUT,\n",
    "    beta=0.1,                      # KL penalty coefficient\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    max_length=512,\n",
    "    max_prompt_length=256,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# Create DPO trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=dpo_config,\n",
    "    train_dataset=dpo_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=LORA_CONFIG,  # Continue with LoRA\n",
    ")\n",
    "\n",
    "# Train!\n",
    "print(\"Training DPO model...\")\n",
    "dpo_trainer.train()\n",
    "dpo_trainer.save_model(DPO_OUTPUT)\n",
    "print(f\"Aligned model saved to {DPO_OUTPUT}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nStep 3: Evaluation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load aligned model\n",
    "aligned_model = AutoModelForCausalLM.from_pretrained(DPO_OUTPUT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(DPO_OUTPUT)\n",
    "\n",
    "# Test generation\n",
    "test_prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do I learn programming?\",\n",
    "    \"Explain machine learning to a 5-year-old.\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    inputs = tokenizer(f\"Human: {prompt}\\\\nAssistant:\", return_tensors=\"pt\")\n",
    "    outputs = aligned_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\\\nPrompt: {prompt}\")\n",
    "    print(f\"Response: {response.split(\\'Assistant:\\')[-1].strip()}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 70)\n",
    "print(\"PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "'''\n",
    "\n",
    "print(complete_code)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation Metrics: Measuring Alignment Success\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              EVALUATION METRICS                                │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  1. WIN RATE (Automated)                                      │\n",
    "    │     Compare aligned model vs baseline on same prompts         │\n",
    "    │     Use reward model or GPT-4 as judge                        │\n",
    "    │     Target: > 50% (better than baseline)                      │\n",
    "    │                                                                │\n",
    "    │  2. REWARD SCORE (Automated)                                  │\n",
    "    │     Average reward model score on held-out prompts            │\n",
    "    │     Higher = better aligned with preferences                  │\n",
    "    │     Watch for reward hacking!                                 │\n",
    "    │                                                                │\n",
    "    │  3. KL DIVERGENCE (Automated)                                 │\n",
    "    │     How different from reference model?                       │\n",
    "    │     Target: 5-15 (enough change but not too much)            │\n",
    "    │                                                                │\n",
    "    │  4. PERPLEXITY (Automated)                                    │\n",
    "    │     Language quality on held-out text                        │\n",
    "    │     Should not increase significantly                        │\n",
    "    │                                                                │\n",
    "    │  5. HUMAN EVALUATION (Gold Standard!)                         │\n",
    "    │     Real humans rate responses                               │\n",
    "    │     Most reliable but expensive                              │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation utilities\n",
    "\n",
    "def compute_win_rate(model_a_scores, model_b_scores):\n",
    "    \"\"\"\n",
    "    Compute win rate of model A over model B.\n",
    "    \n",
    "    Args:\n",
    "        model_a_scores: Scores for model A responses\n",
    "        model_b_scores: Scores for model B responses\n",
    "    \n",
    "    Returns:\n",
    "        Win rate (0-1), tie rate, lose rate\n",
    "    \"\"\"\n",
    "    wins = sum(a > b for a, b in zip(model_a_scores, model_b_scores))\n",
    "    ties = sum(a == b for a, b in zip(model_a_scores, model_b_scores))\n",
    "    losses = sum(a < b for a, b in zip(model_a_scores, model_b_scores))\n",
    "    total = len(model_a_scores)\n",
    "    \n",
    "    return wins/total, ties/total, losses/total\n",
    "\n",
    "\n",
    "def compute_kl_divergence(log_probs_policy, log_probs_ref):\n",
    "    \"\"\"\n",
    "    Compute KL divergence between policy and reference.\n",
    "    \n",
    "    KL(π || π_ref) = E_π[log π - log π_ref]\n",
    "    \"\"\"\n",
    "    kl = (log_probs_policy - log_probs_ref).mean()\n",
    "    return kl.item()\n",
    "\n",
    "\n",
    "# Simulate evaluation results\n",
    "print(\"EVALUATION EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_eval = 100\n",
    "\n",
    "# Simulated scores\n",
    "base_scores = np.random.normal(5, 1.5, n_eval)  # Base model: mean 5\n",
    "sft_scores = np.random.normal(6, 1.5, n_eval)   # SFT: mean 6\n",
    "aligned_scores = np.random.normal(7.5, 1.2, n_eval)  # Aligned: mean 7.5\n",
    "\n",
    "# Compute win rates\n",
    "win_sft, tie_sft, lose_sft = compute_win_rate(sft_scores, base_scores)\n",
    "win_aligned, tie_aligned, lose_aligned = compute_win_rate(aligned_scores, sft_scores)\n",
    "\n",
    "print(\"\\nWin Rate Results:\")\n",
    "print(f\"  SFT vs Base: {win_sft:.1%} wins, {tie_sft:.1%} ties, {lose_sft:.1%} losses\")\n",
    "print(f\"  Aligned vs SFT: {win_aligned:.1%} wins, {tie_aligned:.1%} ties, {lose_aligned:.1%} losses\")\n",
    "\n",
    "print(\"\\nAverage Reward Scores:\")\n",
    "print(f\"  Base Model: {base_scores.mean():.2f} ± {base_scores.std():.2f}\")\n",
    "print(f\"  SFT Model: {sft_scores.mean():.2f} ± {sft_scores.std():.2f}\")\n",
    "print(f\"  Aligned Model: {aligned_scores.mean():.2f} ± {aligned_scores.std():.2f}\")\n",
    "\n",
    "# Simulated KL divergence\n",
    "kl_sft = 3.2  # After SFT\n",
    "kl_aligned = 8.5  # After alignment\n",
    "\n",
    "print(\"\\nKL Divergence from Base:\")\n",
    "print(f\"  SFT Model: {kl_sft:.1f}\")\n",
    "print(f\"  Aligned Model: {kl_aligned:.1f}\")\n",
    "print(f\"  Target range: 5-15 (enough change but stable)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top-left: Reward score distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(base_scores, bins=20, alpha=0.6, label='Base', color='#ef5350')\n",
    "ax1.hist(sft_scores, bins=20, alpha=0.6, label='SFT', color='#ff9800')\n",
    "ax1.hist(aligned_scores, bins=20, alpha=0.6, label='Aligned', color='#4caf50')\n",
    "ax1.set_xlabel('Reward Score', fontsize=11)\n",
    "ax1.set_ylabel('Count', fontsize=11)\n",
    "ax1.set_title('Reward Score Distribution', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Top-right: Win rate comparison\n",
    "ax2 = axes[0, 1]\n",
    "comparisons = ['SFT vs Base', 'Aligned vs SFT']\n",
    "wins = [win_sft * 100, win_aligned * 100]\n",
    "ties = [tie_sft * 100, tie_aligned * 100]\n",
    "losses = [lose_sft * 100, lose_aligned * 100]\n",
    "\n",
    "x = np.arange(len(comparisons))\n",
    "width = 0.25\n",
    "\n",
    "ax2.bar(x - width, wins, width, label='Wins', color='#4caf50')\n",
    "ax2.bar(x, ties, width, label='Ties', color='#ff9800')\n",
    "ax2.bar(x + width, losses, width, label='Losses', color='#ef5350')\n",
    "\n",
    "ax2.set_ylabel('Percentage', fontsize=11)\n",
    "ax2.set_title('Win Rate Analysis', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(comparisons)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Bottom-left: Training progress\n",
    "ax3 = axes[1, 0]\n",
    "steps = np.arange(0, 1001, 100)\n",
    "reward_progress = 5 + 2 * (1 - np.exp(-steps/300)) + np.random.randn(len(steps)) * 0.2\n",
    "kl_progress = 0.5 * steps / 100 + np.random.randn(len(steps)) * 0.5\n",
    "\n",
    "ax3.plot(steps, reward_progress, 'g-', linewidth=2, label='Reward Score')\n",
    "ax3.set_xlabel('Training Steps', fontsize=11)\n",
    "ax3.set_ylabel('Reward Score', fontsize=11, color='g')\n",
    "ax3.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "ax3_twin = ax3.twinx()\n",
    "ax3_twin.plot(steps, kl_progress, 'b--', linewidth=2, label='KL Divergence')\n",
    "ax3_twin.set_ylabel('KL Divergence', fontsize=11, color='b')\n",
    "ax3_twin.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "ax3.set_title('Training Progress', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom-right: Quality metrics\n",
    "ax4 = axes[1, 1]\n",
    "metrics = ['Helpfulness', 'Harmlessness', 'Honesty', 'Coherence']\n",
    "base_metrics = [5, 5, 5, 7]\n",
    "aligned_metrics = [8, 7, 8, 8]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "ax4.bar(x - width/2, base_metrics, width, label='Base', color='#ef5350', alpha=0.8)\n",
    "ax4.bar(x + width/2, aligned_metrics, width, label='Aligned', color='#4caf50', alpha=0.8)\n",
    "\n",
    "ax4.set_ylabel('Score (1-10)', fontsize=11)\n",
    "ax4.set_title('Quality Metrics (Human Evaluation)', fontsize=12, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(metrics)\n",
    "ax4.legend()\n",
    "ax4.set_ylim(0, 10)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Common Pitfalls and Solutions\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              COMMON PITFALLS                                   │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  1. REWARD HACKING                                            │\n",
    "    │     Problem: Model finds exploits in reward model            │\n",
    "    │     Symptoms: High reward but bad outputs                    │\n",
    "    │     Solution: KL penalty, diverse eval, human checks         │\n",
    "    │                                                                │\n",
    "    │  2. MODE COLLAPSE                                             │\n",
    "    │     Problem: Model gives same response to everything         │\n",
    "    │     Symptoms: Very low diversity in outputs                  │\n",
    "    │     Solution: Increase KL penalty, reduce learning rate      │\n",
    "    │                                                                │\n",
    "    │  3. CATASTROPHIC FORGETTING                                   │\n",
    "    │     Problem: Model forgets base knowledge                    │\n",
    "    │     Symptoms: Can't answer basic questions anymore           │\n",
    "    │     Solution: Use LoRA, lower learning rate, more KL         │\n",
    "    │                                                                │\n",
    "    │  4. OVERFITTING TO PREFERENCES                                │\n",
    "    │     Problem: Model memorizes preference data                 │\n",
    "    │     Symptoms: Perfect on train, bad on test                  │\n",
    "    │     Solution: Early stopping, regularization, more data      │\n",
    "    │                                                                │\n",
    "    │  5. LENGTH BIAS                                               │\n",
    "    │     Problem: Model learns \"longer = better\"                  │\n",
    "    │     Symptoms: Verbose, repetitive outputs                    │\n",
    "    │     Solution: Length normalization in reward/loss            │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall detection utilities\n",
    "\n",
    "def check_for_pitfalls(training_stats):\n",
    "    \"\"\"\n",
    "    Analyze training stats for common pitfalls.\n",
    "    \n",
    "    Args:\n",
    "        training_stats: Dict with 'rewards', 'kl', 'loss', 'response_lengths'\n",
    "    \n",
    "    Returns:\n",
    "        List of warnings\n",
    "    \"\"\"\n",
    "    warnings = []\n",
    "    \n",
    "    # Check for reward hacking\n",
    "    if training_stats['rewards'][-1] > 10:\n",
    "        warnings.append(\"⚠️ REWARD HACKING: Suspiciously high reward scores\")\n",
    "    \n",
    "    # Check for high KL (mode collapse risk)\n",
    "    if training_stats['kl'][-1] > 15:\n",
    "        warnings.append(\"⚠️ HIGH KL: Risk of catastrophic forgetting\")\n",
    "    \n",
    "    if training_stats['kl'][-1] < 1:\n",
    "        warnings.append(\"⚠️ LOW KL: Model may not be learning enough\")\n",
    "    \n",
    "    # Check for length bias\n",
    "    avg_length_start = np.mean(training_stats['response_lengths'][:10])\n",
    "    avg_length_end = np.mean(training_stats['response_lengths'][-10:])\n",
    "    if avg_length_end > avg_length_start * 1.5:\n",
    "        warnings.append(\"⚠️ LENGTH BIAS: Responses getting longer\")\n",
    "    \n",
    "    # Check for loss spike\n",
    "    if max(training_stats['loss']) > 3 * training_stats['loss'][0]:\n",
    "        warnings.append(\"⚠️ LOSS SPIKE: Training may be unstable\")\n",
    "    \n",
    "    return warnings\n",
    "\n",
    "\n",
    "# Example: Good training run\n",
    "print(\"PITFALL ANALYSIS EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Good training\n",
    "good_stats = {\n",
    "    'rewards': [5, 5.5, 6, 6.3, 6.5, 6.8, 7, 7.1, 7.2, 7.3],\n",
    "    'kl': [0, 1, 2, 3, 4, 5, 6, 7, 8, 8.5],\n",
    "    'loss': [0.7, 0.6, 0.55, 0.5, 0.48, 0.46, 0.45, 0.44, 0.43, 0.42],\n",
    "    'response_lengths': [100, 105, 108, 110, 112, 115, 118, 120, 122, 125],\n",
    "}\n",
    "\n",
    "print(\"\\nGood Training Run:\")\n",
    "warnings = check_for_pitfalls(good_stats)\n",
    "if not warnings:\n",
    "    print(\"  ✓ No issues detected!\")\n",
    "else:\n",
    "    for w in warnings:\n",
    "        print(f\"  {w}\")\n",
    "\n",
    "# Bad training (reward hacking)\n",
    "bad_stats = {\n",
    "    'rewards': [5, 6, 8, 10, 12, 14, 16, 18, 20, 25],  # Too high!\n",
    "    'kl': [0, 2, 5, 10, 15, 20, 25, 30, 35, 40],  # Too high!\n",
    "    'loss': [0.7, 0.5, 0.3, 0.5, 1.0, 0.3, 2.0, 0.4, 0.3, 0.2],  # Spiky\n",
    "    'response_lengths': [100, 120, 150, 200, 280, 350, 450, 550, 700, 900],  # Growing!\n",
    "}\n",
    "\n",
    "print(\"\\nProblematic Training Run:\")\n",
    "warnings = check_for_pitfalls(bad_stats)\n",
    "for w in warnings:\n",
    "    print(f\"  {w}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Production Tips\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              PRODUCTION TIPS                                   │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  DATA QUALITY:                                                │\n",
    "    │    • Quality >> Quantity (1K good > 100K bad)                │\n",
    "    │    • Clean preference data manually if possible              │\n",
    "    │    • Remove low-quality/contradictory examples               │\n",
    "    │                                                                │\n",
    "    │  TRAINING:                                                    │\n",
    "    │    • Start with smaller model to debug pipeline              │\n",
    "    │    • Use LoRA for memory efficiency                          │\n",
    "    │    • Save checkpoints frequently                             │\n",
    "    │    • Monitor KL divergence closely                           │\n",
    "    │                                                                │\n",
    "    │  EVALUATION:                                                  │\n",
    "    │    • Always test on held-out data                           │\n",
    "    │    • Use multiple metrics (not just reward)                 │\n",
    "    │    • Include human evaluation when possible                 │\n",
    "    │    • Test for harmful outputs explicitly                    │\n",
    "    │                                                                │\n",
    "    │  DEPLOYMENT:                                ",
    "                                                                                                                                                                               │\n",
    "    │    • A/B test against baseline                              │\n",
    "    │    • Monitor outputs in production                          │\n",
    "    │    • Have fallback mechanisms                               │\n",
    "    │    • Plan for iterative improvement                         │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production checklist\n",
    "\n",
    "print(\"PRODUCTION CHECKLIST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checklist = [\n",
    "    (\"Data\", [\n",
    "        \"☐ Clean instruction dataset (remove duplicates, errors)\",\n",
    "        \"☐ Clean preference dataset (remove contradictions)\",\n",
    "        \"☐ Create held-out test sets\",\n",
    "        \"☐ Check for data leakage\",\n",
    "    ]),\n",
    "    (\"SFT Stage\", [\n",
    "        \"☐ Validate base model loads correctly\",\n",
    "        \"☐ Test tokenizer (padding, special tokens)\",\n",
    "        \"☐ Run small training to verify pipeline\",\n",
    "        \"☐ Save intermediate checkpoints\",\n",
    "    ]),\n",
    "    (\"Alignment Stage\", [\n",
    "        \"☐ Choose DPO (simple) or PPO (flexible)\",\n",
    "        \"☐ Set appropriate beta/KL coefficient\",\n",
    "        \"☐ Monitor training metrics (reward, KL, loss)\",\n",
    "        \"☐ Watch for reward hacking\",\n",
    "    ]),\n",
    "    (\"Evaluation\", [\n",
    "        \"☐ Test on held-out prompts\",\n",
    "        \"☐ Compare to baseline (SFT model)\",\n",
    "        \"☐ Check for harmful outputs\",\n",
    "        \"☐ Run human evaluation if possible\",\n",
    "    ]),\n",
    "    (\"Deployment\", [\n",
    "        \"☐ Merge LoRA weights (if using LoRA)\",\n",
    "        \"☐ Optimize for inference (quantization)\",\n",
    "        \"☐ Set up monitoring\",\n",
    "        \"☐ Plan iteration cycle\",\n",
    "    ]),\n",
    "]\n",
    "\n",
    "for category, items in checklist:\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: RLHF Key Takeaways\n",
    "\n",
    "### The Complete Pipeline\n",
    "\n",
    "```\n",
    "Base LLM → SFT → (Reward Model →) PPO/DPO → Aligned Model\n",
    "```\n",
    "\n",
    "### Two Paths to Alignment\n",
    "\n",
    "| Path | Stages | Complexity | When to Use |\n",
    "|------|--------|------------|-------------|\n",
    "| **DPO** | SFT → DPO | Simple | Most projects |\n",
    "| **PPO** | SFT → RM → PPO | Complex | Online learning, fine control |\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "| Metric | Purpose | Target |\n",
    "|--------|---------|--------|\n",
    "| Win Rate | Compare to baseline | > 50% |\n",
    "| Reward Score | Preference alignment | Higher is better |\n",
    "| KL Divergence | Model stability | 5-15 |\n",
    "| Human Eval | True quality | Gold standard |\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "| Pitfall | Symptom | Solution |\n",
    "|---------|---------|----------|\n",
    "| Reward Hacking | High reward, bad output | More KL penalty |\n",
    "| Mode Collapse | Repetitive outputs | Lower learning rate |\n",
    "| Forgetting | Lost base knowledge | Use LoRA |\n",
    "| Length Bias | Verbose responses | Length normalization |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why is DPO recommended over PPO for most projects?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "DPO is simpler (no reward model needed), faster (2 stages vs 3), and often just as effective. PPO is more complex and can be unstable. DPO directly optimizes on preferences without the intermediate reward model step.\n",
    "</details>\n",
    "\n",
    "**2. What does KL divergence measure and why is it important?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "KL divergence measures how different the aligned model is from the reference (SFT) model. It's important because:\n",
    "- Too low: Model isn't learning the preferences\n",
    "- Too high: Risk of catastrophic forgetting or reward hacking\n",
    "- Target range: 5-15 is typically good\n",
    "</details>\n",
    "\n",
    "**3. What is reward hacking and how do you detect it?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Reward hacking is when the model exploits weaknesses in the reward model to get high scores without actually being helpful. Signs include:\n",
    "- Suspiciously high reward scores (> 10)\n",
    "- Outputs that look good to RM but are actually bad\n",
    "- Repetitive or formulaic responses\n",
    "\n",
    "Prevention: Strong KL penalty, diverse evaluation, human checks.\n",
    "</details>\n",
    "\n",
    "**4. Why should you start with a smaller model when developing?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Starting with a smaller model (e.g., GPT-2 instead of Llama-7B) lets you:\n",
    "- Debug the pipeline quickly\n",
    "- Iterate faster on data processing\n",
    "- Test hyperparameters cheaply\n",
    "- Identify issues before expensive training\n",
    "\n",
    "Once the pipeline works, scale up to the larger model.\n",
    "</details>\n",
    "\n",
    "**5. What's the most reliable evaluation method for alignment?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Human evaluation is the gold standard! While automated metrics (reward score, win rate, KL) are useful for development, only humans can truly judge:\n",
    "- Whether responses are actually helpful\n",
    "- Subtle issues like tone, appropriateness\n",
    "- Edge cases and safety concerns\n",
    "\n",
    "Use automated metrics for iteration, human eval for final assessment.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the **RLHF** section! You now understand:\n",
    "\n",
    "- ✅ What RLHF is and why it matters\n",
    "- ✅ How reward models learn human preferences\n",
    "- ✅ How PPO optimizes language models\n",
    "- ✅ DPO as a simpler alternative\n",
    "- ✅ Using the TRL library\n",
    "- ✅ Building complete alignment pipelines\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Move on to **[Applications](../applications/)** to see RL in action across different domains - games, robotics, recommendations, and more!\n",
    "\n",
    "---\n",
    "\n",
    "*RLHF: \"Teaching AI to be helpful, harmless, and honest - one preference at a time!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
