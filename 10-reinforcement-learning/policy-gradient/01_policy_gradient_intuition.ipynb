{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Intuition: Learning Actions Directly\n",
    "\n",
    "Welcome to a fundamentally different approach to RL! Instead of learning values, we'll learn policies directly.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The dance instructor analogy: why learning actions directly makes sense\n",
    "- Limitations of value-based methods (DQN's struggles)\n",
    "- Stochastic vs deterministic policies\n",
    "- The policy gradient theorem (intuition, not math terror!)\n",
    "- When to use policy gradients vs value methods\n",
    "\n",
    "**Prerequisites:** Deep RL section (DQN basics)\n",
    "\n",
    "**Time:** ~25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Dance Instructor Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE DANCE INSTRUCTOR ANALOGY                          │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Imagine learning to dance...                                 │\n",
    "    │                                                                │\n",
    "    │  VALUE-BASED (DQN approach):                                  │\n",
    "    │    Instructor rates every possible move:                      │\n",
    "    │      \"Step left: 7 points, spin: 9 points, jump: 3 points\"   │\n",
    "    │    You calculate which move has highest points               │\n",
    "    │    Then do that move                                         │\n",
    "    │                                                                │\n",
    "    │    Problems:                                                  │\n",
    "    │    • What if you need smooth moves, not discrete steps?      │\n",
    "    │    • What if calculating max is expensive?                   │\n",
    "    │    • Some moves work better with randomness (jazz hands!)    │\n",
    "    │                                                                │\n",
    "    │  POLICY GRADIENT (Direct approach):                          │\n",
    "    │    Instructor says: \"Just dance! I'll tell you if it's good\" │\n",
    "    │    You try moves, get feedback                               │\n",
    "    │    Do MORE of what worked, LESS of what didn't              │\n",
    "    │                                                                │\n",
    "    │    Benefits:                                                  │\n",
    "    │    • Works with smooth, continuous movements                 │\n",
    "    │    • Natural randomness for creativity                       │\n",
    "    │    • Simple: just learn what to do!                         │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, Rectangle, FancyArrowPatch\n",
    "\n",
    "# Visualize Value-Based vs Policy-Based\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Value-Based (DQN)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('VALUE-BASED (DQN)\\n\"Learn values, derive actions\"', fontsize=14, fontweight='bold', color='#1976d2')\n",
    "\n",
    "# State input\n",
    "state_box1 = FancyBboxPatch((1, 6), 2, 2, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#bbdefb', edgecolor='#1976d2', linewidth=2)\n",
    "ax1.add_patch(state_box1)\n",
    "ax1.text(2, 7, 'State', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Q-Network\n",
    "q_box = FancyBboxPatch((4, 5.5), 2, 3, boxstyle=\"round,pad=0.1\",\n",
    "                        facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax1.add_patch(q_box)\n",
    "ax1.text(5, 7.8, 'Q-Network', ha='center', fontsize=10, fontweight='bold')\n",
    "ax1.text(5, 7, 'Q(s, a₁)=5.2', ha='center', fontsize=9)\n",
    "ax1.text(5, 6.4, 'Q(s, a₂)=3.1', ha='center', fontsize=9)\n",
    "ax1.text(5, 5.8, 'Q(s, a₃)=7.4', ha='center', fontsize=9, color='#d32f2f', fontweight='bold')\n",
    "\n",
    "# argmax operation\n",
    "argmax_box = FancyBboxPatch((7, 6), 2, 2, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "ax1.add_patch(argmax_box)\n",
    "ax1.text(8, 7.3, 'argmax', ha='center', fontsize=10, fontweight='bold')\n",
    "ax1.text(8, 6.7, '→ a₃', ha='center', fontsize=11)\n",
    "\n",
    "# Arrows\n",
    "ax1.annotate('', xy=(3.9, 7), xytext=(3.1, 7),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax1.annotate('', xy=(6.9, 7), xytext=(6.1, 7),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Problems\n",
    "ax1.text(5, 4.5, '❌ Needs discrete actions', ha='center', fontsize=10, color='#d32f2f')\n",
    "ax1.text(5, 3.8, '❌ argmax is not differentiable', ha='center', fontsize=10, color='#d32f2f')\n",
    "ax1.text(5, 3.1, '❌ Deterministic (no exploration)', ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "# Right: Policy-Based\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('POLICY-BASED\\n\"Learn actions directly\"', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# State input\n",
    "state_box2 = FancyBboxPatch((1, 6), 2, 2, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax2.add_patch(state_box2)\n",
    "ax2.text(2, 7, 'State', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Policy Network\n",
    "policy_box = FancyBboxPatch((4.5, 5.5), 3, 3, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax2.add_patch(policy_box)\n",
    "ax2.text(6, 7.8, 'Policy Network', ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.text(6, 7, 'π(a₁|s)=0.15', ha='center', fontsize=9)\n",
    "ax2.text(6, 6.4, 'π(a₂|s)=0.10', ha='center', fontsize=9)\n",
    "ax2.text(6, 5.8, 'π(a₃|s)=0.75', ha='center', fontsize=9, color='#388e3c', fontweight='bold')\n",
    "\n",
    "# Sample arrow\n",
    "ax2.annotate('', xy=(3.9, 7), xytext=(3.1, 7),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax2.annotate('', xy=(8.5, 7), xytext=(7.6, 7),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax2.text(9, 7.3, 'Sample', ha='center', fontsize=10)\n",
    "ax2.text(9, 6.7, '→ Action!', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Benefits\n",
    "ax2.text(5.5, 4.5, '✓ Works with continuous actions', ha='center', fontsize=10, color='#388e3c')\n",
    "ax2.text(5.5, 3.8, '✓ Fully differentiable', ha='center', fontsize=10, color='#388e3c')\n",
    "ax2.text(5.5, 3.1, '✓ Natural exploration (stochastic)', ha='center', fontsize=10, color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THE FUNDAMENTAL DIFFERENCE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "VALUE-BASED (DQN):\n",
    "  1. Learn: \"How good is each action?\" → Q(s, a)\n",
    "  2. Then: Take the best action → argmax Q(s, a)\n",
    "  \n",
    "  Problem: You need to compare ALL actions to pick the best!\n",
    "\n",
    "POLICY-BASED:\n",
    "  1. Learn: \"What should I do?\" → π(a|s) directly!\n",
    "  2. No argmax needed - just sample from the policy\n",
    "  \n",
    "  Simpler, and works with infinite action spaces!\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why DQN Struggles: The Continuous Action Problem\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              THE CONTINUOUS ACTION PROBLEM                     │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  DISCRETE ACTIONS (DQN is happy):                             │\n",
    "    │    CartPole: [LEFT, RIGHT] → 2 options, easy argmax!          │\n",
    "    │    Atari: [UP, DOWN, LEFT, RIGHT, FIRE...] → ~18 options      │\n",
    "    │                                                                │\n",
    "    │  CONTINUOUS ACTIONS (DQN struggles):                          │\n",
    "    │    Robot arm: \"How much torque?\" → Any value from -10 to +10 │\n",
    "    │    Car steering: \"What angle?\" → Any value from -30° to +30° │\n",
    "    │    Drone: \"What thrust?\" → Any value from 0 to 100%          │\n",
    "    │                                                                │\n",
    "    │  WHY DQN FAILS:                                               │\n",
    "    │    DQN needs: argmax_a Q(s, a)                                │\n",
    "    │    With continuous a, there are INFINITE options!            │\n",
    "    │    Can't check Q-value for every possible action!            │\n",
    "    │                                                                │\n",
    "    │  POLICY GRADIENT SOLUTION:                                    │\n",
    "    │    Output a probability distribution over actions            │\n",
    "    │    For continuous: output mean μ and std σ of Gaussian       │\n",
    "    │    Sample action from N(μ, σ)                                │\n",
    "    │    No argmax needed!                                         │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize discrete vs continuous action spaces\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Left: Discrete actions (DQN-friendly)\n",
    "ax1 = axes[0]\n",
    "actions = ['LEFT', 'RIGHT']\n",
    "q_values = [3.5, 7.2]\n",
    "colors = ['#90caf9', '#81c784']\n",
    "bars = ax1.bar(actions, q_values, color=colors, edgecolor='black', linewidth=2)\n",
    "bars[1].set_color('#4caf50')  # Highlight max\n",
    "ax1.set_ylabel('Q-value', fontsize=11)\n",
    "ax1.set_title('Discrete Actions\\n(DQN works great!)', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax1.text(1, 7.5, '← argmax!', fontsize=10, fontweight='bold', color='#388e3c')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Middle: Continuous action - the problem\n",
    "ax2 = axes[1]\n",
    "# Show Q-function over continuous action\n",
    "a = np.linspace(-2, 2, 100)\n",
    "q = -0.5 * (a - 0.7)**2 + 2  # Quadratic Q-function\n",
    "ax2.plot(a, q, 'b-', linewidth=2)\n",
    "ax2.fill_between(a, q, alpha=0.2, color='blue')\n",
    "ax2.axvline(x=0.7, color='red', linestyle='--', linewidth=2)\n",
    "ax2.scatter([0.7], [2], c='red', s=100, zorder=5, label='Max (but how to find?)')\n",
    "ax2.set_xlabel('Action (continuous)', fontsize=11)\n",
    "ax2.set_ylabel('Q(s, a)', fontsize=11)\n",
    "ax2.set_title('Continuous Actions\\n(DQN struggles!)', fontsize=12, fontweight='bold', color='#d32f2f')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.text(0, 0.5, '∞ options to check!', fontsize=10, color='#d32f2f', ha='center')\n",
    "\n",
    "# Right: Policy gradient solution\n",
    "ax3 = axes[2]\n",
    "# Show Gaussian policy\n",
    "a = np.linspace(-2, 2, 100)\n",
    "mu, sigma = 0.7, 0.3\n",
    "policy = 1/(sigma*np.sqrt(2*np.pi)) * np.exp(-0.5*((a-mu)/sigma)**2)\n",
    "ax3.plot(a, policy, 'g-', linewidth=2, label=f'π(a|s) ~ N({mu}, {sigma}²)')\n",
    "ax3.fill_between(a, policy, alpha=0.3, color='green')\n",
    "ax3.axvline(x=mu, color='green', linestyle='--', linewidth=2)\n",
    "\n",
    "# Show samples\n",
    "np.random.seed(42)\n",
    "samples = np.random.normal(mu, sigma, 10)\n",
    "ax3.scatter(samples, np.zeros(10), c='green', s=50, zorder=5, marker='|', label='Sampled actions')\n",
    "\n",
    "ax3.set_xlabel('Action (continuous)', fontsize=11)\n",
    "ax3.set_ylabel('π(a|s)', fontsize=11)\n",
    "ax3.set_title('Policy Gradient Solution\\n(Just sample!)', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPOLICY GRADIENT FOR CONTINUOUS ACTIONS:\")\n",
    "print(\"  Instead of Q-values, output distribution parameters:\")\n",
    "print(\"  • μ (mean): Where to center actions\")\n",
    "print(\"  • σ (std): How much to explore\")\n",
    "print(\"  Then simply sample: a ~ N(μ, σ²)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stochastic vs Deterministic Policies\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              STOCHASTIC vs DETERMINISTIC                       │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  DETERMINISTIC POLICY:                                        │\n",
    "    │    π(s) = a                                                   │\n",
    "    │    \"In state s, always do action a\"                          │\n",
    "    │    Example: DQN's argmax policy                              │\n",
    "    │                                                                │\n",
    "    │    Problem: No exploration! Gets stuck in local optima.       │\n",
    "    │                                                                │\n",
    "    │  STOCHASTIC POLICY:                                           │\n",
    "    │    π(a|s) = probability of action a in state s               │\n",
    "    │    \"In state s, here's how likely each action is\"            │\n",
    "    │                                                                │\n",
    "    │    Benefits:                                                  │\n",
    "    │    • Built-in exploration (samples vary)                     │\n",
    "    │    • Can represent mixed strategies                          │\n",
    "    │    • Smooth optimization (no discontinuities)                │\n",
    "    │                                                                │\n",
    "    │  ANALOGY - Rock Paper Scissors:                               │\n",
    "    │    Deterministic: \"Always play Rock\" → Easy to beat!         │\n",
    "    │    Stochastic: \"33% each\" → Optimal mixed strategy!         │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate stochastic vs deterministic policies\n",
    "\n",
    "class DeterministicPolicy(nn.Module):\n",
    "    \"\"\"Always outputs the same action for a given state.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        # Returns Q-values, then we take argmax\n",
    "        return self.net(state)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state)\n",
    "            return q_values.argmax().item()  # Always same action!\n",
    "\n",
    "\n",
    "class StochasticPolicy(nn.Module):\n",
    "    \"\"\"Outputs action probabilities - samples are different each time!\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)  # Output probabilities!\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            probs = self.forward(state)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            return dist.sample().item()  # Different each time!\n",
    "\n",
    "\n",
    "# Compare behavior\n",
    "print(\"COMPARING DETERMINISTIC vs STOCHASTIC POLICIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "state_dim, action_dim = 4, 3\n",
    "det_policy = DeterministicPolicy(state_dim, action_dim)\n",
    "sto_policy = StochasticPolicy(state_dim, action_dim)\n",
    "\n",
    "# Same state\n",
    "test_state = torch.randn(1, state_dim)\n",
    "\n",
    "print(\"\\nFor the SAME state, sampling 10 actions:\")\n",
    "print(\"\\nDeterministic policy:\")\n",
    "det_actions = [det_policy.get_action(test_state) for _ in range(10)]\n",
    "print(f\"  Actions: {det_actions}\")\n",
    "print(f\"  Always the same! ← No exploration!\")\n",
    "\n",
    "print(\"\\nStochastic policy:\")\n",
    "with torch.no_grad():\n",
    "    probs = sto_policy(test_state)\n",
    "    print(f\"  Probabilities: {probs.numpy()[0].round(3)}\")\n",
    "\n",
    "sto_actions = [sto_policy.get_action(test_state) for _ in range(10)]\n",
    "print(f\"  Actions: {sto_actions}\")\n",
    "print(f\"  Varies! ← Natural exploration!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Deterministic\n",
    "ax1 = axes[0]\n",
    "actions = ['A1', 'A2', 'A3']\n",
    "heights = [0, 1, 0]  # Always A2\n",
    "ax1.bar(actions, heights, color=['#90caf9', '#4caf50', '#90caf9'], edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Probability', fontsize=11)\n",
    "ax1.set_ylim(0, 1.2)\n",
    "ax1.set_title('Deterministic Policy\\nπ(s) = A2 (always)', fontsize=12, fontweight='bold')\n",
    "ax1.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.text(1, 1.05, '100%', ha='center', fontsize=10, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Simulate multiple episodes\n",
    "ax1_inset = ax1.inset_axes([0.6, 0.5, 0.35, 0.35])\n",
    "det_samples = [1] * 20  # Always action 1\n",
    "ax1_inset.hist(det_samples, bins=[0, 1, 2, 3], color='#4caf50', edgecolor='black', rwidth=0.8)\n",
    "ax1_inset.set_title('20 Samples', fontsize=9)\n",
    "ax1_inset.set_xticks([0.5, 1.5, 2.5])\n",
    "ax1_inset.set_xticklabels(['A1', 'A2', 'A3'], fontsize=8)\n",
    "\n",
    "# Right: Stochastic\n",
    "ax2 = axes[1]\n",
    "probs = [0.2, 0.5, 0.3]\n",
    "ax2.bar(actions, probs, color=['#90caf9', '#81c784', '#ffcc80'], edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Probability', fontsize=11)\n",
    "ax2.set_ylim(0, 1.2)\n",
    "ax2.set_title('Stochastic Policy\\nπ(a|s) = [0.2, 0.5, 0.3]', fontsize=12, fontweight='bold')\n",
    "ax2.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "for i, p in enumerate(probs):\n",
    "    ax2.text(i, p + 0.03, f'{int(p*100)}%', ha='center', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Simulate samples\n",
    "ax2_inset = ax2.inset_axes([0.6, 0.5, 0.35, 0.35])\n",
    "np.random.seed(42)\n",
    "sto_samples = np.random.choice([0, 1, 2], 20, p=probs)\n",
    "ax2_inset.hist(sto_samples, bins=[0, 1, 2, 3], color=['#90caf9', '#81c784', '#ffcc80'][1], \n",
    "               edgecolor='black', rwidth=0.8)\n",
    "ax2_inset.set_title('20 Samples', fontsize=9)\n",
    "ax2_inset.set_xticks([0.5, 1.5, 2.5])\n",
    "ax2_inset.set_xticklabels(['A1', 'A2', 'A3'], fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSTOCHASTIC POLICIES ARE BETTER FOR:\")\n",
    "print(\"  1. Exploration - naturally try different actions\")\n",
    "print(\"  2. Mixed strategies - optimal in competitive games\")\n",
    "print(\"  3. Smooth optimization - gradients flow nicely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Policy Gradient Theorem: The Key Insight\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              THE POLICY GRADIENT THEOREM                       │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  GOAL: Maximize expected return J(θ)                          │\n",
    "    │        J(θ) = E[total rewards when following π(a|s; θ)]       │\n",
    "    │                                                                │\n",
    "    │  PROBLEM: How do we compute ∇_θ J(θ)?                         │\n",
    "    │           Rewards depend on environment (not differentiable!) │\n",
    "    │                                                                │\n",
    "    │  THE MAGIC THEOREM:                                           │\n",
    "    │                                                                │\n",
    "    │        ∇_θ J(θ) = E[ ∇_θ log π(a|s; θ) × R ]                  │\n",
    "    │                        └─────────────────┘   └─┘              │\n",
    "    │                         Score function    Return              │\n",
    "    │                                                                │\n",
    "    │  INTUITION (Very Important!):                                 │\n",
    "    │    • If action a got HIGH return R:                          │\n",
    "    │      → Increase log π(a|s) → Make a MORE likely              │\n",
    "    │    • If action a got LOW return R:                           │\n",
    "    │      → Decrease log π(a|s) → Make a LESS likely              │\n",
    "    │                                                                │\n",
    "    │  ANALOGY: Like a coach giving feedback                       │\n",
    "    │    \"That move scored! Do more of that!\"                      │\n",
    "    │    \"That move failed. Do less of that.\"                      │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the policy gradient intuition\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Initial policy\n",
    "ax1 = axes[0]\n",
    "actions = ['A1', 'A2', 'A3']\n",
    "initial_probs = [0.33, 0.33, 0.34]\n",
    "ax1.bar(actions, initial_probs, color='#90caf9', edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('π(a|s)', fontsize=11)\n",
    "ax1.set_ylim(0, 0.8)\n",
    "ax1.set_title('Before Training\\n(Uniform Policy)', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Experience: A2 got high return, A3 got low return\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Experience\\n(Trial and Feedback)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Show experiences\n",
    "experiences = [\n",
    "    ('A1', 'R = +5', '#fff3e0', 'Medium'),\n",
    "    ('A2', 'R = +50', '#c8e6c9', 'HIGH! ✓'),\n",
    "    ('A3', 'R = -10', '#ffcdd2', 'Bad ✗'),\n",
    "]\n",
    "\n",
    "for i, (action, ret, color, comment) in enumerate(experiences):\n",
    "    y = 7 - i * 2.5\n",
    "    box = FancyBboxPatch((1, y), 7, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(2.5, y + 0.75, f'Action {action}:', fontsize=11, fontweight='bold', va='center')\n",
    "    ax2.text(5.5, y + 0.75, ret, fontsize=11, va='center')\n",
    "    ax2.text(8.5, y + 0.75, comment, fontsize=10, va='center')\n",
    "\n",
    "ax2.text(5, 0.5, 'Policy gradient uses this\\nfeedback to update!', ha='center', fontsize=10, style='italic')\n",
    "\n",
    "# After update\n",
    "ax3 = axes[2]\n",
    "updated_probs = [0.25, 0.60, 0.15]  # A2 increased, A3 decreased\n",
    "colors = ['#90caf9', '#4caf50', '#ef9a9a']\n",
    "bars = ax3.bar(actions, updated_probs, color=colors, edgecolor='black', linewidth=2)\n",
    "ax3.set_ylabel('π(a|s)', fontsize=11)\n",
    "ax3.set_ylim(0, 0.8)\n",
    "ax3.set_title('After Training\\n(Learned Policy)', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Annotations\n",
    "ax3.annotate('↑ More likely\\n(worked well!)', xy=(1, 0.60), xytext=(1.5, 0.72),\n",
    "             fontsize=9, color='#388e3c', ha='center')\n",
    "ax3.annotate('↓ Less likely\\n(failed)', xy=(2, 0.15), xytext=(2.5, 0.30),\n",
    "             fontsize=9, color='#d32f2f', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPOLICY GRADIENT UPDATE RULE:\")\n",
    "print(\"  If return R > 0: Increase π(a|s) for that action\")\n",
    "print(\"  If return R < 0: Decrease π(a|s) for that action\")\n",
    "print(\"\\n  It's reinforcing good behavior and weakening bad behavior!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the math: log π trick\n",
    "\n",
    "print(\"THE LOG PROBABILITY TRICK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simple policy\n",
    "policy = StochasticPolicy(4, 3)\n",
    "state = torch.randn(1, 4)\n",
    "\n",
    "# Get probabilities and log probabilities\n",
    "probs = policy(state)\n",
    "log_probs = torch.log(probs)\n",
    "\n",
    "print(f\"\\nAction probabilities: π(a|s) = {probs.detach().numpy()[0].round(3)}\")\n",
    "print(f\"Log probabilities: log π(a|s) = {log_probs.detach().numpy()[0].round(3)}\")\n",
    "\n",
    "# The gradient magic\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"WHY LOG PROBABILITIES?\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "We want: ∇_θ E[R × π(a|s; θ)]\n",
    "Problem: R and π are tangled together!\n",
    "\n",
    "Solution: Use the log-derivative trick:\n",
    "  ∇_θ π(a|s; θ) = π(a|s; θ) × ∇_θ log π(a|s; θ)\n",
    "  \n",
    "This lets us write:\n",
    "  ∇_θ E[R × π] = E[R × ∇_θ log π]\n",
    "  \n",
    "Now we can estimate this from samples!\n",
    "  ≈ (1/N) Σ R_i × ∇_θ log π(a_i|s_i; θ)\n",
    "\"\"\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Practical gradient computation\n",
    "print(\"\\nPRACTICAL IMPLEMENTATION:\")\n",
    "action = torch.tensor([1])  # Suppose we took action 1\n",
    "ret = 10.0  # And got return 10\n",
    "\n",
    "# The loss we minimize (negative because we want gradient ASCENT)\n",
    "dist = torch.distributions.Categorical(probs)\n",
    "log_prob = dist.log_prob(action)\n",
    "loss = -log_prob * ret  # Negative for gradient ascent\n",
    "\n",
    "print(f\"  Action taken: {action.item()}\")\n",
    "print(f\"  Log probability: {log_prob.item():.4f}\")\n",
    "print(f\"  Return: {ret}\")\n",
    "print(f\"  Loss: {loss.item():.4f}\")\n",
    "print(\"\\n  Gradient descent on this loss → Makes action more likely!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building a Simple Policy Network\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              POLICY NETWORK ARCHITECTURE                       │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  DISCRETE ACTIONS (e.g., CartPole):                           │\n",
    "    │                                                                │\n",
    "    │    State [4] → Hidden [64] → Hidden [64] → Softmax → [2]      │\n",
    "    │                                                                │\n",
    "    │    Output: [π(LEFT|s), π(RIGHT|s)]                            │\n",
    "    │    Sample action from this distribution                       │\n",
    "    │                                                                │\n",
    "    │  CONTINUOUS ACTIONS (e.g., Pendulum):                         │\n",
    "    │                                                                │\n",
    "    │    State [3] → Hidden [64] → Hidden [64] → [μ, log σ]        │\n",
    "    │                                                                │\n",
    "    │    Output: Parameters of Gaussian N(μ, σ²)                    │\n",
    "    │    Sample action from Gaussian                                │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretePolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy network for DISCRETE action spaces.\n",
    "    \n",
    "    Outputs probability distribution over actions.\n",
    "    Use for: CartPole, Atari, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)  # Outputs sum to 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Returns action probabilities.\"\"\"\n",
    "        return self.network(state)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Sample action from policy.\"\"\"\n",
    "        probs = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "\n",
    "class ContinuousPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy network for CONTINUOUS action spaces.\n",
    "    \n",
    "    Outputs parameters of Gaussian distribution.\n",
    "    Use for: Pendulum, MuJoCo, robotics, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Output mean of Gaussian\n",
    "        self.mean = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        # Output log std (learnable, state-independent for simplicity)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Returns mean and std of Gaussian.\"\"\"\n",
    "        features = self.shared(state)\n",
    "        mean = self.mean(features)\n",
    "        std = torch.exp(self.log_std)  # Ensure positive\n",
    "        return mean, std\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Sample continuous action from Gaussian.\"\"\"\n",
    "        mean, std = self.forward(state)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum()  # Sum over action dims\n",
    "        return action.numpy(), log_prob\n",
    "\n",
    "\n",
    "# Demonstrate both\n",
    "print(\"POLICY NETWORK EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Discrete policy (CartPole-like)\n",
    "print(\"\\n1. DISCRETE POLICY (CartPole):\")\n",
    "discrete_policy = DiscretePolicy(state_dim=4, action_dim=2)\n",
    "state = torch.randn(1, 4)\n",
    "probs = discrete_policy(state)\n",
    "action, log_prob = discrete_policy.get_action(state)\n",
    "print(f\"   State: {state.numpy()[0].round(2)}\")\n",
    "print(f\"   Action probabilities: {probs.detach().numpy()[0].round(3)}\")\n",
    "print(f\"   Sampled action: {action}\")\n",
    "print(f\"   Log probability: {log_prob.item():.4f}\")\n",
    "\n",
    "# Continuous policy (Pendulum-like)\n",
    "print(\"\\n2. CONTINUOUS POLICY (Pendulum):\")\n",
    "continuous_policy = ContinuousPolicy(state_dim=3, action_dim=1)\n",
    "state = torch.randn(1, 3)\n",
    "mean, std = continuous_policy(state)\n",
    "action, log_prob = continuous_policy.get_action(state)\n",
    "print(f\"   State: {state.numpy()[0].round(2)}\")\n",
    "print(f\"   Gaussian mean: {mean.detach().numpy()[0].round(3)}\")\n",
    "print(f\"   Gaussian std: {std.detach().numpy().round(3)}\")\n",
    "print(f\"   Sampled action: {action.round(3)}\")\n",
    "print(f\"   Log probability: {log_prob.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## When to Use Policy Gradients vs Value Methods\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              WHEN TO USE WHAT                                  │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  USE VALUE-BASED (DQN) WHEN:                                  │\n",
    "    │    ✓ Discrete, small action space                            │\n",
    "    │    ✓ Sample efficiency is crucial                            │\n",
    "    │    ✓ You have lots of experience replay data                 │\n",
    "    │    Example: Atari games, board games                         │\n",
    "    │                                                                │\n",
    "    │  USE POLICY GRADIENTS WHEN:                                   │\n",
    "    │    ✓ Continuous action space                                 │\n",
    "    │    ✓ High-dimensional action space                           │\n",
    "    │    ✓ Stochastic policy is beneficial                         │\n",
    "    │    ✓ You want simpler, more direct optimization              │\n",
    "    │    Example: Robotics, control, games with mixed strategies   │\n",
    "    │                                                                │\n",
    "    │  USE ACTOR-CRITIC (Both!) WHEN:                               │\n",
    "    │    ✓ You want the best of both worlds                        │\n",
    "    │    ✓ Continuous actions + sample efficiency                  │\n",
    "    │    Example: PPO, SAC (state-of-the-art for most tasks)       │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 9.5, 'Value-Based vs Policy-Based Methods', ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Value-based column\n",
    "value_box = FancyBboxPatch((0.5, 2), 6, 6.5, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(value_box)\n",
    "ax.text(3.5, 8, 'VALUE-BASED', ha='center', fontsize=14, fontweight='bold', color='#1976d2')\n",
    "ax.text(3.5, 7.3, '(DQN, Double DQN, Dueling)', ha='center', fontsize=10, color='#666')\n",
    "\n",
    "value_points = [\n",
    "    '✓ Sample efficient',\n",
    "    '✓ Off-policy learning',\n",
    "    '✓ Experience replay',\n",
    "    '✗ Discrete actions only',\n",
    "    '✗ Deterministic policy',\n",
    "    '✗ argmax not differentiable'\n",
    "]\n",
    "for i, point in enumerate(value_points):\n",
    "    color = '#388e3c' if point.startswith('✓') else '#d32f2f'\n",
    "    ax.text(1, 6.2 - i*0.8, point, fontsize=10, color=color)\n",
    "\n",
    "# Policy-based column\n",
    "policy_box = FancyBboxPatch((7.5, 2), 6, 6.5, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#e8f5e9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(policy_box)\n",
    "ax.text(10.5, 8, 'POLICY-BASED', ha='center', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "ax.text(10.5, 7.3, '(REINFORCE, A2C, PPO)', ha='center', fontsize=10, color='#666')\n",
    "\n",
    "policy_points = [\n",
    "    '✓ Continuous actions',\n",
    "    '✓ Stochastic policies',\n",
    "    '✓ Differentiable end-to-end',\n",
    "    '✓ Simpler architecture',\n",
    "    '✗ Higher variance',\n",
    "    '✗ On-policy (less efficient)'\n",
    "]\n",
    "for i, point in enumerate(policy_points):\n",
    "    color = '#388e3c' if point.startswith('✓') else '#d32f2f'\n",
    "    ax.text(8, 6.2 - i*0.8, point, fontsize=10, color=color)\n",
    "\n",
    "# Best of both\n",
    "ax.text(7, 0.8, '⭐ ACTOR-CRITIC combines both: Policy + Value function ⭐', \n",
    "        ha='center', fontsize=12, fontweight='bold', color='#7b1fa2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### The Two Approaches\n",
    "\n",
    "| Aspect | Value-Based (DQN) | Policy-Based |\n",
    "|--------|-------------------|-------------|\n",
    "| **Learns** | Q(s, a) values | π(a|s) directly |\n",
    "| **Action Selection** | argmax Q(s, a) | Sample from π(a|s) |\n",
    "| **Action Space** | Discrete only | Any (discrete or continuous) |\n",
    "| **Policy Type** | Deterministic | Stochastic |\n",
    "| **Exploration** | ε-greedy (added) | Built-in (sampling) |\n",
    "\n",
    "### Policy Gradient Theorem\n",
    "\n",
    "```\n",
    "∇_θ J(θ) = E[ ∇_θ log π(a|s; θ) × R ]\n",
    "\n",
    "Intuition:\n",
    "  High return → Increase action probability\n",
    "  Low return → Decrease action probability\n",
    "```\n",
    "\n",
    "### When to Use\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|---------------|\n",
    "| Discrete, small action space | DQN |\n",
    "| Continuous actions | Policy Gradient |\n",
    "| Best performance | Actor-Critic (PPO) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why can't DQN handle continuous action spaces?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "DQN requires computing argmax Q(s, a) to select actions. With continuous actions, there are infinite possible actions to check. You can't evaluate Q(s, a) for every possible real-valued action. Policy gradient methods avoid this by directly outputting the action (or its distribution parameters).\n",
    "</details>\n",
    "\n",
    "**2. What's the difference between deterministic and stochastic policies?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Deterministic: π(s) = a, always outputs the same action for a given state.\n",
    "Stochastic: π(a|s), outputs a probability distribution over actions.\n",
    "\n",
    "Stochastic policies are better for exploration (naturally vary actions) and can represent mixed strategies (important in competitive games like Rock-Paper-Scissors).\n",
    "</details>\n",
    "\n",
    "**3. What does the policy gradient theorem tell us intuitively?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The policy gradient theorem says:\n",
    "- If an action led to a HIGH return, increase its probability\n",
    "- If an action led to a LOW return, decrease its probability\n",
    "\n",
    "It's like a coach reinforcing good moves and discouraging bad ones!\n",
    "</details>\n",
    "\n",
    "**4. Why do we use log probabilities in the policy gradient?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The log-derivative trick allows us to rewrite:\n",
    "∇_θ π(a|s; θ) = π(a|s; θ) × ∇_θ log π(a|s; θ)\n",
    "\n",
    "This lets us estimate the gradient from samples by multiplying log π by returns. Without this trick, we couldn't easily compute gradients through the environment's dynamics.\n",
    "</details>\n",
    "\n",
    "**5. When would you choose policy gradient over DQN?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Choose policy gradient when:\n",
    "- Action space is continuous (robotics, control)\n",
    "- Action space is large or high-dimensional\n",
    "- You want stochastic behavior (exploration, mixed strategies)\n",
    "- You prefer simpler, end-to-end differentiable optimization\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "You now understand the **why** of policy gradients. In the next notebook, we'll implement the simplest policy gradient algorithm: **REINFORCE**!\n",
    "\n",
    "**Continue to:** [Notebook 2: REINFORCE Algorithm](02_reinforce_algorithm.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Policy gradients: \"Instead of asking 'how good is this action?', just learn what to do!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
