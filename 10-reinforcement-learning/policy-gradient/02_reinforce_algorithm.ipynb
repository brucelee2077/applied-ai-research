{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE: The Simplest Policy Gradient Algorithm\n",
    "\n",
    "REINFORCE is where policy gradients begin! It's simple, elegant, and the foundation for everything that follows.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The REINFORCE algorithm step-by-step (with a talent show analogy!)\n",
    "- Why it's called \"Monte Carlo Policy Gradient\"\n",
    "- Implementing REINFORCE from scratch in PyTorch\n",
    "- Training on CartPole and analyzing results\n",
    "- The high variance problem (and why it matters)\n",
    "\n",
    "**Prerequisites:** Notebook 1 (Policy Gradient Intuition)\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Talent Show Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE TALENT SHOW ANALOGY                               │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Imagine you're learning to perform in talent shows...        │\n",
    "    │                                                                │\n",
    "    │  REINFORCE is like this:                                      │\n",
    "    │    1. Go on stage with your current act (follow your policy) │\n",
    "    │    2. Perform the WHOLE act (complete episode)               │\n",
    "    │    3. See the judges' total score (return)                   │\n",
    "    │    4. Update your act based on the score:                    │\n",
    "    │       - High score? \"Do MORE of those moves!\"               │\n",
    "    │       - Low score? \"Do LESS of those moves!\"                │\n",
    "    │    5. Repeat at the next show                                │\n",
    "    │                                                                │\n",
    "    │  KEY INSIGHT:                                                 │\n",
    "    │    You only get feedback AFTER the whole performance!        │\n",
    "    │    No coaching during the act.                               │\n",
    "    │    This is \"MONTE CARLO\" - wait for the episode to end!     │\n",
    "    │                                                                │\n",
    "    │  THE PROBLEM:                                                 │\n",
    "    │    Some nights the audience is tough, some nights easy.      │\n",
    "    │    Same act → different scores! (High variance)              │\n",
    "    │    This makes learning noisy and slow.                       │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, Rectangle\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym\n",
    "\n",
    "# Visualize the REINFORCE algorithm\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 9.5, 'REINFORCE Algorithm Flow', ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Step boxes\n",
    "steps = [\n",
    "    ('1. COLLECT EPISODE', 'Follow policy π(a|s;θ)', '#bbdefb', '#1976d2'),\n",
    "    ('2. COMPUTE RETURNS', 'G_t = Σ γᵏ r_{t+k}', '#c8e6c9', '#388e3c'),\n",
    "    ('3. COMPUTE LOSS', '-log π(a|s) × G', '#fff3e0', '#f57c00'),\n",
    "    ('4. UPDATE POLICY', 'θ ← θ - α∇L', '#e1bee7', '#7b1fa2'),\n",
    "]\n",
    "\n",
    "for i, (title, desc, color, edge) in enumerate(steps):\n",
    "    x = 1 + i * 3.2\n",
    "    box = FancyBboxPatch((x, 5), 2.8, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor=edge, linewidth=3)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 1.4, 6.8, title, ha='center', fontsize=10, fontweight='bold')\n",
    "    ax.text(x + 1.4, 5.8, desc, ha='center', fontsize=9)\n",
    "    \n",
    "    if i < 3:\n",
    "        ax.annotate('', xy=(x + 3, 6.25), xytext=(x + 2.9, 6.25),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Loop back arrow\n",
    "ax.annotate('', xy=(1.5, 5), xytext=(12.5, 5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666',\n",
    "                           connectionstyle='arc3,rad=0.4'))\n",
    "ax.text(7, 3.5, 'Repeat for many episodes', ha='center', fontsize=10, style='italic')\n",
    "\n",
    "# Key properties\n",
    "ax.text(7, 2, 'KEY PROPERTIES:', ha='center', fontsize=12, fontweight='bold')\n",
    "ax.text(7, 1.3, '• Monte Carlo: Wait for episode to end • On-policy: Use current policy to collect data', ha='center', fontsize=10)\n",
    "ax.text(7, 0.6, '• Unbiased gradient estimate • High variance (can be noisy)', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THE REINFORCE UPDATE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "For each timestep t in the episode:\n",
    "    1. Compute return: G_t = r_t + γr_{t+1} + γ²r_{t+2} + ...\n",
    "    2. Compute gradient: ∇_θ log π(a_t|s_t; θ) × G_t\n",
    "    3. Update: θ ← θ + α × gradient\n",
    "\n",
    "This increases the probability of actions that led to high returns!\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The REINFORCE Algorithm\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              REINFORCE PSEUDOCODE                              │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Initialize policy network π(a|s; θ) with random weights      │\n",
    "    │                                                                │\n",
    "    │  FOR each episode:                                            │\n",
    "    │      # Step 1: Collect trajectory                            │\n",
    "    │      trajectory = []                                          │\n",
    "    │      state = env.reset()                                      │\n",
    "    │      WHILE not done:                                          │\n",
    "    │          action ~ π(·|state; θ)  # Sample from policy        │\n",
    "    │          next_state, reward, done = env.step(action)         │\n",
    "    │          trajectory.append((state, action, reward))          │\n",
    "    │          state = next_state                                   │\n",
    "    │                                                                │\n",
    "    │      # Step 2: Compute returns (backwards)                   │\n",
    "    │      returns = []                                             │\n",
    "    │      G = 0                                                    │\n",
    "    │      FOR (s, a, r) in reversed(trajectory):                  │\n",
    "    │          G = r + γ × G                                       │\n",
    "    │          returns.insert(0, G)                                 │\n",
    "    │                                                                │\n",
    "    │      # Step 3: Update policy                                 │\n",
    "    │      loss = 0                                                 │\n",
    "    │      FOR (s, a), G in zip(trajectory, returns):              │\n",
    "    │          loss -= log π(a|s; θ) × G                           │\n",
    "    │      θ ← θ - α × ∇_θ loss                                    │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple policy network for discrete actions.\n",
    "    \n",
    "    Architecture:\n",
    "        State → Hidden (128) → Hidden (128) → Action probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Returns action probabilities.\"\"\"\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.FloatTensor(state)\n",
    "        return self.network(state)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Sample action and return both action and log probability.\n",
    "        \n",
    "        Returns:\n",
    "            action: The sampled action\n",
    "            log_prob: Log probability of the action (for gradient computation)\n",
    "        \"\"\"\n",
    "        probs = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "\n",
    "# Create policy for CartPole\n",
    "policy = PolicyNetwork(state_dim=4, action_dim=2)\n",
    "print(\"POLICY NETWORK\")\n",
    "print(\"=\"*60)\n",
    "print(policy)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in policy.parameters()):,}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute discounted returns for each timestep.\n",
    "    \n",
    "    G_t = r_t + γr_{t+1} + γ²r_{t+2} + ...\n",
    "    \n",
    "    We compute this BACKWARDS for efficiency:\n",
    "        G_T = r_T                    (last step)\n",
    "        G_{T-1} = r_{T-1} + γ × G_T  (second-to-last)\n",
    "        ...and so on\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards from the episode\n",
    "        gamma: Discount factor\n",
    "    \n",
    "    Returns:\n",
    "        List of returns, one for each timestep\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    \n",
    "    # Work backwards through the episode\n",
    "    for reward in reversed(rewards):\n",
    "        G = reward + gamma * G\n",
    "        returns.insert(0, G)  # Insert at beginning to maintain order\n",
    "    \n",
    "    return returns\n",
    "\n",
    "\n",
    "# Demonstrate return computation\n",
    "print(\"COMPUTING RETURNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example: 5-step episode\n",
    "rewards = [1, 1, 1, 1, 1]  # Reward of 1 each step\n",
    "gamma = 0.99\n",
    "\n",
    "returns = compute_returns(rewards, gamma)\n",
    "\n",
    "print(f\"\\nRewards: {rewards}\")\n",
    "print(f\"Gamma: {gamma}\")\n",
    "print(f\"\\nReturns (discounted sum of future rewards):\")\n",
    "for t, (r, G) in enumerate(zip(rewards, returns)):\n",
    "    print(f\"  t={t}: r={r}, G_t={G:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Notice: Earlier timesteps have higher returns!\")\n",
    "print(\"They have more future rewards to collect.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env_name='CartPole-v1', n_episodes=500, gamma=0.99, lr=1e-2,\n",
    "              normalize_returns=True, print_every=50):\n",
    "    \"\"\"\n",
    "    REINFORCE algorithm: Monte Carlo Policy Gradient.\n",
    "    \n",
    "    The simplest policy gradient algorithm!\n",
    "    \n",
    "    Args:\n",
    "        env_name: Gymnasium environment name\n",
    "        n_episodes: Number of episodes to train\n",
    "        gamma: Discount factor\n",
    "        lr: Learning rate\n",
    "        normalize_returns: Whether to normalize returns (reduces variance)\n",
    "        print_every: Print progress every N episodes\n",
    "    \n",
    "    Returns:\n",
    "        policy: Trained policy network\n",
    "        rewards_history: List of episode rewards\n",
    "    \"\"\"\n",
    "    # ========================================\n",
    "    # SETUP\n",
    "    # ========================================\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    policy = PolicyNetwork(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    \n",
    "    rewards_history = []\n",
    "    \n",
    "    # ========================================\n",
    "    # TRAINING LOOP\n",
    "    # ========================================\n",
    "    for episode in range(n_episodes):\n",
    "        # ----------------------------------------\n",
    "        # STEP 1: Collect episode trajectory\n",
    "        # ----------------------------------------\n",
    "        state, _ = env.reset()\n",
    "        log_probs = []  # Store log π(a|s) for each step\n",
    "        rewards = []    # Store rewards for each step\n",
    "        \n",
    "        # Run episode until done\n",
    "        for _ in range(500):  # Max steps\n",
    "            # Sample action from policy\n",
    "            action, log_prob = policy.get_action(state)\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # STEP 2: Compute returns\n",
    "        # ----------------------------------------\n",
    "        returns = compute_returns(rewards, gamma)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        \n",
    "        # Optional: Normalize returns (reduces variance!)\n",
    "        if normalize_returns and len(returns) > 1:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # STEP 3: Compute policy gradient loss\n",
    "        # ----------------------------------------\n",
    "        # loss = -Σ log π(a|s) × G\n",
    "        # Negative because we want to MAXIMIZE expected return\n",
    "        # (gradient descent minimizes, so we negate)\n",
    "        loss = 0\n",
    "        for log_prob, G in zip(log_probs, returns):\n",
    "            loss -= log_prob * G\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # STEP 4: Update policy\n",
    "        # ----------------------------------------\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track progress\n",
    "        episode_reward = sum(rewards)\n",
    "        rewards_history.append(episode_reward)\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(rewards_history[-print_every:])\n",
    "            print(f\"Episode {episode+1:4d} | Avg Reward: {avg_reward:6.1f} | Episode: {episode_reward:3.0f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return policy, rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train REINFORCE on CartPole!\n",
    "print(\"TRAINING REINFORCE ON CARTPOLE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nThis may take a minute...\\n\")\n",
    "\n",
    "policy, rewards_history = reinforce(\n",
    "    env_name='CartPole-v1',\n",
    "    n_episodes=500,\n",
    "    gamma=0.99,\n",
    "    lr=1e-2,\n",
    "    normalize_returns=True,\n",
    "    print_every=50\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Final average (last 50): {np.mean(rewards_history[-50:]):.1f}\")\n",
    "print(f\"Best episode: {max(rewards_history):.0f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Raw rewards and smoothed\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rewards_history, alpha=0.3, color='blue', label='Episode Reward')\n",
    "\n",
    "# Smoothed curve\n",
    "window = 50\n",
    "smoothed = np.convolve(rewards_history, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(rewards_history)), smoothed, color='red', linewidth=2, label=f'{window}-Episode Average')\n",
    "\n",
    "ax1.axhline(y=500, color='green', linestyle='--', linewidth=2, label='Max Score (500)')\n",
    "ax1.set_xlabel('Episode', fontsize=11)\n",
    "ax1.set_ylabel('Reward', fontsize=11)\n",
    "ax1.set_title('REINFORCE Training on CartPole', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Reward distribution over time\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Split into quarters\n",
    "quarters = np.array_split(rewards_history, 4)\n",
    "labels = ['0-25%', '25-50%', '50-75%', '75-100%']\n",
    "positions = [1, 2, 3, 4]\n",
    "\n",
    "bp = ax2.boxplot(quarters, positions=positions, patch_artist=True)\n",
    "colors = ['#ffcdd2', '#fff3e0', '#c8e6c9', '#81c784']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax2.set_xticklabels(labels)\n",
    "ax2.set_xlabel('Training Progress', fontsize=11)\n",
    "ax2.set_ylabel('Episode Reward', fontsize=11)\n",
    "ax2.set_title('Reward Distribution Over Training', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOBSERVATIONS:\")\n",
    "print(\"  • The raw rewards are VERY noisy (high variance!)\")\n",
    "print(\"  • The smoothed curve shows learning progress\")\n",
    "print(\"  • Later training episodes have higher and more consistent rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The High Variance Problem\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              THE HIGH VARIANCE PROBLEM                         │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  REINFORCE multiplies: log π(a|s) × G                         │\n",
    "    │                                                                │\n",
    "    │  PROBLEM: G can vary WILDLY between episodes!                 │\n",
    "    │                                                                │\n",
    "    │  Episode 1: G = 50 (bad luck)                                 │\n",
    "    │  Episode 2: G = 300 (good luck)                               │\n",
    "    │  Episode 3: G = 100 (ok)                                      │\n",
    "    │                                                                │\n",
    "    │  Same action → wildly different gradients!                    │\n",
    "    │                                                                │\n",
    "    │  SOURCES OF VARIANCE:                                         │\n",
    "    │  1. Stochastic policy: Different actions each episode        │\n",
    "    │  2. Stochastic environment: Randomness in transitions        │\n",
    "    │  3. Monte Carlo: Must wait for full episode                  │\n",
    "    │  4. Credit assignment: Early actions credit for late rewards │\n",
    "    │                                                                │\n",
    "    │  WHY THIS MATTERS:                                            │\n",
    "    │  • Slow learning (gradients point in wrong direction)        │\n",
    "    │  • Need more samples to converge                             │\n",
    "    │  • Unstable training                                         │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the variance problem\n",
    "\n",
    "# Run multiple episodes with trained policy to see variance\n",
    "env = gym.make('CartPole-v1')\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "for _ in range(100):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(500):\n",
    "        with torch.no_grad():\n",
    "            action, _ = policy.get_action(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(step + 1)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Visualize variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Distribution of returns\n",
    "ax1 = axes[0]\n",
    "ax1.hist(episode_rewards, bins=20, color='#64b5f6', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(x=np.mean(episode_rewards), color='red', linewidth=2, linestyle='--', \n",
    "            label=f'Mean: {np.mean(episode_rewards):.1f}')\n",
    "ax1.axvline(x=np.mean(episode_rewards) + np.std(episode_rewards), color='orange', linewidth=2, linestyle=':',\n",
    "            label=f'Std: ±{np.std(episode_rewards):.1f}')\n",
    "ax1.axvline(x=np.mean(episode_rewards) - np.std(episode_rewards), color='orange', linewidth=2, linestyle=':')\n",
    "ax1.set_xlabel('Episode Reward', fontsize=11)\n",
    "ax1.set_ylabel('Count', fontsize=11)\n",
    "ax1.set_title('Variance in Episode Returns\\n(Same Policy, Different Outcomes)', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Returns over episodes\n",
    "ax2 = axes[1]\n",
    "ax2.plot(episode_rewards, 'o-', alpha=0.7, markersize=3)\n",
    "ax2.axhline(y=np.mean(episode_rewards), color='red', linewidth=2, linestyle='--', label='Mean')\n",
    "ax2.fill_between(range(100), \n",
    "                 np.mean(episode_rewards) - np.std(episode_rewards),\n",
    "                 np.mean(episode_rewards) + np.std(episode_rewards),\n",
    "                 alpha=0.2, color='red', label='±1 Std')\n",
    "ax2.set_xlabel('Episode', fontsize=11)\n",
    "ax2.set_ylabel('Reward', fontsize=11)\n",
    "ax2.set_title('Episode-to-Episode Variance\\n(Even with trained policy!)', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVARIANCE STATISTICS:\")\n",
    "print(f\"  Mean return: {np.mean(episode_rewards):.1f}\")\n",
    "print(f\"  Std return: {np.std(episode_rewards):.1f}\")\n",
    "print(f\"  Min: {min(episode_rewards):.0f}, Max: {max(episode_rewards):.0f}\")\n",
    "print(f\"\\n  This variance is HUGE! Same policy gives very different outcomes.\")\n",
    "print(f\"  During training, this makes gradient estimates very noisy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show effect of return normalization\n",
    "\n",
    "def reinforce_comparison(env_name='CartPole-v1', n_episodes=300, gamma=0.99, lr=1e-2):\n",
    "    \"\"\"Compare REINFORCE with and without return normalization.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for normalize in [False, True]:\n",
    "        name = \"With Normalization\" if normalize else \"Without Normalization\"\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        _, rewards = reinforce(\n",
    "            env_name=env_name,\n",
    "            n_episodes=n_episodes,\n",
    "            gamma=gamma,\n",
    "            lr=lr,\n",
    "            normalize_returns=normalize,\n",
    "            print_every=100\n",
    "        )\n",
    "        results[name] = rewards\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"COMPARING WITH AND WITHOUT NORMALIZATION\")\n",
    "print(\"=\"*60)\n",
    "results = reinforce_comparison(n_episodes=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "window = 30\n",
    "for name, rewards in results.items():\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    color = '#4caf50' if 'With' in name else '#f44336'\n",
    "    ax.plot(range(window-1, len(rewards)), smoothed, linewidth=2, label=name, color=color)\n",
    "\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Reward (smoothed)', fontsize=11)\n",
    "ax.set_title('Effect of Return Normalization on REINFORCE', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNormalization typically helps by:\")\n",
    "print(\"  • Centering returns around 0 (negative for bad, positive for good)\")\n",
    "print(\"  • Scaling to similar magnitude\")\n",
    "print(\"  • Reducing variance in gradient estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### REINFORCE Algorithm\n",
    "\n",
    "```\n",
    "For each episode:\n",
    "    1. Collect trajectory: (s₀, a₀, r₀), (s₁, a₁, r₁), ...\n",
    "    2. Compute returns: G_t = Σ γᵏ r_{t+k}\n",
    "    3. Update policy: θ ← θ + α × Σ ∇log π(aₜ|sₜ) × Gₜ\n",
    "```\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "| Property | Description |\n",
    "|----------|-------------|\n",
    "| **Monte Carlo** | Wait for episode to end |\n",
    "| **On-policy** | Use current policy to collect data |\n",
    "| **Unbiased** | Gradient estimate is unbiased |\n",
    "| **High Variance** | Returns can vary wildly |\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Simple to implement | High variance |\n",
    "| Unbiased gradients | Needs complete episodes |\n",
    "| Works with continuous actions | Sample inefficient |\n",
    "| Foundation for advanced methods | Slow to converge |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why is REINFORCE called \"Monte Carlo\" policy gradient?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Because it uses complete episode returns (G_t) rather than bootstrapped estimates. Monte Carlo methods wait until the episode ends to compute the true return, rather than estimating it from value functions. This makes the gradient estimate unbiased but high variance.\n",
    "</details>\n",
    "\n",
    "**2. What does the loss `-log π(a|s) × G` mean intuitively?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "- `-log π(a|s)`: How \"surprising\" was this action (lower prob = more surprising = higher loss)\n",
    "- `× G`: Scale by how good the outcome was\n",
    "\n",
    "If G is high and π(a|s) was low, we increase π(a|s) a lot (make this good action more likely).\n",
    "If G is low, we decrease the probability of that action.\n",
    "</details>\n",
    "\n",
    "**3. Why do we normalize returns?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Normalization reduces variance by:\n",
    "1. Centering returns around 0 (negative = below average, positive = above average)\n",
    "2. Scaling to consistent magnitude across episodes\n",
    "\n",
    "This makes gradient updates more stable, even though it technically adds some bias.\n",
    "</details>\n",
    "\n",
    "**4. Why is REINFORCE high variance?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Several sources of variance:\n",
    "1. Stochastic policy samples different actions\n",
    "2. Environment may have randomness\n",
    "3. Returns include ALL future rewards (credit assignment problem)\n",
    "4. Single episode can have very different outcomes\n",
    "\n",
    "The same action might lead to return 50 in one episode and 300 in another!\n",
    "</details>\n",
    "\n",
    "**5. What's the credit assignment problem?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Each action's gradient is weighted by G_t, which includes ALL future rewards. But early actions shouldn't get credit for rewards that came from good late actions! For example, in a 100-step episode, the first action's gradient uses all 100 rewards, even though most of them had nothing to do with that first action.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "REINFORCE works but has high variance. In the next notebook, we'll learn **variance reduction techniques** that make policy gradients practical!\n",
    "\n",
    "**Continue to:** [Notebook 3: Variance Reduction](03_variance_reduction.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*REINFORCE: Simple, elegant, but needs help with variance!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
