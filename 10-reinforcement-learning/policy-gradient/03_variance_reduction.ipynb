{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Reduction: Making Policy Gradients Practical\n",
    "\n",
    "REINFORCE is elegant but has high variance. This notebook covers the key techniques that make policy gradients work in practice!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The exam grading analogy: why baselines help\n",
    "- Baseline subtraction: the variance reduction trick\n",
    "- Why V(s) is the optimal baseline\n",
    "- The advantage function A(s, a)\n",
    "- Implementing baselines in PyTorch\n",
    "- Comparing REINFORCE with and without baseline\n",
    "\n",
    "**Prerequisites:** Notebook 2 (REINFORCE Algorithm)\n",
    "\n",
    "**Time:** ~25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Exam Grading Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE EXAM GRADING ANALOGY                              │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Imagine you're a student getting exam scores...              │\n",
    "    │                                                                │\n",
    "    │  WITHOUT BASELINE (Raw Scores):                               │\n",
    "    │    \"You scored 75/100 on the exam\"                           │\n",
    "    │    Is this good? Bad? Hard to tell!                          │\n",
    "    │    • Easy exam: 75 is bad (everyone got 90+)                 │\n",
    "    │    • Hard exam: 75 is great (average was 50)                 │\n",
    "    │                                                                │\n",
    "    │  WITH BASELINE (Curve Grading):                               │\n",
    "    │    \"You scored 75, class average was 60\"                     │\n",
    "    │    Advantage = 75 - 60 = +15                                 │\n",
    "    │    This is CLEARLY good! You beat the baseline!              │\n",
    "    │                                                                │\n",
    "    │  HOW THIS HELPS:                                              │\n",
    "    │    • If return > baseline: Action was BETTER than average    │\n",
    "    │    • If return < baseline: Action was WORSE than average     │\n",
    "    │    • Much clearer signal for learning!                       │\n",
    "    │                                                                │\n",
    "    │  THE MAGIC: Subtracting baseline doesn't change expected     │\n",
    "    │  gradient, but REDUCES VARIANCE!                             │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym\n",
    "\n",
    "# Visualize why baselines help\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Without baseline (high variance)\n",
    "ax1 = axes[0]\n",
    "np.random.seed(42)\n",
    "returns = np.random.normal(100, 30, 50)  # Mean 100, std 30\n",
    "\n",
    "ax1.bar(range(len(returns)), returns, color='#64b5f6', edgecolor='black', linewidth=0.5)\n",
    "ax1.axhline(y=0, color='black', linewidth=2)\n",
    "ax1.axhline(y=100, color='red', linewidth=2, linestyle='--', label='Mean: 100')\n",
    "ax1.set_xlabel('Episode', fontsize=11)\n",
    "ax1.set_ylabel('Return G', fontsize=11)\n",
    "ax1.set_title('Without Baseline\\n(All positive → Always increase!)', fontsize=12, fontweight='bold', color='#d32f2f')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.text(25, 20, 'Problem: ALL returns are positive!\\nEven bad actions get reinforced!', \n",
    "         ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "# Right: With baseline (centered)\n",
    "ax2 = axes[1]\n",
    "baseline = 100\n",
    "advantages = returns - baseline  # Subtract baseline\n",
    "\n",
    "colors = ['#4caf50' if a > 0 else '#f44336' for a in advantages]\n",
    "ax2.bar(range(len(advantages)), advantages, color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax2.axhline(y=0, color='black', linewidth=2, label='Baseline')\n",
    "ax2.set_xlabel('Episode', fontsize=11)\n",
    "ax2.set_ylabel('Advantage (G - baseline)', fontsize=11)\n",
    "ax2.set_title('With Baseline\\n(Clear good vs bad signal!)', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.text(25, -50, 'Better: Green = above average (reinforce!)\\nRed = below average (discourage!)', \n",
    "         ha='center', fontsize=10, color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWHY BASELINES HELP:\")\n",
    "print(f\"  Without baseline: All returns positive (mean {np.mean(returns):.0f})\")\n",
    "print(f\"  With baseline: Centered around 0 (mean {np.mean(advantages):.0f})\")\n",
    "print(f\"\\n  Variance unchanged: {np.std(returns):.1f} vs {np.std(advantages):.1f}\")\n",
    "print(f\"  But now we can clearly distinguish good from bad!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Math: Why Baselines Don't Add Bias\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              BASELINE SUBTRACTION MATH                         │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  ORIGINAL GRADIENT:                                           │\n",
    "    │    ∇_θ J = E[ ∇_θ log π(a|s) × G ]                            │\n",
    "    │                                                                │\n",
    "    │  WITH BASELINE b(s):                                          │\n",
    "    │    ∇_θ J = E[ ∇_θ log π(a|s) × (G - b(s)) ]                   │\n",
    "    │                                                                │\n",
    "    │  WHY DOESN'T THIS ADD BIAS?                                   │\n",
    "    │                                                                │\n",
    "    │    E[ ∇_θ log π(a|s) × b(s) ]                                 │\n",
    "    │    = b(s) × E[ ∇_θ log π(a|s) ]  (b doesn't depend on a)     │\n",
    "    │    = b(s) × ∇_θ Σ_a π(a|s)       (by definition of log grad) │\n",
    "    │    = b(s) × ∇_θ 1               (probabilities sum to 1)     │\n",
    "    │    = 0                                                        │\n",
    "    │                                                                │\n",
    "    │  KEY INSIGHT: Any baseline that doesn't depend on a can be   │\n",
    "    │  subtracted without changing the expected gradient!          │\n",
    "    │                                                                │\n",
    "    │  OPTIMAL BASELINE:                                            │\n",
    "    │    b*(s) = E[G|s] = V(s)                                     │\n",
    "    │    The value function minimizes variance!                    │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate variance reduction mathematically\n",
    "\n",
    "print(\"VARIANCE REDUCTION DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simulate returns from a state\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# True expected return from this state\n",
    "true_value = 50\n",
    "\n",
    "# Actual returns have variance\n",
    "returns = np.random.normal(true_value, 20, n_samples)\n",
    "\n",
    "# Simulate log probabilities (doesn't matter what they are for this demo)\n",
    "log_probs = np.random.randn(n_samples)\n",
    "\n",
    "# Gradient estimates without baseline\n",
    "grad_no_baseline = log_probs * returns\n",
    "\n",
    "# Gradient estimates with baseline (using true V(s))\n",
    "baseline = true_value\n",
    "grad_with_baseline = log_probs * (returns - baseline)\n",
    "\n",
    "print(f\"\\nTrue V(s) = {true_value}\")\n",
    "print(f\"\\nGradient WITHOUT baseline:\")\n",
    "print(f\"  Mean: {np.mean(grad_no_baseline):.2f}\")\n",
    "print(f\"  Variance: {np.var(grad_no_baseline):.2f}\")\n",
    "print(f\"  Std: {np.std(grad_no_baseline):.2f}\")\n",
    "\n",
    "print(f\"\\nGradient WITH baseline (V(s)):\")\n",
    "print(f\"  Mean: {np.mean(grad_with_baseline):.2f}\")\n",
    "print(f\"  Variance: {np.var(grad_with_baseline):.2f}\")\n",
    "print(f\"  Std: {np.std(grad_with_baseline):.2f}\")\n",
    "\n",
    "variance_reduction = (np.var(grad_no_baseline) - np.var(grad_with_baseline)) / np.var(grad_no_baseline)\n",
    "print(f\"\\n  Variance reduced by: {variance_reduction*100:.1f}%!\")\n",
    "print(f\"  Same expected value, much lower variance!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the variance reduction\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Distribution of gradient estimates\n",
    "ax1 = axes[0]\n",
    "ax1.hist(grad_no_baseline, bins=50, alpha=0.5, color='red', label='No baseline', density=True)\n",
    "ax1.hist(grad_with_baseline, bins=50, alpha=0.5, color='green', label='With baseline', density=True)\n",
    "ax1.axvline(x=0, color='black', linewidth=2, linestyle='--')\n",
    "ax1.set_xlabel('Gradient Estimate', fontsize=11)\n",
    "ax1.set_ylabel('Density', fontsize=11)\n",
    "ax1.set_title('Distribution of Gradient Estimates', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Effect of baseline choice\n",
    "ax2 = axes[1]\n",
    "baselines = np.linspace(0, 100, 50)\n",
    "variances = []\n",
    "\n",
    "for b in baselines:\n",
    "    grad = log_probs * (returns - b)\n",
    "    variances.append(np.var(grad))\n",
    "\n",
    "ax2.plot(baselines, variances, 'b-', linewidth=2)\n",
    "ax2.axvline(x=true_value, color='red', linewidth=2, linestyle='--', label=f'V(s) = {true_value}')\n",
    "ax2.scatter([true_value], [min(variances)], c='red', s=100, zorder=5, label='Minimum variance')\n",
    "ax2.set_xlabel('Baseline Value', fontsize=11)\n",
    "ax2.set_ylabel('Gradient Variance', fontsize=11)\n",
    "ax2.set_title('V(s) is the Optimal Baseline!', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHT:\")\n",
    "print(\"  The value function V(s) is the optimal baseline!\")\n",
    "print(\"  It minimizes gradient variance while keeping it unbiased.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Advantage Function\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              THE ADVANTAGE FUNCTION                            │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  DEFINITION:                                                  │\n",
    "    │    A(s, a) = Q(s, a) - V(s)                                   │\n",
    "    │                                                                │\n",
    "    │  INTUITION:                                                   │\n",
    "    │    • Q(s, a): Expected return from taking a in state s       │\n",
    "    │    • V(s): Expected return from state s (under policy)       │\n",
    "    │    • A(s, a): How much BETTER is action a than average?      │\n",
    "    │                                                                │\n",
    "    │  PROPERTIES:                                                  │\n",
    "    │    • A(s, a) > 0: Action a is BETTER than average            │\n",
    "    │    • A(s, a) < 0: Action a is WORSE than average             │\n",
    "    │    • E[A(s, a)] = 0: Advantages sum to zero!                 │\n",
    "    │                                                                │\n",
    "    │  IN PRACTICE:                                                 │\n",
    "    │    We estimate A using returns and learned V(s):             │\n",
    "    │    A_t ≈ G_t - V(s_t)                                        │\n",
    "    │                                                                │\n",
    "    │  THE POLICY GRADIENT BECOMES:                                 │\n",
    "    │    ∇_θ J = E[ ∇_θ log π(a|s) × A(s, a) ]                     │\n",
    "    │                                                                │\n",
    "    │  This is what Actor-Critic methods use!                      │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the advantage function\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Example: 3 actions from some state\n",
    "actions = ['Action A\\n(Good)', 'Action B\\n(Average)', 'Action C\\n(Bad)']\n",
    "q_values = [80, 50, 20]  # Q(s, a)\n",
    "v_value = 50  # V(s) = average over all actions\n",
    "advantages = [q - v_value for q in q_values]  # A(s, a) = Q(s, a) - V(s)\n",
    "\n",
    "x = np.arange(len(actions))\n",
    "width = 0.35\n",
    "\n",
    "# Bar chart\n",
    "bars_q = ax.bar(x - width/2, q_values, width, label='Q(s, a)', color='#64b5f6', edgecolor='black')\n",
    "colors_a = ['#4caf50' if a > 0 else '#f44336' for a in advantages]\n",
    "bars_a = ax.bar(x + width/2, advantages, width, label='A(s, a)', color=colors_a, edgecolor='black')\n",
    "\n",
    "ax.axhline(y=v_value, color='orange', linewidth=2, linestyle='--', label=f'V(s) = {v_value}')\n",
    "ax.axhline(y=0, color='black', linewidth=1)\n",
    "\n",
    "ax.set_ylabel('Value', fontsize=11)\n",
    "ax.set_title('Q-Values vs Advantages\\nA(s,a) = Q(s,a) - V(s)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(actions)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Annotations\n",
    "for i, (q, a) in enumerate(zip(q_values, advantages)):\n",
    "    sign = '+' if a > 0 else ''\n",
    "    ax.annotate(f'A={sign}{a}', xy=(i + width/2, a), xytext=(i + width/2 + 0.1, a + 5),\n",
    "                fontsize=10, fontweight='bold', color=colors_a[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nADVANTAGE INTERPRETATION:\")\n",
    "print(f\"  V(s) = {v_value} (expected return from this state)\")\n",
    "for action, q, a in zip(actions, q_values, advantages):\n",
    "    sign = '+' if a > 0 else ''\n",
    "    verdict = \"REINFORCE\" if a > 0 else \"DISCOURAGE\"\n",
    "    action_name = action.split('\\n')[0]\n",
    "    print(f\"  {action_name}: Q={q}, A={sign}{a} → {verdict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implementing REINFORCE with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyWithBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy network with value function baseline.\n",
    "    \n",
    "    Two heads on shared features:\n",
    "    - Policy head: π(a|s) for action selection\n",
    "    - Value head: V(s) for baseline\n",
    "    \n",
    "    This architecture is used in Actor-Critic!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared feature extraction\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Policy head: outputs action probabilities\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Value head: outputs V(s) estimate\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Returns both policy and value.\"\"\"\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.FloatTensor(state)\n",
    "        \n",
    "        features = self.shared(state)\n",
    "        action_probs = self.policy_head(features)\n",
    "        value = self.value_head(features)\n",
    "        \n",
    "        return action_probs, value\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Sample action and return action, log_prob, and value.\"\"\"\n",
    "        action_probs, value = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob, value\n",
    "\n",
    "\n",
    "# Demonstrate\n",
    "policy_baseline = PolicyWithBaseline(state_dim=4, action_dim=2)\n",
    "print(\"POLICY WITH BASELINE\")\n",
    "print(\"=\"*60)\n",
    "print(policy_baseline)\n",
    "\n",
    "# Test forward pass\n",
    "test_state = torch.randn(1, 4)\n",
    "probs, value = policy_baseline(test_state)\n",
    "print(f\"\\nTest state: {test_state.numpy()[0].round(2)}\")\n",
    "print(f\"Action probs: {probs.detach().numpy()[0].round(3)}\")\n",
    "print(f\"V(s): {value.item():.3f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_with_baseline(env_name='CartPole-v1', n_episodes=500, gamma=0.99, \n",
    "                            lr=1e-2, print_every=50):\n",
    "    \"\"\"\n",
    "    REINFORCE with learned value function baseline.\n",
    "    \n",
    "    Key differences from vanilla REINFORCE:\n",
    "    1. Learn V(s) alongside π(a|s)\n",
    "    2. Use advantage A = G - V(s) instead of raw return\n",
    "    3. Two losses: policy loss + value loss\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    policy = PolicyWithBaseline(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    \n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # Collect episode\n",
    "        # ----------------------------------------\n",
    "        for _ in range(500):\n",
    "            action, log_prob, value = policy.get_action(state)\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # Compute returns and advantages\n",
    "        # ----------------------------------------\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        \n",
    "        values = torch.cat(values)\n",
    "        \n",
    "        # Advantage = Return - Baseline\n",
    "        advantages = returns - values.detach()  # detach to not backprop through baseline\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # Compute losses\n",
    "        # ----------------------------------------\n",
    "        # Policy loss: -log π × A\n",
    "        policy_loss = 0\n",
    "        for log_prob, adv in zip(log_probs, advantages):\n",
    "            policy_loss -= log_prob * adv\n",
    "        \n",
    "        # Value loss: (V(s) - G)²\n",
    "        value_loss = nn.functional.mse_loss(values, returns)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = policy_loss + 0.5 * value_loss  # 0.5 weight for value loss\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # Update\n",
    "        # ----------------------------------------\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track progress\n",
    "        episode_reward = sum(rewards)\n",
    "        rewards_history.append(episode_reward)\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(rewards_history[-print_every:])\n",
    "            print(f\"Episode {episode+1:4d} | Avg Reward: {avg_reward:6.1f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return policy, rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and compare\n",
    "print(\"COMPARING VANILLA REINFORCE vs REINFORCE WITH BASELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Training REINFORCE with Baseline...\\n\")\n",
    "policy_with, rewards_with = reinforce_with_baseline(\n",
    "    n_episodes=500, print_every=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also train vanilla REINFORCE for comparison\n",
    "def reinforce_vanilla(env_name='CartPole-v1', n_episodes=500, gamma=0.99, lr=1e-2):\n",
    "    \"\"\"Vanilla REINFORCE without baseline.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    policy = nn.Sequential(\n",
    "        nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "        nn.Linear(128, 128), nn.ReLU(),\n",
    "        nn.Linear(128, action_dim), nn.Softmax(dim=-1)\n",
    "    )\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    \n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        \n",
    "        for _ in range(500):\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            probs = policy(state_tensor)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_probs.append(dist.log_prob(action))\n",
    "            \n",
    "            state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        \n",
    "        # Vanilla REINFORCE loss (no baseline!)\n",
    "        loss = 0\n",
    "        for log_prob, G in zip(log_probs, returns):\n",
    "            loss -= log_prob * G\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        rewards_history.append(sum(rewards))\n",
    "    \n",
    "    env.close()\n",
    "    return rewards_history\n",
    "\n",
    "print(\"\\n2. Training Vanilla REINFORCE (no baseline)...\")\n",
    "rewards_vanilla = reinforce_vanilla(n_episodes=500)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Learning curves\n",
    "ax1 = axes[0]\n",
    "window = 50\n",
    "\n",
    "smoothed_with = np.convolve(rewards_with, np.ones(window)/window, mode='valid')\n",
    "smoothed_vanilla = np.convolve(rewards_vanilla, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax1.plot(range(window-1, len(rewards_vanilla)), smoothed_vanilla, \n",
    "         color='red', linewidth=2, label='Vanilla REINFORCE')\n",
    "ax1.plot(range(window-1, len(rewards_with)), smoothed_with, \n",
    "         color='green', linewidth=2, label='With Baseline')\n",
    "ax1.axhline(y=500, color='gray', linestyle='--', linewidth=1, label='Max Score')\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=11)\n",
    "ax1.set_ylabel('Reward (50-ep moving avg)', fontsize=11)\n",
    "ax1.set_title('Learning Speed: Baseline Helps!', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Variance comparison\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Compute rolling std\n",
    "rolling_std_vanilla = [np.std(rewards_vanilla[max(0,i-50):i+1]) for i in range(len(rewards_vanilla))]\n",
    "rolling_std_with = [np.std(rewards_with[max(0,i-50):i+1]) for i in range(len(rewards_with))]\n",
    "\n",
    "ax2.plot(rolling_std_vanilla, color='red', alpha=0.5, linewidth=1, label='Vanilla')\n",
    "ax2.plot(rolling_std_with, color='green', alpha=0.5, linewidth=1, label='With Baseline')\n",
    "\n",
    "# Smoothed std\n",
    "ax2.plot(np.convolve(rolling_std_vanilla, np.ones(30)/30, mode='valid'), \n",
    "         color='red', linewidth=2)\n",
    "ax2.plot(np.convolve(rolling_std_with, np.ones(30)/30, mode='valid'), \n",
    "         color='green', linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Episode', fontsize=11)\n",
    "ax2.set_ylabel('Rolling Std (50 episodes)', fontsize=11)\n",
    "ax2.set_title('Variance: Baseline Reduces It!', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFINAL STATISTICS:\")\n",
    "print(f\"  Vanilla REINFORCE - Final avg: {np.mean(rewards_vanilla[-50:]):.1f}\")\n",
    "print(f\"  With Baseline - Final avg: {np.mean(rewards_with[-50:]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### Baseline Subtraction\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Baseline** | Value to subtract from returns |\n",
    "| **Purpose** | Reduce variance without adding bias |\n",
    "| **Optimal** | V(s) = expected return from state |\n",
    "\n",
    "### The Advantage Function\n",
    "\n",
    "```\n",
    "A(s, a) = Q(s, a) - V(s)\n",
    "        = \"How much better is this action than average?\"\n",
    "\n",
    "A > 0: Action is better than average → Reinforce!\n",
    "A < 0: Action is worse than average → Discourage!\n",
    "```\n",
    "\n",
    "### Policy Gradient with Baseline\n",
    "\n",
    "```\n",
    "∇_θ J = E[ ∇_θ log π(a|s) × A(s, a) ]\n",
    "      = E[ ∇_θ log π(a|s) × (G - V(s)) ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why does subtracting a baseline not add bias?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Because E[∇log π(a|s) × b(s)] = 0 when b doesn't depend on the action a. The math shows that ∇Σπ(a|s) = ∇1 = 0 since probabilities sum to 1. So we can subtract any state-dependent baseline without changing the expected gradient!\n",
    "</details>\n",
    "\n",
    "**2. Why is V(s) the optimal baseline?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "V(s) minimizes the variance of the gradient estimate. Intuitively, it's the expected return from state s, so (G - V(s)) measures how much better or worse this specific trajectory was compared to average. This gives a clearer signal than raw returns.\n",
    "</details>\n",
    "\n",
    "**3. What is the advantage function?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "A(s, a) = Q(s, a) - V(s) measures how much better action a is compared to the average action in state s. Positive advantage means the action is better than average; negative means worse. This is what Actor-Critic methods optimize!\n",
    "</details>\n",
    "\n",
    "**4. Why do we have two losses (policy and value)?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "- Policy loss trains π(a|s) to take actions with high advantage\n",
    "- Value loss trains V(s) to accurately predict returns (better baseline)\n",
    "\n",
    "Both are needed: without good V(s) estimates, the baseline is useless. Without policy loss, we never learn which actions are good.\n",
    "</details>\n",
    "\n",
    "**5. How much does baseline help in practice?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "A LOT! Without baseline, all returns are typically positive, so all actions get reinforced (just by different amounts). With baseline, we clearly distinguish better-than-average (reinforce) from worse-than-average (discourage). This dramatically speeds up learning and reduces variance.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "We've now seen how to reduce variance with baselines. But we still wait for full episodes! In the next notebook, we'll learn **Actor-Critic** methods that update after every step!\n",
    "\n",
    "**Continue to:** [Notebook 4: Actor-Critic Methods](04_actor_critic.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Baselines: Simple idea, huge impact!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
