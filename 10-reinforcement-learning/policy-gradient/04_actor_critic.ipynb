{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic: The Best of Both Worlds\n",
    "\n",
    "Actor-Critic methods combine policy gradients with value functions, getting the benefits of both!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The acting coach analogy: why immediate feedback matters\n",
    "- The actor-critic architecture (policy + value)\n",
    "- TD learning for the critic (bootstrapping)\n",
    "- Why actor-critic beats REINFORCE\n",
    "- Implementing actor-critic from scratch\n",
    "- Online vs episodic updates\n",
    "\n",
    "**Prerequisites:** Notebook 3 (Variance Reduction)\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Acting Coach Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE ACTING COACH ANALOGY                              │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Imagine learning to act in a play...                         │\n",
    "    │                                                                │\n",
    "    │  REINFORCE (Monte Carlo):                                     │\n",
    "    │    Perform the ENTIRE play, then get feedback:                │\n",
    "    │    \"The audience gave you 7/10 for the whole performance\"     │\n",
    "    │    Which scene was good? Which was bad? Hard to tell!         │\n",
    "    │                                                                │\n",
    "    │  ACTOR-CRITIC (TD Learning):                                  │\n",
    "    │    A coach watches EACH scene and gives feedback:             │\n",
    "    │    \"That scene was better than I expected! Good job!\"         │\n",
    "    │    \"That scene was worse than expected. Try differently.\"     │\n",
    "    │    You improve after EVERY scene, not just at the end!        │\n",
    "    │                                                                │\n",
    "    │  THE TWO ROLES:                                               │\n",
    "    │    ACTOR (Policy): You, the performer, deciding how to act    │\n",
    "    │    CRITIC (Value): The coach, judging how good things are     │\n",
    "    │                                                                │\n",
    "    │  WHY THIS WORKS:                                              │\n",
    "    │    • Immediate feedback → faster learning                     │\n",
    "    │    • Critic reduces variance by predicting outcomes           │\n",
    "    │    • No need to wait for episode to end!                      │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, Rectangle\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym\n",
    "\n",
    "# Visualize Actor-Critic vs REINFORCE\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: REINFORCE (wait for episode end)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('REINFORCE (Monte Carlo)\\n\"Wait for the whole play\"', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Episode timeline\n",
    "for i in range(5):\n",
    "    x = 1.5 + i * 1.5\n",
    "    box = FancyBboxPatch((x, 5.5), 1, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#bbdefb', edgecolor='#1976d2', linewidth=2)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(x + 0.5, 6.25, f'Step {i+1}', ha='center', fontsize=9)\n",
    "    if i < 4:\n",
    "        ax1.annotate('', xy=(x + 1.1, 6.25), xytext=(x + 1.4, 6.25),\n",
    "                     arrowprops=dict(arrowstyle='->', lw=1, color='#666'))\n",
    "\n",
    "# Final feedback arrow\n",
    "ax1.annotate('', xy=(5, 4.5), xytext=(8.5, 5.4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f',\n",
    "                            connectionstyle='arc3,rad=-0.3'))\n",
    "ax1.text(5, 3.5, 'Total Return: G = 47', ha='center', fontsize=11, \n",
    "         color='#d32f2f', fontweight='bold')\n",
    "ax1.text(5, 2.5, '\"How was the whole thing?\"', ha='center', fontsize=10, \n",
    "         style='italic', color='#666')\n",
    "\n",
    "ax1.text(5, 1.5, '❌ Wait until end\\n❌ High variance\\n❌ Slow credit assignment', \n",
    "         ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "# Right: Actor-Critic (immediate feedback)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Actor-Critic (TD Learning)\\n\"Feedback after each scene\"', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Episode timeline with feedback arrows\n",
    "for i in range(5):\n",
    "    x = 1.5 + i * 1.5\n",
    "    box = FancyBboxPatch((x, 5.5), 1, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(x + 0.5, 6.25, f'Step {i+1}', ha='center', fontsize=9)\n",
    "    \n",
    "    # Immediate feedback arrow\n",
    "    ax2.annotate('', xy=(x + 0.5, 5.4), xytext=(x + 0.5, 4.2),\n",
    "                 arrowprops=dict(arrowstyle='->', lw=1, color='#388e3c'))\n",
    "    \n",
    "    if i < 4:\n",
    "        ax2.annotate('', xy=(x + 1.1, 6.25), xytext=(x + 1.4, 6.25),\n",
    "                     arrowprops=dict(arrowstyle='->', lw=1, color='#666'))\n",
    "\n",
    "# TD errors\n",
    "td_errors = ['+2', '+1', '-1', '+3', '+2']\n",
    "for i, td in enumerate(td_errors):\n",
    "    x = 1.5 + i * 1.5\n",
    "    color = '#388e3c' if '+' in td else '#d32f2f'\n",
    "    ax2.text(x + 0.5, 3.8, f'δ={td}', ha='center', fontsize=9, color=color, fontweight='bold')\n",
    "\n",
    "ax2.text(5, 2.5, '\"Better than expected!\" or \"Worse than expected!\"', \n",
    "         ha='center', fontsize=10, style='italic', color='#666')\n",
    "\n",
    "ax2.text(5, 1.5, '✓ Immediate updates\\n✓ Lower variance\\n✓ Faster learning', \n",
    "         ha='center', fontsize=10, color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHT:\")\n",
    "print(\"  REINFORCE: Learn from total return (high variance, slow)\")\n",
    "print(\"  Actor-Critic: Learn from TD error (lower variance, fast)\")\n",
    "print(\"  TD error δ = 'How much better/worse than expected?'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Actor-Critic Architecture\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              ACTOR-CRITIC ARCHITECTURE                         │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │                     ┌─────────────┐                            │\n",
    "    │                     │    STATE    │                            │\n",
    "    │                     │     s_t     │                            │\n",
    "    │                     └──────┬──────┘                            │\n",
    "    │                            │                                   │\n",
    "    │                 ┌──────────┴──────────┐                        │\n",
    "    │                 │                     │                        │\n",
    "    │          ┌──────▼──────┐       ┌──────▼──────┐                 │\n",
    "    │          │   ACTOR     │       │   CRITIC    │                 │\n",
    "    │          │   π(a|s;θ)  │       │   V(s;w)    │                 │\n",
    "    │          │             │       │             │                 │\n",
    "    │          │ \"What to    │       │ \"How good   │                 │\n",
    "    │          │   do?\"      │       │  is this?\"  │                 │\n",
    "    │          └──────┬──────┘       └──────┬──────┘                 │\n",
    "    │                 │                     │                        │\n",
    "    │                 ▼                     ▼                        │\n",
    "    │            Action a_t          Value V(s_t)                    │\n",
    "    │                                                                │\n",
    "    │  ACTOR updates using policy gradient + critic's feedback       │\n",
    "    │  CRITIC updates using TD learning                              │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Actor-Critic architecture in detail\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('Actor-Critic Architecture: Two Networks, One Goal', fontsize=16, fontweight='bold')\n",
    "\n",
    "# State input\n",
    "state_box = FancyBboxPatch((5.5, 10), 3, 1, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(state_box)\n",
    "ax.text(7, 10.5, 'State s', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Shared features (optional in some implementations)\n",
    "shared_box = FancyBboxPatch((5.5, 8), 3, 1, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax.add_patch(shared_box)\n",
    "ax.text(7, 8.5, 'Shared Features\\n(optional)', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Actor\n",
    "actor_box = FancyBboxPatch((1.5, 5), 4, 2, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(actor_box)\n",
    "ax.text(3.5, 6.3, 'ACTOR', ha='center', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax.text(3.5, 5.7, 'Policy π(a|s; θ)', ha='center', fontsize=10)\n",
    "ax.text(3.5, 5.2, '\"What action to take?\"', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# Critic\n",
    "critic_box = FancyBboxPatch((8.5, 5), 4, 2, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#e1bee7', edgecolor='#7b1fa2', linewidth=3)\n",
    "ax.add_patch(critic_box)\n",
    "ax.text(10.5, 6.3, 'CRITIC', ha='center', fontsize=12, fontweight='bold', color='#7b1fa2')\n",
    "ax.text(10.5, 5.7, 'Value V(s; w)', ha='center', fontsize=10)\n",
    "ax.text(10.5, 5.2, '\"How good is this state?\"', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# Arrows from shared to actor/critic\n",
    "ax.annotate('', xy=(5.5, 8), xytext=(7, 8),\n",
    "             arrowprops=dict(arrowstyle='-', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(3.5, 7.1), xytext=(5.5, 8),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(10.5, 7.1), xytext=(8.5, 8),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(7, 9.9), xytext=(7, 9.1),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Actor output\n",
    "action_box = FancyBboxPatch((2, 2.5), 3, 1, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax.add_patch(action_box)\n",
    "ax.text(3.5, 3, 'Action a', ha='center', fontsize=10, fontweight='bold')\n",
    "ax.annotate('', xy=(3.5, 3.6), xytext=(3.5, 4.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#388e3c'))\n",
    "\n",
    "# Critic output\n",
    "value_box = FancyBboxPatch((9, 2.5), 3, 1, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#e1bee7', edgecolor='#7b1fa2', linewidth=2)\n",
    "ax.add_patch(value_box)\n",
    "ax.text(10.5, 3, 'V(s)', ha='center', fontsize=10, fontweight='bold')\n",
    "ax.annotate('', xy=(10.5, 3.6), xytext=(10.5, 4.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#7b1fa2'))\n",
    "\n",
    "# TD Error computation\n",
    "td_box = FancyBboxPatch((5.5, 0.5), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=3)\n",
    "ax.add_patch(td_box)\n",
    "ax.text(7, 1.5, 'TD Error δ', ha='center', fontsize=11, fontweight='bold', color='#d32f2f')\n",
    "ax.text(7, 1, 'δ = r + γV(s\\') - V(s)', ha='center', fontsize=9)\n",
    "\n",
    "# Arrow from V to TD\n",
    "ax.annotate('', xy=(8.4, 1.25), xytext=(8.9, 2.4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#7b1fa2'))\n",
    "\n",
    "# Feedback arrows\n",
    "ax.annotate('', xy=(5.4, 1.5), xytext=(3.5, 2.4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f',\n",
    "                            connectionstyle='arc3,rad=0.2'))\n",
    "ax.text(3.5, 1.3, 'Updates Actor', ha='center', fontsize=9, color='#d32f2f')\n",
    "\n",
    "ax.annotate('', xy=(10.5, 2.4), xytext=(8.6, 1.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f',\n",
    "                            connectionstyle='arc3,rad=-0.2'))\n",
    "ax.text(10.5, 1.3, 'Updates Critic', ha='center', fontsize=9, color='#d32f2f')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nACTOR-CRITIC SUMMARY:\")\n",
    "print(\"  Actor: Learns WHAT to do (policy)\")\n",
    "print(\"  Critic: Learns HOW GOOD things are (value function)\")\n",
    "print(\"  TD Error: Tells actor if action was better/worse than expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The TD Error: The Critic's Feedback\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              THE TD ERROR (Temporal Difference)                │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  TD ERROR FORMULA:                                            │\n",
    "    │    δ = r + γV(s') - V(s)                                      │\n",
    "    │        └─────────┘   └──┘                                     │\n",
    "    │         TD target   Current                                   │\n",
    "    │         (what we    estimate                                  │\n",
    "    │          got)       (what we                                  │\n",
    "    │                      expected)                                │\n",
    "    │                                                                │\n",
    "    │  INTERPRETATION:                                              │\n",
    "    │    δ > 0: \"Better than expected!\"  → Reinforce this action   │\n",
    "    │    δ < 0: \"Worse than expected!\"   → Discourage this action  │\n",
    "    │    δ ≈ 0: \"About as expected\"      → Keep doing this         │\n",
    "    │                                                                │\n",
    "    │  WHY TD ERROR?                                                │\n",
    "    │    • Like advantage A(s,a), but computed INSTANTLY            │\n",
    "    │    • No need to wait for episode to end                       │\n",
    "    │    • Lower variance (bootstraps from learned V)               │\n",
    "    │                                                                │\n",
    "    │  USED FOR:                                                    │\n",
    "    │    • Actor: -log π(a|s) × δ                                  │\n",
    "    │    • Critic: (δ)² = (r + γV(s') - V(s))²                     │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate TD error computation\n",
    "\n",
    "print(\"TD ERROR DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simulated scenario\n",
    "gamma = 0.99\n",
    "\n",
    "print(\"\\nScenario: Agent in a game...\\n\")\n",
    "\n",
    "examples = [\n",
    "    # (V(s), r, V(s'), description)\n",
    "    (10.0, 5.0, 8.0, \"Got reward AND reached good state\"),\n",
    "    (10.0, 1.0, 5.0, \"Got small reward, reached worse state\"),\n",
    "    (5.0,  0.0, 15.0, \"No reward but reached great state\"),\n",
    "    (10.0, -2.0, 8.0, \"Got penalty, state about same\"),\n",
    "]\n",
    "\n",
    "for i, (V_s, r, V_next, desc) in enumerate(examples, 1):\n",
    "    td_target = r + gamma * V_next\n",
    "    td_error = td_target - V_s\n",
    "    \n",
    "    interpretation = \"✓ Better than expected!\" if td_error > 0.5 else \\\n",
    "                     \"✗ Worse than expected!\" if td_error < -0.5 else \\\n",
    "                     \"≈ About as expected\"\n",
    "    \n",
    "    print(f\"Example {i}: {desc}\")\n",
    "    print(f\"  V(s) = {V_s:.1f}, r = {r:.1f}, V(s') = {V_next:.1f}\")\n",
    "    print(f\"  TD target = r + γV(s') = {r:.1f} + {gamma}×{V_next:.1f} = {td_target:.2f}\")\n",
    "    print(f\"  TD error δ = {td_target:.2f} - {V_s:.1f} = {td_error:+.2f}\")\n",
    "    print(f\"  → {interpretation}\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TD error tells the actor: 'That action was better/worse than I expected!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TD error interpretation\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# TD error values\n",
    "td_errors = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Color gradient based on TD error\n",
    "colors = plt.cm.RdYlGn((td_errors + 5) / 10)  # Red to Green\n",
    "\n",
    "for i in range(len(td_errors) - 1):\n",
    "    ax.axvspan(td_errors[i], td_errors[i+1], \n",
    "               facecolor=colors[i], alpha=0.8)\n",
    "\n",
    "ax.axvline(x=0, color='black', linewidth=3, linestyle='--')\n",
    "\n",
    "# Annotations\n",
    "ax.text(-2.5, 0.7, 'WORSE than expected\\n\"Discourage this action\"', \n",
    "        ha='center', fontsize=12, fontweight='bold', color='white')\n",
    "ax.text(2.5, 0.7, 'BETTER than expected\\n\"Reinforce this action\"', \n",
    "        ha='center', fontsize=12, fontweight='bold', color='white')\n",
    "ax.text(0, 0.3, 'As expected', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('TD Error δ = r + γV(s\\') - V(s)', fontsize=12)\n",
    "ax.set_title('TD Error: The Critic\\'s Feedback to the Actor', fontsize=14, fontweight='bold')\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparing Update Rules\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              REINFORCE vs ACTOR-CRITIC UPDATES                 │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  REINFORCE (with baseline):                                   │\n",
    "    │    ∇θ J ≈ Σ_t ∇θ log π(a_t|s_t) × (G_t - V(s_t))             │\n",
    "    │                                    └────────────┘              │\n",
    "    │                                    Monte Carlo return          │\n",
    "    │                                    (wait for episode end)      │\n",
    "    │                                                                │\n",
    "    │  ACTOR-CRITIC:                                                │\n",
    "    │    ∇θ J ≈ ∇θ log π(a_t|s_t) × δ_t                            │\n",
    "    │                                └──┘                            │\n",
    "    │                                TD error                        │\n",
    "    │                                (instant feedback!)             │\n",
    "    │                                                                │\n",
    "    │  KEY DIFFERENCE:                                              │\n",
    "    │    REINFORCE: Uses G_t (actual return) - UNBIASED but HIGH VAR│\n",
    "    │    Actor-Critic: Uses δ_t (TD error) - SOME BIAS but LOW VAR │\n",
    "    │                                                                │\n",
    "    │  ONLINE VS EPISODIC:                                          │\n",
    "    │    REINFORCE: Episodic (update after each episode)            │\n",
    "    │    Actor-Critic: Online (update after EACH step!)             │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the bias-variance tradeoff\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Bias-Variance Spectrum\n",
    "ax1 = axes[0]\n",
    "\n",
    "methods = ['Monte Carlo\\n(REINFORCE)', 'N-step TD\\n(n=10)', 'N-step TD\\n(n=3)', 'TD(0)\\n(Actor-Critic)']\n",
    "bias = [0, 1, 2, 3]\n",
    "variance = [5, 3, 2, 1]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, bias, width, label='Bias', color='#ef5350', edgecolor='black')\n",
    "bars2 = ax1.bar(x + width/2, variance, width, label='Variance', color='#42a5f5', edgecolor='black')\n",
    "\n",
    "ax1.set_ylabel('Relative Amount', fontsize=11)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(methods)\n",
    "ax1.set_title('Bias-Variance Tradeoff', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Right: Update frequency\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Update Frequency', fontsize=14, fontweight='bold')\n",
    "\n",
    "# REINFORCE: episodic\n",
    "ax2.text(1, 8.5, 'REINFORCE (Episodic):', fontsize=11, fontweight='bold')\n",
    "for i in range(5):\n",
    "    color = '#bbdefb' if i < 4 else '#4caf50'\n",
    "    box = FancyBboxPatch((1 + i*1.5, 7), 1, 1, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor=color, edgecolor='#1976d2', linewidth=1)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(1.5 + i*1.5, 7.5, f's{i+1}', ha='center', fontsize=9)\n",
    "ax2.annotate('Update!', xy=(8.5, 7.5), xytext=(9.5, 7.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#4caf50'),\n",
    "             fontsize=10, fontweight='bold', color='#4caf50')\n",
    "\n",
    "# Actor-Critic: online\n",
    "ax2.text(1, 4.5, 'Actor-Critic (Online):', fontsize=11, fontweight='bold')\n",
    "for i in range(5):\n",
    "    box = FancyBboxPatch((1 + i*1.5, 3), 1, 1, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=1)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(1.5 + i*1.5, 3.5, f's{i+1}', ha='center', fontsize=9)\n",
    "    # Update arrow after each\n",
    "    ax2.annotate('', xy=(1.5 + i*1.5, 2.9), xytext=(1.5 + i*1.5, 2.3),\n",
    "                 arrowprops=dict(arrowstyle='->', lw=1, color='#4caf50'))\n",
    "    ax2.text(1.5 + i*1.5, 2, '↓', ha='center', fontsize=10, color='#4caf50')\n",
    "\n",
    "ax2.text(5, 1, '5 updates vs 1 update for same episode!', \n",
    "         ha='center', fontsize=11, fontweight='bold', color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implementing Actor-Critic from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic Network with shared features.\n",
    "    \n",
    "    Architecture:\n",
    "        State → Shared Layers → Actor Head (policy)\n",
    "                             → Critic Head (value)\n",
    "    \n",
    "    Actor: \"What action should I take?\" → π(a|s)\n",
    "    Critic: \"How good is this state?\" → V(s)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ========================================\n",
    "        # SHARED FEATURE EXTRACTION\n",
    "        # Both actor and critic see the same features\n",
    "        # ========================================\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # ACTOR HEAD: Policy π(a|s)\n",
    "        # Outputs probability distribution over actions\n",
    "        # ========================================\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # CRITIC HEAD: Value V(s)\n",
    "        # Outputs single scalar value\n",
    "        # ========================================\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass returns both policy and value.\n",
    "        \n",
    "        Returns:\n",
    "            action_probs: π(a|s) - probability for each action\n",
    "            value: V(s) - estimated state value\n",
    "        \"\"\"\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.FloatTensor(state)\n",
    "        \n",
    "        features = self.shared(state)\n",
    "        action_probs = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        \n",
    "        return action_probs, value\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Sample action and return action, log_prob, and value.\n",
    "        \n",
    "        This is used during environment interaction.\n",
    "        \"\"\"\n",
    "        action_probs, value = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob, value\n",
    "\n",
    "\n",
    "# Demonstrate the network\n",
    "print(\"ACTOR-CRITIC NETWORK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "state_dim = 4  # CartPole\n",
    "action_dim = 2  # Left, Right\n",
    "ac = ActorCritic(state_dim, action_dim)\n",
    "\n",
    "print(ac)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in ac.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_state = torch.randn(1, state_dim)\n",
    "probs, value = ac(test_state)\n",
    "action, log_prob, value = ac.get_action(test_state)\n",
    "\n",
    "print(f\"\\nTest state: {test_state.numpy()[0].round(2)}\")\n",
    "print(f\"Action probabilities: {probs.detach().numpy()[0].round(3)}\")\n",
    "print(f\"State value V(s): {value.item():.3f}\")\n",
    "print(f\"Sampled action: {action}\")\n",
    "print(f\"Log probability: {log_prob.item():.3f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic(env_name='CartPole-v1', n_episodes=500, gamma=0.99, \n",
    "                       lr=1e-3, print_every=50):\n",
    "    \"\"\"\n",
    "    Train Actor-Critic with ONLINE updates (update after each step!).\n",
    "    \n",
    "    Key differences from REINFORCE:\n",
    "    1. Update after EACH step (not after episode)\n",
    "    2. Use TD error instead of full return\n",
    "    3. Lower variance, faster learning\n",
    "    \n",
    "    Args:\n",
    "        env_name: Gymnasium environment\n",
    "        n_episodes: Number of episodes to train\n",
    "        gamma: Discount factor\n",
    "        lr: Learning rate\n",
    "        print_every: Print progress every N episodes\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained actor-critic model\n",
    "        rewards_history: List of episode rewards\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    # Create Actor-Critic network\n",
    "    model = ActorCritic(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(500):  # Max steps\n",
    "            # ========================================\n",
    "            # STEP 1: Get action from actor\n",
    "            # ========================================\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            action_probs, value = model(state_tensor)\n",
    "            dist = torch.distributions.Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            \n",
    "            # ========================================\n",
    "            # STEP 2: Take action in environment\n",
    "            # ========================================\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            # ========================================\n",
    "            # STEP 3: Compute TD error (critic's feedback)\n",
    "            # δ = r + γV(s') - V(s)\n",
    "            # ========================================\n",
    "            next_state_tensor = torch.FloatTensor(next_state)\n",
    "            _, next_value = model(next_state_tensor)\n",
    "            \n",
    "            # TD target: r + γV(s')  (0 if terminal)\n",
    "            td_target = reward + gamma * next_value * (1 - done)\n",
    "            \n",
    "            # TD error: δ = TD_target - V(s)\n",
    "            td_error = td_target - value\n",
    "            \n",
    "            # ========================================\n",
    "            # STEP 4: Compute losses\n",
    "            # ========================================\n",
    "            # Actor loss: -log π(a|s) × δ\n",
    "            # We use td_error as advantage estimate\n",
    "            actor_loss = -log_prob * td_error.detach()  # Don't backprop through δ for actor\n",
    "            \n",
    "            # Critic loss: (δ)² = (r + γV(s') - V(s))²\n",
    "            critic_loss = td_error.pow(2)\n",
    "            \n",
    "            # Total loss (weight critic loss to balance)\n",
    "            loss = actor_loss + 0.5 * critic_loss\n",
    "            \n",
    "            # ========================================\n",
    "            # STEP 5: Update IMMEDIATELY (online!)\n",
    "            # ========================================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Track progress\n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(rewards_history[-print_every:])\n",
    "            print(f\"Episode {episode+1:4d} | Avg Reward: {avg_reward:6.1f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return model, rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Actor-Critic!\n",
    "print(\"TRAINING ACTOR-CRITIC ON CARTPOLE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThis uses ONLINE updates (after each step!)\\n\")\n",
    "\n",
    "model, rewards_history = train_actor_critic(\n",
    "    env_name='CartPole-v1',\n",
    "    n_episodes=500,\n",
    "    gamma=0.99,\n",
    "    lr=1e-3,\n",
    "    print_every=100\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Final average (last 50): {np.mean(rewards_history[-50:]):.1f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Learning curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rewards_history, alpha=0.3, color='blue', label='Episode Reward')\n",
    "\n",
    "# Smoothed curve\n",
    "window = 50\n",
    "smoothed = np.convolve(rewards_history, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(rewards_history)), smoothed, \n",
    "         color='red', linewidth=2, label=f'{window}-Episode Average')\n",
    "\n",
    "ax1.axhline(y=500, color='green', linestyle='--', linewidth=2, label='Max Score')\n",
    "ax1.set_xlabel('Episode', fontsize=11)\n",
    "ax1.set_ylabel('Reward', fontsize=11)\n",
    "ax1.set_title('Actor-Critic Training on CartPole', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Reward distribution over time\n",
    "ax2 = axes[1]\n",
    "quarters = np.array_split(rewards_history, 4)\n",
    "labels = ['0-25%', '25-50%', '50-75%', '75-100%']\n",
    "positions = [1, 2, 3, 4]\n",
    "\n",
    "bp = ax2.boxplot(quarters, positions=positions, patch_artist=True)\n",
    "colors = ['#ffcdd2', '#fff3e0', '#c8e6c9', '#81c784']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax2.set_xticklabels(labels)\n",
    "ax2.set_xlabel('Training Progress', fontsize=11)\n",
    "ax2.set_ylabel('Episode Reward', fontsize=11)\n",
    "ax2.set_title('Reward Distribution Over Training', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Actor-Critic vs REINFORCE: Side by Side\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              REINFORCE vs ACTOR-CRITIC                         │\n",
    "    ├──────────────────────┬─────────────────────────────────────────┤\n",
    "    │    REINFORCE         │    ACTOR-CRITIC                         │\n",
    "    ├──────────────────────┼─────────────────────────────────────────┤\n",
    "    │ Monte Carlo          │ Temporal Difference (TD)                │\n",
    "    │                      │                                         │\n",
    "    │ Uses: G_t (return)   │ Uses: δ_t (TD error)                   │\n",
    "    │                      │                                         │\n",
    "    │ Update: End of       │ Update: After EACH step                │\n",
    "    │         episode      │         (online!)                       │\n",
    "    │                      │                                         │\n",
    "    │ Unbiased            │ Some bias (bootstrapping)               │\n",
    "    │                      │                                         │\n",
    "    │ HIGH variance       │ LOWER variance                          │\n",
    "    │                      │                                         │\n",
    "    │ Slower learning     │ Faster learning                         │\n",
    "    │                      │                                         │\n",
    "    │ Needs complete      │ Works with continuing                   │\n",
    "    │ episodes            │ tasks                                   │\n",
    "    └──────────────────────┴─────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Actor-Critic with REINFORCE\n",
    "\n",
    "def reinforce_for_comparison(env_name='CartPole-v1', n_episodes=500, gamma=0.99, lr=1e-3):\n",
    "    \"\"\"REINFORCE with baseline for fair comparison.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    model = ActorCritic(state_dim, action_dim)  # Same architecture\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        \n",
    "        # Collect full episode\n",
    "        for _ in range(500):\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            probs, value = model(state_tensor)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            log_probs.append(dist.log_prob(action))\n",
    "            values.append(value)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Compute returns (Monte Carlo)\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        values = torch.cat(values)\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages = returns - values.detach()\n",
    "        \n",
    "        # Losses (update at end of episode)\n",
    "        actor_loss = 0\n",
    "        for log_prob, adv in zip(log_probs, advantages):\n",
    "            actor_loss -= log_prob * adv\n",
    "        critic_loss = nn.functional.mse_loss(values, returns)\n",
    "        loss = actor_loss + 0.5 * critic_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        rewards_history.append(sum(rewards))\n",
    "    \n",
    "    env.close()\n",
    "    return rewards_history\n",
    "\n",
    "print(\"COMPARING REINFORCE vs ACTOR-CRITIC\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. Training Actor-Critic (online updates)...\")\n",
    "_, ac_rewards = train_actor_critic(n_episodes=300, print_every=100)\n",
    "\n",
    "print(\"\\n2. Training REINFORCE (episodic updates)...\")\n",
    "reinforce_rewards = reinforce_for_comparison(n_episodes=300)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "window = 30\n",
    "\n",
    "# Left: Learning curves\n",
    "ax1 = axes[0]\n",
    "smoothed_ac = np.convolve(ac_rewards, np.ones(window)/window, mode='valid')\n",
    "smoothed_rf = np.convolve(reinforce_rewards, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax1.plot(range(window-1, len(ac_rewards)), smoothed_ac, \n",
    "         'g-', linewidth=2, label='Actor-Critic (online)')\n",
    "ax1.plot(range(window-1, len(reinforce_rewards)), smoothed_rf, \n",
    "         'r-', linewidth=2, label='REINFORCE (episodic)')\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=11)\n",
    "ax1.set_ylabel('Reward (smoothed)', fontsize=11)\n",
    "ax1.set_title('Learning Speed Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Rolling variance\n",
    "ax2 = axes[1]\n",
    "rolling_std_ac = [np.std(ac_rewards[max(0,i-50):i+1]) for i in range(len(ac_rewards))]\n",
    "rolling_std_rf = [np.std(reinforce_rewards[max(0,i-50):i+1]) for i in range(len(reinforce_rewards))]\n",
    "\n",
    "ax2.plot(np.convolve(rolling_std_ac, np.ones(30)/30, mode='valid'), \n",
    "         'g-', linewidth=2, label='Actor-Critic')\n",
    "ax2.plot(np.convolve(rolling_std_rf, np.ones(30)/30, mode='valid'), \n",
    "         'r-', linewidth=2, label='REINFORCE')\n",
    "\n",
    "ax2.set_xlabel('Episode', fontsize=11)\n",
    "ax2.set_ylabel('Rolling Std (50 episodes)', fontsize=11)\n",
    "ax2.set_title('Variance Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFINAL STATISTICS:\")\n",
    "print(f\"  Actor-Critic - Final avg: {np.mean(ac_rewards[-50:]):.1f}\")\n",
    "print(f\"  REINFORCE - Final avg: {np.mean(reinforce_rewards[-50:]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### Actor-Critic Architecture\n",
    "\n",
    "| Component | Role | Formula |\n",
    "|-----------|------|--------|\n",
    "| **Actor** | Policy | π(a|s; θ) |\n",
    "| **Critic** | Value function | V(s; w) |\n",
    "\n",
    "### TD Error (Critic's Feedback)\n",
    "\n",
    "```\n",
    "δ = r + γV(s') - V(s)\n",
    "\n",
    "δ > 0: Better than expected → Reinforce action\n",
    "δ < 0: Worse than expected → Discourage action\n",
    "```\n",
    "\n",
    "### Update Rules\n",
    "\n",
    "```\n",
    "Actor:  θ ← θ + α × ∇θ log π(a|s) × δ\n",
    "Critic: w ← w + β × δ × ∇w V(s)\n",
    "```\n",
    "\n",
    "### Advantages Over REINFORCE\n",
    "\n",
    "| Aspect | REINFORCE | Actor-Critic |\n",
    "|--------|-----------|-------------|\n",
    "| Updates | Episodic | Online (each step) |\n",
    "| Variance | High | Lower |\n",
    "| Bias | Unbiased | Some bias |\n",
    "| Speed | Slower | Faster |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What is the actor and what is the critic?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The actor is the policy network π(a|s) that decides which action to take. The critic is the value network V(s) that estimates how good a state is. Together, they form a complete learning system: the critic tells the actor whether its actions are better or worse than expected.\n",
    "</details>\n",
    "\n",
    "**2. What is the TD error and why is it useful?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "TD error δ = r + γV(s') - V(s) measures the difference between what we got (reward + estimated future value) and what we expected (current value estimate). It tells us if an action was better or worse than expected, providing immediate feedback for the actor without waiting for the episode to end.\n",
    "</details>\n",
    "\n",
    "**3. Why does actor-critic have lower variance than REINFORCE?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "REINFORCE uses the full return G_t, which varies wildly because it includes all future rewards with their randomness. Actor-critic uses the TD error δ, which only looks one step ahead and uses the critic's estimate V(s') for future rewards. By \"bootstrapping\" from V, we get a more stable (lower variance) learning signal.\n",
    "</details>\n",
    "\n",
    "**4. What's the tradeoff between REINFORCE and actor-critic?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "REINFORCE has no bias (uses true returns) but high variance (slow, noisy learning). Actor-critic has some bias (bootstrapping from learned V is imperfect) but lower variance (faster, more stable learning). In practice, the variance reduction usually outweighs the small bias, making actor-critic preferred.\n",
    "</details>\n",
    "\n",
    "**5. Why are online updates beneficial?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Online updates (after each step) mean:\n",
    "1. More updates per episode → faster learning\n",
    "2. Credit assignment happens immediately → clearer signal\n",
    "3. Works with continuing tasks (no need for episode boundaries)\n",
    "4. Lower memory requirements (don't store full episodes)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Actor-critic is the foundation for modern RL algorithms. In the next notebook, we'll learn about **A2C and A3C** which scale actor-critic to multiple parallel environments!\n",
    "\n",
    "**Continue to:** [Notebook 5: A2C and A3C](05_a2c_a3c.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Actor-Critic: The critic coaches the actor, one step at a time!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
