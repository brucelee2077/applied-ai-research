{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C and A3C: Scaling Actor-Critic\n",
    "\n",
    "A2C (Advantage Actor-Critic) and A3C (Asynchronous A3C) scale actor-critic to multiple parallel environments for much faster training!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The parallel training analogy: why more environments help\n",
    "- A2C: synchronous parallel training\n",
    "- A3C: asynchronous parallel training (historical)\n",
    "- Generalized Advantage Estimation (GAE)\n",
    "- Implementing A2C from scratch\n",
    "- Using Stable-Baselines3 for production A2C\n",
    "\n",
    "**Prerequisites:** Notebook 4 (Actor-Critic)\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Parallel Training Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE PARALLEL TRAINING ANALOGY                         │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Imagine training for a marathon...                           │\n",
    "    │                                                                │\n",
    "    │  SINGLE ENVIRONMENT (Basic Actor-Critic):                     │\n",
    "    │    You run one route, analyze, adjust, repeat.                │\n",
    "    │    Slow! You only see one type of terrain at a time.          │\n",
    "    │                                                                │\n",
    "    │  PARALLEL ENVIRONMENTS (A2C):                                 │\n",
    "    │    4 clones of you run 4 different routes simultaneously!     │\n",
    "    │    Every 10 minutes, all report back:                         │\n",
    "    │      Clone 1: \"Hills were tough\"                             │\n",
    "    │      Clone 2: \"Flat terrain was easy\"                        │\n",
    "    │      Clone 3: \"Rain made it slippery\"                        │\n",
    "    │      Clone 4: \"Crowds helped motivation\"                     │\n",
    "    │    You learn from ALL experiences at once!                    │\n",
    "    │                                                                │\n",
    "    │  WHY THIS HELPS:                                              │\n",
    "    │    • 4x more experience per update → faster learning          │\n",
    "    │    • Diverse experiences → more stable gradients              │\n",
    "    │    • Better exploration → see more situations                 │\n",
    "    │                                                                │\n",
    "    │  A2C: All clones finish, then update (SYNCHRONOUS)           │\n",
    "    │  A3C: Clones update as soon as they finish (ASYNCHRONOUS)    │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, Rectangle, FancyArrowPatch\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym\n",
    "\n",
    "# Visualize parallel environments\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Single environment\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Single Environment\\n(Basic Actor-Critic)', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Timeline\n",
    "for i in range(4):\n",
    "    x = 1 + i * 2\n",
    "    box = FancyBboxPatch((x, 5), 1.5, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(x + 0.75, 5.75, f'Exp {i+1}', ha='center', fontsize=9)\n",
    "    if i < 3:\n",
    "        ax1.annotate('', xy=(x + 1.6, 5.75), xytext=(x + 1.9, 5.75),\n",
    "                     arrowprops=dict(arrowstyle='->', lw=1, color='#666'))\n",
    "\n",
    "ax1.text(5, 3.5, 'Sequential: One experience at a time', ha='center', fontsize=11, color='#d32f2f')\n",
    "ax1.text(5, 2.5, 'Time to collect 4 experiences: ████████', ha='center', fontsize=10)\n",
    "\n",
    "# Right: Parallel environments (A2C)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Parallel Environments\\n(A2C)', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# 4 parallel environments\n",
    "for i in range(4):\n",
    "    y = 7.5 - i * 1.5\n",
    "    box = FancyBboxPatch((3, y), 1.5, 1, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(3.75, y + 0.5, f'Env {i+1}', ha='center', fontsize=9)\n",
    "    ax2.text(2.5, y + 0.5, f'→', ha='center', fontsize=12)\n",
    "\n",
    "# Arrows converging to update\n",
    "update_box = FancyBboxPatch((6, 4), 2.5, 2, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#fff3e0', edgecolor='#f57c00', linewidth=3)\n",
    "ax2.add_patch(update_box)\n",
    "ax2.text(7.25, 5.3, 'Batch', ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.text(7.25, 4.7, 'Update', ha='center', fontsize=10)\n",
    "\n",
    "for i in range(4):\n",
    "    y = 7.5 - i * 1.5 + 0.5\n",
    "    ax2.annotate('', xy=(5.9, 5), xytext=(4.6, y),\n",
    "                 arrowprops=dict(arrowstyle='->', lw=1, color='#388e3c'))\n",
    "\n",
    "ax2.text(5, 2, 'Parallel: All experiences simultaneously!', ha='center', fontsize=11, color='#388e3c')\n",
    "ax2.text(5, 1, 'Time to collect 4 experiences: ██', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPARALLEL ENVIRONMENTS BENEFITS:\")\n",
    "print(\"  1. 4x faster data collection (4 envs = 4x experience)\")\n",
    "print(\"  2. More diverse gradients (different situations)\")\n",
    "print(\"  3. Lower variance (batch of experiences averages out noise)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## A2C vs A3C: Synchronous vs Asynchronous\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              A2C vs A3C COMPARISON                             │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  A3C (Asynchronous - 2016):                                   │\n",
    "    │    ┌────┐ ┌────┐ ┌────┐ ┌────┐                               │\n",
    "    │    │Env1│ │Env2│ │Env3│ │Env4│  Each has own copy of policy  │\n",
    "    │    └──┬─┘ └─┬──┘ └──┬─┘ └─┬──┘                               │\n",
    "    │       ↓     ↓      ↓     ↓     Update whenever ready         │\n",
    "    │    ┌────────────────────────┐                                │\n",
    "    │    │   Shared Parameters    │  But may use stale params!     │\n",
    "    │    └────────────────────────┘                                │\n",
    "    │                                                                │\n",
    "    │  A2C (Synchronous - simpler):                                 │\n",
    "    │    ┌────┐ ┌────┐ ┌────┐ ┌────┐                               │\n",
    "    │    │Env1│ │Env2│ │Env3│ │Env4│  All use SAME policy         │\n",
    "    │    └──┬─┘ └─┬──┘ └──┬─┘ └─┬──┘                               │\n",
    "    │       └─────┴───┬───┴─────┘     Wait for all, then update    │\n",
    "    │                 ↓                                            │\n",
    "    │    ┌────────────────────────┐                                │\n",
    "    │    │   Batch Update Once    │  All params fresh!             │\n",
    "    │    └────────────────────────┘                                │\n",
    "    │                                                                │\n",
    "    │  MODERN PREFERENCE: A2C (simpler, just as effective on GPU)  │\n",
    "    │    A3C was designed for CPU parallelism                      │\n",
    "    │    A2C works better with GPU batch processing                │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize A2C vs A3C architecture\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "# Left: A3C (Asynchronous)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('A3C: Asynchronous\\n(Historical, CPU-focused)', fontsize=14, fontweight='bold', color='#7b1fa2')\n",
    "\n",
    "# Global parameters\n",
    "global_box = FancyBboxPatch((3.5, 7.5), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#e1bee7', edgecolor='#7b1fa2', linewidth=3)\n",
    "ax1.add_patch(global_box)\n",
    "ax1.text(5, 8.25, 'Global Parameters', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Workers\n",
    "colors = ['#ffcdd2', '#c8e6c9', '#bbdefb', '#fff3e0']\n",
    "for i in range(4):\n",
    "    x = 1.5 + i * 2\n",
    "    worker_box = FancyBboxPatch((x, 4), 1.5, 2, boxstyle=\"round,pad=0.1\",\n",
    "                                 facecolor=colors[i], edgecolor='#333', linewidth=2)\n",
    "    ax1.add_patch(worker_box)\n",
    "    ax1.text(x + 0.75, 5.3, f'Worker {i+1}', ha='center', fontsize=9, fontweight='bold')\n",
    "    ax1.text(x + 0.75, 4.7, 'Local copy', ha='center', fontsize=8)\n",
    "    ax1.text(x + 0.75, 4.3, '+ Env', ha='center', fontsize=8)\n",
    "    \n",
    "    # Arrows (different heights to show async)\n",
    "    offset = [0, 0.3, -0.2, 0.1][i]\n",
    "    ax1.annotate('', xy=(x + 0.75, 6.1 + offset), xytext=(x + 0.75, 7.4),\n",
    "                 arrowprops=dict(arrowstyle='<->', lw=1, color='#7b1fa2'))\n",
    "\n",
    "ax1.text(5, 2.5, 'Updates happen independently!', ha='center', fontsize=10, color='#7b1fa2')\n",
    "ax1.text(5, 1.8, 'Problem: May use stale parameters', ha='center', fontsize=9, color='#d32f2f')\n",
    "\n",
    "# Right: A2C (Synchronous)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('A2C: Synchronous\\n(Modern, GPU-friendly)', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Single policy\n",
    "policy_box = FancyBboxPatch((3.5, 7.5), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax2.add_patch(policy_box)\n",
    "ax2.text(5, 8.25, 'Single Policy', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Vectorized environments\n",
    "env_box = FancyBboxPatch((2, 4), 6, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#fff3e0', edgecolor='#f57c00', linewidth=3)\n",
    "ax2.add_patch(env_box)\n",
    "ax2.text(5, 5.5, 'Vectorized Environments', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "for i in range(4):\n",
    "    x = 2.5 + i * 1.3\n",
    "    mini_box = FancyBboxPatch((x, 4.3), 1, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                               facecolor='#ffe0b2', edgecolor='#f57c00', linewidth=1)\n",
    "    ax2.add_patch(mini_box)\n",
    "    ax2.text(x + 0.5, 4.7, f'E{i+1}', ha='center', fontsize=9)\n",
    "\n",
    "# Single synchronized arrow\n",
    "ax2.annotate('', xy=(5, 6.1), xytext=(5, 7.4),\n",
    "             arrowprops=dict(arrowstyle='<->', lw=3, color='#388e3c'))\n",
    "\n",
    "ax2.text(5, 2.5, 'All envs step together, then batch update', ha='center', fontsize=10, color='#388e3c')\n",
    "ax2.text(5, 1.8, 'Simpler, better GPU utilization!', ha='center', fontsize=9, color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWHY A2C IS PREFERRED TODAY:\")\n",
    "print(\"  • Simpler implementation (no async complications)\")\n",
    "print(\"  • Better GPU utilization (batch processing)\")\n",
    "print(\"  • Equally effective in practice\")\n",
    "print(\"  • A3C's async was designed for CPU parallelism\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Generalized Advantage Estimation (GAE)\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              GENERALIZED ADVANTAGE ESTIMATION                  │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  RECALL THE BIAS-VARIANCE TRADEOFF:                           │\n",
    "    │    TD(0): A_t = r_t + γV(s_{t+1}) - V(s_t)                    │\n",
    "    │           Low variance, but biased (1-step lookahead)         │\n",
    "    │                                                                │\n",
    "    │    MC:    A_t = G_t - V(s_t)                                  │\n",
    "    │           Unbiased, but high variance (full return)           │\n",
    "    │                                                                │\n",
    "    │  GAE: GET THE BEST OF BOTH!                                   │\n",
    "    │    A_t^GAE = Σ (γλ)^l × δ_{t+l}                               │\n",
    "    │                                                                │\n",
    "    │    where δ_t = r_t + γV(s_{t+1}) - V(s_t) (TD error)         │\n",
    "    │                                                                │\n",
    "    │  THE λ PARAMETER:                                             │\n",
    "    │    λ = 0: Pure TD(0) - low variance, more bias                │\n",
    "    │    λ = 1: Pure MC - high variance, no bias                    │\n",
    "    │    λ = 0.95: Good balance (commonly used)                     │\n",
    "    │                                                                │\n",
    "    │  INTUITION:                                                   │\n",
    "    │    GAE is like a weighted average of n-step returns:          │\n",
    "    │    \"Look ahead many steps, but trust closer steps more\"       │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation (GAE).\n",
    "    \n",
    "    GAE provides a balance between bias and variance:\n",
    "    A_t^GAE = Σ_{l=0}^{∞} (γλ)^l × δ_{t+l}\n",
    "    \n",
    "    where δ_t = r_t + γV(s_{t+1}) - V(s_t)\n",
    "    \n",
    "    Args:\n",
    "        rewards: List/array of rewards\n",
    "        values: List/array of value estimates V(s)\n",
    "        dones: List/array of done flags\n",
    "        gamma: Discount factor\n",
    "        lam: GAE lambda (0 = TD(0), 1 = MC)\n",
    "    \n",
    "    Returns:\n",
    "        advantages: GAE advantage estimates\n",
    "        returns: Estimated returns (advantages + values)\n",
    "    \"\"\"\n",
    "    n_steps = len(rewards)\n",
    "    advantages = np.zeros(n_steps)\n",
    "    \n",
    "    # Compute GAE backwards\n",
    "    gae = 0\n",
    "    for t in reversed(range(n_steps)):\n",
    "        # For the last step, next_value is 0\n",
    "        if t == n_steps - 1:\n",
    "            next_value = 0\n",
    "        else:\n",
    "            next_value = values[t + 1]\n",
    "        \n",
    "        # TD error: δ_t = r_t + γV(s_{t+1}) - V(s_t)\n",
    "        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]\n",
    "        \n",
    "        # GAE: A_t = δ_t + γλ × A_{t+1}\n",
    "        gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
    "        advantages[t] = gae\n",
    "    \n",
    "    # Returns = advantages + values\n",
    "    returns = advantages + np.array(values)\n",
    "    \n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "# Demonstrate GAE\n",
    "print(\"GAE DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example episode\n",
    "rewards = [1, 1, 1, 1, 10]  # Big reward at end\n",
    "values = [5, 5.5, 6, 6.5, 7]  # V(s) estimates\n",
    "dones = [0, 0, 0, 0, 1]  # Episode ends at step 5\n",
    "\n",
    "print(f\"\\nRewards: {rewards}\")\n",
    "print(f\"Values:  {values}\")\n",
    "\n",
    "print(\"\\nGAE with different λ values:\")\n",
    "for lam in [0.0, 0.5, 0.95, 1.0]:\n",
    "    adv, ret = compute_gae(rewards, values, dones, gamma=0.99, lam=lam)\n",
    "    print(f\"  λ={lam:.2f}: advantages = {adv.round(2)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"λ=0: Only looks 1 step ahead (low variance, more bias)\")\n",
    "print(\"λ=1: Uses full return (high variance, no bias)\")\n",
    "print(\"λ=0.95: Good balance (commonly used in PPO)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of lambda\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Lambda spectrum\n",
    "ax1 = axes[0]\n",
    "lambdas = [0.0, 0.5, 0.9, 0.95, 1.0]\n",
    "labels = ['TD(0)', 'Mixed', 'GAE', 'Standard', 'MC']\n",
    "bias = [3, 2, 1, 0.5, 0]\n",
    "variance = [0, 1, 2, 2.5, 4]\n",
    "\n",
    "x = np.arange(len(lambdas))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, bias, width, label='Bias', color='#ef5350', edgecolor='black')\n",
    "bars2 = ax1.bar(x + width/2, variance, width, label='Variance', color='#42a5f5', edgecolor='black')\n",
    "\n",
    "ax1.set_ylabel('Relative Amount', fontsize=11)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f'λ={l}\\n({lab})' for l, lab in zip(lambdas, labels)])\n",
    "ax1.set_title('GAE: Bias-Variance Tradeoff via λ', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Highlight optimal region\n",
    "ax1.axvspan(2.5, 3.5, alpha=0.2, color='green')\n",
    "ax1.text(3, 3.5, 'Sweet spot!', ha='center', fontsize=10, color='green', fontweight='bold')\n",
    "\n",
    "# Right: Weighting visualization\n",
    "ax2 = axes[1]\n",
    "steps = np.arange(10)\n",
    "\n",
    "gamma, lam = 0.99, 0.95\n",
    "weights = [(gamma * lam) ** i for i in steps]\n",
    "\n",
    "ax2.bar(steps, weights, color='#64b5f6', edgecolor='black')\n",
    "ax2.set_xlabel('Steps into Future', fontsize=11)\n",
    "ax2.set_ylabel('Weight (γλ)^t', fontsize=11)\n",
    "ax2.set_title(f'GAE Weights (γ={gamma}, λ={lam})', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "ax2.text(5, 0.5, 'Closer steps get more weight!\\nThis reduces variance.', \n",
    "         ha='center', fontsize=10, color='#1976d2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implementing A2C from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic network for A2C.\n",
    "    Same architecture as before, but used with vectorized environments.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.FloatTensor(state)\n",
    "        \n",
    "        features = self.shared(state)\n",
    "        action_logits = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        \n",
    "        return action_logits, value\n",
    "    \n",
    "    def get_action_and_value(self, state):\n",
    "        \"\"\"Sample action and return everything needed for training.\"\"\"\n",
    "        action_logits, value = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits=action_logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedEnv:\n",
    "    \"\"\"\n",
    "    Simple vectorized environment wrapper.\n",
    "    Runs multiple environments in parallel (synchronously).\n",
    "    \n",
    "    This is a simplified version - use SubprocVecEnv for real parallelism!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env_name, n_envs):\n",
    "        self.envs = [gym.make(env_name) for _ in range(n_envs)]\n",
    "        self.n_envs = n_envs\n",
    "        \n",
    "        # Get environment info\n",
    "        self.observation_space = self.envs[0].observation_space\n",
    "        self.action_space = self.envs[0].action_space\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all environments and return stacked observations.\"\"\"\n",
    "        observations = []\n",
    "        for env in self.envs:\n",
    "            obs, _ = env.reset()\n",
    "            observations.append(obs)\n",
    "        return np.array(observations)\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"Step all environments with given actions.\"\"\"\n",
    "        observations = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        \n",
    "        for env, action in zip(self.envs, actions):\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Auto-reset on done\n",
    "            if done:\n",
    "                obs, _ = env.reset()\n",
    "            \n",
    "            observations.append(obs)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "        \n",
    "        return np.array(observations), np.array(rewards), np.array(dones)\n",
    "    \n",
    "    def close(self):\n",
    "        for env in self.envs:\n",
    "            env.close()\n",
    "\n",
    "\n",
    "# Demonstrate vectorized environments\n",
    "print(\"VECTORIZED ENVIRONMENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_envs = 4\n",
    "vec_env = VectorizedEnv('CartPole-v1', n_envs)\n",
    "\n",
    "# Reset all environments\n",
    "observations = vec_env.reset()\n",
    "print(f\"\\nNumber of parallel environments: {n_envs}\")\n",
    "print(f\"Observation shape: {observations.shape}\")\n",
    "print(f\"  → {n_envs} environments × {observations.shape[1]} state dims\")\n",
    "\n",
    "# Take actions in all environments\n",
    "actions = np.random.randint(0, 2, size=n_envs)\n",
    "next_obs, rewards, dones = vec_env.step(actions)\n",
    "\n",
    "print(f\"\\nActions taken: {actions}\")\n",
    "print(f\"Rewards: {rewards}\")\n",
    "print(f\"Dones: {dones}\")\n",
    "\n",
    "vec_env.close()\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_a2c(env_name='CartPole-v1', n_envs=4, n_steps=5, n_updates=1000,\n",
    "              gamma=0.99, lam=0.95, lr=7e-4, ent_coef=0.01, vf_coef=0.5,\n",
    "              print_every=100):\n",
    "    \"\"\"\n",
    "    Train A2C (Advantage Actor-Critic) with vectorized environments.\n",
    "    \n",
    "    Key components:\n",
    "    1. Vectorized environments: Multiple envs running in parallel\n",
    "    2. N-step returns: Collect n_steps before updating\n",
    "    3. GAE: Generalized Advantage Estimation for better advantages\n",
    "    4. Entropy bonus: Encourage exploration\n",
    "    \n",
    "    Args:\n",
    "        env_name: Environment name\n",
    "        n_envs: Number of parallel environments\n",
    "        n_steps: Steps to collect before each update\n",
    "        n_updates: Number of update iterations\n",
    "        gamma: Discount factor\n",
    "        lam: GAE lambda\n",
    "        lr: Learning rate\n",
    "        ent_coef: Entropy coefficient (exploration bonus)\n",
    "        vf_coef: Value function coefficient\n",
    "        print_every: Print interval\n",
    "    \"\"\"\n",
    "    # ========================================\n",
    "    # SETUP\n",
    "    # ========================================\n",
    "    vec_env = VectorizedEnv(env_name, n_envs)\n",
    "    state_dim = vec_env.observation_space.shape[0]\n",
    "    action_dim = vec_env.action_space.n\n",
    "    \n",
    "    model = ActorCritic(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize\n",
    "    observations = vec_env.reset()\n",
    "    rewards_history = []\n",
    "    episode_rewards = np.zeros(n_envs)  # Track ongoing episode rewards\n",
    "    \n",
    "    # ========================================\n",
    "    # TRAINING LOOP\n",
    "    # ========================================\n",
    "    for update in range(n_updates):\n",
    "        # Storage for this rollout\n",
    "        mb_obs = []\n",
    "        mb_actions = []\n",
    "        mb_log_probs = []\n",
    "        mb_values = []\n",
    "        mb_rewards = []\n",
    "        mb_dones = []\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # STEP 1: Collect n_steps of experience\n",
    "        # ----------------------------------------\n",
    "        for step in range(n_steps):\n",
    "            with torch.no_grad():\n",
    "                obs_tensor = torch.FloatTensor(observations)\n",
    "                actions, log_probs, _, values = model.get_action_and_value(obs_tensor)\n",
    "            \n",
    "            # Store\n",
    "            mb_obs.append(observations.copy())\n",
    "            mb_actions.append(actions.numpy())\n",
    "            mb_log_probs.append(log_probs.numpy())\n",
    "            mb_values.append(values.squeeze().numpy())\n",
    "            \n",
    "            # Step environments\n",
    "            observations, rewards, dones = vec_env.step(actions.numpy())\n",
    "            \n",
    "            mb_rewards.append(rewards)\n",
    "            mb_dones.append(dones)\n",
    "            \n",
    "            # Track episode rewards\n",
    "            episode_rewards += rewards\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:\n",
    "                    rewards_history.append(episode_rewards[i])\n",
    "                    episode_rewards[i] = 0\n",
    "        \n",
    "        # Convert to arrays: shape (n_steps, n_envs)\n",
    "        mb_obs = np.array(mb_obs)\n",
    "        mb_actions = np.array(mb_actions)\n",
    "        mb_log_probs = np.array(mb_log_probs)\n",
    "        mb_values = np.array(mb_values)\n",
    "        mb_rewards = np.array(mb_rewards)\n",
    "        mb_dones = np.array(mb_dones)\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # STEP 2: Compute GAE advantages\n",
    "        # ----------------------------------------\n",
    "        # Get value of last state for bootstrapping\n",
    "        with torch.no_grad():\n",
    "            _, last_values = model(torch.FloatTensor(observations))\n",
    "            last_values = last_values.squeeze().numpy()\n",
    "        \n",
    "        # Compute advantages for each environment\n",
    "        mb_advantages = np.zeros_like(mb_rewards)\n",
    "        mb_returns = np.zeros_like(mb_rewards)\n",
    "        \n",
    "        for env_idx in range(n_envs):\n",
    "            # Append last value for GAE computation\n",
    "            values_with_last = np.append(mb_values[:, env_idx], last_values[env_idx])\n",
    "            \n",
    "            gae = 0\n",
    "            for t in reversed(range(n_steps)):\n",
    "                delta = (mb_rewards[t, env_idx] + \n",
    "                         gamma * values_with_last[t+1] * (1 - mb_dones[t, env_idx]) -\n",
    "                         values_with_last[t])\n",
    "                gae = delta + gamma * lam * (1 - mb_dones[t, env_idx]) * gae\n",
    "                mb_advantages[t, env_idx] = gae\n",
    "            \n",
    "            mb_returns[:, env_idx] = mb_advantages[:, env_idx] + mb_values[:, env_idx]\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # STEP 3: Flatten and compute loss\n",
    "        # ----------------------------------------\n",
    "        # Flatten: (n_steps, n_envs) → (n_steps * n_envs,)\n",
    "        batch_obs = mb_obs.reshape(-1, state_dim)\n",
    "        batch_actions = mb_actions.flatten()\n",
    "        batch_log_probs_old = mb_log_probs.flatten()\n",
    "        batch_advantages = mb_advantages.flatten()\n",
    "        batch_returns = mb_returns.flatten()\n",
    "        \n",
    "        # Normalize advantages\n",
    "        batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std() + 1e-8)\n",
    "        \n",
    "        # Forward pass\n",
    "        obs_tensor = torch.FloatTensor(batch_obs)\n",
    "        actions_tensor = torch.LongTensor(batch_actions)\n",
    "        \n",
    "        action_logits, values = model(obs_tensor)\n",
    "        dist = torch.distributions.Categorical(logits=action_logits)\n",
    "        log_probs = dist.log_prob(actions_tensor)\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # STEP 4: Compute A2C losses\n",
    "        # ----------------------------------------\n",
    "        advantages_tensor = torch.FloatTensor(batch_advantages)\n",
    "        returns_tensor = torch.FloatTensor(batch_returns)\n",
    "        \n",
    "        # Policy loss (actor): -log π × A\n",
    "        policy_loss = -(log_probs * advantages_tensor).mean()\n",
    "        \n",
    "        # Value loss (critic): (V - R)²\n",
    "        value_loss = ((values.squeeze() - returns_tensor) ** 2).mean()\n",
    "        \n",
    "        # Entropy loss (exploration bonus): -H[π]\n",
    "        entropy_loss = -entropy\n",
    "        \n",
    "        # Total loss\n",
    "        loss = policy_loss + vf_coef * value_loss + ent_coef * entropy_loss\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # STEP 5: Update\n",
    "        # ----------------------------------------\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping for stability\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if (update + 1) % print_every == 0 and len(rewards_history) > 0:\n",
    "            avg_reward = np.mean(rewards_history[-100:]) if len(rewards_history) >= 100 else np.mean(rewards_history)\n",
    "            print(f\"Update {update+1:4d} | Episodes: {len(rewards_history):4d} | Avg Reward: {avg_reward:6.1f}\")\n",
    "    \n",
    "    vec_env.close()\n",
    "    return model, rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train A2C!\n",
    "print(\"TRAINING A2C ON CARTPOLE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nUsing 4 parallel environments with GAE\\n\")\n",
    "\n",
    "model, rewards_history = train_a2c(\n",
    "    env_name='CartPole-v1',\n",
    "    n_envs=4,\n",
    "    n_steps=5,\n",
    "    n_updates=500,\n",
    "    gamma=0.99,\n",
    "    lam=0.95,\n",
    "    lr=7e-4,\n",
    "    print_every=100\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if len(rewards_history) > 0:\n",
    "    print(f\"Total episodes completed: {len(rewards_history)}\")\n",
    "    print(f\"Final average (last 100): {np.mean(rewards_history[-100:]):.1f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "if len(rewards_history) > 10:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Left: Learning curve\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(rewards_history, alpha=0.3, color='blue', label='Episode Reward')\n",
    "    \n",
    "    window = min(50, len(rewards_history) // 3)\n",
    "    if window > 1:\n",
    "        smoothed = np.convolve(rewards_history, np.ones(window)/window, mode='valid')\n",
    "        ax1.plot(range(window-1, len(rewards_history)), smoothed, \n",
    "                 color='red', linewidth=2, label=f'{window}-Episode Average')\n",
    "    \n",
    "    ax1.axhline(y=500, color='green', linestyle='--', linewidth=2, label='Max Score')\n",
    "    ax1.set_xlabel('Episode', fontsize=11)\n",
    "    ax1.set_ylabel('Reward', fontsize=11)\n",
    "    ax1.set_title('A2C Training on CartPole', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Right: Episode completion rate\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    # Bin episodes into groups\n",
    "    n_bins = min(10, len(rewards_history) // 5)\n",
    "    if n_bins > 1:\n",
    "        bins = np.array_split(rewards_history, n_bins)\n",
    "        bin_means = [np.mean(b) for b in bins]\n",
    "        bin_stds = [np.std(b) for b in bins]\n",
    "        \n",
    "        x = range(n_bins)\n",
    "        ax2.bar(x, bin_means, yerr=bin_stds, capsize=5, \n",
    "                color='#64b5f6', edgecolor='black', alpha=0.8)\n",
    "        ax2.set_xlabel('Training Progress (bins)', fontsize=11)\n",
    "        ax2.set_ylabel('Average Reward', fontsize=11)\n",
    "        ax2.set_title('Reward Improvement Over Training', fontsize=14, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough episodes to plot. Try increasing n_updates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using Stable-Baselines3 for Production A2C\n",
    "\n",
    "For real projects, use a well-tested library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Stable-Baselines3 is available\n",
    "try:\n",
    "    from stable_baselines3 import A2C\n",
    "    from stable_baselines3.common.env_util import make_vec_env\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    SB3_AVAILABLE = True\n",
    "    print(\"✓ Stable-Baselines3 is installed!\")\n",
    "except ImportError:\n",
    "    SB3_AVAILABLE = False\n",
    "    print(\"Stable-Baselines3 not installed.\")\n",
    "    print(\"\\nTo install, run:\")\n",
    "    print(\"  pip install stable-baselines3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    print(\"TRAINING A2C WITH STABLE-BASELINES3\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create vectorized environment\n",
    "    vec_env = make_vec_env('CartPole-v1', n_envs=4)\n",
    "    \n",
    "    # Create A2C agent\n",
    "    model = A2C(\n",
    "        'MlpPolicy',      # Use MLP policy network\n",
    "        vec_env,          # Vectorized environment\n",
    "        verbose=1,        # Print training info\n",
    "        learning_rate=7e-4,\n",
    "        n_steps=5,        # Steps before each update\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,  # GAE lambda\n",
    "        ent_coef=0.01,    # Entropy coefficient\n",
    "        vf_coef=0.5,      # Value function coefficient\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining for 25,000 steps...\")\n",
    "    model.learn(total_timesteps=25000)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating trained agent...\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=10)\n",
    "    print(f\"Mean reward: {mean_reward:.1f} ± {std_reward:.1f}\")\n",
    "    \n",
    "    vec_env.close()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"\\n(Showing example code - install SB3 to run)\")\n",
    "    example_code = '''\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Create 4 parallel environments\n",
    "vec_env = make_vec_env('CartPole-v1', n_envs=4)\n",
    "\n",
    "# Create and train A2C agent\n",
    "model = A2C('MlpPolicy', vec_env, verbose=1)\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Save the model\n",
    "model.save('a2c_cartpole')\n",
    "\n",
    "# Load and use later\n",
    "model = A2C.load('a2c_cartpole')\n",
    "'''\n",
    "    print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### A2C vs A3C\n",
    "\n",
    "| Aspect | A2C | A3C |\n",
    "|--------|-----|-----|\n",
    "| **Updates** | Synchronous | Asynchronous |\n",
    "| **Parameters** | Single copy | Local copies |\n",
    "| **Hardware** | GPU-friendly | CPU-focused |\n",
    "| **Modern Use** | ✓ Preferred | Historical |\n",
    "\n",
    "### GAE (Generalized Advantage Estimation)\n",
    "\n",
    "```\n",
    "A_t^GAE = Σ (γλ)^l × δ_{t+l}\n",
    "\n",
    "λ = 0: TD(0) - low variance, more bias\n",
    "λ = 1: MC - high variance, no bias  \n",
    "λ = 0.95: Sweet spot!\n",
    "```\n",
    "\n",
    "### A2C Components\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| Vectorized Envs | Parallel experience collection |\n",
    "| N-step Returns | Batch updates |\n",
    "| GAE | Better advantage estimates |\n",
    "| Entropy Bonus | Encourage exploration |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why use parallel environments?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Parallel environments provide:\n",
    "1. N× faster data collection (N envs = N× experience)\n",
    "2. More diverse experiences in each batch\n",
    "3. Lower variance gradients (batch averages out noise)\n",
    "4. Better exploration (different situations simultaneously)\n",
    "</details>\n",
    "\n",
    "**2. What's the difference between A2C and A3C?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "A3C (Asynchronous): Workers update independently, may use stale parameters. Designed for CPU parallelism.\n",
    "\n",
    "A2C (Synchronous): All workers wait, then batch update with fresh parameters. Simpler and better for GPUs.\n",
    "\n",
    "A2C is preferred today because GPUs are common and it's simpler while being equally effective.\n",
    "</details>\n",
    "\n",
    "**3. What does GAE lambda control?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "GAE lambda (λ) controls the bias-variance tradeoff:\n",
    "- λ = 0: TD(0), only look 1 step ahead (low variance, more bias)\n",
    "- λ = 1: Monte Carlo, use full return (no bias, high variance)\n",
    "- λ = 0.95: Good balance (commonly used)\n",
    "\n",
    "It's a weighted average of n-step returns, trusting closer steps more.\n",
    "</details>\n",
    "\n",
    "**4. Why add an entropy bonus?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The entropy bonus encourages exploration by preventing the policy from becoming too deterministic too quickly. Higher entropy = more random actions = more exploration. The coefficient (ent_coef) balances exploration vs exploitation.\n",
    "</details>\n",
    "\n",
    "**5. When should you use A2C vs other algorithms?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "A2C is good for:\n",
    "- Simple environments where sample efficiency isn't critical\n",
    "- When you want a baseline policy gradient method\n",
    "- Educational purposes (simpler than PPO)\n",
    "\n",
    "For most real tasks, PPO is preferred (next section!) because it's more stable and sample efficient.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the **Policy Gradient** section! You now understand:\n",
    "\n",
    "- ✅ Why policy gradients exist (continuous actions, stochastic policies)\n",
    "- ✅ REINFORCE algorithm (Monte Carlo policy gradient)\n",
    "- ✅ Variance reduction with baselines\n",
    "- ✅ Actor-Critic methods (TD learning + policy gradient)\n",
    "- ✅ A2C/A3C (parallel environments, GAE)\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Move on to **[Advanced Algorithms](../advanced-algorithms/)** to learn about:\n",
    "- Trust Region Methods (TRPO)\n",
    "- PPO (Proximal Policy Optimization) - the most popular algorithm!\n",
    "- SAC (Soft Actor-Critic) for continuous control\n",
    "\n",
    "---\n",
    "\n",
    "*A2C: \"Why train one agent when you can train four at once?\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
