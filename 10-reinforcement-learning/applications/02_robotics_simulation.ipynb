{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robotics Simulation: Teaching Robots to Move\n",
    "\n",
    "From balancing pendulums to walking humanoids, RL makes robots move!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The dance instructor analogy: how RL teaches robots to move\n",
    "- Continuous vs discrete control\n",
    "- Popular robotics environments (Pendulum, MuJoCo)\n",
    "- Why SAC excels at continuous control\n",
    "- Reward shaping for locomotion\n",
    "- Sim-to-real transfer challenges\n",
    "\n",
    "**Prerequisites:** Game playing notebook, PPO basics\n",
    "\n",
    "**Time:** ~35 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Dance Instructor Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE DANCE INSTRUCTOR ANALOGY                          │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Teaching a robot to walk is like teaching someone to dance...│\n",
    "    │                                                                │\n",
    "    │  THE BODY (Robot State):                                      │\n",
    "    │    Joint angles: How bent is each joint?                     │\n",
    "    │    Joint velocities: How fast is each joint moving?          │\n",
    "    │    Body orientation: Is the robot upright?                   │\n",
    "    │    Contact sensors: Are feet touching ground?                │\n",
    "    │                                                                │\n",
    "    │  THE MUSCLES (Continuous Actions):                            │\n",
    "    │    Not just \"move left\" or \"move right\"                      │\n",
    "    │    But \"apply 0.73 torque to hip, 0.21 to knee\"              │\n",
    "    │    Smooth, continuous control is harder!                     │\n",
    "    │                                                                │\n",
    "    │  THE INSTRUCTOR (Reward Signal):                              │\n",
    "    │    \"Good! You moved forward\" (+1)                            │\n",
    "    │    \"Oops, you fell\" (-10)                                    │\n",
    "    │    \"Using too much energy\" (-0.1)                            │\n",
    "    │    \"Nice and smooth!\" (+0.5)                                 │\n",
    "    │                                                                │\n",
    "    │  THE LEARNING:                                                │\n",
    "    │    Practice → Feedback → Adjust → Master the dance!         │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, Rectangle, Wedge\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check gymnasium availability\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    GYM_AVAILABLE = True\n",
    "    print(\"✓ Gymnasium is installed!\")\n",
    "except ImportError:\n",
    "    GYM_AVAILABLE = False\n",
    "    print(\"✗ Gymnasium not installed.\")\n",
    "\n",
    "# Check stable-baselines3\n",
    "try:\n",
    "    from stable_baselines3 import PPO, SAC, TD3\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    SB3_AVAILABLE = True\n",
    "    print(\"✓ Stable-Baselines3 is installed!\")\n",
    "except ImportError:\n",
    "    SB3_AVAILABLE = False\n",
    "    print(\"✗ Stable-Baselines3 not installed.\")\n",
    "\n",
    "# Check for MuJoCo\n",
    "MUJOCO_AVAILABLE = False\n",
    "try:\n",
    "    env = gym.make('HalfCheetah-v4')\n",
    "    env.close()\n",
    "    MUJOCO_AVAILABLE = True\n",
    "    print(\"✓ MuJoCo environments available!\")\n",
    "except:\n",
    "    print(\"✗ MuJoCo not available (optional).\")\n",
    "    print(\"  Install with: pip install gymnasium[mujoco]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize discrete vs continuous control\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Discrete actions (games)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Discrete Control (Games)', fontsize=14, fontweight='bold', color='#1976d2')\n",
    "\n",
    "# Draw discrete action choices\n",
    "discrete_actions = [\n",
    "    (2, 7, 'LEFT', '←'),\n",
    "    (5, 7, 'NONE', '•'),\n",
    "    (8, 7, 'RIGHT', '→'),\n",
    "]\n",
    "\n",
    "for x, y, label, symbol in discrete_actions:\n",
    "    box = FancyBboxPatch((x-1, y-0.5), 2, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(x, y+0.3, symbol, ha='center', fontsize=20)\n",
    "    ax1.text(x, y-0.2, label, ha='center', fontsize=9)\n",
    "\n",
    "ax1.text(5, 5, 'Choose ONE action', ha='center', fontsize=12, style='italic')\n",
    "ax1.text(5, 4, 'Like pressing a button', ha='center', fontsize=10, color='#666')\n",
    "\n",
    "# Example environments\n",
    "ax1.text(5, 2.5, 'Examples:', ha='center', fontsize=11, fontweight='bold')\n",
    "ax1.text(5, 1.8, 'CartPole, LunarLander, Atari', ha='center', fontsize=10, color='#666')\n",
    "\n",
    "# Right: Continuous actions (robotics)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Continuous Control (Robotics)', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Draw continuous slider\n",
    "ax2.plot([2, 8], [7, 7], 'k-', linewidth=4)\n",
    "ax2.text(2, 6.3, '-1.0', ha='center', fontsize=10)\n",
    "ax2.text(5, 6.3, '0.0', ha='center', fontsize=10)\n",
    "ax2.text(8, 6.3, '+1.0', ha='center', fontsize=10)\n",
    "\n",
    "# Current value marker\n",
    "current_val = 6.3\n",
    "circle = Circle((current_val, 7), 0.3, facecolor='#388e3c', edgecolor='black', linewidth=2)\n",
    "ax2.add_patch(circle)\n",
    "ax2.text(current_val, 7.8, '0.43', ha='center', fontsize=10, fontweight='bold', color='#388e3c')\n",
    "\n",
    "ax2.text(5, 5, 'Choose ANY value in range', ha='center', fontsize=12, style='italic')\n",
    "ax2.text(5, 4, 'Like turning a dial smoothly', ha='center', fontsize=10, color='#666')\n",
    "\n",
    "# Example environments\n",
    "ax2.text(5, 2.5, 'Examples:', ha='center', fontsize=11, fontweight='bold')\n",
    "ax2.text(5, 1.8, 'Pendulum, HalfCheetah, Humanoid', ha='center', fontsize=10, color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY DIFFERENCE:\")\n",
    "print(\"  Discrete: action ∈ {0, 1, 2, 3} (finite choices)\")\n",
    "print(\"  Continuous: action ∈ [-1, +1] (infinite choices!)\")\n",
    "print(\"  Continuous control is MUCH harder to learn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Robotics Environments: A Tour\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              ROBOTICS ENVIRONMENTS                             │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  PENDULUM (Difficulty: ⭐)                                     │\n",
    "    │    Task: Swing up and balance a pendulum                      │\n",
    "    │    State: [cos(θ), sin(θ), angular_velocity]                  │\n",
    "    │    Action: Torque ∈ [-2, +2]                                  │\n",
    "    │    Great for learning continuous control!                    │\n",
    "    │                                                                │\n",
    "    │  HALFCHEETAH (Difficulty: ⭐⭐)                                │\n",
    "    │    Task: Make a 2D cheetah run fast                          │\n",
    "    │    State: 17D (joint angles, velocities, body orientation)   │\n",
    "    │    Action: 6 joint torques                                   │\n",
    "    │    Classic locomotion benchmark                              │\n",
    "    │                                                                │\n",
    "    │  ANT (Difficulty: ⭐⭐⭐)                                      │\n",
    "    │    Task: 4-legged robot walking                              │\n",
    "    │    State: 111D (lots of sensors!)                            │\n",
    "    │    Action: 8 joint torques                                   │\n",
    "    │    Coordination challenge                                    │\n",
    "    │                                                                │\n",
    "    │  HUMANOID (Difficulty: ⭐⭐⭐⭐)                               │\n",
    "    │    Task: Bipedal walking                                     │\n",
    "    │    State: 376D (huge state space!)                           │\n",
    "    │    Action: 17 joint torques                                  │\n",
    "    │    Very hard - balancing + coordination                      │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore robotics environments\n",
    "\n",
    "print(\"ROBOTICS ENVIRONMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if GYM_AVAILABLE:\n",
    "    environments = {\n",
    "        'Pendulum-v1': {\n",
    "            'description': 'Swing up and balance pendulum',\n",
    "            'difficulty': '⭐',\n",
    "            'state_dim': 3,\n",
    "            'action_dim': 1,\n",
    "            'best_algorithm': 'SAC, TD3, PPO',\n",
    "            'requires_mujoco': False,\n",
    "        },\n",
    "        'HalfCheetah-v4': {\n",
    "            'description': '2D cheetah locomotion',\n",
    "            'difficulty': '⭐⭐',\n",
    "            'state_dim': 17,\n",
    "            'action_dim': 6,\n",
    "            'best_algorithm': 'SAC, TD3',\n",
    "            'requires_mujoco': True,\n",
    "        },\n",
    "        'Ant-v4': {\n",
    "            'description': '4-legged robot locomotion',\n",
    "            'difficulty': '⭐⭐⭐',\n",
    "            'state_dim': 111,\n",
    "            'action_dim': 8,\n",
    "            'best_algorithm': 'SAC, TD3',\n",
    "            'requires_mujoco': True,\n",
    "        },\n",
    "        'Humanoid-v4': {\n",
    "            'description': 'Bipedal walking',\n",
    "            'difficulty': '⭐⭐⭐⭐',\n",
    "            'state_dim': 376,\n",
    "            'action_dim': 17,\n",
    "            'best_algorithm': 'SAC + careful tuning',\n",
    "            'requires_mujoco': True,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    for env_name, info in environments.items():\n",
    "        print(f\"\\n{env_name}\")\n",
    "        print(f\"  Description: {info['description']}\")\n",
    "        print(f\"  Difficulty: {info['difficulty']}\")\n",
    "        print(f\"  State dim: {info['state_dim']}, Action dim: {info['action_dim']}\")\n",
    "        print(f\"  Best algorithms: {info['best_algorithm']}\")\n",
    "        \n",
    "        if info['requires_mujoco'] and not MUJOCO_AVAILABLE:\n",
    "            print(f\"  (Requires MuJoCo - not installed)\")\n",
    "        else:\n",
    "            try:\n",
    "                env = gym.make(env_name)\n",
    "                print(f\"  Observation: {env.observation_space}\")\n",
    "                print(f\"  Action: {env.action_space}\")\n",
    "                env.close()\n",
    "            except Exception as e:\n",
    "                print(f\"  (Environment not available)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize complexity progression\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('Robotics Environment Complexity', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Robots as boxes with increasing complexity\n",
    "robots = [\n",
    "    (1.5, 5, 'Pendulum', '1 joint', '#4caf50', 3, 1),\n",
    "    (5, 5, 'HalfCheetah', '6 joints', '#ff9800', 17, 6),\n",
    "    (8.5, 5, 'Ant', '8 joints', '#f44336', 111, 8),\n",
    "    (12, 5, 'Humanoid', '17 joints', '#9c27b0', 376, 17),\n",
    "]\n",
    "\n",
    "for i, (x, y, name, joints, color, state_dim, action_dim) in enumerate(robots):\n",
    "    # Robot body (size increases with complexity)\n",
    "    size = 1.5 + i * 0.3\n",
    "    box = FancyBboxPatch((x - size/2, y - size/2), size, size, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor='black', linewidth=2, alpha=0.8)\n",
    "    ax.add_patch(box)\n",
    "    \n",
    "    ax.text(x, y + size/2 + 0.8, name, ha='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(x, y + size/2 + 0.3, joints, ha='center', fontsize=10, color='#666')\n",
    "    \n",
    "    # State/Action dimensions\n",
    "    ax.text(x, y - size/2 - 0.5, f'State: {state_dim}D', ha='center', fontsize=9)\n",
    "    ax.text(x, y - size/2 - 1, f'Action: {action_dim}D', ha='center', fontsize=9)\n",
    "\n",
    "# Difficulty arrow\n",
    "ax.annotate('', xy=(13, 2), xytext=(1, 2),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#333'))\n",
    "ax.text(7, 1.3, 'Increasing Complexity →', ha='center', fontsize=12, style='italic')\n",
    "\n",
    "# Training time indicators\n",
    "times = ['~50K steps', '~1M steps', '~3M steps', '~10M+ steps']\n",
    "for i, ((x, y, _, _, _, _, _), time) in enumerate(zip(robots, times)):\n",
    "    ax.text(x, 3.3, time, ha='center', fontsize=9, color='#666')\n",
    "\n",
    "ax.text(7, 3.8, 'Typical Training Time:', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why SAC Excels at Continuous Control\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              SAC: SOFT ACTOR-CRITIC                            │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  WHY SAC IS GREAT FOR ROBOTICS:                               │\n",
    "    │                                                                │\n",
    "    │  1. SAMPLE EFFICIENCY (Off-Policy)                            │\n",
    "    │     Reuses past experiences from replay buffer               │\n",
    "    │     Important: Robot simulation is slow!                     │\n",
    "    │                                                                │\n",
    "    │  2. EXPLORATION (Entropy Bonus)                               │\n",
    "    │     Objective: Maximize reward + entropy                     │\n",
    "    │     J = E[R + α × H(π)]                                      │\n",
    "    │     Prevents premature convergence to bad gaits              │\n",
    "    │                                                                │\n",
    "    │  3. STABILITY                                                 │\n",
    "    │     Twin Q-networks reduce overestimation                    │\n",
    "    │     Automatic temperature tuning                             │\n",
    "    │     Soft updates for smooth learning                         │\n",
    "    │                                                                │\n",
    "    │  COMPARISON:                                                  │\n",
    "    │    • PPO: Stable but sample inefficient                      │\n",
    "    │    • TD3: Sample efficient but less exploration              │\n",
    "    │    • SAC: Best of both worlds for robotics!                  │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC vs PPO on Pendulum\n",
    "\n",
    "print(\"TRAINING ON PENDULUM: SAC vs PPO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if SB3_AVAILABLE and GYM_AVAILABLE:\n",
    "    results = {}\n",
    "    \n",
    "    for name, AlgoClass in [('SAC', SAC), ('PPO', PPO)]:\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        env = gym.make('Pendulum-v1')\n",
    "        model = AlgoClass('MlpPolicy', env, verbose=0)\n",
    "        \n",
    "        # Train\n",
    "        model.learn(total_timesteps=20000)\n",
    "        \n",
    "        # Evaluate\n",
    "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "        results[name] = (mean_reward, std_reward)\n",
    "        print(f\"  Result: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "        \n",
    "        env.close()\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    names = list(results.keys())\n",
    "    means = [results[n][0] for n in names]\n",
    "    stds = [results[n][1] for n in names]\n",
    "    colors = ['#388e3c', '#1976d2']\n",
    "    \n",
    "    bars = ax.bar(names, means, yerr=stds, capsize=5, color=colors,\n",
    "                  edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Pendulum reward range: -16.2 to 0 (higher is better)\n",
    "    ax.axhline(y=-200, color='red', linestyle='--', label='Random baseline (~-1200)')\n",
    "    \n",
    "    ax.set_ylabel('Mean Reward (higher = better)', fontsize=12)\n",
    "    ax.set_title('Pendulum: SAC vs PPO\\n(20K training steps)', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, mean in zip(bars, means):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 50,\n",
    "                f'{mean:.0f}', ha='center', fontsize=12, fontweight='bold', color='white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKEY INSIGHT:\")\n",
    "    print(\"  SAC often learns faster on continuous control tasks!\")\n",
    "    print(\"  Entropy bonus helps explore the action space smoothly.\")\n",
    "else:\n",
    "    print(\"Install required libraries to train agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reward Shaping for Locomotion\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              REWARD SHAPING                                    │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  BAD REWARD (too simple):                                     │\n",
    "    │    reward = +1 if reached_goal else 0                        │\n",
    "    │    Problem: Sparse, hard to learn                            │\n",
    "    │                                                                │\n",
    "    │  GOOD REWARD (shaped):                                        │\n",
    "    │    reward = velocity_forward                # Move fast!     │\n",
    "    │           - 0.1 * |control|^2               # Use energy     │\n",
    "    │           - 10 * (z < threshold)            # Don't fall     │\n",
    "    │           + 0.1 * still_alive               # Stay up!       │\n",
    "    │                                                                │\n",
    "    │  COMMON REWARD COMPONENTS:                                    │\n",
    "    │    • Forward velocity: Encourage movement                    │\n",
    "    │    • Control cost: Penalize jerky motions                    │\n",
    "    │    • Alive bonus: Reward staying upright                     │\n",
    "    │    • Impact cost: Penalize hard contacts                     │\n",
    "    │    • Smoothness: Reward consistent motion                    │\n",
    "    │                                                                │\n",
    "    │  TIP: Good reward shaping is an ART!                         │\n",
    "    │       Too complex → hard to optimize                         │\n",
    "    │       Too simple → wrong behavior                            │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate reward components\n",
    "\n",
    "print(\"LOCOMOTION REWARD BREAKDOWN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def compute_locomotion_reward(\n",
    "    velocity_forward,\n",
    "    control_magnitude,\n",
    "    height,\n",
    "    alive,\n",
    "    ctrl_cost_weight=0.1,\n",
    "    alive_bonus=1.0,\n",
    "    height_threshold=0.5,\n",
    "    fall_penalty=10.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute locomotion reward with multiple components.\n",
    "    \n",
    "    This is similar to what MuJoCo envs use.\n",
    "    \"\"\"\n",
    "    # Forward progress\n",
    "    velocity_reward = velocity_forward\n",
    "    \n",
    "    # Control cost (penalize large actions)\n",
    "    ctrl_cost = ctrl_cost_weight * np.sum(control_magnitude ** 2)\n",
    "    \n",
    "    # Alive bonus\n",
    "    alive_reward = alive_bonus if alive else 0\n",
    "    \n",
    "    # Fall penalty\n",
    "    fall_cost = fall_penalty if height < height_threshold else 0\n",
    "    \n",
    "    total = velocity_reward - ctrl_cost + alive_reward - fall_cost\n",
    "    \n",
    "    return {\n",
    "        'velocity': velocity_reward,\n",
    "        'ctrl_cost': -ctrl_cost,\n",
    "        'alive': alive_reward,\n",
    "        'fall': -fall_cost,\n",
    "        'total': total,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example scenarios\n",
    "scenarios = [\n",
    "    (\"Walking nicely\", 2.0, np.array([0.3, 0.2, 0.1]), 1.2, True),\n",
    "    (\"Running hard\", 5.0, np.array([0.9, 0.8, 0.7]), 1.0, True),\n",
    "    (\"Standing still\", 0.0, np.array([0.1, 0.1, 0.1]), 1.2, True),\n",
    "    (\"Falling down\", 1.0, np.array([0.5, 0.5, 0.5]), 0.3, False),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Scenario':<18} {'Velocity':>10} {'Ctrl Cost':>10} {'Alive':>8} {'Fall':>8} {'TOTAL':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, vel, ctrl, height, alive in scenarios:\n",
    "    r = compute_locomotion_reward(vel, ctrl, height, alive)\n",
    "    print(f\"{name:<18} {r['velocity']:>10.2f} {r['ctrl_cost']:>10.2f} {r['alive']:>8.2f} {r['fall']:>8.2f} {r['total']:>10.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"  • Running hard has high velocity but high control cost\")\n",
    "print(\"  • Walking nicely balances speed and efficiency\")\n",
    "print(\"  • Falling incurs a large penalty!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reward components\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "scenarios_names = [\"Walking nicely\", \"Running hard\", \"Standing still\", \"Falling down\"]\n",
    "scenarios_data = [\n",
    "    (2.0, np.array([0.3, 0.2, 0.1]), 1.2, True),\n",
    "    (5.0, np.array([0.9, 0.8, 0.7]), 1.0, True),\n",
    "    (0.0, np.array([0.1, 0.1, 0.1]), 1.2, True),\n",
    "    (1.0, np.array([0.5, 0.5, 0.5]), 0.3, False),\n",
    "]\n",
    "\n",
    "# Compute rewards\n",
    "all_rewards = []\n",
    "for vel, ctrl, height, alive in scenarios_data:\n",
    "    r = compute_locomotion_reward(vel, ctrl, height, alive)\n",
    "    all_rewards.append(r)\n",
    "\n",
    "# Stacked bar chart\n",
    "x = np.arange(len(scenarios_names))\n",
    "width = 0.6\n",
    "\n",
    "velocity_vals = [r['velocity'] for r in all_rewards]\n",
    "ctrl_vals = [r['ctrl_cost'] for r in all_rewards]\n",
    "alive_vals = [r['alive'] for r in all_rewards]\n",
    "fall_vals = [r['fall'] for r in all_rewards]\n",
    "\n",
    "# Plot positive components\n",
    "ax.bar(x, velocity_vals, width, label='Velocity Reward', color='#4caf50')\n",
    "ax.bar(x, alive_vals, width, bottom=velocity_vals, label='Alive Bonus', color='#2196f3')\n",
    "\n",
    "# Plot negative components (below zero)\n",
    "ax.bar(x, ctrl_vals, width, label='Control Cost', color='#ff9800')\n",
    "ax.bar(x, fall_vals, width, bottom=ctrl_vals, label='Fall Penalty', color='#f44336')\n",
    "\n",
    "# Total line\n",
    "totals = [r['total'] for r in all_rewards]\n",
    "ax.plot(x, totals, 'ko-', markersize=10, linewidth=2, label='Total Reward')\n",
    "\n",
    "ax.axhline(y=0, color='black', linewidth=1)\n",
    "ax.set_xlabel('Scenario', fontsize=12)\n",
    "ax.set_ylabel('Reward', fontsize=12)\n",
    "ax.set_title('Locomotion Reward Components', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(scenarios_names, rotation=15)\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sim-to-Real Transfer\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              SIM-TO-REAL TRANSFER                              │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  THE PROBLEM:                                                 │\n",
    "    │    Training in simulation: ✓ Fast, cheap, safe               │\n",
    "    │    Running on real robot: ? Does it still work?              │\n",
    "    │                                                                │\n",
    "    │  THE REALITY GAP:                                             │\n",
    "    │    • Friction is different                                   │\n",
    "    │    • Sensors are noisy                                       │\n",
    "    │    • Motors have delays                                      │\n",
    "    │    • Physics isn't perfect                                   │\n",
    "    │                                                                │\n",
    "    │  SOLUTIONS:                                                   │\n",
    "    │                                                                │\n",
    "    │  1. DOMAIN RANDOMIZATION                                      │\n",
    "    │     Train with random friction, mass, etc.                   │\n",
    "    │     Robot learns to be robust!                               │\n",
    "    │                                                                │\n",
    "    │  2. SYSTEM IDENTIFICATION                                     │\n",
    "    │     Measure real robot parameters                            │\n",
    "    │     Make simulation match reality                            │\n",
    "    │                                                                │\n",
    "    │  3. SIM-TO-REAL FINE-TUNING                                   │\n",
    "    │     Train mostly in sim                                      │\n",
    "    │     Fine-tune on real robot (carefully!)                     │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sim-to-real challenge\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('The Sim-to-Real Challenge', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Simulation box\n",
    "sim_box = FancyBboxPatch((0.5, 5.5), 5, 4, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(sim_box)\n",
    "ax.text(3, 9, 'SIMULATION', ha='center', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax.text(3, 8.3, '✓ Fast (1000x real-time)', ha='center', fontsize=10)\n",
    "ax.text(3, 7.7, '✓ Cheap (no hardware)', ha='center', fontsize=10)\n",
    "ax.text(3, 7.1, '✓ Safe (can crash)', ha='center', fontsize=10)\n",
    "ax.text(3, 6.3, '✗ Not perfect physics', ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "# Real world box\n",
    "real_box = FancyBboxPatch((8.5, 5.5), 5, 4, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#fff3e0', edgecolor='#f57c00', linewidth=3)\n",
    "ax.add_patch(real_box)\n",
    "ax.text(11, 9, 'REAL WORLD', ha='center', fontsize=12, fontweight='bold', color='#f57c00')\n",
    "ax.text(11, 8.3, '✗ Slow (real-time)', ha='center', fontsize=10, color='#d32f2f')\n",
    "ax.text(11, 7.7, '✗ Expensive (hardware)', ha='center', fontsize=10, color='#d32f2f')\n",
    "ax.text(11, 7.1, '✗ Dangerous (can break)', ha='center', fontsize=10, color='#d32f2f')\n",
    "ax.text(11, 6.3, '✓ True physics', ha='center', fontsize=10)\n",
    "\n",
    "# Gap in the middle\n",
    "ax.annotate('', xy=(8.4, 7.5), xytext=(5.6, 7.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#666'))\n",
    "ax.text(7, 8.2, 'THE GAP', ha='center', fontsize=11, fontweight='bold', color='#d32f2f')\n",
    "ax.text(7, 7.8, '(Reality is different!)', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Solutions\n",
    "solutions = [\n",
    "    (2, 3.5, 'Domain\\nRandomization', 'Train with varied\\nparameters', '#4caf50'),\n",
    "    (7, 3.5, 'System\\nIdentification', 'Match sim to\\nreal robot', '#2196f3'),\n",
    "    (12, 3.5, 'Fine-Tuning', 'Train in sim,\\ntune on real', '#9c27b0'),\n",
    "]\n",
    "\n",
    "ax.text(7, 4.8, 'SOLUTIONS:', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "for x, y, title, desc, color in solutions:\n",
    "    box = FancyBboxPatch((x-1.5, y-1.5), 3, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor='black', linewidth=2, alpha=0.3)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y+0.5, title, ha='center', fontsize=10, fontweight='bold')\n",
    "    ax.text(x, y-0.5, desc, ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSIM-TO-REAL SUCCESS STORIES:\")\n",
    "print(\"  • OpenAI: Robot hand solving Rubik's cube (domain randomization)\")\n",
    "print(\"  • Boston Dynamics: Walking robots (careful system ID)\")\n",
    "print(\"  • Google: Robot manipulation (sim + real fine-tuning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete Example: Training a Walking Agent\n",
    "\n",
    "Let's train SAC on a continuous control task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete training example on Pendulum\n",
    "\n",
    "print(\"TRAINING SAC ON PENDULUM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if SB3_AVAILABLE and GYM_AVAILABLE:\n",
    "    # Create environment\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    \n",
    "    print(\"\\nEnvironment Details:\")\n",
    "    print(f\"  Observation space: {env.observation_space}\")\n",
    "    print(f\"  Action space: {env.action_space}\")\n",
    "    \n",
    "    # Create SAC model with good defaults for robotics\n",
    "    model = SAC(\n",
    "        'MlpPolicy',\n",
    "        env,\n",
    "        learning_rate=3e-4,\n",
    "        buffer_size=100000,\n",
    "        batch_size=256,\n",
    "        tau=0.005,               # Soft update coefficient\n",
    "        gamma=0.99,              # Discount factor\n",
    "        learning_starts=1000,    # Random exploration first\n",
    "        verbose=0,\n",
    "    )\n",
    "    \n",
    "    # Evaluate before training\n",
    "    print(\"\\nEvaluating before training...\")\n",
    "    mean_before, std_before = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "    print(f\"  Mean reward: {mean_before:.2f} ± {std_before:.2f}\")\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nTraining for 30K timesteps...\")\n",
    "    model.learn(total_timesteps=30000, progress_bar=True)\n",
    "    \n",
    "    # Evaluate after training\n",
    "    print(\"\\nEvaluating after training...\")\n",
    "    mean_after, std_after = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    print(f\"  Mean reward: {mean_after:.2f} ± {std_after:.2f}\")\n",
    "    \n",
    "    improvement = mean_after - mean_before\n",
    "    print(f\"\\nImprovement: {improvement:+.2f}\")\n",
    "    \n",
    "    # Run an episode and record actions\n",
    "    print(\"\\nRecording trained policy...\")\n",
    "    obs, _ = env.reset()\n",
    "    states = [obs]\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(200):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        states.append(obs)\n",
    "        actions.append(action[0])\n",
    "        rewards.append(reward)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Visualize learned behavior\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "    \n",
    "    # Actions\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(actions, 'b-', linewidth=1.5)\n",
    "    ax1.set_ylabel('Action (Torque)', fontsize=11)\n",
    "    ax1.set_title('Learned Pendulum Control', fontsize=14, fontweight='bold')\n",
    "    ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(-2.5, 2.5)\n",
    "    \n",
    "    # Angle (from cos and sin)\n",
    "    angles = [np.arctan2(s[1], s[0]) for s in states[:-1]]\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(angles, 'g-', linewidth=1.5)\n",
    "    ax2.set_ylabel('Angle (rad)', fontsize=11)\n",
    "    ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Target (upright)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Rewards\n",
    "    ax3 = axes[2]\n",
    "    ax3.plot(rewards, 'orange', linewidth=1.5)\n",
    "    ax3.set_xlabel('Timestep', fontsize=11)\n",
    "    ax3.set_ylabel('Reward', fontsize=11)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nLEARNED BEHAVIOR:\")\n",
    "    print(\"  The agent learns to swing up and balance!\")\n",
    "    print(\"  Notice how actions become small once balanced.\")\n",
    "else:\n",
    "    print(\"Install required libraries to train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Robotics Simulation with RL\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| Continuous Control | Actions are real numbers, not discrete choices |\n",
    "| Reward Shaping | Combining multiple objectives (speed, energy, stability) |\n",
    "| Sim-to-Real | Training in simulation, deploying on real robots |\n",
    "\n",
    "### Algorithm Comparison\n",
    "\n",
    "| Algorithm | Sample Efficiency | Stability | Best For |\n",
    "|-----------|------------------|-----------|----------|\n",
    "| **SAC** | High | High | Default for robotics |\n",
    "| **TD3** | High | Medium | When SAC is unstable |\n",
    "| **PPO** | Low | Very High | When stability is critical |\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# SAC for robotics\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "env = gym.make('Pendulum-v1')  # or HalfCheetah-v4, etc.\n",
    "model = SAC('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=100_000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why is continuous control harder than discrete control?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Continuous control has infinite possible actions (any value in a range), while discrete control has finite choices. This makes:\n",
    "- Exploration harder (can't try all actions)\n",
    "- Learning slower (larger search space)\n",
    "- Coordination more complex (multiple continuous values)\n",
    "</details>\n",
    "\n",
    "**2. Why does SAC work well for robotics?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "SAC has three key advantages:\n",
    "1. Sample efficiency: Uses replay buffer to reuse experiences\n",
    "2. Entropy bonus: Encourages exploration of continuous action space\n",
    "3. Stability: Twin Q-networks and soft updates\n",
    "</details>\n",
    "\n",
    "**3. What is the \"reality gap\" in sim-to-real transfer?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The reality gap is the difference between simulation and the real world:\n",
    "- Different friction, mass, dynamics\n",
    "- Sensor noise and delays\n",
    "- Imperfect physics modeling\n",
    "\n",
    "A policy trained in simulation may fail on a real robot because it learned to exploit unrealistic simulation quirks.\n",
    "</details>\n",
    "\n",
    "**4. What's domain randomization and why does it help?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Domain randomization trains with randomly varied simulation parameters (friction, mass, etc.). This forces the policy to be robust to variations, making it more likely to work in the real world where exact parameters are unknown.\n",
    "</details>\n",
    "\n",
    "**5. Why include control cost in the reward?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Without control cost, robots learn jerky, energy-wasting motions. Penalizing large control signals:\n",
    "- Encourages smooth, efficient movement\n",
    "- Reduces wear on real motors\n",
    "- Produces more natural-looking gaits\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Robotics shows RL in physical systems. In the next notebook, we'll explore a very different application: **recommendation systems** where RL learns user preferences!\n",
    "\n",
    "**Continue to:** [Notebook 3: Recommendation Systems](03_recommendation_systems.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*\"Teaching robots to walk is like teaching children - patience, practice, and lots of falling down!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
