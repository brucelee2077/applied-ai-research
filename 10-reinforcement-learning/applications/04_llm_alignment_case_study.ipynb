{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Alignment Case Study: From Raw Model to Helpful Assistant\n",
    "\n",
    "A complete, practical walkthrough of aligning a language model!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The apprenticeship analogy: the full alignment journey\n",
    "- How to prepare and explore alignment datasets\n",
    "- SFT: Teaching the model to follow instructions\n",
    "- DPO: Refining the model with human preferences\n",
    "- Evaluation: Measuring alignment success\n",
    "- Best practices and common pitfalls\n",
    "\n",
    "**Prerequisites:** RLHF notebooks (01-06)\n",
    "\n",
    "**Time:** ~40 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Apprenticeship Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE APPRENTICESHIP ANALOGY                            │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Training an AI assistant is like training an apprentice...   │\n",
    "    │                                                                │\n",
    "    │  STAGE 1: EDUCATION (Pre-training)                            │\n",
    "    │    Read millions of books and documents                      │\n",
    "    │    Learn language, facts, patterns                           │\n",
    "    │    But: Doesn't know how to be helpful yet!                  │\n",
    "    │                                                                │\n",
    "    │  STAGE 2: JOB TRAINING (SFT)                                  │\n",
    "    │    \"Here's how we answer customer questions\"                 │\n",
    "    │    \"Here's the format we use\"                                │\n",
    "    │    Learn: What good responses LOOK like                      │\n",
    "    │                                                                │\n",
    "    │  STAGE 3: MENTORSHIP (DPO/RLHF)                               │\n",
    "    │    \"This response is better than that one\"                   │\n",
    "    │    \"This is too verbose, that's rude\"                        │\n",
    "    │    Learn: What humans PREFER                                 │\n",
    "    │                                                                │\n",
    "    │  RESULT:                                                      │\n",
    "    │    An assistant that's helpful, harmless, and honest!        │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, Rectangle, Arrow\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check libraries\n",
    "print(\"LIBRARY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "libraries = {\n",
    "    'torch': 'PyTorch (neural networks)',\n",
    "    'transformers': 'Transformers (LLM models)',\n",
    "    'datasets': 'Datasets (data loading)',\n",
    "    'trl': 'TRL (training)',\n",
    "    'peft': 'PEFT (efficient fine-tuning)',\n",
    "}\n",
    "\n",
    "available = {}\n",
    "for module, desc in libraries.items():\n",
    "    try:\n",
    "        lib = __import__(module)\n",
    "        available[module] = True\n",
    "        version = getattr(lib, '__version__', 'installed')\n",
    "        print(f\"  ✓ {desc}: {version}\")\n",
    "    except ImportError:\n",
    "        available[module] = False\n",
    "        print(f\"  ✗ {desc}: Not installed\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the alignment pipeline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('LLM Alignment Pipeline: The Complete Journey', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Stages\n",
    "stages = [\n",
    "    (2, 9, 'Pre-trained\\nLLM', 'Knows language\\nbut unguided', '#e3f2fd', '#1976d2'),\n",
    "    (6, 9, 'SFT Model', 'Follows\\ninstructions', '#fff3e0', '#f57c00'),\n",
    "    (10, 9, 'Aligned\\nModel', 'Helpful &\\nHarmless', '#c8e6c9', '#388e3c'),\n",
    "]\n",
    "\n",
    "for x, y, title, desc, fcolor, ecolor in stages:\n",
    "    box = FancyBboxPatch((x-1.3, y-1), 2.6, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=fcolor, edgecolor=ecolor, linewidth=3)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y+0.8, title, ha='center', fontsize=10, fontweight='bold')\n",
    "    ax.text(x, y-0.4, desc, ha='center', fontsize=8, color='#666')\n",
    "\n",
    "# Training stages below\n",
    "training = [\n",
    "    (4, 6, 'SFT', 'Supervised\\nFine-Tuning', 'Instruction\\nData', '#fff3e0', '#f57c00'),\n",
    "    (8, 6, 'DPO', 'Direct Preference\\nOptimization', 'Preference\\nData', '#c8e6c9', '#388e3c'),\n",
    "]\n",
    "\n",
    "for x, y, title, method, data, fcolor, ecolor in training:\n",
    "    # Training method box\n",
    "    box = FancyBboxPatch((x-1.3, y-0.5), 2.6, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=fcolor, edgecolor=ecolor, linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y+0.9, title, ha='center', fontsize=11, fontweight='bold', color=ecolor)\n",
    "    ax.text(x, y+0.1, method, ha='center', fontsize=8)\n",
    "    \n",
    "    # Data box\n",
    "    data_box = FancyBboxPatch((x-1, y-2.5), 2, 1.3, boxstyle=\"round,pad=0.1\",\n",
    "                               facecolor='#f5f5f5', edgecolor='#999', linewidth=1)\n",
    "    ax.add_patch(data_box)\n",
    "    ax.text(x, y-1.9, data, ha='center', fontsize=8)\n",
    "    \n",
    "    # Arrow from data to training\n",
    "    ax.annotate('', xy=(x, y-0.6), xytext=(x, y-1.1),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5, color='#666'))\n",
    "\n",
    "# Horizontal arrows between stages\n",
    "ax.annotate('', xy=(4.6, 9.5), xytext=(3.4, 9.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(8.6, 9.5), xytext=(7.4, 9.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Diagonal arrows from models to training\n",
    "ax.annotate('', xy=(4, 7.6), xytext=(2.5, 7.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=1.5, color='#999'))\n",
    "ax.annotate('', xy=(8, 7.6), xytext=(6.5, 7.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=1.5, color='#999'))\n",
    "\n",
    "# Evaluation at the bottom\n",
    "eval_box = FancyBboxPatch((5, 0.5), 4, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#e1bee7', edgecolor='#7b1fa2', linewidth=2)\n",
    "ax.add_patch(eval_box)\n",
    "ax.text(7, 1.5, 'Evaluation', ha='center', fontsize=10, fontweight='bold', color='#7b1fa2')\n",
    "ax.text(7, 0.9, 'Win rate, Human eval, KL divergence', ha='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nALIGNMENT PIPELINE:\")\n",
    "print(\"  1. Pre-trained LLM → knows language but not helpful\")\n",
    "print(\"  2. SFT → learns to follow instructions\")\n",
    "print(\"  3. DPO/RLHF → learns human preferences\")\n",
    "print(\"  4. Evaluation → verify it's actually better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Understanding Alignment Datasets\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              ALIGNMENT DATASETS                                │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  SFT DATASETS (Instruction-following):                        │\n",
    "    │    Format: {instruction, response}                           │\n",
    "    │    Examples:                                                  │\n",
    "    │      - OpenAssistant (conversation data)                     │\n",
    "    │      - Dolly-15k (instruction tuning)                        │\n",
    "    │      - Alpaca (GPT-generated instructions)                   │\n",
    "    │                                                                │\n",
    "    │  PREFERENCE DATASETS (DPO/RLHF):                              │\n",
    "    │    Format: {prompt, chosen_response, rejected_response}      │\n",
    "    │    Examples:                                                  │\n",
    "    │      - Anthropic HH-RLHF (helpful & harmless)                │\n",
    "    │      - Stanford SHP (Stack Exchange preferences)             │\n",
    "    │      - UltraFeedback (diverse preferences)                   │\n",
    "    │                                                                │\n",
    "    │  KEY INSIGHT:                                                 │\n",
    "    │    Quality matters more than quantity!                       │\n",
    "    │    1K great examples > 100K noisy examples                   │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore preference data format\n",
    "\n",
    "print(\"UNDERSTANDING PREFERENCE DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulated example from Anthropic HH-RLHF\n",
    "example_preference = {\n",
    "    \"prompt\": \"Human: How can I improve my public speaking skills?\",\n",
    "    \n",
    "    \"chosen\": \"\"\"Assistant: Here are some effective ways to improve your public speaking:\n",
    "\n",
    "1. **Practice regularly** - Record yourself and watch for filler words, pacing, and body language.\n",
    "\n",
    "2. **Know your material** - Thorough preparation builds confidence.\n",
    "\n",
    "3. **Start small** - Practice with friends or small groups before larger audiences.\n",
    "\n",
    "4. **Focus on your audience** - Think about helping them, not about yourself.\n",
    "\n",
    "5. **Join a group** - Organizations like Toastmasters provide supportive practice environments.\n",
    "\n",
    "Would you like me to elaborate on any of these points?\"\"\",\n",
    "    \n",
    "    \"rejected\": \"\"\"Assistant: Just practice more and you'll get better at it. \n",
    "Public speaking isn't that hard if you just do it.\"\"\"\n",
    "}\n",
    "\n",
    "print(\"\\nEXAMPLE PREFERENCE PAIR:\")\n",
    "print(\"-\"*70)\n",
    "print(f\"\\nPROMPT:\\n{example_preference['prompt']}\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"\\nCHOSEN (Better) Response:\")\n",
    "print(example_preference['chosen'][:300] + \"...\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"\\nREJECTED (Worse) Response:\")\n",
    "print(example_preference['rejected'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nWHY CHOSEN IS BETTER:\")\n",
    "print(\"  ✓ Structured with clear points\")\n",
    "print(\"  ✓ Actionable advice\")\n",
    "print(\"  ✓ Specific recommendations\")\n",
    "print(\"  ✓ Offers to help more\")\n",
    "print(\"\\nWHY REJECTED IS WORSE:\")\n",
    "print(\"  ✗ Vague and unhelpful\")\n",
    "print(\"  ✗ Dismissive tone\")\n",
    "print(\"  ✗ No concrete suggestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what makes good vs bad responses\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "# Left: Good response characteristics\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('CHOSEN Response\\n(What makes it good)', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "\n",
    "good_traits = [\n",
    "    (5, 8.5, 'Helpful', 'Addresses the actual need'),\n",
    "    (5, 7, 'Specific', 'Concrete, actionable advice'),\n",
    "    (5, 5.5, 'Structured', 'Clear organization'),\n",
    "    (5, 4, 'Respectful', 'Takes user seriously'),\n",
    "    (5, 2.5, 'Engaging', 'Offers further help'),\n",
    "]\n",
    "\n",
    "for x, y, trait, desc in good_traits:\n",
    "    box = FancyBboxPatch((x-2.5, y-0.5), 5, 1, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(x, y+0.2, f'✓ {trait}', ha='center', fontsize=10, fontweight='bold', color='#388e3c')\n",
    "    ax1.text(x, y-0.3, desc, ha='center', fontsize=8, color='#666')\n",
    "\n",
    "# Right: Bad response characteristics\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('REJECTED Response\\n(What makes it bad)', fontsize=12, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "bad_traits = [\n",
    "    (5, 8.5, 'Unhelpful', \"Doesn't address the need\"),\n",
    "    (5, 7, 'Vague', 'No concrete guidance'),\n",
    "    (5, 5.5, 'Dismissive', 'Minimizes the concern'),\n",
    "    (5, 4, 'Short', 'Low effort response'),\n",
    "    (5, 2.5, 'Closed', 'No follow-up offered'),\n",
    "]\n",
    "\n",
    "for x, y, trait, desc in bad_traits:\n",
    "    box = FancyBboxPatch((x-2.5, y-0.5), 5, 1, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(x, y+0.2, f'✗ {trait}', ha='center', fontsize=10, fontweight='bold', color='#d32f2f')\n",
    "    ax2.text(x, y-0.3, desc, ha='center', fontsize=8, color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHT:\")\n",
    "print(\"  DPO learns from these COMPARISONS, not from labels!\")\n",
    "print(\"  The model learns: 'Make responses more like chosen, less like rejected'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Supervised Fine-Tuning (SFT)\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          SFT: TEACHING TO FOLLOW INSTRUCTIONS                  │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  PURPOSE:                                                     │\n",
    "    │    Transform: \"Smart but aimless\" → \"Instruction-following\"  │\n",
    "    │                                                                │\n",
    "    │  WHAT IT LEARNS:                                              │\n",
    "    │    - The STRUCTURE of helpful responses                      │\n",
    "    │    - How to INTERPRET instructions                           │\n",
    "    │    - Appropriate TONE and FORMAT                             │\n",
    "    │                                                                │\n",
    "    │  DATA FORMAT:                                                 │\n",
    "    │    \"Human: How do I bake a cake?\"                            │\n",
    "    │    \"Assistant: Here's a simple recipe...\"                    │\n",
    "    │                                                                │\n",
    "    │  TRAINING:                                                    │\n",
    "    │    Standard next-token prediction on instruction data        │\n",
    "    │    Loss = -log P(correct_next_token)                         │\n",
    "    │                                                                │\n",
    "    │  KEY TIPS:                                                    │\n",
    "    │    • Quality data matters more than quantity                 │\n",
    "    │    • Use LoRA for efficient training                         │\n",
    "    │    • 1-3 epochs is usually enough                            │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT Training Code (Template)\n",
    "\n",
    "print(\"SFT TRAINING TEMPLATE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sft_code = '''\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# 1. Load base model (use a small one for demonstration)\n",
    "model_name = \"gpt2\"  # Or \"meta-llama/Llama-2-7b-hf\" for real use\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Load instruction dataset\n",
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "\n",
    "# 3. Configure LoRA (efficient fine-tuning)\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                           # Rank of LoRA matrices\n",
    "    lora_alpha=32,                  # Scaling factor\n",
    "    lora_dropout=0.05,              # Regularization\n",
    "    target_modules=[\"c_attn\"],      # GPT-2 attention (varies by model)\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 4. Configure training\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./sft_model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_seq_length=512,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# 5. Create trainer and train\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./sft_model_final\")\n",
    "'''\n",
    "\n",
    "print(sft_code)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate SFT training curves\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulated training metrics\n",
    "steps = np.arange(0, 1001, 50)\n",
    "loss = 3.5 * np.exp(-steps/300) + 1.5 + np.random.randn(len(steps)) * 0.1\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Loss curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(steps, loss, 'b-', linewidth=2)\n",
    "ax1.fill_between(steps, loss - 0.1, loss + 0.1, alpha=0.2)\n",
    "ax1.set_xlabel('Training Steps', fontsize=11)\n",
    "ax1.set_ylabel('Loss', fontsize=11)\n",
    "ax1.set_title('SFT Training Loss', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=1.6, color='red', linestyle='--', alpha=0.5, label='Target')\n",
    "ax1.legend()\n",
    "\n",
    "# Right: Before/After comparison\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('SFT Effect', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Before SFT\n",
    "before_box = FancyBboxPatch((0.5, 5.5), 4, 4, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "ax2.add_patch(before_box)\n",
    "ax2.text(2.5, 9, 'Before SFT', ha='center', fontsize=11, fontweight='bold', color='#d32f2f')\n",
    "ax2.text(2.5, 8, 'User: What is 2+2?', ha='center', fontsize=9)\n",
    "ax2.text(2.5, 7.2, 'Model: The sum of two', ha='center', fontsize=9)\n",
    "ax2.text(2.5, 6.6, 'numbers when added...', ha='center', fontsize=9)\n",
    "ax2.text(2.5, 5.8, '(Rambles, no answer)', ha='center', fontsize=8, color='#666', style='italic')\n",
    "\n",
    "# After SFT\n",
    "after_box = FancyBboxPatch((5.5, 5.5), 4, 4, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax2.add_patch(after_box)\n",
    "ax2.text(7.5, 9, 'After SFT', ha='center', fontsize=11, fontweight='bold', color='#388e3c')\n",
    "ax2.text(7.5, 8, 'User: What is 2+2?', ha='center', fontsize=9)\n",
    "ax2.text(7.5, 7.2, 'Model: 2+2 equals 4.', ha='center', fontsize=9)\n",
    "ax2.text(7.5, 6.4, 'Is there anything else', ha='center', fontsize=9)\n",
    "ax2.text(7.5, 5.8, \"I can help you with?\", ha='center', fontsize=9)\n",
    "\n",
    "# Arrow\n",
    "ax2.annotate('', xy=(5.4, 7.5), xytext=(4.6, 7.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Impact summary\n",
    "ax2.text(5, 4, 'SFT teaches the model HOW to respond', ha='center', fontsize=10, style='italic')\n",
    "ax2.text(5, 3, 'Format, structure, helpfulness', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Direct Preference Optimization (DPO)\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          DPO: LEARNING FROM PREFERENCES                        │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  PURPOSE:                                                     │\n",
    "    │    Transform: \"Follows instructions\" → \"Aligned with humans\" │\n",
    "    │                                                                │\n",
    "    │  WHAT IT LEARNS:                                              │\n",
    "    │    - Which responses humans PREFER                           │\n",
    "    │    - Subtle quality differences                              │\n",
    "    │    - Safety and helpfulness tradeoffs                        │\n",
    "    │                                                                │\n",
    "    │  THE DPO LOSS:                                                │\n",
    "    │    L = -log σ(β × [log π(y_w)/π_ref(y_w)                     │\n",
    "    │                  - log π(y_l)/π_ref(y_l)])                   │\n",
    "    │                                                                │\n",
    "    │    \"Increase probability of chosen response\"                 │\n",
    "    │    \"Decrease probability of rejected response\"               │\n",
    "    │    \"Don't drift too far from reference (KL constraint)\"      │\n",
    "    │                                                                │\n",
    "    │  KEY ADVANTAGE:                                               │\n",
    "    │    No reward model needed! Direct supervised training!       │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO Training Code (Template)\n",
    "\n",
    "print(\"DPO TRAINING TEMPLATE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dpo_code = '''\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "# 1. Load SFT model (from previous step)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./sft_model_final\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./sft_model_final\")\n",
    "\n",
    "# 2. Load preference dataset\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:5000]\")\n",
    "\n",
    "# 3. Format data for DPO\n",
    "def format_for_dpo(example):\n",
    "    \"\"\"Convert to DPO format: prompt, chosen, rejected.\"\"\"\n",
    "    # Parse Anthropic format (Human/Assistant turns)\n",
    "    chosen = example[\"chosen\"]\n",
    "    rejected = example[\"rejected\"]\n",
    "    \n",
    "    # Extract prompt and responses\n",
    "    if \"\\\\nAssistant:\" in chosen:\n",
    "        prompt = chosen.split(\"\\\\nAssistant:\")[0].replace(\"Human: \", \"\")\n",
    "        chosen_response = chosen.split(\"\\\\nAssistant:\")[-1]\n",
    "        rejected_response = rejected.split(\"\\\\nAssistant:\")[-1]\n",
    "    else:\n",
    "        prompt = chosen[:100]\n",
    "        chosen_response = chosen\n",
    "        rejected_response = rejected\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt.strip(),\n",
    "        \"chosen\": chosen_response.strip(),\n",
    "        \"rejected\": rejected_response.strip(),\n",
    "    }\n",
    "\n",
    "dpo_dataset = dataset.map(format_for_dpo)\n",
    "\n",
    "# 4. Configure DPO training\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"./dpo_model\",\n",
    "    beta=0.1,                          # KL penalty coefficient\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    max_length=512,\n",
    "    max_prompt_length=256,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# 5. Create trainer (reference model created automatically)\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dpo_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./dpo_model_final\")\n",
    "'''\n",
    "\n",
    "print(dpo_code)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DPO training dynamics\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "steps = np.arange(0, 501, 25)\n",
    "\n",
    "# Simulated metrics\n",
    "chosen_rewards = -0.5 + 0.8 * (1 - np.exp(-steps/150)) + np.random.randn(len(steps)) * 0.1\n",
    "rejected_rewards = -0.3 - 0.5 * (1 - np.exp(-steps/150)) + np.random.randn(len(steps)) * 0.1\n",
    "margin = chosen_rewards - rejected_rewards\n",
    "kl_div = 0.1 + steps/100 + np.random.randn(len(steps)) * 0.2\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Left: Reward margins\n",
    "ax1 = axes[0]\n",
    "ax1.plot(steps, chosen_rewards, 'g-', linewidth=2, label='Chosen reward')\n",
    "ax1.plot(steps, rejected_rewards, 'r-', linewidth=2, label='Rejected reward')\n",
    "ax1.fill_between(steps, rejected_rewards, chosen_rewards, alpha=0.2, color='green')\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Training Steps', fontsize=11)\n",
    "ax1.set_ylabel('Implicit Reward', fontsize=11)\n",
    "ax1.set_title('DPO Reward Dynamics', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Middle: Margin\n",
    "ax2 = axes[1]\n",
    "ax2.plot(steps, margin, 'b-', linewidth=2)\n",
    "ax2.fill_between(steps, 0, margin, alpha=0.2)\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Training Steps', fontsize=11)\n",
    "ax2.set_ylabel('Reward Margin', fontsize=11)\n",
    "ax2.set_title('Chosen - Rejected Margin', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: KL divergence\n",
    "ax3 = axes[2]\n",
    "ax3.plot(steps, kl_div, 'orange', linewidth=2)\n",
    "ax3.axhline(y=5, color='red', linestyle='--', alpha=0.5, label='Warning threshold')\n",
    "ax3.axhline(y=15, color='red', linestyle='-', alpha=0.5, label='Danger zone')\n",
    "ax3.set_xlabel('Training Steps', fontsize=11)\n",
    "ax3.set_ylabel('KL Divergence', fontsize=11)\n",
    "ax3.set_title('KL from Reference', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim(0, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDPO TRAINING DYNAMICS:\")\n",
    "print(\"  • Chosen reward INCREASES (model prefers good responses)\")\n",
    "print(\"  • Rejected reward DECREASES (model avoids bad responses)\")\n",
    "print(\"  • Margin GROWS (clearer preference signal)\")\n",
    "print(\"  • KL divergence should stay moderate (not too high!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Evaluation - Did It Work?\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          EVALUATION METHODS                                    │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  1. AUTOMATED METRICS                                         │\n",
    "    │     Win Rate: Compare to baseline on same prompts            │\n",
    "    │     Reward Score: Average reward model score                 │\n",
    "    │     Perplexity: Language quality (shouldn't degrade)         │\n",
    "    │                                                                │\n",
    "    │  2. LLM-AS-JUDGE                                              │\n",
    "    │     Use GPT-4 or Claude to evaluate response quality         │\n",
    "    │     Cheaper than human eval, correlates well                 │\n",
    "    │                                                                │\n",
    "    │  3. HUMAN EVALUATION (Gold Standard)                          │\n",
    "    │     Real humans rate helpfulness, harmlessness               │\n",
    "    │     Most reliable but expensive                              │\n",
    "    │                                                                │\n",
    "    │  4. SAFETY TESTING                                            │\n",
    "    │     Red teaming: Try to make model produce harmful output   │\n",
    "    │     Check for refusals on dangerous requests                 │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated evaluation results\n",
    "\n",
    "print(\"EVALUATION RESULTS (Simulated)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate evaluation on 100 prompts\n",
    "n_eval = 100\n",
    "\n",
    "# Win rates (aligned model vs baseline)\n",
    "base_scores = np.random.normal(5, 1.5, n_eval)\n",
    "sft_scores = np.random.normal(6.5, 1.2, n_eval)\n",
    "aligned_scores = np.random.normal(7.8, 1.0, n_eval)\n",
    "\n",
    "# Compute win rates\n",
    "def compute_win_rate(model_scores, baseline_scores):\n",
    "    wins = sum(m > b for m, b in zip(model_scores, baseline_scores))\n",
    "    ties = sum(m == b for m, b in zip(model_scores, baseline_scores))\n",
    "    return wins / len(model_scores), ties / len(model_scores)\n",
    "\n",
    "win_sft, tie_sft = compute_win_rate(sft_scores, base_scores)\n",
    "win_aligned, tie_aligned = compute_win_rate(aligned_scores, sft_scores)\n",
    "\n",
    "print(\"\\nWIN RATE ANALYSIS:\")\n",
    "print(f\"  SFT vs Base:     {win_sft:.1%} wins\")\n",
    "print(f\"  Aligned vs SFT:  {win_aligned:.1%} wins\")\n",
    "\n",
    "print(\"\\nAVERAGE QUALITY SCORES (1-10):\")\n",
    "print(f\"  Base Model:    {base_scores.mean():.2f} ± {base_scores.std():.2f}\")\n",
    "print(f\"  SFT Model:     {sft_scores.mean():.2f} ± {sft_scores.std():.2f}\")\n",
    "print(f\"  Aligned Model: {aligned_scores.mean():.2f} ± {aligned_scores.std():.2f}\")\n",
    "\n",
    "print(\"\\nSAFETY EVALUATION:\")\n",
    "safety_results = {\n",
    "    'Harmful request refusal': (85, 95),  # (SFT, Aligned)\n",
    "    'Honest uncertainty': (70, 88),\n",
    "    'No hallucination': (75, 82),\n",
    "}\n",
    "\n",
    "for metric, (sft_rate, aligned_rate) in safety_results.items():\n",
    "    improvement = aligned_rate - sft_rate\n",
    "    print(f\"  {metric}: SFT={sft_rate}%, Aligned={aligned_rate}% (+{improvement}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Left: Score distributions\n",
    "ax1 = axes[0]\n",
    "ax1.hist(base_scores, bins=15, alpha=0.5, label='Base', color='red')\n",
    "ax1.hist(sft_scores, bins=15, alpha=0.5, label='SFT', color='orange')\n",
    "ax1.hist(aligned_scores, bins=15, alpha=0.5, label='Aligned', color='green')\n",
    "ax1.set_xlabel('Quality Score', fontsize=11)\n",
    "ax1.set_ylabel('Count', fontsize=11)\n",
    "ax1.set_title('Response Quality Distribution', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Middle: Win rate visualization\n",
    "ax2 = axes[1]\n",
    "comparisons = ['SFT vs Base', 'Aligned vs SFT']\n",
    "win_rates = [win_sft * 100, win_aligned * 100]\n",
    "lose_rates = [100 - win_sft * 100, 100 - win_aligned * 100]\n",
    "\n",
    "x = np.arange(len(comparisons))\n",
    "width = 0.35\n",
    "\n",
    "ax2.bar(x - width/2, win_rates, width, label='Wins', color='#4caf50')\n",
    "ax2.bar(x + width/2, lose_rates, width, label='Losses', color='#f44336')\n",
    "ax2.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax2.set_ylabel('Percentage', fontsize=11)\n",
    "ax2.set_title('Win Rate Comparison', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(comparisons)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Right: Safety metrics\n",
    "ax3 = axes[2]\n",
    "metrics = list(safety_results.keys())\n",
    "sft_vals = [v[0] for v in safety_results.values()]\n",
    "aligned_vals = [v[1] for v in safety_results.values()]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "ax3.bar(x - width/2, sft_vals, width, label='SFT', color='#ff9800')\n",
    "ax3.bar(x + width/2, aligned_vals, width, label='Aligned', color='#4caf50')\n",
    "\n",
    "ax3.set_ylabel('Success Rate (%)', fontsize=11)\n",
    "ax3.set_title('Safety Metrics', fontsize=12, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(['Refusal', 'Uncertainty', 'No Halluc.'], rotation=15)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "ax3.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(\"  • Aligned model wins ~75% of comparisons\")\n",
    "print(\"  • Quality scores improved from 5 → 6.5 → 7.8\")\n",
    "print(\"  • Safety metrics improved across the board\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Common Pitfalls and Best Practices\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              PITFALLS TO AVOID                                 │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  1. REWARD HACKING                                            │\n",
    "    │     Model exploits reward signal without being helpful       │\n",
    "    │     Fix: Stronger KL penalty, diverse evaluation             │\n",
    "    │                                                                │\n",
    "    │  2. MODE COLLAPSE                                             │\n",
    "    │     Model gives same response to everything                  │\n",
    "    │     Fix: Lower learning rate, increase KL penalty            │\n",
    "    │                                                                │\n",
    "    │  3. CATASTROPHIC FORGETTING                                   │\n",
    "    │     Model forgets base knowledge                             │\n",
    "    │     Fix: Use LoRA, smaller learning rate                     │\n",
    "    │                                                                │\n",
    "    │  4. LENGTH GAMING                                             │\n",
    "    │     Model learns \"longer = better\"                           │\n",
    "    │     Fix: Length normalization in reward                      │\n",
    "    │                                                                │\n",
    "    │  5. SYCOPHANCY                                                │\n",
    "    │     Model agrees with user even when wrong                   │\n",
    "    │     Fix: Include honesty examples in training data           │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pitfalls and solutions\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('Common Alignment Pitfalls and Solutions', fontsize=16, fontweight='bold')\n",
    "\n",
    "pitfalls = [\n",
    "    (2.5, 9.5, 'Reward\\nHacking', 'Model exploits reward\\nwithout being helpful', 'Increase β,\\ndiverse eval'),\n",
    "    (7, 9.5, 'Mode\\nCollapse', 'Same response\\nto everything', 'Lower LR,\\nmore KL'),\n",
    "    (11.5, 9.5, 'Catastrophic\\nForgetting', 'Forgets base\\nknowledge', 'Use LoRA,\\nlower LR'),\n",
    "    (4.75, 5.5, 'Length\\nGaming', 'Learns longer\\nis better', 'Length\\nnormalization'),\n",
    "    (9.25, 5.5, 'Sycophancy', 'Agrees even\\nwhen wrong', 'Honesty\\ntraining data'),\n",
    "]\n",
    "\n",
    "for x, y, title, problem, solution in pitfalls:\n",
    "    # Problem box (red)\n",
    "    prob_box = FancyBboxPatch((x-1.5, y), 3, 2, boxstyle=\"round,pad=0.1\",\n",
    "                               facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "    ax.add_patch(prob_box)\n",
    "    ax.text(x, y+1.6, title, ha='center', fontsize=9, fontweight='bold', color='#d32f2f')\n",
    "    ax.text(x, y+0.5, problem, ha='center', fontsize=7)\n",
    "    \n",
    "    # Solution box (green) below\n",
    "    sol_box = FancyBboxPatch((x-1.3, y-2.3), 2.6, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                              facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "    ax.add_patch(sol_box)\n",
    "    ax.text(x, y-1.3, 'FIX:', ha='center', fontsize=8, fontweight='bold', color='#388e3c')\n",
    "    ax.text(x, y-1.9, solution, ha='center', fontsize=8)\n",
    "    \n",
    "    # Arrow\n",
    "    ax.annotate('', xy=(x, y-0.7), xytext=(x, y-0.1),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5, color='#666'))\n",
    "\n",
    "# Best practices section\n",
    "ax.text(7, 2, 'BEST PRACTICES', ha='center', fontsize=12, fontweight='bold', color='#1976d2')\n",
    "practices = [\n",
    "    'Start small (test pipeline on GPT-2 first)',\n",
    "    'Monitor KL divergence throughout training',\n",
    "    'Use diverse evaluation (automated + human)',\n",
    "    'Save checkpoints frequently',\n",
    "]\n",
    "for i, practice in enumerate(practices):\n",
    "    ax.text(7, 1.3 - i*0.4, f'• {practice}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: The Alignment Recipe\n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "| Stage | Purpose | Data Needed | Output |\n",
    "|-------|---------|-------------|--------|\n",
    "| Pre-train | Learn language | Internet text | Base LLM |\n",
    "| SFT | Learn to follow | Instruction pairs | Instruction model |\n",
    "| DPO | Learn preferences | Preference pairs | Aligned model |\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "| Parameter | SFT | DPO |\n",
    "|-----------|-----|-----|\n",
    "| Learning rate | 2e-4 | 5e-5 |\n",
    "| Batch size | 4 | 2 |\n",
    "| Epochs | 1-3 | 1 |\n",
    "| β (KL penalty) | N/A | 0.1 |\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# Minimal alignment pipeline\n",
    "from trl import SFTTrainer, DPOTrainer\n",
    "\n",
    "# Step 1: SFT\n",
    "sft_trainer = SFTTrainer(model, args, sft_dataset)\n",
    "sft_trainer.train()\n",
    "\n",
    "# Step 2: DPO\n",
    "dpo_trainer = DPOTrainer(sft_model, args, preference_dataset)\n",
    "dpo_trainer.train()\n",
    "\n",
    "# Step 3: Evaluate\n",
    "# Compare to baseline on held-out prompts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What's the difference between SFT and DPO?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "SFT (Supervised Fine-Tuning) teaches the model WHAT good responses look like by training on instruction-response pairs. It's like showing an apprentice examples of good work.\n",
    "\n",
    "DPO (Direct Preference Optimization) teaches the model WHICH responses humans prefer by training on preference pairs (chosen vs rejected). It's like mentorship - \"this is better than that.\"\n",
    "\n",
    "SFT comes first (foundation), DPO refines (polish).\n",
    "</details>\n",
    "\n",
    "**2. Why use LoRA instead of full fine-tuning?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "LoRA (Low-Rank Adaptation) is more efficient:\n",
    "- Uses ~10-100x less memory\n",
    "- Trains ~0.1% of parameters\n",
    "- Reduces catastrophic forgetting\n",
    "- Enables training large models on consumer GPUs\n",
    "\n",
    "Full fine-tuning updates ALL parameters, which requires massive compute and risks losing base knowledge.\n",
    "</details>\n",
    "\n",
    "**3. What is reward hacking and how do you prevent it?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Reward hacking is when the model finds ways to maximize the reward signal without actually being helpful. Examples:\n",
    "- Writing longer responses (if length correlates with reward)\n",
    "- Using specific phrases that score high\n",
    "- Being sycophantic (agreeing with everything)\n",
    "\n",
    "Prevention:\n",
    "- Increase β (KL penalty) to stay close to reference\n",
    "- Use diverse evaluation metrics\n",
    "- Include human evaluation\n",
    "- Length normalization\n",
    "</details>\n",
    "\n",
    "**4. Why is KL divergence monitoring important?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "KL divergence measures how different the trained model is from the reference model.\n",
    "\n",
    "- Too low (< 1): Model isn't learning much from preferences\n",
    "- Good range (5-15): Meaningful improvement while staying stable\n",
    "- Too high (> 15): Risk of reward hacking or catastrophic forgetting\n",
    "\n",
    "High KL means the model has drifted far from its original behavior, which can cause unexpected failures.\n",
    "</details>\n",
    "\n",
    "**5. What makes a good preference dataset?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Good preference data has:\n",
    "- Clear differences between chosen and rejected (not ambiguous)\n",
    "- Diversity of topics and response types\n",
    "- Consistent labeling (no contradictions)\n",
    "- Quality matters more than quantity (1K good > 100K noisy)\n",
    "\n",
    "The chosen response should be clearly better - helpful, accurate, and well-structured - while rejected should have clear flaws.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "You've now seen a complete LLM alignment case study! In the final notebook, we'll explore **Multi-Agent Systems** - what happens when multiple RL agents interact.\n",
    "\n",
    "**Continue to:** [Notebook 5: Multi-Agent Systems](05_multi_agent_systems.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*\"Alignment is not a one-time fix - it's an ongoing journey of understanding and improving.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
