{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Reinforcement Learning: When AI Agents Interact\n",
    "\n",
    "What happens when multiple learning agents share the same world?\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The dance troupe analogy: agents learning to coordinate\n",
    "- Game theory foundations: Nash equilibrium and strategic thinking\n",
    "- Cooperative, competitive, and mixed-motive scenarios\n",
    "- The non-stationarity challenge: learning in a changing world\n",
    "- Key algorithms: Independent Q-Learning, QMIX, MADDPG\n",
    "- Centralized Training with Decentralized Execution (CTDE)\n",
    "- Emergent behaviors and communication\n",
    "\n",
    "**Prerequisites:** All previous RL notebooks\n",
    "\n",
    "**Time:** ~40 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Dance Troupe Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE DANCE TROUPE ANALOGY                              │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Imagine a dance troupe learning a new routine...             │\n",
    "    │                                                                │\n",
    "    │  EACH DANCER (Agent):                                         │\n",
    "    │    Has their own moves to learn                              │\n",
    "    │    Watches what others are doing                             │\n",
    "    │    Adjusts based on the group                                │\n",
    "    │                                                                │\n",
    "    │  THE CHALLENGE:                                               │\n",
    "    │    Everyone is learning at the same time!                    │\n",
    "    │    Your partner's moves keep changing                        │\n",
    "    │    What worked yesterday might not work today                │\n",
    "    │                                                                │\n",
    "    │  COOPERATIVE DANCE:                                          │\n",
    "    │    All dancers want the show to succeed                      │\n",
    "    │    Shared reward: Audience applause                          │\n",
    "    │                                                                │\n",
    "    │  COMPETITIVE DANCE:                                          │\n",
    "    │    Dance battle! One winner                                  │\n",
    "    │    Your success = opponent's failure                         │\n",
    "    │                                                                │\n",
    "    │  MIXED:                                                       │\n",
    "    │    Team competition                                          │\n",
    "    │    Cooperate with team, compete with rivals                  │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, Rectangle, FancyArrowPatch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-agent settings\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Cooperative\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('COOPERATIVE', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Agents working together\n",
    "positions = [(3, 5), (5, 7), (7, 5), (5, 3)]\n",
    "for i, (x, y) in enumerate(positions):\n",
    "    circle = Circle((x, y), 0.8, facecolor='#4caf50', edgecolor='black', linewidth=2)\n",
    "    ax1.add_patch(circle)\n",
    "    ax1.text(x, y, f'A{i+1}', ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "# Connections (cooperation)\n",
    "for i, (x1, y1) in enumerate(positions):\n",
    "    for j, (x2, y2) in enumerate(positions):\n",
    "        if i < j:\n",
    "            ax1.plot([x1, x2], [y1, y2], 'g-', linewidth=1, alpha=0.5)\n",
    "\n",
    "# Shared goal\n",
    "goal = FancyBboxPatch((3.5, 0.5), 3, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                       facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax1.add_patch(goal)\n",
    "ax1.text(5, 1.1, 'Shared Goal', ha='center', fontsize=10, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Competitive\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('COMPETITIVE', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Two opposing agents\n",
    "circle1 = Circle((3, 5), 1, facecolor='#f44336', edgecolor='black', linewidth=2)\n",
    "circle2 = Circle((7, 5), 1, facecolor='#2196f3', edgecolor='black', linewidth=2)\n",
    "ax2.add_patch(circle1)\n",
    "ax2.add_patch(circle2)\n",
    "ax2.text(3, 5, 'A1', ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "ax2.text(7, 5, 'A2', ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "\n",
    "# Conflict\n",
    "ax2.annotate('', xy=(4.2, 5), xytext=(5.8, 5),\n",
    "             arrowprops=dict(arrowstyle='<->', lw=3, color='#d32f2f'))\n",
    "ax2.text(5, 3, 'Zero-Sum', ha='center', fontsize=10, style='italic', color='#666')\n",
    "ax2.text(5, 2.3, 'Your win = My loss', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Mixed\n",
    "ax3 = axes[2]\n",
    "ax3.set_xlim(0, 10)\n",
    "ax3.set_ylim(0, 10)\n",
    "ax3.axis('off')\n",
    "ax3.set_title('MIXED', fontsize=14, fontweight='bold', color='#7b1fa2')\n",
    "\n",
    "# Team 1\n",
    "for i, (x, y) in enumerate([(2, 6), (2, 4)]):\n",
    "    circle = Circle((x, y), 0.6, facecolor='#f44336', edgecolor='black', linewidth=2)\n",
    "    ax3.add_patch(circle)\n",
    "ax3.plot([2, 2], [6, 4], 'g-', linewidth=2)  # Cooperation\n",
    "ax3.text(2, 2.5, 'Team A', ha='center', fontsize=9, color='#f44336', fontweight='bold')\n",
    "\n",
    "# Team 2\n",
    "for i, (x, y) in enumerate([(8, 6), (8, 4)]):\n",
    "    circle = Circle((x, y), 0.6, facecolor='#2196f3', edgecolor='black', linewidth=2)\n",
    "    ax3.add_patch(circle)\n",
    "ax3.plot([8, 8], [6, 4], 'g-', linewidth=2)  # Cooperation\n",
    "ax3.text(8, 2.5, 'Team B', ha='center', fontsize=9, color='#2196f3', fontweight='bold')\n",
    "\n",
    "# Competition between teams\n",
    "ax3.annotate('', xy=(3, 5), xytext=(7, 5),\n",
    "             arrowprops=dict(arrowstyle='<->', lw=2, color='#d32f2f'))\n",
    "ax3.text(5, 5.5, 'Compete', ha='center', fontsize=9, color='#d32f2f')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMULTI-AGENT SETTINGS:\")\n",
    "print(\"  Cooperative: Agents share rewards, work toward common goal\")\n",
    "print(\"  Competitive: One agent's gain is another's loss\")\n",
    "print(\"  Mixed: Both cooperation and competition (teams, alliances)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Game Theory Foundations: Strategic Thinking\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              GAME THEORY BASICS                                │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  NORMAL FORM GAME:                                            │\n",
    "    │    Players choose actions simultaneously                      │\n",
    "    │    Payoffs depend on ALL players' actions                    │\n",
    "    │                                                                │\n",
    "    │  PRISONER'S DILEMMA:                                          │\n",
    "    │                        Player 2                               │\n",
    "    │                    Cooperate  Defect                          │\n",
    "    │    Player 1  Cooperate  (3,3)    (0,5)                       │\n",
    "    │              Defect     (5,0)    (1,1)                       │\n",
    "    │                                                                │\n",
    "    │    Dilemma: Both defecting is worse than both cooperating!   │\n",
    "    │                                                                │\n",
    "    │  NASH EQUILIBRIUM:                                            │\n",
    "    │    No player can improve by unilaterally changing strategy   │\n",
    "    │    In Prisoner's Dilemma: (Defect, Defect) is Nash!         │\n",
    "    │                                                                │\n",
    "    │  PARETO OPTIMALITY:                                           │\n",
    "    │    No way to make someone better off without hurting others  │\n",
    "    │    (Cooperate, Cooperate) is Pareto optimal but not Nash!   │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement classic games\n",
    "\n",
    "class MatrixGame:\n",
    "    \"\"\"\n",
    "    Two-player normal-form game.\n",
    "    \n",
    "    Players simultaneously choose actions.\n",
    "    Payoffs determined by the payoff matrices.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, payoff_matrix_1, payoff_matrix_2, action_names=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            payoff_matrix_1: Payoffs for player 1 (rows=P1, cols=P2)\n",
    "            payoff_matrix_2: Payoffs for player 2\n",
    "            action_names: Names for actions (optional)\n",
    "        \"\"\"\n",
    "        self.payoff_1 = np.array(payoff_matrix_1)\n",
    "        self.payoff_2 = np.array(payoff_matrix_2)\n",
    "        self.n_actions_1 = self.payoff_1.shape[0]\n",
    "        self.n_actions_2 = self.payoff_1.shape[1]\n",
    "        self.action_names = action_names or [f'Action {i}' for i in range(self.n_actions_1)]\n",
    "        \n",
    "    def play(self, action_1, action_2):\n",
    "        \"\"\"Execute a round of the game.\"\"\"\n",
    "        return self.payoff_1[action_1, action_2], self.payoff_2[action_1, action_2]\n",
    "    \n",
    "    def find_nash_equilibria(self):\n",
    "        \"\"\"\n",
    "        Find pure strategy Nash equilibria.\n",
    "        \n",
    "        Nash equilibrium: Neither player can improve by deviating.\n",
    "        \"\"\"\n",
    "        nash = []\n",
    "        \n",
    "        for i in range(self.n_actions_1):\n",
    "            for j in range(self.n_actions_2):\n",
    "                # Check if P1 can improve by changing action\n",
    "                p1_can_improve = any(\n",
    "                    self.payoff_1[k, j] > self.payoff_1[i, j] \n",
    "                    for k in range(self.n_actions_1) if k != i\n",
    "                )\n",
    "                \n",
    "                # Check if P2 can improve by changing action\n",
    "                p2_can_improve = any(\n",
    "                    self.payoff_2[i, k] > self.payoff_2[i, j] \n",
    "                    for k in range(self.n_actions_2) if k != j\n",
    "                )\n",
    "                \n",
    "                if not p1_can_improve and not p2_can_improve:\n",
    "                    nash.append((i, j))\n",
    "        \n",
    "        return nash\n",
    "    \n",
    "    def is_pareto_optimal(self, action_1, action_2):\n",
    "        \"\"\"\n",
    "        Check if outcome is Pareto optimal.\n",
    "        \n",
    "        Pareto optimal: Can't make anyone better off without hurting someone.\n",
    "        \"\"\"\n",
    "        p1, p2 = self.play(action_1, action_2)\n",
    "        \n",
    "        for i in range(self.n_actions_1):\n",
    "            for j in range(self.n_actions_2):\n",
    "                other_p1, other_p2 = self.play(i, j)\n",
    "                # Check if (i,j) Pareto dominates (action_1, action_2)\n",
    "                if (other_p1 >= p1 and other_p2 >= p2 and \n",
    "                    (other_p1 > p1 or other_p2 > p2)):\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "\n",
    "# Create classic games\n",
    "print(\"CLASSIC GAMES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prisoner's Dilemma\n",
    "# Actions: 0 = Cooperate, 1 = Defect\n",
    "prisoners_dilemma = MatrixGame(\n",
    "    payoff_matrix_1=[[3, 0], [5, 1]],\n",
    "    payoff_matrix_2=[[3, 5], [0, 1]],\n",
    "    action_names=['Cooperate', 'Defect']\n",
    ")\n",
    "\n",
    "print(\"\\nPRISONER'S DILEMMA:\")\n",
    "print(\"                    Player 2\")\n",
    "print(\"                Cooperate  Defect\")\n",
    "print(\"Player 1  Cooperate  (3,3)    (0,5)\")\n",
    "print(\"          Defect     (5,0)    (1,1)\")\n",
    "\n",
    "nash = prisoners_dilemma.find_nash_equilibria()\n",
    "print(f\"\\nNash Equilibria: {[(prisoners_dilemma.action_names[i], prisoners_dilemma.action_names[j]) for i, j in nash]}\")\n",
    "\n",
    "print(\"\\nPareto Optimal Outcomes:\")\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        if prisoners_dilemma.is_pareto_optimal(i, j):\n",
    "            p1, p2 = prisoners_dilemma.play(i, j)\n",
    "            print(f\"  ({prisoners_dilemma.action_names[i]}, {prisoners_dilemma.action_names[j]}): ({p1}, {p2})\")\n",
    "\n",
    "print(\"\\nTHE DILEMMA:\")\n",
    "print(\"  Nash equilibrium (Defect, Defect) gives (1,1)\")\n",
    "print(\"  But (Cooperate, Cooperate) gives (3,3) - better for both!\")\n",
    "print(\"  Individual rationality leads to collective irrationality!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Prisoner's Dilemma\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Prisoner's Dilemma: Payoff Matrix\", fontsize=16, fontweight='bold')\n",
    "\n",
    "# Draw payoff matrix\n",
    "cell_width = 2.5\n",
    "cell_height = 2\n",
    "start_x = 3\n",
    "start_y = 3\n",
    "\n",
    "outcomes = [\n",
    "    [(3, 3), (0, 5)],\n",
    "    [(5, 0), (1, 1)]\n",
    "]\n",
    "colors = [\n",
    "    ['#c8e6c9', '#ffcdd2'],  # (C,C) green, (C,D) red for P1\n",
    "    ['#ffcdd2', '#fff3e0']   # (D,C) red for P2, (D,D) orange (Nash)\n",
    "]\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        x = start_x + j * cell_width\n",
    "        y = start_y + (1-i) * cell_height\n",
    "        \n",
    "        # Cell\n",
    "        rect = Rectangle((x, y), cell_width, cell_height, \n",
    "                         facecolor=colors[i][j], edgecolor='black', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Payoffs\n",
    "        p1, p2 = outcomes[i][j]\n",
    "        ax.text(x + cell_width/2, y + cell_height/2, f'({p1}, {p2})',\n",
    "               ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Labels\n",
    "ax.text(start_x + cell_width, start_y + 2*cell_height + 0.5, 'Player 2',\n",
    "       ha='center', fontsize=12, fontweight='bold')\n",
    "ax.text(start_x + cell_width/2, start_y + 2*cell_height + 0.2, 'Cooperate',\n",
    "       ha='center', fontsize=10)\n",
    "ax.text(start_x + 1.5*cell_width, start_y + 2*cell_height + 0.2, 'Defect',\n",
    "       ha='center', fontsize=10)\n",
    "\n",
    "ax.text(start_x - 0.8, start_y + cell_height, 'Player 1',\n",
    "       ha='center', va='center', fontsize=12, fontweight='bold', rotation=90)\n",
    "ax.text(start_x - 0.3, start_y + 1.5*cell_height, 'Cooperate',\n",
    "       ha='right', va='center', fontsize=10)\n",
    "ax.text(start_x - 0.3, start_y + 0.5*cell_height, 'Defect',\n",
    "       ha='right', va='center', fontsize=10)\n",
    "\n",
    "# Annotations\n",
    "# Nash equilibrium\n",
    "ax.annotate('Nash\\nEquilibrium', xy=(start_x + 2*cell_width - 0.3, start_y + 0.3),\n",
    "           xytext=(start_x + 2*cell_width + 1.5, start_y - 0.5),\n",
    "           fontsize=10, color='#d32f2f', fontweight='bold',\n",
    "           arrowprops=dict(arrowstyle='->', color='#d32f2f', lw=2))\n",
    "\n",
    "# Pareto optimal\n",
    "ax.annotate('Pareto\\nOptimal!', xy=(start_x + 0.3, start_y + 2*cell_height - 0.3),\n",
    "           xytext=(start_x - 1.5, start_y + 2*cell_height + 1),\n",
    "           fontsize=10, color='#388e3c', fontweight='bold',\n",
    "           arrowprops=dict(arrowstyle='->', color='#388e3c', lw=2))\n",
    "\n",
    "# Legend\n",
    "ax.text(5, 1, 'The Dilemma: Nash ≠ Pareto Optimal', ha='center',\n",
    "       fontsize=12, style='italic', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Non-Stationarity Challenge\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              NON-STATIONARITY                                  │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  THE PROBLEM:                                                 │\n",
    "    │    In single-agent RL, the environment is fixed              │\n",
    "    │    In multi-agent RL, other agents ARE the environment!      │\n",
    "    │    And they're learning too - the world keeps changing!      │\n",
    "    │                                                                │\n",
    "    │  EXAMPLE:                                                     │\n",
    "    │    Agent 1 learns: \"Attack works great!\"                     │\n",
    "    │    Agent 2 learns: \"I should defend more\"                    │\n",
    "    │    Agent 1: \"Attack doesn't work anymore!\"                   │\n",
    "    │    Agent 2: \"They stopped attacking, I can be aggressive!\"   │\n",
    "    │    ... and the cycle continues                               │\n",
    "    │                                                                │\n",
    "    │  CONSEQUENCES:                                                │\n",
    "    │    • Q-values can oscillate forever                          │\n",
    "    │    • Convergence guarantees break down                       │\n",
    "    │    • Training can be unstable                                │\n",
    "    │                                                                │\n",
    "    │  SOLUTIONS:                                                   │\n",
    "    │    • Centralized training (CTDE)                             │\n",
    "    │    • Opponent modeling                                       │\n",
    "    │    • Self-play curriculum                                    │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate non-stationarity in iterated games\n",
    "\n",
    "class IteratedPrisonersDilemma:\n",
    "    \"\"\"\n",
    "    Iterated Prisoner's Dilemma with learning agents.\n",
    "    \n",
    "    Both agents learn simultaneously, creating non-stationarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.payoff = np.array([\n",
    "            [(3, 3), (0, 5)],\n",
    "            [(5, 0), (1, 1)]\n",
    "        ])\n",
    "        \n",
    "    def step(self, action_1, action_2):\n",
    "        \"\"\"Play one round.\"\"\"\n",
    "        return self.payoff[action_1, action_2]\n",
    "\n",
    "\n",
    "class LearningAgent:\n",
    "    \"\"\"\n",
    "    Simple Q-learning agent.\n",
    "    \n",
    "    Learns action values through trial and error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions=2, learning_rate=0.1, epsilon=0.1):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.q_values = np.zeros(n_actions)  # Q(action)\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        return np.argmax(self.q_values)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update Q-value for action.\"\"\"\n",
    "        self.q_values[action] += self.lr * (reward - self.q_values[action])\n",
    "\n",
    "\n",
    "def simulate_iterated_game(n_rounds=1000, track_interval=10):\n",
    "    \"\"\"\n",
    "    Simulate iterated game with two learning agents.\n",
    "    \n",
    "    This demonstrates non-stationarity:\n",
    "    - Both agents learn simultaneously\n",
    "    - Each agent's optimal strategy depends on the other\n",
    "    - The \"best\" action keeps changing!\n",
    "    \"\"\"\n",
    "    game = IteratedPrisonersDilemma()\n",
    "    agent_1 = LearningAgent(learning_rate=0.1, epsilon=0.15)\n",
    "    agent_2 = LearningAgent(learning_rate=0.1, epsilon=0.15)\n",
    "    \n",
    "    history = {\n",
    "        'q_cooperate_1': [],\n",
    "        'q_defect_1': [],\n",
    "        'q_cooperate_2': [],\n",
    "        'q_defect_2': [],\n",
    "        'cooperation_rate': [],\n",
    "    }\n",
    "    \n",
    "    cooperations = 0\n",
    "    \n",
    "    for t in range(n_rounds):\n",
    "        # Both agents choose simultaneously\n",
    "        a1 = agent_1.select_action()\n",
    "        a2 = agent_2.select_action()\n",
    "        \n",
    "        # Get rewards\n",
    "        r1, r2 = game.step(a1, a2)\n",
    "        \n",
    "        # Both agents update\n",
    "        agent_1.update(a1, r1)\n",
    "        agent_2.update(a2, r2)\n",
    "        \n",
    "        # Track cooperation\n",
    "        if a1 == 0 and a2 == 0:\n",
    "            cooperations += 1\n",
    "        \n",
    "        # Record history\n",
    "        if t % track_interval == 0:\n",
    "            history['q_cooperate_1'].append(agent_1.q_values[0])\n",
    "            history['q_defect_1'].append(agent_1.q_values[1])\n",
    "            history['q_cooperate_2'].append(agent_2.q_values[0])\n",
    "            history['q_defect_2'].append(agent_2.q_values[1])\n",
    "            history['cooperation_rate'].append(cooperations / (t + 1))\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Run simulation\n",
    "print(\"NON-STATIONARITY DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = simulate_iterated_game(n_rounds=2000)\n",
    "\n",
    "print(\"\\nSimulating two learning agents in Iterated Prisoner's Dilemma...\")\n",
    "print(\"Watch how Q-values oscillate as agents adapt to each other!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize non-stationarity\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "steps = np.arange(len(history['q_cooperate_1'])) * 10\n",
    "\n",
    "# Top-left: Agent 1 Q-values\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(steps, history['q_cooperate_1'], 'g-', label='Q(Cooperate)', linewidth=2)\n",
    "ax1.plot(steps, history['q_defect_1'], 'r-', label='Q(Defect)', linewidth=2)\n",
    "ax1.set_xlabel('Round', fontsize=11)\n",
    "ax1.set_ylabel('Q-Value', fontsize=11)\n",
    "ax1.set_title('Agent 1: Q-Values Over Time', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Top-right: Agent 2 Q-values\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(steps, history['q_cooperate_2'], 'g-', label='Q(Cooperate)', linewidth=2)\n",
    "ax2.plot(steps, history['q_defect_2'], 'r-', label='Q(Defect)', linewidth=2)\n",
    "ax2.set_xlabel('Round', fontsize=11)\n",
    "ax2.set_ylabel('Q-Value', fontsize=11)\n",
    "ax2.set_title('Agent 2: Q-Values Over Time', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom-left: Cooperation rate\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(steps, history['cooperation_rate'], 'b-', linewidth=2)\n",
    "ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Round', fontsize=11)\n",
    "ax3.set_ylabel('Cooperation Rate', fontsize=11)\n",
    "ax3.set_title('Mutual Cooperation Rate', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom-right: Q-value difference\n",
    "ax4 = axes[1, 1]\n",
    "q_diff_1 = np.array(history['q_defect_1']) - np.array(history['q_cooperate_1'])\n",
    "q_diff_2 = np.array(history['q_defect_2']) - np.array(history['q_cooperate_2'])\n",
    "ax4.plot(steps, q_diff_1, 'r-', label='Agent 1: Q(D)-Q(C)', linewidth=2, alpha=0.7)\n",
    "ax4.plot(steps, q_diff_2, 'b-', label='Agent 2: Q(D)-Q(C)', linewidth=2, alpha=0.7)\n",
    "ax4.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax4.set_xlabel('Round', fontsize=11)\n",
    "ax4.set_ylabel('Q-Value Difference', fontsize=11)\n",
    "ax4.set_title('Defection Preference (Q(Defect) - Q(Cooperate))', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOBSERVATIONS:\")\n",
    "print(\"  • Q-values oscillate - they don't converge to fixed values!\")\n",
    "print(\"  • When one agent cooperates more, the other learns to defect\")\n",
    "print(\"  • This creates cycles of cooperation and defection\")\n",
    "print(\"  • This is NON-STATIONARITY: the optimal action keeps changing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi-Agent Algorithms\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              MULTI-AGENT ALGORITHMS                            │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  1. INDEPENDENT Q-LEARNING (IQL)                              │\n",
    "    │     Each agent learns independently                          │\n",
    "    │     Treats other agents as part of environment               │\n",
    "    │     Simple but suffers from non-stationarity                 │\n",
    "    │                                                                │\n",
    "    │  2. CENTRALIZED TRAINING, DECENTRALIZED EXECUTION (CTDE)     │\n",
    "    │     Training: Agents share information                       │\n",
    "    │     Execution: Each agent acts independently                 │\n",
    "    │     Best of both worlds!                                     │\n",
    "    │                                                                │\n",
    "    │  3. QMIX (for cooperative)                                    │\n",
    "    │     Learns individual Q-values                               │\n",
    "    │     Mixes them into a global Q-value                         │\n",
    "    │     Constraint: Monotonic mixing (argmax preserved)          │\n",
    "    │                                                                │\n",
    "    │  4. MADDPG (Multi-Agent DDPG)                                 │\n",
    "    │     Actor-Critic for continuous control                      │\n",
    "    │     Centralized critic, decentralized actors                 │\n",
    "    │     Works for cooperative, competitive, mixed                │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CTDE\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Training (centralized)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('TRAINING: Centralized', fontsize=14, fontweight='bold', color='#1976d2')\n",
    "\n",
    "# Central coordinator\n",
    "coord = FancyBboxPatch((3.5, 6.5), 3, 2, boxstyle=\"round,pad=0.1\",\n",
    "                        facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=3)\n",
    "ax1.add_patch(coord)\n",
    "ax1.text(5, 7.8, 'Central', ha='center', fontsize=10, fontweight='bold')\n",
    "ax1.text(5, 7.2, 'Coordinator', ha='center', fontsize=10)\n",
    "\n",
    "# Agents\n",
    "for i, (x, y) in enumerate([(2, 3), (5, 3), (8, 3)]):\n",
    "    circle = Circle((x, y), 0.8, facecolor='#4caf50', edgecolor='black', linewidth=2)\n",
    "    ax1.add_patch(circle)\n",
    "    ax1.text(x, y, f'A{i+1}', ha='center', va='center', fontweight='bold', color='white')\n",
    "    # Connection to coordinator\n",
    "    ax1.plot([x, 5], [y+0.8, 6.5], 'b-', linewidth=2, alpha=0.5)\n",
    "\n",
    "# Labels\n",
    "ax1.text(5, 5, 'Share observations,\\nactions, rewards', ha='center', fontsize=9, color='#666')\n",
    "ax1.text(5, 1.5, 'Full information access', ha='center', fontsize=10, style='italic')\n",
    "\n",
    "# Right: Execution (decentralized)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('EXECUTION: Decentralized', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Environment\n",
    "env = FancyBboxPatch((2, 6.5), 6, 2, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax2.add_patch(env)\n",
    "ax2.text(5, 7.5, 'Environment', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Agents (independent)\n",
    "for i, (x, y) in enumerate([(2, 3), (5, 3), (8, 3)]):\n",
    "    circle = Circle((x, y), 0.8, facecolor='#4caf50', edgecolor='black', linewidth=2)\n",
    "    ax2.add_patch(circle)\n",
    "    ax2.text(x, y, f'A{i+1}', ha='center', va='center', fontweight='bold', color='white')\n",
    "    # Individual connection to environment\n",
    "    ax2.annotate('', xy=(x, 6.5), xytext=(x, 3.8),\n",
    "                arrowprops=dict(arrowstyle='<->', lw=1.5, color='#666'))\n",
    "    ax2.text(x, 5, f'obs{i+1}\\n↓\\nact{i+1}', ha='center', fontsize=7, color='#666')\n",
    "\n",
    "ax2.text(5, 1.5, 'Each agent acts on local observation only', ha='center', fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCTDE (Centralized Training, Decentralized Execution):\")\n",
    "print(\"  Training: Share all information (states, actions, rewards)\")\n",
    "print(\"  Execution: Each agent uses only its local observation\")\n",
    "print(\"  Benefit: Stable training + practical deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple cooperative multi-agent environment\n",
    "\n",
    "class CooperativeGridWorld:\n",
    "    \"\"\"\n",
    "    Cooperative multi-agent grid world.\n",
    "    \n",
    "    Two agents must meet at a goal location.\n",
    "    Reward only given when BOTH agents reach the goal.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.goal = (size-1, size-1)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset agents to starting positions.\"\"\"\n",
    "        self.agent_1_pos = (0, 0)\n",
    "        self.agent_2_pos = (self.size-1, 0)\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"Return observations for both agents.\"\"\"\n",
    "        return {\n",
    "            'agent_1': (*self.agent_1_pos, *self.goal),\n",
    "            'agent_2': (*self.agent_2_pos, *self.goal),\n",
    "        }\n",
    "    \n",
    "    def step(self, action_1, action_2):\n",
    "        \"\"\"\n",
    "        Execute actions for both agents.\n",
    "        \n",
    "        Actions: 0=up, 1=right, 2=down, 3=left, 4=stay\n",
    "        \"\"\"\n",
    "        # Move agent 1\n",
    "        self.agent_1_pos = self._move(self.agent_1_pos, action_1)\n",
    "        # Move agent 2\n",
    "        self.agent_2_pos = self._move(self.agent_2_pos, action_2)\n",
    "        \n",
    "        # Check if both at goal\n",
    "        done = (self.agent_1_pos == self.goal and self.agent_2_pos == self.goal)\n",
    "        \n",
    "        # Shared reward\n",
    "        reward = 10.0 if done else -0.1\n",
    "        \n",
    "        return self._get_obs(), reward, done\n",
    "    \n",
    "    def _move(self, pos, action):\n",
    "        \"\"\"Move agent based on action.\"\"\"\n",
    "        x, y = pos\n",
    "        if action == 0:  # up\n",
    "            y = min(y + 1, self.size - 1)\n",
    "        elif action == 1:  # right\n",
    "            x = min(x + 1, self.size - 1)\n",
    "        elif action == 2:  # down\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == 3:  # left\n",
    "            x = max(x - 1, 0)\n",
    "        # action 4 = stay\n",
    "        return (x, y)\n",
    "\n",
    "\n",
    "class CooperativeAgent:\n",
    "    \"\"\"\n",
    "    Q-learning agent for cooperative task.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dims=4, n_actions=5, lr=0.1, gamma=0.99, epsilon=0.2):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        # Simple state discretization\n",
    "        self.q_table = {}\n",
    "        \n",
    "    def get_q(self, state, action):\n",
    "        \"\"\"Get Q-value.\"\"\"\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        q_values = [self.get_q(state, a) for a in range(self.n_actions)]\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Q-learning update.\"\"\"\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            next_q = max(self.get_q(next_state, a) for a in range(self.n_actions))\n",
    "            target = reward + self.gamma * next_q\n",
    "        \n",
    "        old_q = self.get_q(state, action)\n",
    "        self.q_table[(state, action)] = old_q + self.lr * (target - old_q)\n",
    "\n",
    "\n",
    "# Train cooperative agents\n",
    "print(\"COOPERATIVE MULTI-AGENT TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = CooperativeGridWorld(size=5)\n",
    "agent_1 = CooperativeAgent()\n",
    "agent_2 = CooperativeAgent()\n",
    "\n",
    "n_episodes = 1000\n",
    "episode_rewards = []\n",
    "success_count = 0\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(50):  # Max steps per episode\n",
    "        # Get actions\n",
    "        action_1 = agent_1.select_action(obs['agent_1'])\n",
    "        action_2 = agent_2.select_action(obs['agent_2'])\n",
    "        \n",
    "        # Step environment\n",
    "        next_obs, reward, done = env.step(action_1, action_2)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update both agents with shared reward\n",
    "        agent_1.update(obs['agent_1'], action_1, reward, next_obs['agent_1'], done)\n",
    "        agent_2.update(obs['agent_2'], action_2, reward, next_obs['agent_2'], done)\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        if done:\n",
    "            success_count += 1\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Success rate: {success_count/n_episodes:.1%}\")\n",
    "print(f\"Average reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cooperative training\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Learning curve\n",
    "ax1 = axes[0]\n",
    "window = 50\n",
    "smoothed = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(smoothed, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Episode', fontsize=11)\n",
    "ax1.set_ylabel('Total Reward', fontsize=11)\n",
    "ax1.set_title('Cooperative Learning Curve', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Visualize final policy\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(-0.5, env.size - 0.5)\n",
    "ax2.set_ylim(-0.5, env.size - 0.5)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_title('Final Learned Paths', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Draw grid\n",
    "for i in range(env.size + 1):\n",
    "    ax2.axhline(y=i - 0.5, color='gray', linewidth=0.5)\n",
    "    ax2.axvline(x=i - 0.5, color='gray', linewidth=0.5)\n",
    "\n",
    "# Draw goal\n",
    "goal = Rectangle((env.goal[0]-0.4, env.goal[1]-0.4), 0.8, 0.8,\n",
    "                  facecolor='#4caf50', alpha=0.5)\n",
    "ax2.add_patch(goal)\n",
    "ax2.text(env.goal[0], env.goal[1], 'GOAL', ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Simulate one episode with trained agents\n",
    "obs = env.reset()\n",
    "path_1 = [env.agent_1_pos]\n",
    "path_2 = [env.agent_2_pos]\n",
    "\n",
    "agent_1.epsilon = 0  # Greedy\n",
    "agent_2.epsilon = 0\n",
    "\n",
    "for _ in range(20):\n",
    "    action_1 = agent_1.select_action(obs['agent_1'])\n",
    "    action_2 = agent_2.select_action(obs['agent_2'])\n",
    "    obs, _, done = env.step(action_1, action_2)\n",
    "    path_1.append(env.agent_1_pos)\n",
    "    path_2.append(env.agent_2_pos)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Plot paths\n",
    "path_1 = np.array(path_1)\n",
    "path_2 = np.array(path_2)\n",
    "ax2.plot(path_1[:, 0], path_1[:, 1], 'r-o', linewidth=2, markersize=8, label='Agent 1')\n",
    "ax2.plot(path_2[:, 0], path_2[:, 1], 'b-s', linewidth=2, markersize=8, label='Agent 2')\n",
    "\n",
    "# Mark start positions\n",
    "ax2.plot(0, 0, 'r*', markersize=15, label='A1 Start')\n",
    "ax2.plot(env.size-1, 0, 'b*', markersize=15, label='A2 Start')\n",
    "\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.set_xlabel('X', fontsize=11)\n",
    "ax2.set_ylabel('Y', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCOOPERATIVE LEARNING:\")\n",
    "print(\"  Both agents learned to navigate to the shared goal!\")\n",
    "print(\"  Success requires COORDINATION - not just individual skill.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Emergent Behaviors\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              EMERGENT BEHAVIORS                                │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  WHAT IS EMERGENCE?                                           │\n",
    "    │    Complex behaviors that arise from simple rules             │\n",
    "    │    Not explicitly programmed - learned through interaction    │\n",
    "    │                                                                │\n",
    "    │  EXAMPLES IN MULTI-AGENT RL:                                  │\n",
    "    │                                                                │\n",
    "    │  • COOPERATION                                                │\n",
    "    │    Agents learn to help each other                           │\n",
    "    │    Division of labor emerges                                 │\n",
    "    │                                                                │\n",
    "    │  • COMMUNICATION                                              │\n",
    "    │    Agents develop shared signals                             │\n",
    "    │    \"Language\" emerges from scratch                           │\n",
    "    │                                                                │\n",
    "    │  • DECEPTION                                                  │\n",
    "    │    In competitive settings, agents learn to mislead          │\n",
    "    │    Feints and bluffs emerge naturally                        │\n",
    "    │                                                                │\n",
    "    │  • SPECIALIZATION                                             │\n",
    "    │    Different agents develop different roles                  │\n",
    "    │    \"Scout\", \"Fighter\", \"Gatherer\"                            │\n",
    "    │                                                                │\n",
    "    │  FAMOUS EXAMPLES:                                             │\n",
    "    │    • OpenAI Hide and Seek: Tool use emerged                  │\n",
    "    │    • AlphaStar: Novel Starcraft strategies                   │\n",
    "    │    • Emergent communication in cooperative games             │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple emergence demonstration: Predator-Prey\n",
    "\n",
    "class PredatorPreyWorld:\n",
    "    \"\"\"\n",
    "    Simple predator-prey environment.\n",
    "    \n",
    "    Demonstrates emergent chase/flee behaviors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=10):\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to random positions.\"\"\"\n",
    "        self.predator = np.array([np.random.randint(self.size), np.random.randint(self.size)], dtype=float)\n",
    "        self.prey = np.array([np.random.randint(self.size), np.random.randint(self.size)], dtype=float)\n",
    "        # Ensure they don't start at same position\n",
    "        while np.array_equal(self.predator.astype(int), self.prey.astype(int)):\n",
    "            self.prey = np.array([np.random.randint(self.size), np.random.randint(self.size)], dtype=float)\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"Return observations.\"\"\"\n",
    "        return {\n",
    "            'predator': np.concatenate([self.predator, self.prey - self.predator]),\n",
    "            'prey': np.concatenate([self.prey, self.predator - self.prey]),\n",
    "        }\n",
    "    \n",
    "    def step(self, predator_action, prey_action):\n",
    "        \"\"\"\n",
    "        Execute actions.\n",
    "        \n",
    "        Actions are direction vectors (dx, dy) normalized to speed.\n",
    "        \"\"\"\n",
    "        predator_speed = 0.8\n",
    "        prey_speed = 0.6  # Prey is slower\n",
    "        \n",
    "        # Move predator\n",
    "        self.predator += predator_speed * np.clip(predator_action, -1, 1)\n",
    "        self.predator = np.clip(self.predator, 0, self.size - 1)\n",
    "        \n",
    "        # Move prey\n",
    "        self.prey += prey_speed * np.clip(prey_action, -1, 1)\n",
    "        self.prey = np.clip(self.prey, 0, self.size - 1)\n",
    "        \n",
    "        # Check catch\n",
    "        distance = np.linalg.norm(self.predator - self.prey)\n",
    "        caught = distance < 0.5\n",
    "        \n",
    "        # Rewards\n",
    "        predator_reward = 10.0 if caught else -distance / self.size\n",
    "        prey_reward = -10.0 if caught else distance / self.size\n",
    "        \n",
    "        return self._get_obs(), predator_reward, prey_reward, caught\n",
    "\n",
    "\n",
    "# Simulate with simple policies\n",
    "print(\"EMERGENT BEHAVIOR: PREDATOR-PREY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = PredatorPreyWorld(size=10)\n",
    "\n",
    "# Record one episode\n",
    "obs = env.reset()\n",
    "predator_path = [env.predator.copy()]\n",
    "prey_path = [env.prey.copy()]\n",
    "\n",
    "for step in range(100):\n",
    "    # Simple heuristic policies (chase/flee)\n",
    "    # Predator: Move toward prey\n",
    "    direction_to_prey = obs['predator'][2:4]  # (prey - predator)\n",
    "    if np.linalg.norm(direction_to_prey) > 0:\n",
    "        predator_action = direction_to_prey / np.linalg.norm(direction_to_prey)\n",
    "    else:\n",
    "        predator_action = np.array([0, 0])\n",
    "    \n",
    "    # Prey: Move away from predator + some randomness\n",
    "    direction_from_predator = obs['prey'][2:4]  # (predator - prey)\n",
    "    if np.linalg.norm(direction_from_predator) > 0:\n",
    "        prey_action = -direction_from_predator / np.linalg.norm(direction_from_predator)\n",
    "    else:\n",
    "        prey_action = np.random.randn(2)\n",
    "    prey_action += 0.2 * np.random.randn(2)  # Some randomness\n",
    "    \n",
    "    obs, _, _, caught = env.step(predator_action, prey_action)\n",
    "    predator_path.append(env.predator.copy())\n",
    "    prey_path.append(env.prey.copy())\n",
    "    \n",
    "    if caught:\n",
    "        print(f\"Prey caught at step {step}!\")\n",
    "        break\n",
    "\n",
    "predator_path = np.array(predator_path)\n",
    "prey_path = np.array(prey_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predator-prey dynamics\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Trajectories\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(-0.5, env.size - 0.5)\n",
    "ax1.set_ylim(-0.5, env.size - 0.5)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.set_title('Predator-Prey Chase', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot trajectories\n",
    "ax1.plot(predator_path[:, 0], predator_path[:, 1], 'r-', linewidth=2, alpha=0.7, label='Predator')\n",
    "ax1.plot(prey_path[:, 0], prey_path[:, 1], 'b-', linewidth=2, alpha=0.7, label='Prey')\n",
    "\n",
    "# Start and end points\n",
    "ax1.plot(predator_path[0, 0], predator_path[0, 1], 'rs', markersize=15, label='Predator Start')\n",
    "ax1.plot(prey_path[0, 0], prey_path[0, 1], 'bs', markersize=15, label='Prey Start')\n",
    "ax1.plot(predator_path[-1, 0], predator_path[-1, 1], 'r*', markersize=20)\n",
    "ax1.plot(prey_path[-1, 0], prey_path[-1, 1], 'b*', markersize=20)\n",
    "\n",
    "ax1.set_xlabel('X', fontsize=11)\n",
    "ax1.set_ylabel('Y', fontsize=11)\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Distance over time\n",
    "ax2 = axes[1]\n",
    "distances = np.linalg.norm(predator_path - prey_path, axis=1)\n",
    "ax2.plot(distances, 'purple', linewidth=2)\n",
    "ax2.axhline(y=0.5, color='red', linestyle='--', label='Catch threshold')\n",
    "ax2.set_xlabel('Step', fontsize=11)\n",
    "ax2.set_ylabel('Distance', fontsize=11)\n",
    "ax2.set_title('Distance Between Agents', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEMERGENT BEHAVIORS OBSERVED:\")\n",
    "print(\"  • Predator: Pursuit strategy - direct chase toward prey\")\n",
    "print(\"  • Prey: Evasion strategy - flee away from predator\")\n",
    "print(\"  • Neither behavior was programmed explicitly!\")\n",
    "print(\"  • With RL, even more complex strategies emerge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Real-World Applications\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              MULTI-AGENT RL APPLICATIONS                       │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  GAMES:                                                       │\n",
    "    │    • AlphaGo/AlphaZero: Self-play for board games            │\n",
    "    │    • AlphaStar: Grandmaster StarCraft II                     │\n",
    "    │    • OpenAI Five: Dota 2                                     │\n",
    "    │    • Pluribus: Poker (6-player)                              │\n",
    "    │                                                                │\n",
    "    │  ROBOTICS:                                                    │\n",
    "    │    • Multi-robot coordination                                │\n",
    "    │    • Swarm robotics                                          │\n",
    "    │    • Warehouse automation                                    │\n",
    "    │                                                                │\n",
    "    │  AUTONOMOUS VEHICLES:                                         │\n",
    "    │    • Traffic coordination                                    │\n",
    "    │    • Intersection management                                 │\n",
    "    │    • Fleet management                                        │\n",
    "    │                                                                │\n",
    "    │  ECONOMICS:                                                   │\n",
    "    │    • Market simulation                                       │\n",
    "    │    • Auction design                                          │\n",
    "    │    • Resource allocation                                     │\n",
    "    │                                                                │\n",
    "    │  LLM AGENTS:                                                  │\n",
    "    │    • Multi-agent debate for reasoning                        │\n",
    "    │    • Collaborative problem-solving                           │\n",
    "    │    • Agent-to-agent negotiation                              │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize applications\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('Multi-Agent RL: Real-World Applications', fontsize=16, fontweight='bold')\n",
    "\n",
    "applications = [\n",
    "    (2, 9, 'Games', 'AlphaStar, OpenAI Five\\nDota 2, StarCraft', '#e91e63'),\n",
    "    (7, 9, 'Robotics', 'Swarm coordination\\nWarehouse automation', '#4caf50'),\n",
    "    (12, 9, 'Autonomous\\nVehicles', 'Traffic coordination\\nFleet management', '#2196f3'),\n",
    "    (4.5, 5, 'Economics', 'Market simulation\\nAuction design', '#ff9800'),\n",
    "    (9.5, 5, 'LLM Agents', 'Multi-agent debate\\nCollaborative AI', '#9c27b0'),\n",
    "]\n",
    "\n",
    "for x, y, title, desc, color in applications:\n",
    "    box = FancyBboxPatch((x-1.5, y-1.5), 3, 3, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor='black', linewidth=2, alpha=0.8)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y+0.8, title, ha='center', fontsize=11, fontweight='bold', color='white')\n",
    "    ax.text(x, y-0.4, desc, ha='center', fontsize=8, color='white')\n",
    "\n",
    "# Key challenges\n",
    "ax.text(7, 2, 'Key Challenges: Non-stationarity | Credit Assignment | Scalability | Communication',\n",
    "       ha='center', fontsize=11, style='italic', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Multi-Agent RL\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Cooperative** | Agents share rewards, work toward common goal |\n",
    "| **Competitive** | Zero-sum: one agent's gain is another's loss |\n",
    "| **Mixed** | Both cooperation and competition (teams) |\n",
    "| **Non-stationarity** | Environment changes as agents learn |\n",
    "| **CTDE** | Train together, execute independently |\n",
    "\n",
    "### Game Theory Concepts\n",
    "\n",
    "| Concept | Definition |\n",
    "|---------|------------|\n",
    "| **Nash Equilibrium** | No agent can improve by changing alone |\n",
    "| **Pareto Optimal** | Can't improve one without hurting another |\n",
    "| **Best Response** | Optimal action given others' strategies |\n",
    "\n",
    "### Key Algorithms\n",
    "\n",
    "| Algorithm | Type | Key Idea |\n",
    "|-----------|------|----------|\n",
    "| IQL | Independent | Each agent learns separately |\n",
    "| QMIX | Cooperative | Mix individual Q-values |\n",
    "| MADDPG | General | Centralized critic, decentralized actors |\n",
    "| COMA | Cooperative | Counterfactual baselines |\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# Multi-agent training loop\n",
    "for episode in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    for step in range(max_steps):\n",
    "        # Each agent selects action based on its observation\n",
    "        actions = [agent.select_action(obs[i]) for i, agent in enumerate(agents)]\n",
    "        \n",
    "        # Environment steps with joint action\n",
    "        next_obs, rewards, done = env.step(actions)\n",
    "        \n",
    "        # Each agent updates\n",
    "        for i, agent in enumerate(agents):\n",
    "            agent.update(obs[i], actions[i], rewards[i], next_obs[i])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What is non-stationarity and why is it a problem?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Non-stationarity means the environment changes over time. In multi-agent RL, other agents ARE part of the environment, and they're learning too. This means:\n",
    "- The optimal action keeps changing\n",
    "- Q-values may never converge\n",
    "- Standard RL convergence guarantees don't apply\n",
    "\n",
    "Solution: CTDE, opponent modeling, or self-play.\n",
    "</details>\n",
    "\n",
    "**2. What's the difference between Nash equilibrium and Pareto optimality?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Nash Equilibrium: No player can improve by changing their strategy alone (stability concept).\n",
    "\n",
    "Pareto Optimality: No way to make someone better off without making someone else worse off (efficiency concept).\n",
    "\n",
    "Key insight: Nash equilibria are NOT always Pareto optimal! (Prisoner's Dilemma: Defect-Defect is Nash but Cooperate-Cooperate is Pareto better)\n",
    "</details>\n",
    "\n",
    "**3. What is CTDE and why is it useful?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "CTDE = Centralized Training, Decentralized Execution\n",
    "\n",
    "Training: All agents share information (observations, actions, rewards)\n",
    "Execution: Each agent acts only on its local observation\n",
    "\n",
    "Benefits:\n",
    "- Stable training (addresses non-stationarity)\n",
    "- Practical deployment (no communication needed at runtime)\n",
    "- Best of both worlds!\n",
    "</details>\n",
    "\n",
    "**4. What are emergent behaviors in multi-agent systems?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Emergent behaviors are complex strategies that arise from simple rules without being explicitly programmed. Examples:\n",
    "\n",
    "- Cooperation: Agents learn to help each other\n",
    "- Communication: Agents develop shared signals\n",
    "- Deception: Agents learn to mislead opponents\n",
    "- Specialization: Different roles emerge (scout, fighter, etc.)\n",
    "\n",
    "Famous example: OpenAI Hide and Seek - agents learned to use tools!\n",
    "</details>\n",
    "\n",
    "**5. When would you use QMIX vs MADDPG?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "QMIX:\n",
    "- Cooperative settings only\n",
    "- Discrete actions\n",
    "- Factored Q-function with monotonicity constraint\n",
    "\n",
    "MADDPG:\n",
    "- Any setting (cooperative, competitive, mixed)\n",
    "- Continuous actions\n",
    "- Actor-Critic framework\n",
    "\n",
    "Rule of thumb: QMIX for cooperative discrete, MADDPG for everything else.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the entire **Reinforcement Learning** curriculum!\n",
    "\n",
    "### Your Journey:\n",
    "\n",
    "```\n",
    "Fundamentals → Classic Algorithms → Deep RL → Policy Gradients\n",
    "     ↓                                              ↓\n",
    "  MDPs, Value Functions              REINFORCE, Actor-Critic\n",
    "     ↓                                              ↓\n",
    "     └──────────────────────────────────────────────┘\n",
    "                           ↓\n",
    "                   Advanced Algorithms\n",
    "                   (PPO, SAC, TRPO)\n",
    "                           ↓\n",
    "                         RLHF\n",
    "               (Reward Models, DPO, TRL)\n",
    "                           ↓\n",
    "                     Applications\n",
    "          (Games, Robotics, Recommendations,\n",
    "           LLM Alignment, Multi-Agent)\n",
    "```\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "- ✅ **Fundamentals**: MDPs, policies, value functions, Bellman equations\n",
    "- ✅ **Classic Algorithms**: Q-learning, SARSA, Monte Carlo, TD learning\n",
    "- ✅ **Deep RL**: DQN, experience replay, target networks\n",
    "- ✅ **Policy Gradients**: REINFORCE, Actor-Critic, A2C/A3C\n",
    "- ✅ **Advanced**: PPO, SAC, TRPO\n",
    "- ✅ **RLHF**: Reward modeling, PPO for LLMs, DPO\n",
    "- ✅ **Applications**: Games, robotics, recommendations, alignment, multi-agent\n",
    "\n",
    "### Where to Go Next:\n",
    "\n",
    "1. **Practice**: Implement algorithms from scratch\n",
    "2. **Explore**: Try PettingZoo, RLlib for multi-agent\n",
    "3. **Apply**: Use RLHF to fine-tune your own models\n",
    "4. **Research**: Read papers on emergent communication, social dilemmas\n",
    "\n",
    "---\n",
    "\n",
    "*\"The future of AI is not just individual intelligence, but the collective intelligence of many agents working together!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
