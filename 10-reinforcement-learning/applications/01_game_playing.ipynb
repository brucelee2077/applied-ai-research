{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Playing with RL: Teaching AI to Master Games\n",
    "\n",
    "From classic Atari to board games, RL has revolutionized game AI!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The arcade cabinet analogy: how RL learns to play games\n",
    "- Classic control environments (CartPole, LunarLander, etc.)\n",
    "- Training agents with Stable-Baselines3\n",
    "- Evaluating and visualizing game performance\n",
    "- Hyperparameter tuning for different games\n",
    "- The journey from simple games to superhuman AI\n",
    "\n",
    "**Prerequisites:** Policy gradient and PPO notebooks\n",
    "\n",
    "**Time:** ~35 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Arcade Cabinet Analogy\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          THE ARCADE CABINET ANALOGY                            â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Imagine teaching someone to play arcade games...             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE SCREEN (State):                                          â”‚\n",
    "    â”‚    What the player sees                                       â”‚\n",
    "    â”‚    Position of characters, obstacles, enemies                 â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE JOYSTICK (Actions):                                      â”‚\n",
    "    â”‚    Discrete: Up, Down, Left, Right, Fire                     â”‚\n",
    "    â”‚    Or: Jump, Duck, Attack                                    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE SCORE (Reward):                                          â”‚\n",
    "    â”‚    Points for collecting items                               â”‚\n",
    "    â”‚    Penalties for losing lives                                â”‚\n",
    "    â”‚    Bonus for completing levels                               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE PLAYER (RL Agent):                                       â”‚\n",
    "    â”‚    Learns by trial and error                                 â”‚\n",
    "    â”‚    Gets better with practice                                 â”‚\n",
    "    â”‚    Develops strategies and reflexes                          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  LEARNING PROCESS:                                            â”‚\n",
    "    â”‚    Play game â†’ See score â†’ Adjust strategy â†’ Repeat!        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, Rectangle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check gymnasium availability\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    GYM_AVAILABLE = True\n",
    "    print(\"âœ“ Gymnasium is installed!\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        import gym\n",
    "        GYM_AVAILABLE = True\n",
    "        print(\"âœ“ OpenAI Gym is installed (legacy version)\")\n",
    "    except ImportError:\n",
    "        GYM_AVAILABLE = False\n",
    "        print(\"âœ— Gymnasium not installed.\")\n",
    "        print(\"  Install with: pip install gymnasium\")\n",
    "\n",
    "# Check stable-baselines3\n",
    "try:\n",
    "    from stable_baselines3 import PPO, DQN, A2C\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    SB3_AVAILABLE = True\n",
    "    print(\"âœ“ Stable-Baselines3 is installed!\")\n",
    "except ImportError:\n",
    "    SB3_AVAILABLE = False\n",
    "    print(\"âœ— Stable-Baselines3 not installed.\")\n",
    "    print(\"  Install with: pip install stable-baselines3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the RL game playing concept\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('RL for Game Playing: The Learning Loop', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Game screen (environment)\n",
    "screen_box = FancyBboxPatch((0.5, 6), 5, 5, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(screen_box)\n",
    "ax.text(3, 10.3, 'GAME ENVIRONMENT', ha='center', fontsize=11, fontweight='bold', color='#1976d2')\n",
    "ax.text(3, 9.5, '(CartPole, LunarLander, etc.)', ha='center', fontsize=9)\n",
    "\n",
    "# Draw a simple game scene\n",
    "# Ground\n",
    "ax.plot([1, 5], [6.8, 6.8], 'brown', linewidth=2)\n",
    "# Cart\n",
    "cart = Rectangle((2.5, 6.9), 1, 0.3, facecolor='#4caf50', edgecolor='black')\n",
    "ax.add_patch(cart)\n",
    "# Pole\n",
    "ax.plot([3, 2.5], [7.2, 8.5], 'brown', linewidth=4)\n",
    "# Wheels\n",
    "wheel1 = Circle((2.7, 6.85), 0.15, facecolor='black')\n",
    "wheel2 = Circle((3.3, 6.85), 0.15, facecolor='black')\n",
    "ax.add_patch(wheel1)\n",
    "ax.add_patch(wheel2)\n",
    "\n",
    "# Agent (neural network)\n",
    "agent_box = FancyBboxPatch((8, 6.5), 5, 4, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(agent_box)\n",
    "ax.text(10.5, 9.8, 'RL AGENT', ha='center', fontsize=11, fontweight='bold', color='#388e3c')\n",
    "ax.text(10.5, 9.1, '(Neural Network Policy)', ha='center', fontsize=9)\n",
    "\n",
    "# Draw simple neural net\n",
    "for i in range(3):\n",
    "    c = Circle((9, 7.5 + i*0.5), 0.15, facecolor='#388e3c', edgecolor='black')\n",
    "    ax.add_patch(c)\n",
    "for i in range(4):\n",
    "    c = Circle((10.5, 7.25 + i*0.5), 0.15, facecolor='#388e3c', edgecolor='black')\n",
    "    ax.add_patch(c)\n",
    "for i in range(2):\n",
    "    c = Circle((12, 7.75 + i*0.5), 0.15, facecolor='#388e3c', edgecolor='black')\n",
    "    ax.add_patch(c)\n",
    "\n",
    "# State arrow\n",
    "ax.annotate('', xy=(7.9, 9), xytext=(5.6, 9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#1976d2'))\n",
    "ax.text(6.75, 9.5, 'State\\n(observation)', ha='center', fontsize=9, color='#1976d2')\n",
    "\n",
    "# Action arrow\n",
    "ax.annotate('', xy=(5.6, 7), xytext=(7.9, 7),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#388e3c'))\n",
    "ax.text(6.75, 7.5, 'Action\\n(left/right)', ha='center', fontsize=9, color='#388e3c')\n",
    "\n",
    "# Reward box\n",
    "reward_box = FancyBboxPatch((5.5, 3), 3, 2, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#fff3e0', edgecolor='#f57c00', linewidth=3)\n",
    "ax.add_patch(reward_box)\n",
    "ax.text(7, 4.3, 'REWARD', ha='center', fontsize=11, fontweight='bold', color='#f57c00')\n",
    "ax.text(7, 3.5, '+1 for balance\\n-10 for falling', ha='center', fontsize=9)\n",
    "\n",
    "# Reward arrows\n",
    "ax.annotate('', xy=(5.5, 4), xytext=(3, 5.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#f57c00'))\n",
    "ax.annotate('', xy=(10.5, 6.4), xytext=(8.5, 5.1),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#f57c00'))\n",
    "\n",
    "# Learning update\n",
    "ax.text(7, 1.5, 'Agent learns: \"Actions that maximize reward are best!\"',\n",
    "        ha='center', fontsize=11, style='italic', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRL GAME PLAYING LOOP:\")\n",
    "print(\"  1. Environment sends state (what agent sees)\")\n",
    "print(\"  2. Agent chooses action based on policy\")\n",
    "print(\"  3. Environment returns reward and next state\")\n",
    "print(\"  4. Agent updates policy to get more reward\")\n",
    "print(\"  5. Repeat until agent masters the game!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Classic Control Environments\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              CLASSIC CONTROL ENVIRONMENTS                      â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  CARTPOLE (Difficulty: â­)                                     â”‚\n",
    "    â”‚    Task: Balance a pole on a moving cart                     â”‚\n",
    "    â”‚    State: Cart position, velocity, pole angle, angular vel   â”‚\n",
    "    â”‚    Actions: Push left (0) or right (1)                       â”‚\n",
    "    â”‚    Reward: +1 for each timestep pole stays balanced          â”‚\n",
    "    â”‚    Success: 500 timesteps (solved in ~100 episodes)          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  LUNARLANDER (Difficulty: â­â­)                                â”‚\n",
    "    â”‚    Task: Land a spacecraft safely between flags              â”‚\n",
    "    â”‚    State: Position, velocity, angle, leg contact             â”‚\n",
    "    â”‚    Actions: Nothing, fire left, fire main, fire right        â”‚\n",
    "    â”‚    Reward: +100 to +140 for landing, -100 for crash         â”‚\n",
    "    â”‚    Success: Average reward > 200 (solved in ~500 episodes)   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  MOUNTAINCAR (Difficulty: â­â­â­)                              â”‚\n",
    "    â”‚    Task: Drive car up a steep hill                           â”‚\n",
    "    â”‚    State: Position, velocity                                 â”‚\n",
    "    â”‚    Actions: Push left, nothing, push right                   â”‚\n",
    "    â”‚    Reward: -1 each step (sparse reward - hard!)              â”‚\n",
    "    â”‚    Challenge: Car can't climb directly - must gain momentum  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  ACROBOT (Difficulty: â­â­â­)                                  â”‚\n",
    "    â”‚    Task: Swing double pendulum to reach target               â”‚\n",
    "    â”‚    State: Joint angles and angular velocities                â”‚\n",
    "    â”‚    Actions: Torque on middle joint                           â”‚\n",
    "    â”‚    Reward: -1 until target reached (sparse reward)           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore available environments\n",
    "\n",
    "print(\"CLASSIC CONTROL ENVIRONMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if GYM_AVAILABLE:\n",
    "    environments = {\n",
    "        'CartPole-v1': {\n",
    "            'description': 'Balance a pole on a cart',\n",
    "            'difficulty': 'â­',\n",
    "            'timesteps_to_solve': '~50K',\n",
    "            'best_algorithm': 'DQN, PPO, A2C',\n",
    "        },\n",
    "        'LunarLander-v2': {\n",
    "            'description': 'Land a spacecraft safely',\n",
    "            'difficulty': 'â­â­',\n",
    "            'timesteps_to_solve': '~500K',\n",
    "            'best_algorithm': 'PPO, DQN',\n",
    "        },\n",
    "        'MountainCar-v0': {\n",
    "            'description': 'Drive car up a hill',\n",
    "            'difficulty': 'â­â­â­',\n",
    "            'timesteps_to_solve': '~200K',\n",
    "            'best_algorithm': 'DQN with exploration bonus',\n",
    "        },\n",
    "        'Acrobot-v1': {\n",
    "            'description': 'Swing double pendulum',\n",
    "            'difficulty': 'â­â­â­',\n",
    "            'timesteps_to_solve': '~100K',\n",
    "            'best_algorithm': 'A2C, PPO',\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    for env_name, info in environments.items():\n",
    "        print(f\"\\n{env_name}\")\n",
    "        print(f\"  Description: {info['description']}\")\n",
    "        print(f\"  Difficulty: {info['difficulty']}\")\n",
    "        print(f\"  Timesteps to solve: {info['timesteps_to_solve']}\")\n",
    "        print(f\"  Best algorithms: {info['best_algorithm']}\")\n",
    "        \n",
    "        try:\n",
    "            env = gym.make(env_name)\n",
    "            print(f\"  Observation space: {env.observation_space}\")\n",
    "            print(f\"  Action space: {env.action_space}\")\n",
    "            env.close()\n",
    "        except Exception as e:\n",
    "            print(f\"  (Environment not available: {e})\")\n",
    "else:\n",
    "    print(\"Install gymnasium to explore environments!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize environment comparison\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Environment data\n",
    "env_data = [\n",
    "    ('CartPole', 4, 2, 'Easy', '#4caf50'),\n",
    "    ('LunarLander', 8, 4, 'Medium', '#ff9800'),\n",
    "    ('MountainCar', 2, 3, 'Hard (sparse)', '#f44336'),\n",
    "    ('Acrobot', 6, 3, 'Hard (sparse)', '#f44336'),\n",
    "]\n",
    "\n",
    "# Top left: State space comparison\n",
    "ax1 = axes[0, 0]\n",
    "names = [e[0] for e in env_data]\n",
    "state_dims = [e[1] for e in env_data]\n",
    "colors = [e[4] for e in env_data]\n",
    "bars = ax1.bar(names, state_dims, color=colors, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('State Dimensions', fontsize=11)\n",
    "ax1.set_title('State Space Complexity', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "for bar, dim in zip(bars, state_dims):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2, \n",
    "             str(dim), ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Top right: Action space comparison\n",
    "ax2 = axes[0, 1]\n",
    "action_dims = [e[2] for e in env_data]\n",
    "bars = ax2.bar(names, action_dims, color=colors, edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Number of Actions', fontsize=11)\n",
    "ax2.set_title('Action Space Size', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "for bar, dim in zip(bars, action_dims):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "             str(dim), ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Bottom left: Training time comparison\n",
    "ax3 = axes[1, 0]\n",
    "timesteps = [50, 500, 200, 100]  # in thousands\n",
    "bars = ax3.bar(names, timesteps, color=colors, edgecolor='black', linewidth=2)\n",
    "ax3.set_ylabel('Timesteps (thousands)', fontsize=11)\n",
    "ax3.set_title('Typical Training Time to Solve', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "for bar, ts in zip(bars, timesteps):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 15,\n",
    "             f'{ts}K', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Bottom right: Difficulty factors\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "ax4.set_title('Difficulty Breakdown', fontsize=12, fontweight='bold')\n",
    "\n",
    "difficulty_text = \"\"\"\n",
    "WHAT MAKES GAMES HARD FOR RL:\n",
    "\n",
    "â€¢ Sparse Rewards (MountainCar, Acrobot)\n",
    "  - Only get reward at the end\n",
    "  - Agent must explore without guidance\n",
    "  - Solution: Curiosity, reward shaping\n",
    "\n",
    "â€¢ Large State Space (LunarLander)\n",
    "  - More dimensions = harder to learn\n",
    "  - Need more samples to cover space\n",
    "  - Solution: Function approximation\n",
    "\n",
    "â€¢ Credit Assignment\n",
    "  - Which action caused the reward?\n",
    "  - Delayed consequences\n",
    "  - Solution: Value functions, GAE\n",
    "\"\"\"\n",
    "ax4.text(0.1, 0.9, difficulty_text, transform=ax4.transAxes, fontsize=10,\n",
    "         verticalalignment='top', family='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Playing Games with Random Policy\n",
    "\n",
    "Let's first see how a random agent performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_random_episodes(env_name, n_episodes=10):\n",
    "    \"\"\"\n",
    "    Play episodes with random actions.\n",
    "    \n",
    "    This establishes a baseline - how well do we do\n",
    "    with no learning at all?\n",
    "    \"\"\"\n",
    "    if not GYM_AVAILABLE:\n",
    "        print(\"Gymnasium not available\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        env = gym.make(env_name)\n",
    "    except:\n",
    "        print(f\"Environment {env_name} not available\")\n",
    "        return None\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Random action!\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'lengths': episode_lengths,\n",
    "        'mean_reward': np.mean(episode_rewards),\n",
    "        'std_reward': np.std(episode_rewards),\n",
    "        'mean_length': np.mean(episode_lengths),\n",
    "    }\n",
    "\n",
    "\n",
    "# Test random policy on different environments\n",
    "print(\"RANDOM POLICY BASELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if GYM_AVAILABLE:\n",
    "    test_envs = ['CartPole-v1', 'MountainCar-v0', 'Acrobot-v1']\n",
    "    baselines = {}\n",
    "    \n",
    "    for env_name in test_envs:\n",
    "        result = play_random_episodes(env_name, n_episodes=20)\n",
    "        if result:\n",
    "            baselines[env_name] = result\n",
    "            print(f\"\\n{env_name}:\")\n",
    "            print(f\"  Mean reward: {result['mean_reward']:.2f} Â± {result['std_reward']:.2f}\")\n",
    "            print(f\"  Mean episode length: {result['mean_length']:.1f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\\nRANDOM POLICY IS BAD! Let's train RL agents...\")\n",
    "else:\n",
    "    print(\"Install gymnasium to run experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training with Stable-Baselines3\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              STABLE-BASELINES3 WORKFLOW                        â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  STEP 1: Create Environment                                   â”‚\n",
    "    â”‚    env = gym.make('CartPole-v1')                             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  STEP 2: Choose Algorithm                                     â”‚\n",
    "    â”‚    â€¢ DQN - Good for discrete actions, sample efficient       â”‚\n",
    "    â”‚    â€¢ PPO - Stable, works for most problems                   â”‚\n",
    "    â”‚    â€¢ A2C - Fast training, good for simple environments       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  STEP 3: Create Model                                         â”‚\n",
    "    â”‚    model = PPO('MlpPolicy', env, verbose=1)                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  STEP 4: Train!                                               â”‚\n",
    "    â”‚    model.learn(total_timesteps=100_000)                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  STEP 5: Evaluate                                             â”‚\n",
    "    â”‚    mean_reward, std = evaluate_policy(model, env)            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete training example\n",
    "\n",
    "print(\"TRAINING RL AGENT ON CARTPOLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if SB3_AVAILABLE and GYM_AVAILABLE:\n",
    "    # Create environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    print(\"\\n1. Environment created\")\n",
    "    print(f\"   Observation space: {env.observation_space}\")\n",
    "    print(f\"   Action space: {env.action_space}\")\n",
    "    \n",
    "    # Create PPO model\n",
    "    model = PPO(\n",
    "        'MlpPolicy',           # Neural network policy\n",
    "        env,                   # Environment\n",
    "        learning_rate=3e-4,    # Learning rate\n",
    "        n_steps=2048,          # Steps per update\n",
    "        batch_size=64,         # Minibatch size\n",
    "        n_epochs=10,           # Epochs per update\n",
    "        gamma=0.99,            # Discount factor\n",
    "        verbose=0,             # Quiet mode\n",
    "    )\n",
    "    \n",
    "    print(\"\\n2. PPO model created\")\n",
    "    print(f\"   Policy: {model.policy}\")\n",
    "    \n",
    "    # Evaluate before training\n",
    "    print(\"\\n3. Evaluating BEFORE training...\")\n",
    "    mean_reward_before, std_before = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    print(f\"   Mean reward: {mean_reward_before:.2f} Â± {std_before:.2f}\")\n",
    "    \n",
    "    # Train!\n",
    "    print(\"\\n4. Training for 50,000 timesteps...\")\n",
    "    model.learn(total_timesteps=50_000, progress_bar=True)\n",
    "    \n",
    "    # Evaluate after training\n",
    "    print(\"\\n5. Evaluating AFTER training...\")\n",
    "    mean_reward_after, std_after = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    print(f\"   Mean reward: {mean_reward_after:.2f} Â± {std_after:.2f}\")\n",
    "    \n",
    "    # Improvement\n",
    "    improvement = mean_reward_after - mean_reward_before\n",
    "    print(f\"\\n6. Improvement: +{improvement:.2f} reward!\")\n",
    "    \n",
    "    if mean_reward_after >= 475:\n",
    "        print(\"   ðŸŽ‰ Environment SOLVED! (threshold: 475)\")\n",
    "    \n",
    "    env.close()\n",
    "else:\n",
    "    print(\"Install stable-baselines3 and gymnasium to train agents\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different algorithms\n",
    "\n",
    "def train_and_evaluate(algorithm_class, env_name, timesteps=30000, n_eval=10):\n",
    "    \"\"\"Train an algorithm and return results.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = algorithm_class('MlpPolicy', env, verbose=0)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    \n",
    "    # Evaluate\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=n_eval)\n",
    "    \n",
    "    env.close()\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "\n",
    "print(\"ALGORITHM COMPARISON ON CARTPOLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if SB3_AVAILABLE and GYM_AVAILABLE:\n",
    "    algorithms = [\n",
    "        ('PPO', PPO),\n",
    "        ('A2C', A2C),\n",
    "        ('DQN', DQN),\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    for name, algo_class in algorithms:\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        mean_r, std_r = train_and_evaluate(algo_class, 'CartPole-v1', timesteps=30000)\n",
    "        results[name] = (mean_r, std_r)\n",
    "        print(f\"  Result: {mean_r:.2f} Â± {std_r:.2f}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    names = list(results.keys())\n",
    "    means = [results[n][0] for n in names]\n",
    "    stds = [results[n][1] for n in names]\n",
    "    colors = ['#4caf50', '#2196f3', '#ff9800']\n",
    "    \n",
    "    bars = ax.bar(names, means, yerr=stds, capsize=5, color=colors, \n",
    "                  edgecolor='black', linewidth=2)\n",
    "    \n",
    "    ax.axhline(y=475, color='red', linestyle='--', label='Solved threshold (475)')\n",
    "    ax.axhline(y=results.get('Random', (25, 0))[0] if 'Random' in results else 25, \n",
    "               color='gray', linestyle=':', label='Random baseline (~25)')\n",
    "    \n",
    "    ax.set_ylabel('Mean Reward', fontsize=12)\n",
    "    ax.set_title('Algorithm Comparison on CartPole-v1\\n(30K training timesteps)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(0, 520)\n",
    "    \n",
    "    for bar, mean in zip(bars, means):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20,\n",
    "                f'{mean:.0f}', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Install required libraries to compare algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              HYPERPARAMETER GUIDE                              â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  PPO HYPERPARAMETERS:                                         â”‚\n",
    "    â”‚    learning_rate: Start with 3e-4, decrease if unstable      â”‚\n",
    "    â”‚    n_steps: 2048 (how many steps before update)              â”‚\n",
    "    â”‚    batch_size: 64-256 (minibatch for gradient)               â”‚\n",
    "    â”‚    n_epochs: 10 (passes through collected data)              â”‚\n",
    "    â”‚    gamma: 0.99 (discount factor - higher for long tasks)     â”‚\n",
    "    â”‚    clip_range: 0.2 (PPO clipping - usually don't change)     â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  DQN HYPERPARAMETERS:                                         â”‚\n",
    "    â”‚    learning_rate: 1e-4 to 1e-3                                â”‚\n",
    "    â”‚    buffer_size: 100000+ (replay buffer)                      â”‚\n",
    "    â”‚    batch_size: 32-128                                        â”‚\n",
    "    â”‚    exploration_fraction: 0.1-0.2                             â”‚\n",
    "    â”‚    target_update_interval: 1000-10000                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  GENERAL TIPS:                                                â”‚\n",
    "    â”‚    â€¢ Start with defaults, then tune                          â”‚\n",
    "    â”‚    â€¢ Learning rate is most important                         â”‚\n",
    "    â”‚    â€¢ More timesteps usually helps                            â”‚\n",
    "    â”‚    â€¢ Use multiple seeds for reliable results                 â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning example\n",
    "\n",
    "def tune_learning_rate(lr_values, env_name='CartPole-v1', timesteps=20000):\n",
    "    \"\"\"Compare different learning rates.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for lr in lr_values:\n",
    "        env = gym.make(env_name)\n",
    "        model = PPO('MlpPolicy', env, learning_rate=lr, verbose=0)\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "        \n",
    "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "        results[lr] = (mean_reward, std_reward)\n",
    "        env.close()\n",
    "        \n",
    "        print(f\"  LR={lr}: {mean_reward:.2f} Â± {std_reward:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"LEARNING RATE TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if SB3_AVAILABLE and GYM_AVAILABLE:\n",
    "    lr_values = [1e-4, 3e-4, 1e-3, 3e-3]\n",
    "    print(f\"\\nTesting learning rates: {lr_values}\")\n",
    "    print(\"\\nTraining...\")\n",
    "    \n",
    "    lr_results = tune_learning_rate(lr_values)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    lrs = list(lr_results.keys())\n",
    "    means = [lr_results[lr][0] for lr in lrs]\n",
    "    stds = [lr_results[lr][1] for lr in lrs]\n",
    "    \n",
    "    # Convert to log scale labels\n",
    "    lr_labels = [f'{lr:.0e}' for lr in lrs]\n",
    "    \n",
    "    colors = ['#4caf50' if m > 400 else '#ff9800' if m > 200 else '#f44336' for m in means]\n",
    "    bars = ax.bar(lr_labels, means, yerr=stds, capsize=5, color=colors,\n",
    "                  edgecolor='black', linewidth=2)\n",
    "    \n",
    "    ax.axhline(y=475, color='green', linestyle='--', alpha=0.5, label='Solved')\n",
    "    ax.set_xlabel('Learning Rate', fontsize=12)\n",
    "    ax.set_ylabel('Mean Reward', fontsize=12)\n",
    "    ax.set_title('Effect of Learning Rate on CartPole Performance', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Find best LR\n",
    "    best_lr = lrs[np.argmax(means)]\n",
    "    ax.text(0.5, 0.95, f'Best LR: {best_lr:.0e}', transform=ax.transAxes,\n",
    "            fontsize=12, fontweight='bold', color='#388e3c',\n",
    "            verticalalignment='top', horizontalalignment='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Install required libraries to tune hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## From Games to Superhuman AI: The Journey\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              HISTORY OF RL IN GAMES                            â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  1992: TD-GAMMON                                              â”‚\n",
    "    â”‚    â€¢ Backgammon at expert level                               â”‚\n",
    "    â”‚    â€¢ Temporal difference learning                             â”‚\n",
    "    â”‚    â€¢ First neural network + RL success                       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  2013: ATARI DQN (DeepMind)                                   â”‚\n",
    "    â”‚    â€¢ Played 49 Atari games from pixels                       â”‚\n",
    "    â”‚    â€¢ Superhuman on many games                                â”‚\n",
    "    â”‚    â€¢ Deep RL revolution begins                               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  2016: ALPHAGO                                                â”‚\n",
    "    â”‚    â€¢ Beat world champion at Go                               â”‚\n",
    "    â”‚    â€¢ Monte Carlo tree search + deep RL                       â”‚\n",
    "    â”‚    â€¢ \"Moment that changed AI\"                                â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  2019: ALPHASTAR                                              â”‚\n",
    "    â”‚    â€¢ Grandmaster level at StarCraft II                       â”‚\n",
    "    â”‚    â€¢ Complex strategy and real-time control                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  2024: CLAUDE, GPT (RLHF)                                     â”‚\n",
    "    â”‚    â€¢ Language models aligned with RL                         â”‚\n",
    "    â”‚    â€¢ Same PPO algorithm, different domain                    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RL history in games\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(1990, 2026)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('Evolution of RL in Games', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Timeline\n",
    "ax.axhline(y=5, xmin=0.02, xmax=0.98, color='#333', linewidth=3)\n",
    "\n",
    "# Milestones\n",
    "milestones = [\n",
    "    (1992, 'TD-Gammon', 'Backgammon\\nExpert level', '#64b5f6'),\n",
    "    (2013, 'Atari DQN', '49 Atari games\\nfrom pixels', '#81c784'),\n",
    "    (2016, 'AlphaGo', 'Beat world\\nGo champion', '#ffb74d'),\n",
    "    (2019, 'AlphaStar', 'Grandmaster\\nStarCraft II', '#ef5350'),\n",
    "    (2024, 'RLHF LLMs', 'ChatGPT, Claude\\naligned with RL', '#ce93d8'),\n",
    "]\n",
    "\n",
    "for i, (year, name, desc, color) in enumerate(milestones):\n",
    "    # Circle on timeline\n",
    "    circle = Circle((year, 5), 0.4, facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    \n",
    "    # Alternate above/below\n",
    "    y_offset = 2.5 if i % 2 == 0 else -2.5\n",
    "    text_y = 5 + y_offset\n",
    "    \n",
    "    # Connector line\n",
    "    ax.plot([year, year], [5, text_y + (0.8 if y_offset > 0 else -0.8)], \n",
    "            color=color, linewidth=2)\n",
    "    \n",
    "    # Text box\n",
    "    box = FancyBboxPatch((year-2.5, text_y - 0.8 if y_offset > 0 else text_y + 0.3), \n",
    "                          5, 1.6, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor='black', alpha=0.8)\n",
    "    ax.add_patch(box)\n",
    "    \n",
    "    ax.text(year, text_y + 0.4 if y_offset > 0 else text_y + 1.0, \n",
    "            f'{year}: {name}', ha='center', fontsize=10, fontweight='bold')\n",
    "    ax.text(year, text_y - 0.3 if y_offset > 0 else text_y + 0.3,\n",
    "            desc, ha='center', fontsize=9)\n",
    "\n",
    "# Add complexity arrow\n",
    "ax.annotate('', xy=(2025, 1), xytext=(1992, 1),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.text(2008, 0.5, 'Increasing Complexity & Intelligence â†’', ha='center', fontsize=11, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHT:\")\n",
    "print(\"  The same core RL algorithms (value functions, policy gradients, PPO)\")\n",
    "print(\"  have scaled from simple games to superhuman AI in complex domains!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Game Playing with RL\n",
    "\n",
    "### Key Environments\n",
    "\n",
    "| Environment | Difficulty | Best Algorithm | Key Challenge |\n",
    "|-------------|------------|----------------|---------------|\n",
    "| CartPole | Easy | PPO, DQN, A2C | None (great for learning) |\n",
    "| LunarLander | Medium | PPO, DQN | Fuel management |\n",
    "| MountainCar | Hard | DQN + bonus | Sparse reward |\n",
    "| Acrobot | Hard | A2C, PPO | Credit assignment |\n",
    "\n",
    "### Algorithm Selection\n",
    "\n",
    "| Algorithm | Pros | Cons | Use When |\n",
    "|-----------|------|------|----------|\n",
    "| **PPO** | Stable, works everywhere | Slower | Default choice |\n",
    "| **DQN** | Sample efficient | Only discrete | Need efficiency |\n",
    "| **A2C** | Fast training | Less stable | Quick prototyping |\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# The SB3 workflow\n",
    "from stable_baselines3 import PPO\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=50_000)\n",
    "mean_reward, _ = evaluate_policy(model, env)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What makes MountainCar harder than CartPole?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "MountainCar has sparse rewards - the agent only gets reward (+0 instead of -1) when reaching the goal. This makes exploration very difficult since random actions never lead to success. The car must learn to build momentum by rocking back and forth, which requires long-horizon planning.\n",
    "</details>\n",
    "\n",
    "**2. When would you choose DQN over PPO?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "DQN is more sample-efficient than PPO because it uses a replay buffer to reuse past experiences. Choose DQN when:\n",
    "- You have discrete actions\n",
    "- Environment interactions are expensive\n",
    "- You need efficient use of data\n",
    "\n",
    "Choose PPO when you need stability or have continuous actions.\n",
    "</details>\n",
    "\n",
    "**3. What is the 'solved' threshold and why does it matter?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The solved threshold is a benchmark score that indicates mastery of an environment. For CartPole, it's 475 (out of 500 maximum). It matters because:\n",
    "- Provides an objective success criterion\n",
    "- Allows comparison between algorithms\n",
    "- Indicates when to stop training\n",
    "\n",
    "Different environments have different thresholds based on their difficulty.\n",
    "</details>\n",
    "\n",
    "**4. Why is learning rate the most important hyperparameter?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Learning rate controls how much the policy changes with each update:\n",
    "- Too high: Unstable training, oscillating performance\n",
    "- Too low: Slow learning, may not converge in time\n",
    "- Just right: Steady improvement toward optimal policy\n",
    "\n",
    "Other hyperparameters matter, but a bad learning rate can completely prevent learning.\n",
    "</details>\n",
    "\n",
    "**5. How did RL scale from simple games to superhuman Go?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The same core principles scaled up:\n",
    "1. Value functions â†’ Neural network value approximation\n",
    "2. Policy gradients â†’ Deep policy networks\n",
    "3. Exploration â†’ Monte Carlo tree search\n",
    "4. Self-play â†’ Learning from playing against itself\n",
    "\n",
    "Plus massive compute and clever engineering. AlphaGo used policy networks, value networks, and MCTS - all concepts we've covered!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Game playing is just the beginning! In the next notebook, we'll explore **robotics simulation** - applying RL to continuous control problems like walking and manipulation.\n",
    "\n",
    "**Continue to:** [Notebook 2: Robotics Simulation](02_robotics_simulation.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*\"From Pong to Go, RL has shown that learning to play is learning to think!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
