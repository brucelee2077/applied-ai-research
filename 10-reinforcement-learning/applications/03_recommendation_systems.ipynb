{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Systems: Teaching AI to Know What You Like\n",
    "\n",
    "From Netflix to Spotify, RL powers personalized recommendations!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The DJ analogy: how RL learns user preferences\n",
    "- Multi-armed bandits: the foundation of recommendation RL\n",
    "- Exploration vs exploitation: the core tradeoff\n",
    "- UCB and Thompson Sampling: smart exploration\n",
    "- Contextual bandits: personalized recommendations\n",
    "- Full RL for sequential recommendations\n",
    "\n",
    "**Prerequisites:** Game playing and robotics notebooks\n",
    "\n",
    "**Time:** ~35 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The DJ Analogy\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          THE DJ ANALOGY                                        â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Imagine you're a DJ at a party...                            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  YOUR GOAL:                                                   â”‚\n",
    "    â”‚    Keep the crowd dancing (maximize engagement)              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE CHALLENGE:                                               â”‚\n",
    "    â”‚    You don't know what songs they like!                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  EXPLOIT (play safe):                                        â”‚\n",
    "    â”‚    Play songs that worked before                             â”‚\n",
    "    â”‚    \"They liked pop, keep playing pop\"                        â”‚\n",
    "    â”‚    â†’ Might miss songs they'd love even more!                â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  EXPLORE (take risks):                                       â”‚\n",
    "    â”‚    Try new genres, new artists                               â”‚\n",
    "    â”‚    \"Let's see if they like jazz\"                             â”‚\n",
    "    â”‚    â†’ Might find hidden gems or clear the floor!             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE RL CONNECTION:                                          â”‚\n",
    "    â”‚    Songs = Actions (what to recommend)                       â”‚\n",
    "    â”‚    Dancing = Reward (user engagement)                        â”‚\n",
    "    â”‚    Balance explore/exploit to maximize total dancing!        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, Rectangle, Wedge\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the explore-exploit dilemma\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Pure exploitation\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Pure EXPLOITATION', fontsize=14, fontweight='bold', color='#1976d2')\n",
    "\n",
    "# Known good option\n",
    "box1 = FancyBboxPatch((2, 4), 2.5, 4, boxstyle=\"round,pad=0.1\",\n",
    "                       facecolor='#4caf50', edgecolor='#2e7d32', linewidth=3)\n",
    "ax1.add_patch(box1)\n",
    "ax1.text(3.25, 7, 'Pop Music', ha='center', fontsize=10, fontweight='bold', color='white')\n",
    "ax1.text(3.25, 6, '70% like', ha='center', fontsize=9, color='white')\n",
    "ax1.text(3.25, 5, '(Known)', ha='center', fontsize=8, color='white')\n",
    "\n",
    "# Unknown options (grayed out)\n",
    "for i, (x, name) in enumerate([(5.5, 'Jazz?'), (5.5, 'Rock?'), (5.5, 'Classical?')]):\n",
    "    y = 6.5 - i * 2\n",
    "    box = FancyBboxPatch((x, y), 2, 1.3, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#bdbdbd', edgecolor='#757575', linewidth=1, alpha=0.5)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(x + 1, y + 0.65, name, ha='center', fontsize=8, color='#666')\n",
    "\n",
    "ax1.text(5, 1.5, 'Safe but limited!', ha='center', fontsize=11, style='italic', color='#666')\n",
    "ax1.text(5, 0.8, 'What if Jazz is 90%?', ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "# Right: Pure exploration\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Pure EXPLORATION', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Random sampling\n",
    "genres = ['Pop', 'Jazz', 'Rock', 'Classical']\n",
    "colors = ['#4caf50', '#2196f3', '#ff9800', '#9c27b0']\n",
    "for i, (genre, color) in enumerate(zip(genres, colors)):\n",
    "    x = 2 + (i % 2) * 3\n",
    "    y = 5 + (i // 2) * 2.5\n",
    "    box = FancyBboxPatch((x, y), 2.5, 1.8, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(x + 1.25, y + 1.2, genre, ha='center', fontsize=9, fontweight='bold', color='white')\n",
    "    ax2.text(x + 1.25, y + 0.5, '???', ha='center', fontsize=9, color='white')\n",
    "\n",
    "ax2.text(5, 3, 'Random!', ha='center', fontsize=12, fontweight='bold', color='#666')\n",
    "ax2.text(5, 1.5, 'Learn everything but...', ha='center', fontsize=11, style='italic', color='#666')\n",
    "ax2.text(5, 0.8, 'Waste time on bad options!', ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTHE DILEMMA:\")\n",
    "print(\"  Exploit too much â†’ Might miss better options\")\n",
    "print(\"  Explore too much â†’ Waste time on bad options\")\n",
    "print(\"  SOLUTION: Multi-armed bandits balance both!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi-Armed Bandits: The Foundation\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          MULTI-ARMED BANDIT                                    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Imagine a casino with K slot machines (\"one-armed bandits\")  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚      [ğŸ°]    [ğŸ°]    [ğŸ°]    [ğŸ°]    [ğŸ°]                      â”‚\n",
    "    â”‚      Arm 1   Arm 2   Arm 3   Arm 4   Arm 5                    â”‚\n",
    "    â”‚       ???     ???     ???     ???     ???                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  EACH ARM:                                                    â”‚\n",
    "    â”‚    Has an unknown probability of paying out                  â”‚\n",
    "    â”‚    You learn by pulling and observing                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  YOUR GOAL:                                                   â”‚\n",
    "    â”‚    Maximize total winnings over T pulls                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  IN RECOMMENDATIONS:                                          â”‚\n",
    "    â”‚    Arms = Products/Videos/Songs to recommend                 â”‚\n",
    "    â”‚    Reward = User clicks, purchases, engagement               â”‚\n",
    "    â”‚    Goal = Maximize user satisfaction                         â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBanditEnvironment:\n",
    "    \"\"\"\n",
    "    Simulates a multi-armed bandit problem.\n",
    "    \n",
    "    Each arm has a fixed probability of giving reward.\n",
    "    This is like different products having different click rates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms=5, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.n_arms = n_arms\n",
    "        # True probabilities (unknown to the agent)\n",
    "        self.true_probs = np.random.uniform(0.2, 0.8, n_arms)\n",
    "        self.best_arm = np.argmax(self.true_probs)\n",
    "        \n",
    "    def pull(self, arm):\n",
    "        \"\"\"Pull an arm and get reward (0 or 1).\"\"\"\n",
    "        return 1 if np.random.random() < self.true_probs[arm] else 0\n",
    "    \n",
    "    def get_optimal_value(self):\n",
    "        \"\"\"Return the value of always picking the best arm.\"\"\"\n",
    "        return self.true_probs[self.best_arm]\n",
    "\n",
    "\n",
    "# Create environment\n",
    "env = MultiArmedBanditEnvironment(n_arms=5)\n",
    "\n",
    "print(\"MULTI-ARMED BANDIT ENVIRONMENT\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nTrue probabilities (hidden from agent):\")\n",
    "for i, p in enumerate(env.true_probs):\n",
    "    indicator = \" â† BEST\" if i == env.best_arm else \"\"\n",
    "    print(f\"  Arm {i+1}: {p:.2%}{indicator}\")\n",
    "print(f\"\\nOptimal strategy would give: {env.get_optimal_value():.2%} per pull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Epsilon-Greedy: Simple but Effective\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          EPSILON-GREEDY ALGORITHM                              â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE IDEA:                                                    â”‚\n",
    "    â”‚    With probability Îµ: Explore (random arm)                  â”‚\n",
    "    â”‚    With probability 1-Îµ: Exploit (best arm so far)           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  EXAMPLE (Îµ = 0.1):                                           â”‚\n",
    "    â”‚    90% of the time: Pick the arm with highest estimated valueâ”‚\n",
    "    â”‚    10% of the time: Pick a random arm                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  UPDATE RULE:                                                 â”‚\n",
    "    â”‚    Q(a) â† Q(a) + (1/n) Ã— (reward - Q(a))                     â”‚\n",
    "    â”‚    Running average of rewards for each arm                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  PROS:                                                        â”‚\n",
    "    â”‚    âœ“ Simple to implement                                     â”‚\n",
    "    â”‚    âœ“ Guaranteed exploration                                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  CONS:                                                        â”‚\n",
    "    â”‚    âœ— Explores uniformly (even clearly bad arms)              â”‚\n",
    "    â”‚    âœ— Fixed Îµ doesn't adapt over time                         â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    \"\"\"\n",
    "    Epsilon-Greedy bandit algorithm.\n",
    "    \n",
    "    The simplest exploration strategy:\n",
    "    - Explore with probability epsilon\n",
    "    - Exploit (pick best) with probability 1-epsilon\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms, epsilon=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all estimates to zero.\"\"\"\n",
    "        self.counts = np.zeros(self.n_arms)  # How many times each arm pulled\n",
    "        self.values = np.zeros(self.n_arms)  # Estimated value of each arm\n",
    "        \n",
    "    def select_arm(self):\n",
    "        \"\"\"Select which arm to pull.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: random arm\n",
    "            return np.random.randint(self.n_arms)\n",
    "        else:\n",
    "            # Exploit: best arm so far\n",
    "            return np.argmax(self.values)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        \"\"\"\n",
    "        Update estimate for an arm after observing reward.\n",
    "        \n",
    "        Uses incremental mean update:\n",
    "        Q(a) = Q(a) + (1/n) * (r - Q(a))\n",
    "        \"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        # Incremental update of running average\n",
    "        self.values[arm] += (reward - self.values[arm]) / n\n",
    "        \n",
    "    def get_name(self):\n",
    "        return f\"Îµ-Greedy (Îµ={self.epsilon})\"\n",
    "\n",
    "\n",
    "# Demonstrate epsilon-greedy\n",
    "print(\"EPSILON-GREEDY DEMONSTRATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "agent = EpsilonGreedy(n_arms=5, epsilon=0.1)\n",
    "env = MultiArmedBanditEnvironment(n_arms=5)\n",
    "\n",
    "# Run for 100 steps\n",
    "total_reward = 0\n",
    "for t in range(100):\n",
    "    arm = agent.select_arm()\n",
    "    reward = env.pull(arm)\n",
    "    agent.update(arm, reward)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"\\nAfter 100 pulls:\")\n",
    "print(f\"  Total reward: {total_reward}\")\n",
    "print(f\"  Average reward: {total_reward/100:.2%}\")\n",
    "print(f\"  Optimal would be: {env.get_optimal_value():.2%}\")\n",
    "\n",
    "print(\"\\nEstimated values vs True values:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Arm {i+1}: Est={agent.values[i]:.2%}, True={env.true_probs[i]:.2%}, Pulls={int(agent.counts[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## UCB: Upper Confidence Bound\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          UCB (UPPER CONFIDENCE BOUND)                          â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE IDEA: \"Optimism in the face of uncertainty\"              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  FORMULA:                                                     â”‚\n",
    "    â”‚    UCB(a) = Q(a) + c Ã— âˆš(ln(t) / N(a))                       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    Where:                                                     â”‚\n",
    "    â”‚    - Q(a): Estimated value of arm a                          â”‚\n",
    "    â”‚    - N(a): Number of times arm a was pulled                  â”‚\n",
    "    â”‚    - t: Total number of pulls so far                         â”‚\n",
    "    â”‚    - c: Exploration coefficient                              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WHY IT WORKS:                                                â”‚\n",
    "    â”‚    - Arms with few pulls have HIGH uncertainty bonus         â”‚\n",
    "    â”‚    - As arm is pulled more, bonus SHRINKS                    â”‚\n",
    "    â”‚    - Naturally balances explore/exploit!                     â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  VISUALIZATION:                                               â”‚\n",
    "    â”‚    Arm 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  Q=0.6, bonus=0.3  â†’  UCB=0.9       â”‚\n",
    "    â”‚    Arm 2: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  Q=0.3, bonus=0.5  â†’  UCB=0.8       â”‚\n",
    "    â”‚    (Arm 2 gets picked despite lower Q because uncertain!)   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB:\n",
    "    \"\"\"\n",
    "    Upper Confidence Bound algorithm.\n",
    "    \n",
    "    Key idea: Be optimistic about uncertain arms!\n",
    "    Add an uncertainty bonus that decreases as we pull more.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms, c=2.0):\n",
    "        self.n_arms = n_arms\n",
    "        self.c = c  # Exploration coefficient\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.counts = np.zeros(self.n_arms)\n",
    "        self.values = np.zeros(self.n_arms)\n",
    "        self.total_pulls = 0\n",
    "        \n",
    "    def select_arm(self):\n",
    "        \"\"\"Select arm with highest UCB value.\"\"\"\n",
    "        self.total_pulls += 1\n",
    "        \n",
    "        # First, pull each arm once to initialize\n",
    "        for arm in range(self.n_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        \n",
    "        # Compute UCB for each arm\n",
    "        ucb_values = np.zeros(self.n_arms)\n",
    "        for arm in range(self.n_arms):\n",
    "            # Uncertainty bonus: decreases as we pull more\n",
    "            bonus = self.c * np.sqrt(np.log(self.total_pulls) / self.counts[arm])\n",
    "            ucb_values[arm] = self.values[arm] + bonus\n",
    "            \n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        \"\"\"Update estimate after observing reward.\"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        self.values[arm] += (reward - self.values[arm]) / n\n",
    "        \n",
    "    def get_name(self):\n",
    "        return f\"UCB (c={self.c})\"\n",
    "\n",
    "\n",
    "# Demonstrate UCB\n",
    "print(\"UCB DEMONSTRATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "agent = UCB(n_arms=5, c=2.0)\n",
    "env = MultiArmedBanditEnvironment(n_arms=5)\n",
    "\n",
    "# Run for 100 steps\n",
    "total_reward = 0\n",
    "for t in range(100):\n",
    "    arm = agent.select_arm()\n",
    "    reward = env.pull(arm)\n",
    "    agent.update(arm, reward)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"\\nAfter 100 pulls:\")\n",
    "print(f\"  Total reward: {total_reward}\")\n",
    "print(f\"  Average reward: {total_reward/100:.2%}\")\n",
    "\n",
    "print(\"\\nPull distribution (UCB focuses on promising arms):\")\n",
    "for i in range(5):\n",
    "    bar = 'â–ˆ' * int(agent.counts[i] / 2)\n",
    "    best = \" â† BEST\" if i == env.best_arm else \"\"\n",
    "    print(f\"  Arm {i+1}: {bar} ({int(agent.counts[i])} pulls){best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Thompson Sampling: The Bayesian Approach\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          THOMPSON SAMPLING                                     â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE IDEA: Maintain probability distributions, sample!        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  FOR EACH ARM:                                                â”‚\n",
    "    â”‚    Keep track of: successes (Î±) and failures (Î²)             â”‚\n",
    "    â”‚    Belief: Probability follows Beta(Î±, Î²) distribution       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TO SELECT AN ARM:                                            â”‚\n",
    "    â”‚    1. Sample Î¸Ìƒáµ¢ ~ Beta(Î±áµ¢, Î²áµ¢) for each arm                â”‚\n",
    "    â”‚    2. Pick arm with highest sample: argmax Î¸Ìƒáµ¢                â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WHY IT WORKS:                                                â”‚\n",
    "    â”‚    - Uncertain arms have wide distributions                  â”‚\n",
    "    â”‚    - Sometimes sample high â†’ explore!                        â”‚\n",
    "    â”‚    - Confident good arms sample high often â†’ exploit!        â”‚\n",
    "    â”‚    - Naturally balances based on uncertainty!                â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  ADVANTAGE:                                                   â”‚\n",
    "    â”‚    Often achieves BEST empirical performance!                â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling:\n",
    "    \"\"\"\n",
    "    Thompson Sampling algorithm.\n",
    "    \n",
    "    Bayesian approach: maintain belief about each arm,\n",
    "    sample from beliefs to select actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        # Beta distribution parameters: starts with Beta(1,1) = uniform\n",
    "        self.alpha = np.ones(self.n_arms)  # Successes + 1\n",
    "        self.beta = np.ones(self.n_arms)   # Failures + 1\n",
    "        self.counts = np.zeros(self.n_arms)\n",
    "        \n",
    "    def select_arm(self):\n",
    "        \"\"\"Sample from each arm's distribution, pick highest.\"\"\"\n",
    "        # Sample from Beta distribution for each arm\n",
    "        samples = np.random.beta(self.alpha, self.beta)\n",
    "        return np.argmax(samples)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        \"\"\"Update belief after observing reward.\"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        if reward == 1:\n",
    "            self.alpha[arm] += 1\n",
    "        else:\n",
    "            self.beta[arm] += 1\n",
    "            \n",
    "    def get_name(self):\n",
    "        return \"Thompson Sampling\"\n",
    "    \n",
    "    @property\n",
    "    def values(self):\n",
    "        \"\"\"Return expected value (mean of Beta).\"\"\"\n",
    "        return self.alpha / (self.alpha + self.beta)\n",
    "\n",
    "\n",
    "# Demonstrate Thompson Sampling\n",
    "print(\"THOMPSON SAMPLING DEMONSTRATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "agent = ThompsonSampling(n_arms=5)\n",
    "env = MultiArmedBanditEnvironment(n_arms=5)\n",
    "\n",
    "# Run for 100 steps\n",
    "total_reward = 0\n",
    "for t in range(100):\n",
    "    arm = agent.select_arm()\n",
    "    reward = env.pull(arm)\n",
    "    agent.update(arm, reward)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"\\nAfter 100 pulls:\")\n",
    "print(f\"  Total reward: {total_reward}\")\n",
    "print(f\"  Average reward: {total_reward/100:.2%}\")\n",
    "\n",
    "print(\"\\nBelief distributions (Î±, Î²):\")\n",
    "for i in range(5):\n",
    "    mean = agent.alpha[i] / (agent.alpha[i] + agent.beta[i])\n",
    "    print(f\"  Arm {i+1}: Î±={agent.alpha[i]:.0f}, Î²={agent.beta[i]:.0f}, Mean={mean:.2%}, True={env.true_probs[i]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Thompson Sampling beliefs\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 4))\n",
    "fig.suptitle('Thompson Sampling: Belief Distributions After 100 Pulls', fontsize=14, fontweight='bold')\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    # Plot Beta distribution\n",
    "    y = stats.beta.pdf(x, agent.alpha[i], agent.beta[i])\n",
    "    color = '#4caf50' if i == env.best_arm else '#2196f3'\n",
    "    ax.fill_between(x, y, alpha=0.3, color=color)\n",
    "    ax.plot(x, y, color=color, linewidth=2)\n",
    "    \n",
    "    # Mark true probability\n",
    "    ax.axvline(env.true_probs[i], color='red', linestyle='--', linewidth=2, label='True')\n",
    "    \n",
    "    title = f'Arm {i+1}'\n",
    "    if i == env.best_arm:\n",
    "        title += ' (BEST)'\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.set_xlabel('Probability', fontsize=9)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\naxes[0].set_ylabel('Density', fontsize=10)\naxes[-1].legend(loc='upper right', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHT:\")\n",
    "print(\"  - Peaked distributions = confident (many pulls)\")\n",
    "print(\"  - Wide distributions = uncertain (few pulls)\")\n",
    "print(\"  - Red dashed line = true probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Algorithm Comparison: Who Wins?\n",
    "\n",
    "Let's compare all three algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(algorithm_class, env, n_steps=1000, **kwargs):\n",
    "    \"\"\"\n",
    "    Run a bandit experiment and track cumulative reward.\n",
    "    \"\"\"\n",
    "    agent = algorithm_class(n_arms=env.n_arms, **kwargs)\n",
    "    rewards = []\n",
    "    \n",
    "    for t in range(n_steps):\n",
    "        arm = agent.select_arm()\n",
    "        reward = env.pull(arm)\n",
    "        agent.update(arm, reward)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return np.cumsum(rewards), agent.get_name()\n",
    "\n",
    "\n",
    "def compute_regret(cumulative_rewards, optimal_value, n_steps):\n",
    "    \"\"\"\n",
    "    Compute regret: how much worse than optimal.\n",
    "    \n",
    "    Regret = optimal_total - actual_total\n",
    "    \"\"\"\n",
    "    optimal_rewards = optimal_value * np.arange(1, n_steps + 1)\n",
    "    return optimal_rewards - cumulative_rewards\n",
    "\n",
    "\n",
    "# Run comparison\n",
    "print(\"ALGORITHM COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_steps = 1000\n",
    "env = MultiArmedBanditEnvironment(n_arms=10)  # 10 products\n",
    "optimal = env.get_optimal_value()\n",
    "\n",
    "algorithms = [\n",
    "    (EpsilonGreedy, {'epsilon': 0.1}),\n",
    "    (EpsilonGreedy, {'epsilon': 0.01}),\n",
    "    (UCB, {'c': 2.0}),\n",
    "    (ThompsonSampling, {}),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for algo_class, kwargs in algorithms:\n",
    "    cum_rewards, name = run_experiment(algo_class, env, n_steps, **kwargs)\n",
    "    regret = compute_regret(cum_rewards, optimal, n_steps)\n",
    "    results.append((name, cum_rewards, regret))\n",
    "    print(f\"  {name}: Total reward = {cum_rewards[-1]:.0f}, Final regret = {regret[-1]:.1f}\")\n",
    "\n",
    "print(f\"\\nOptimal total would be: {optimal * n_steps:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = ['#f44336', '#ff9800', '#4caf50', '#2196f3']\n",
    "\n",
    "# Left: Cumulative reward\n",
    "ax1 = axes[0]\n",
    "for (name, cum_rewards, regret), color in zip(results, colors):\n",
    "    ax1.plot(cum_rewards, label=name, color=color, linewidth=2)\n",
    "\n",
    "# Plot optimal\n",
    "ax1.plot(optimal * np.arange(1, n_steps + 1), 'k--', label='Optimal', linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Time Step', fontsize=11)\n",
    "ax1.set_ylabel('Cumulative Reward', fontsize=11)\n",
    "ax1.set_title('Cumulative Reward Over Time', fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Regret\n",
    "ax2 = axes[1]\n",
    "for (name, cum_rewards, regret), color in zip(results, colors):\n",
    "    ax2.plot(regret, label=name, color=color, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Time Step', fontsize=11)\n",
    "ax2.set_ylabel('Cumulative Regret', fontsize=11)\n",
    "ax2.set_title('Regret Over Time (lower is better)', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"  â€¢ Thompson Sampling often has lowest regret\")\n",
    "print(\"  â€¢ UCB provides good theoretical guarantees\")\n",
    "print(\"  â€¢ Îµ-Greedy is simple but competitive\")\n",
    "print(\"  â€¢ All algorithms improve with more data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Contextual Bandits: Personalized Recommendations\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          CONTEXTUAL BANDITS                                    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  PROBLEM WITH BASIC BANDITS:                                  â”‚\n",
    "    â”‚    Same recommendation for everyone!                         â”‚\n",
    "    â”‚    But different users have different preferences...         â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  CONTEXTUAL BANDITS:                                          â”‚\n",
    "    â”‚    Observe context (user features) before choosing action    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  EXAMPLE:                                                     â”‚\n",
    "    â”‚    Context: [age=25, genre_pref=action, time=evening]        â”‚\n",
    "    â”‚    Action: Which movie to recommend?                         â”‚\n",
    "    â”‚    Reward: Did they watch it?                                â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  MODEL:                                                       â”‚\n",
    "    â”‚    r(context, action) = context^T Ã— Î¸_action                 â”‚\n",
    "    â”‚    Linear model predicting reward from context               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  ALGORITHMS:                                                  â”‚\n",
    "    â”‚    â€¢ LinUCB: UCB with linear reward model                    â”‚\n",
    "    â”‚    â€¢ Linear Thompson Sampling                                â”‚\n",
    "    â”‚    â€¢ Neural bandits (deep learning)                          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearContextualBandit:\n",
    "    \"\"\"\n",
    "    Simple contextual bandit with linear reward model.\n",
    "    \n",
    "    For each arm, learns: reward = context @ weights\n",
    "    Uses Ridge regression for stability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms, context_dim, alpha=1.0):\n",
    "        self.n_arms = n_arms\n",
    "        self.context_dim = context_dim\n",
    "        self.alpha = alpha  # Exploration parameter\n",
    "        \n",
    "        # Initialize for each arm\n",
    "        self.A = [np.eye(context_dim) for _ in range(n_arms)]  # Design matrix\n",
    "        self.b = [np.zeros(context_dim) for _ in range(n_arms)]  # Reward vector\n",
    "        \n",
    "    def select_arm(self, context):\n",
    "        \"\"\"Select arm using LinUCB strategy.\"\"\"\n",
    "        ucb_values = np.zeros(self.n_arms)\n",
    "        \n",
    "        for arm in range(self.n_arms):\n",
    "            A_inv = np.linalg.inv(self.A[arm])\n",
    "            theta = A_inv @ self.b[arm]\n",
    "            \n",
    "            # UCB: mean + uncertainty\n",
    "            mean = context @ theta\n",
    "            uncertainty = self.alpha * np.sqrt(context @ A_inv @ context)\n",
    "            ucb_values[arm] = mean + uncertainty\n",
    "            \n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def update(self, arm, context, reward):\n",
    "        \"\"\"Update model after observing reward.\"\"\"\n",
    "        self.A[arm] += np.outer(context, context)\n",
    "        self.b[arm] += reward * context\n",
    "\n",
    "\n",
    "# Simulate contextual recommendation\n",
    "print(\"CONTEXTUAL BANDIT: MOVIE RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Context features: [age_young, likes_action, evening_time]\n",
    "# 4 movies to recommend\n",
    "n_arms = 4\n",
    "context_dim = 3\n",
    "movie_names = ['Action Blockbuster', 'Romantic Comedy', 'Documentary', 'Horror']\n",
    "\n",
    "# True preferences (hidden)\n",
    "# Different user types prefer different movies!\n",
    "true_weights = np.array([\n",
    "    [0.8, 0.9, 0.3],   # Action: young + action fans + any time\n",
    "    [0.3, 0.1, 0.7],   # Romance: older + non-action + evening\n",
    "    [0.4, 0.1, 0.2],   # Documentary: moderate preferences\n",
    "    [0.6, 0.5, 0.8],   # Horror: young + evening\n",
    "])\n",
    "\n",
    "agent = LinearContextualBandit(n_arms, context_dim, alpha=0.5)\n",
    "\n",
    "# Simulate users\n",
    "total_reward = 0\n",
    "n_users = 500\n",
    "\n",
    "for _ in range(n_users):\n",
    "    # Random user context\n",
    "    context = np.random.uniform(0, 1, context_dim)\n",
    "    \n",
    "    # Agent picks a movie\n",
    "    arm = agent.select_arm(context)\n",
    "    \n",
    "    # True reward (with noise)\n",
    "    true_prob = 1 / (1 + np.exp(-context @ true_weights[arm]))\n",
    "    reward = 1 if np.random.random() < true_prob else 0\n",
    "    \n",
    "    agent.update(arm, context, reward)\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"\\nAfter {n_users} users:\")\n",
    "print(f\"  Total clicks: {total_reward}\")\n",
    "print(f\"  Click rate: {total_reward/n_users:.1%}\")\n",
    "\n",
    "# Show learned preferences\n",
    "print(\"\\nLearned movie preferences:\")\n",
    "for arm in range(n_arms):\n",
    "    A_inv = np.linalg.inv(agent.A[arm])\n",
    "    theta = A_inv @ agent.b[arm]\n",
    "    print(f\"  {movie_names[arm]}:\")\n",
    "    print(f\"    Young: {theta[0]:.2f}, Action fan: {theta[1]:.2f}, Evening: {theta[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize contextual recommendations\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('Contextual Bandits: Personalized Recommendations', fontsize=16, fontweight='bold')\n",
    "\n",
    "# User types\n",
    "users = [\n",
    "    (1, 7, 'Young Action Fan', '[1, 1, 0]', 'Action\\nBlockbuster', '#f44336'),\n",
    "    (5.5, 7, 'Older Evening\\nViewer', '[0, 0, 1]', 'Romantic\\nComedy', '#e91e63'),\n",
    "    (10, 7, 'Young Night Owl', '[1, 0, 1]', 'Horror', '#9c27b0'),\n",
    "]\n",
    "\n",
    "for x, y, user_type, context, rec, color in users:\n",
    "    # User box\n",
    "    user_box = FancyBboxPatch((x-0.8, y), 1.6, 2, boxstyle=\"round,pad=0.1\",\n",
    "                               facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "    ax.add_patch(user_box)\n",
    "    ax.text(x, y+1.6, user_type, ha='center', fontsize=8, fontweight='bold')\n",
    "    ax.text(x, y+0.5, context, ha='center', fontsize=7, family='monospace')\n",
    "    \n",
    "    # Arrow\n",
    "    ax.annotate('', xy=(x, y-0.7), xytext=(x, y-0.1),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "    \n",
    "    # Recommendation\n",
    "    rec_box = FancyBboxPatch((x-0.8, y-2.5), 1.6, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=2, alpha=0.8)\n",
    "    ax.add_patch(rec_box)\n",
    "    ax.text(x, y-1.5, rec, ha='center', fontsize=8, fontweight='bold', color='white')\n",
    "\n",
    "# Legend\n",
    "ax.text(6, 3, 'Same algorithm â†’ Different recommendations based on context!', \n",
    "        ha='center', fontsize=11, style='italic')\n",
    "\n",
    "# Context explanation\n",
    "ax.text(6, 1.5, 'Context = [age_young, likes_action, evening_time]',\n",
    "        ha='center', fontsize=10, family='monospace', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Full RL for Sequential Recommendations\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          FULL RL: BEYOND BANDITS                               â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  BANDITS LIMITATION:                                          â”‚\n",
    "    â”‚    - Each recommendation is independent                      â”‚\n",
    "    â”‚    - Don't consider long-term effects                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  REALITY:                                                     â”‚\n",
    "    â”‚    - Recommendations affect future user state                â”‚\n",
    "    â”‚    - Showing action movies â†’ user expects more action        â”‚\n",
    "    â”‚    - Need to optimize for long-term engagement               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  FULL RL FORMULATION:                                         â”‚\n",
    "    â”‚    State: User history, preferences, context                 â”‚\n",
    "    â”‚    Action: Which item to recommend                           â”‚\n",
    "    â”‚    Reward: Immediate engagement (clicks, watch time)         â”‚\n",
    "    â”‚    Transition: How user state changes after interaction      â”‚\n",
    "    â”‚    Goal: Maximize long-term engagement                       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  ALGORITHMS:                                                  â”‚\n",
    "    â”‚    â€¢ DQN for recommendations                                 â”‚\n",
    "    â”‚    â€¢ Actor-Critic methods                                    â”‚\n",
    "    â”‚    â€¢ Offline RL (learning from logs)                         â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sequential recommendation simulation\n",
    "\n",
    "class UserState:\n",
    "    \"\"\"\n",
    "    Simulates a user whose preferences evolve.\n",
    "    \n",
    "    - Users have genre preferences\n",
    "    - Preferences shift based on what they watch\n",
    "    - Boredom increases if same genre repeated\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_genres=4):\n",
    "        self.n_genres = n_genres\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        # Initial genre preferences (softmax normalized)\n",
    "        self.preferences = np.random.dirichlet(np.ones(self.n_genres))\n",
    "        self.boredom = np.zeros(self.n_genres)  # Boredom per genre\n",
    "        self.history = []  # Recent recommendations\n",
    "        return self.get_state()\n",
    "        \n",
    "    def get_state(self):\n",
    "        \"\"\"Return state vector.\"\"\"\n",
    "        return np.concatenate([self.preferences, self.boredom])\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        User receives recommendation and responds.\n",
    "        \n",
    "        Returns: reward, done\n",
    "        \"\"\"\n",
    "        # Probability of engagement\n",
    "        base_prob = self.preferences[action]\n",
    "        boredom_penalty = 0.3 * self.boredom[action]\n",
    "        prob = max(0.1, base_prob - boredom_penalty)\n",
    "        \n",
    "        # User engages or not\n",
    "        engaged = np.random.random() < prob\n",
    "        reward = 1.0 if engaged else 0.0\n",
    "        \n",
    "        # Update state\n",
    "        self.history.append(action)\n",
    "        \n",
    "        # Boredom increases for watched genre, decreases for others\n",
    "        if engaged:\n",
    "            self.boredom[action] = min(1, self.boredom[action] + 0.2)\n",
    "            # Preferences shift slightly toward watched genre\n",
    "            self.preferences[action] = min(0.5, self.preferences[action] + 0.05)\n",
    "        \n",
    "        # Other genres' boredom decays\n",
    "        for g in range(self.n_genres):\n",
    "            if g != action:\n",
    "                self.boredom[g] = max(0, self.boredom[g] - 0.1)\n",
    "        \n",
    "        # Normalize preferences\n",
    "        self.preferences = self.preferences / self.preferences.sum()\n",
    "        \n",
    "        # Session ends after some interactions\n",
    "        done = len(self.history) >= 20\n",
    "        \n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "\n",
    "# Compare greedy vs diverse strategies\n",
    "print(\"SEQUENTIAL RECOMMENDATION: GREEDY vs DIVERSE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def greedy_policy(state, n_genres=4):\n",
    "    \"\"\"Always pick highest preference.\"\"\"\n",
    "    return np.argmax(state[:n_genres])\n",
    "\n",
    "def diverse_policy(state, n_genres=4):\n",
    "    \"\"\"Consider preferences AND boredom.\"\"\"\n",
    "    preferences = state[:n_genres]\n",
    "    boredom = state[n_genres:]\n",
    "    scores = preferences - 0.5 * boredom\n",
    "    return np.argmax(scores)\n",
    "\n",
    "# Run comparison\n",
    "n_episodes = 100\n",
    "n_genres = 4\n",
    "genre_names = ['Action', 'Comedy', 'Drama', 'Sci-Fi']\n",
    "\n",
    "results = {'Greedy': [], 'Diverse': []}\n",
    "\n",
    "for policy_name, policy in [('Greedy', greedy_policy), ('Diverse', diverse_policy)]:\n",
    "    for _ in range(n_episodes):\n",
    "        user = UserState(n_genres)\n",
    "        state = user.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = policy(state, n_genres)\n",
    "            state, reward, done = user.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "        results[policy_name].append(total_reward)\n",
    "\n",
    "print(\"\\nResults (average over 100 sessions):\")\n",
    "for name, rewards in results.items():\n",
    "    print(f\"  {name}: {np.mean(rewards):.2f} Â± {np.std(rewards):.2f} engagements\")\n",
    "\n",
    "print(\"\\nINSIGHT:\")\n",
    "print(\"  Diverse policy considers long-term effects (boredom)\")\n",
    "print(\"  Sometimes recommending second-best prevents boredom!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Reward distribution\n",
    "ax1 = axes[0]\n",
    "positions = [0, 1]\n",
    "bp = ax1.boxplot([results['Greedy'], results['Diverse']], positions=positions,\n",
    "                  patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('#ff9800')\n",
    "bp['boxes'][1].set_facecolor('#4caf50')\n",
    "\n",
    "ax1.set_xticks(positions)\n",
    "ax1.set_xticklabels(['Greedy', 'Diverse'])\n",
    "ax1.set_ylabel('Total Engagements per Session', fontsize=11)\n",
    "ax1.set_title('Engagement Comparison', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Right: Recommendation diversity\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Track one session for each\n",
    "diversity_data = {}\n",
    "for policy_name, policy in [('Greedy', greedy_policy), ('Diverse', diverse_policy)]:\n",
    "    user = UserState(n_genres)\n",
    "    state = user.reset()\n",
    "    actions = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = policy(state, n_genres)\n",
    "        actions.append(action)\n",
    "        state, reward, done = user.step(action)\n",
    "    \n",
    "    diversity_data[policy_name] = actions\n",
    "\n",
    "# Plot as bar charts\n",
    "x = np.arange(n_genres)\n",
    "width = 0.35\n",
    "\n",
    "greedy_counts = [diversity_data['Greedy'].count(i) for i in range(n_genres)]\n",
    "diverse_counts = [diversity_data['Diverse'].count(i) for i in range(n_genres)]\n",
    "\n",
    "ax2.bar(x - width/2, greedy_counts, width, label='Greedy', color='#ff9800')\n",
    "ax2.bar(x + width/2, diverse_counts, width, label='Diverse', color='#4caf50')\n",
    "\n",
    "ax2.set_xlabel('Genre', fontsize=11)\n",
    "ax2.set_ylabel('Times Recommended', fontsize=11)\n",
    "ax2.set_title('Recommendation Diversity (One Session)', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(genre_names)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY OBSERVATION:\")\n",
    "print(\"  Greedy policy recommends same genre repeatedly (boredom!)\")\n",
    "print(\"  Diverse policy spreads recommendations across genres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Real-World Applications\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              RL IN PRODUCTION RECOMMENDATION SYSTEMS           â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  NETFLIX:                                                     â”‚\n",
    "    â”‚    â€¢ Bandit algorithms for thumbnail selection               â”‚\n",
    "    â”‚    â€¢ RL for personalized row ordering                        â”‚\n",
    "    â”‚    â€¢ Contextual bandits for \"Because you watched\"            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  SPOTIFY:                                                     â”‚\n",
    "    â”‚    â€¢ Bandits for Discover Weekly                             â”‚\n",
    "    â”‚    â€¢ Thompson Sampling for new song recommendations          â”‚\n",
    "    â”‚    â€¢ RL for playlist generation                              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  YOUTUBE:                                                     â”‚\n",
    "    â”‚    â€¢ Deep RL for video recommendations                       â”‚\n",
    "    â”‚    â€¢ Balancing watch time vs diversity                       â”‚\n",
    "    â”‚    â€¢ Long-term user satisfaction optimization                â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  E-COMMERCE (Amazon, etc.):                                   â”‚\n",
    "    â”‚    â€¢ Bandits for product ranking                             â”‚\n",
    "    â”‚    â€¢ RL for personalized pricing                             â”‚\n",
    "    â”‚    â€¢ Sequential recommendations in cart                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize real-world applications\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('RL in Real-World Recommendation Systems', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Companies and their RL uses\n",
    "companies = [\n",
    "    (2, 7.5, 'Netflix', 'Thumbnail selection\\nRow ordering', '#e50914'),\n",
    "    (6, 7.5, 'Spotify', 'Discover Weekly\\nNew songs', '#1db954'),\n",
    "    (10, 7.5, 'YouTube', 'Video ranking\\nWatch time opt.', '#ff0000'),\n",
    "    (4, 4, 'Amazon', 'Product ranking\\nPricing', '#ff9900'),\n",
    "    (8, 4, 'TikTok', 'Feed ranking\\nCreator exposure', '#000000'),\n",
    "]\n",
    "\n",
    "for x, y, name, use, color in companies:\n",
    "    # Company box\n",
    "    box = FancyBboxPatch((x-1.3, y-0.5), 2.6, 2.2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor='black', linewidth=2, alpha=0.8)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y+1.2, name, ha='center', fontsize=11, fontweight='bold', color='white')\n",
    "    ax.text(x, y+0.2, use, ha='center', fontsize=8, color='white')\n",
    "\n",
    "# Common techniques\n",
    "ax.text(6, 1.5, 'Common Techniques:', ha='center', fontsize=12, fontweight='bold')\n",
    "techniques = ['Multi-Armed Bandits', 'Contextual Bandits', 'Deep RL', 'Offline RL']\n",
    "for i, tech in enumerate(techniques):\n",
    "    ax.text(2 + i * 3.2, 0.8, f'â€¢ {tech}', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: RL for Recommendations\n",
    "\n",
    "### Algorithm Comparison\n",
    "\n",
    "| Algorithm | Explore Strategy | Pros | Cons |\n",
    "|-----------|-----------------|------|------|\n",
    "| **Îµ-Greedy** | Random with prob Îµ | Simple | Uniform exploration |\n",
    "| **UCB** | Optimism bonus | Theoretical guarantees | Can over-explore |\n",
    "| **Thompson** | Sample from beliefs | Often best empirical | Computationally heavier |\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "| Scenario | Best Approach |\n",
    "|----------|---------------|\n",
    "| Simple A/B testing | Îµ-Greedy or Thompson |\n",
    "| Need guarantees | UCB |\n",
    "| Personalization needed | Contextual Bandits |\n",
    "| Long-term optimization | Full RL (DQN, Actor-Critic) |\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# Simple bandit\n",
    "class EpsilonGreedy:\n",
    "    def select_arm(self):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_arms)  # Explore\n",
    "        return np.argmax(self.values)  # Exploit\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What is the explore-exploit tradeoff?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The explore-exploit tradeoff is the fundamental tension in bandits:\n",
    "- Exploit: Use what you know works (pick best arm so far)\n",
    "- Explore: Try uncertain options to learn more\n",
    "\n",
    "Too much exploitation â†’ might miss better options\n",
    "Too much exploration â†’ waste time on bad options\n",
    "</details>\n",
    "\n",
    "**2. How does UCB balance exploration and exploitation?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "UCB adds an \"optimism bonus\" to each arm:\n",
    "UCB(a) = estimated_value(a) + uncertainty_bonus(a)\n",
    "\n",
    "Arms with few pulls have HIGH uncertainty bonus â†’ get explored\n",
    "As arms are pulled more, bonus decreases â†’ exploitation wins\n",
    "This naturally balances the tradeoff!\n",
    "</details>\n",
    "\n",
    "**3. Why is Thompson Sampling often the best in practice?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Thompson Sampling maintains probability distributions (beliefs) for each arm:\n",
    "- Sample from each arm's distribution\n",
    "- Pick the arm with highest sample\n",
    "\n",
    "This naturally handles uncertainty:\n",
    "- Uncertain arms have wide distributions â†’ sometimes sample high\n",
    "- Confident good arms sample high often â†’ exploitation\n",
    "- Elegant balance without tuning exploration parameters!\n",
    "</details>\n",
    "\n",
    "**4. What's the difference between bandits and full RL for recommendations?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Bandits: Each recommendation is independent, optimize immediate reward\n",
    "\n",
    "Full RL: Recommendations affect future user state:\n",
    "- User preferences evolve\n",
    "- Boredom accumulates if same type repeated\n",
    "- Need to optimize long-term engagement\n",
    "\n",
    "Full RL considers how today's recommendation affects tomorrow's user!\n",
    "</details>\n",
    "\n",
    "**5. How do contextual bandits enable personalization?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Contextual bandits observe user features (context) before choosing:\n",
    "- Context: age, preferences, time, device, etc.\n",
    "- Learn: which items work for which contexts\n",
    "- Result: Different users get different recommendations!\n",
    "\n",
    "Example: Young action fan in evening â†’ recommend action movie\n",
    "Same time, older documentary fan â†’ recommend documentary\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Recommendation systems show RL for personalization. In the next notebook, we'll explore a detailed case study: **LLM Alignment** using RLHF!\n",
    "\n",
    "**Continue to:** [Notebook 4: LLM Alignment Case Study](04_llm_alignment_case_study.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*\"The best recommendation is the one that feels like you read the user's mind!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
