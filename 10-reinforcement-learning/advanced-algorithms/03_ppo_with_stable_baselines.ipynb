{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO with Stable-Baselines3: Production-Ready RL\n",
    "\n",
    "Now that you understand PPO, let's use a production-ready implementation!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The \"chef's kitchen\" analogy: when to use libraries vs scratch\n",
    "- Stable-Baselines3 architecture and design\n",
    "- Training PPO with SB3 on various environments\n",
    "- Customizing hyperparameters\n",
    "- Saving, loading, and evaluating models\n",
    "- Monitoring training with callbacks\n",
    "\n",
    "**Prerequisites:** Notebook 2 (PPO From Scratch)\n",
    "\n",
    "**Time:** ~25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Chef's Kitchen Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          THE CHEF'S KITCHEN ANALOGY                            │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  FROM SCRATCH (Previous notebook):                            │\n",
    "    │    Like cooking at home with raw ingredients                  │\n",
    "    │    • You understand every step                               │\n",
    "    │    • Full control over everything                            │\n",
    "    │    • Time-consuming                                          │\n",
    "    │    • May have subtle bugs                                    │\n",
    "    │                                                                │\n",
    "    │  STABLE-BASELINES3 (This notebook):                          │\n",
    "    │    Like a professional kitchen with prep done                │\n",
    "    │    • Battle-tested implementation                            │\n",
    "    │    • Optimized for performance                               │\n",
    "    │    • Rich features (logging, callbacks, saving)              │\n",
    "    │    • Used by researchers and industry                        │\n",
    "    │                                                                │\n",
    "    │  WHEN TO USE WHAT:                                            │\n",
    "    │    From scratch: Learning, custom algorithms, research       │\n",
    "    │    SB3: Production, reproducibility, standard benchmarks     │\n",
    "    │                                                                │\n",
    "    │  ANALOGY:                                                     │\n",
    "    │    Knowing how to cook helps you use restaurant kitchen     │\n",
    "    │    better - you understand what the tools are doing!        │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym\n",
    "\n",
    "# Check if Stable-Baselines3 is available\n",
    "try:\n",
    "    from stable_baselines3 import PPO, A2C\n",
    "    from stable_baselines3.common.env_util import make_vec_env\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "    from stable_baselines3.common.monitor import Monitor\n",
    "    from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "    import torch\n",
    "    SB3_AVAILABLE = True\n",
    "    print(\"✓ Stable-Baselines3 is installed!\")\n",
    "    print(f\"  Version info: Using PyTorch {torch.__version__}\")\n",
    "except ImportError:\n",
    "    SB3_AVAILABLE = False\n",
    "    print(\"✗ Stable-Baselines3 not installed.\")\n",
    "    print(\"\\nTo install, run:\")\n",
    "    print(\"  pip install stable-baselines3[extra]\")\n",
    "    print(\"\\nThis will install SB3 with extra dependencies including tensorboard.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Stable-Baselines3 architecture\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('Stable-Baselines3 Architecture', fontsize=16, fontweight='bold')\n",
    "\n",
    "# User code\n",
    "user_box = FancyBboxPatch((1, 9.5), 12, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(user_box)\n",
    "ax.text(7, 10.25, 'YOUR CODE', ha='center', fontsize=12, fontweight='bold', color='#1976d2')\n",
    "ax.text(7, 9.8, 'model = PPO(\"MlpPolicy\", env) → model.learn() → model.predict()', \n",
    "        ha='center', fontsize=10)\n",
    "\n",
    "# SB3 components\n",
    "components = [\n",
    "    ('Algorithms\\n(PPO, A2C, SAC...)', 2, 7, '#c8e6c9', '#388e3c'),\n",
    "    ('Policies\\n(MLP, CNN)', 6, 7, '#fff3e0', '#f57c00'),\n",
    "    ('Vectorized Envs\\n(DummyVec, Subproc)', 10, 7, '#e1bee7', '#7b1fa2'),\n",
    "]\n",
    "\n",
    "for text, x, y, fcolor, ecolor in components:\n",
    "    box = FancyBboxPatch((x, y), 3, 1.8, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=fcolor, edgecolor=ecolor, linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 1.5, y + 0.9, text, ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Lower components\n",
    "lower_components = [\n",
    "    ('Rollout Buffer\\n(stores transitions)', 2, 4.5, '#bbdefb', '#1976d2'),\n",
    "    ('Callbacks\\n(logging, eval)', 6, 4.5, '#ffcdd2', '#d32f2f'),\n",
    "    ('Utils\\n(save, load, evaluate)', 10, 4.5, '#dcedc8', '#689f38'),\n",
    "]\n",
    "\n",
    "for text, x, y, fcolor, ecolor in lower_components:\n",
    "    box = FancyBboxPatch((x, y), 3, 1.8, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=fcolor, edgecolor=ecolor, linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 1.5, y + 0.9, text, ha='center', va='center', fontsize=9)\n",
    "\n",
    "# PyTorch + Gym foundation\n",
    "foundation_box = FancyBboxPatch((1, 2), 12, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                                 facecolor='#fafafa', edgecolor='#666', linewidth=2)\n",
    "ax.add_patch(foundation_box)\n",
    "ax.text(7, 2.75, 'Built on: PyTorch + Gymnasium', ha='center', fontsize=11)\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(7, 9.4), xytext=(7, 8.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSTABLE-BASELINES3 FEATURES:\")\n",
    "print(\"  • Clean, modular implementation\")\n",
    "print(\"  • Support for PPO, A2C, SAC, TD3, DQN...\")\n",
    "print(\"  • Vectorized environments for parallelism\")\n",
    "print(\"  • TensorBoard integration\")\n",
    "print(\"  • Easy save/load functionality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Start: Training PPO in 3 Lines\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              PPO IN 3 LINES!                                   │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  from stable_baselines3 import PPO                           │\n",
    "    │                                                                │\n",
    "    │  model = PPO('MlpPolicy', 'CartPole-v1')  # Create           │\n",
    "    │  model.learn(total_timesteps=10000)       # Train            │\n",
    "    │  model.save('ppo_cartpole')               # Save             │\n",
    "    │                                                                │\n",
    "    │  That's it! Production-ready PPO.                            │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    print(\"QUICK START: PPO IN 3 LINES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Line 1: Create the model\n",
    "    model = PPO('MlpPolicy', 'CartPole-v1', verbose=0)\n",
    "    print(\"\\n1. Created PPO model with MlpPolicy\")\n",
    "    \n",
    "    # Line 2: Train\n",
    "    print(\"\\n2. Training for 10,000 timesteps...\")\n",
    "    model.learn(total_timesteps=10_000)\n",
    "    print(\"   Done!\")\n",
    "    \n",
    "    # Line 3: Save (we'll skip actual saving for demo)\n",
    "    print(\"\\n3. Model ready to save with: model.save('ppo_cartpole')\")\n",
    "    \n",
    "    # Bonus: Evaluate\n",
    "    env = gym.make('CartPole-v1')\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    print(f\"\\nEvaluation: {mean_reward:.1f} ± {std_reward:.1f} reward\")\n",
    "    env.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"Install Stable-Baselines3 to run this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Understanding the PPO Hyperparameters\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              PPO HYPERPARAMETERS EXPLAINED                     │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  CORE PPO PARAMETERS:                                         │\n",
    "    │    clip_range=0.2      Clipping ε (how much policy can change)│\n",
    "    │    n_steps=2048        Steps before update (rollout length)   │\n",
    "    │    batch_size=64       Minibatch size for SGD                 │\n",
    "    │    n_epochs=10         Epochs per update                      │\n",
    "    │                                                                │\n",
    "    │  LEARNING PARAMETERS:                                         │\n",
    "    │    learning_rate=3e-4  Adam learning rate                     │\n",
    "    │    gamma=0.99          Discount factor                        │\n",
    "    │    gae_lambda=0.95     GAE lambda (bias-variance tradeoff)    │\n",
    "    │                                                                │\n",
    "    │  LOSS COEFFICIENTS:                                           │\n",
    "    │    ent_coef=0.0        Entropy bonus (exploration)            │\n",
    "    │    vf_coef=0.5         Value function loss weight             │\n",
    "    │                                                                │\n",
    "    │  ENVIRONMENT:                                                 │\n",
    "    │    n_envs              Number of parallel environments        │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    print(\"PPO HYPERPARAMETERS IN STABLE-BASELINES3\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create PPO with custom hyperparameters\n",
    "    custom_model = PPO(\n",
    "        policy='MlpPolicy',\n",
    "        env='CartPole-v1',\n",
    "        \n",
    "        # Core PPO\n",
    "        learning_rate=3e-4,      # Adam learning rate\n",
    "        n_steps=2048,            # Steps per rollout\n",
    "        batch_size=64,           # Minibatch size\n",
    "        n_epochs=10,             # Epochs per update\n",
    "        clip_range=0.2,          # PPO clipping epsilon\n",
    "        \n",
    "        # Advantage estimation\n",
    "        gamma=0.99,              # Discount factor\n",
    "        gae_lambda=0.95,         # GAE lambda\n",
    "        \n",
    "        # Loss coefficients\n",
    "        ent_coef=0.01,           # Entropy coefficient\n",
    "        vf_coef=0.5,             # Value function coefficient\n",
    "        max_grad_norm=0.5,       # Gradient clipping\n",
    "        \n",
    "        # Misc\n",
    "        verbose=0,               # 0: none, 1: training info\n",
    "        seed=42,                 # Random seed\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel created with custom hyperparameters:\")\n",
    "    print(f\"  learning_rate: {custom_model.learning_rate}\")\n",
    "    print(f\"  n_steps: {custom_model.n_steps}\")\n",
    "    print(f\"  batch_size: {custom_model.batch_size}\")\n",
    "    print(f\"  n_epochs: {custom_model.n_epochs}\")\n",
    "    print(f\"  clip_range: {custom_model.clip_range}\")\n",
    "    print(f\"  gamma: {custom_model.gamma}\")\n",
    "    print(f\"  gae_lambda: {custom_model.gae_lambda}\")\n",
    "    print(f\"  ent_coef: {custom_model.ent_coef}\")\n",
    "    print(f\"  vf_coef: {custom_model.vf_coef}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"Install Stable-Baselines3 to run this example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter effects\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top left: Clip range effect\n",
    "ax1 = axes[0, 0]\n",
    "clip_ranges = [0.1, 0.2, 0.3, 0.4]\n",
    "stability = [0.95, 0.9, 0.7, 0.4]  # Made up for illustration\n",
    "learning_speed = [0.5, 0.8, 0.9, 0.95]\n",
    "\n",
    "x = np.arange(len(clip_ranges))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, stability, width, label='Stability', color='#4caf50')\n",
    "ax1.bar(x + width/2, learning_speed, width, label='Learning Speed', color='#2196f3')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f'ε={c}' for c in clip_ranges])\n",
    "ax1.set_ylabel('Score (normalized)', fontsize=10)\n",
    "ax1.set_title('Clip Range (ε) Effect', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.axvline(x=1, color='red', linestyle='--', alpha=0.5)\n",
    "ax1.text(1, 1.05, 'Default', ha='center', fontsize=9, color='red')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Top right: n_steps effect\n",
    "ax2 = axes[0, 1]\n",
    "n_steps_vals = [128, 256, 512, 1024, 2048, 4096]\n",
    "variance = [0.9, 0.75, 0.6, 0.45, 0.35, 0.3]\n",
    "memory = [0.1, 0.15, 0.25, 0.4, 0.6, 0.85]\n",
    "\n",
    "ax2.plot(n_steps_vals, variance, 'o-', linewidth=2, color='#f44336', label='Gradient Variance')\n",
    "ax2.plot(n_steps_vals, memory, 's-', linewidth=2, color='#ff9800', label='Memory Usage')\n",
    "ax2.axvline(x=2048, color='green', linestyle='--', alpha=0.5)\n",
    "ax2.text(2048, 1.0, 'Default', ha='center', fontsize=9, color='green')\n",
    "ax2.set_xlabel('n_steps', fontsize=10)\n",
    "ax2.set_ylabel('Score (normalized)', fontsize=10)\n",
    "ax2.set_title('Rollout Length (n_steps) Effect', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom left: Entropy coefficient\n",
    "ax3 = axes[1, 0]\n",
    "ent_coefs = [0.0, 0.001, 0.01, 0.05, 0.1]\n",
    "exploration = [0.2, 0.4, 0.7, 0.85, 0.95]\n",
    "exploitation = [0.95, 0.9, 0.75, 0.5, 0.3]\n",
    "\n",
    "ax3.fill_between(ent_coefs, 0, exploration, alpha=0.3, color='#2196f3', label='Exploration')\n",
    "ax3.fill_between(ent_coefs, 0, exploitation, alpha=0.3, color='#4caf50', label='Exploitation')\n",
    "ax3.plot(ent_coefs, exploration, 'b-', linewidth=2)\n",
    "ax3.plot(ent_coefs, exploitation, 'g-', linewidth=2)\n",
    "ax3.axvline(x=0.01, color='red', linestyle='--', alpha=0.5)\n",
    "ax3.text(0.01, 1.0, 'Good\\nbalance', ha='center', fontsize=9, color='red')\n",
    "ax3.set_xlabel('Entropy Coefficient', fontsize=10)\n",
    "ax3.set_ylabel('Behavior', fontsize=10)\n",
    "ax3.set_title('Entropy Coefficient (ent_coef) Effect', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom right: Learning rate\n",
    "ax4 = axes[1, 1]\n",
    "epochs = np.arange(100)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulated learning curves\n",
    "lr_low = 100 * (1 - np.exp(-epochs/80)) + np.random.randn(100) * 5\n",
    "lr_good = 100 * (1 - np.exp(-epochs/30)) + np.random.randn(100) * 3\n",
    "lr_high = 100 * (1 - np.exp(-epochs/10)) * np.exp(-epochs/50) + np.random.randn(100) * 10\n",
    "\n",
    "ax4.plot(epochs, lr_low, alpha=0.7, linewidth=2, label='lr=1e-5 (too slow)', color='#2196f3')\n",
    "ax4.plot(epochs, lr_good, alpha=0.7, linewidth=2, label='lr=3e-4 (good)', color='#4caf50')\n",
    "ax4.plot(epochs, lr_high, alpha=0.7, linewidth=2, label='lr=1e-2 (unstable)', color='#f44336')\n",
    "ax4.set_xlabel('Epoch', fontsize=10)\n",
    "ax4.set_ylabel('Performance', fontsize=10)\n",
    "ax4.set_title('Learning Rate Effect', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHYPERPARAMETER TUNING TIPS:\")\n",
    "print(\"  • Start with defaults - they're well-tuned!\")\n",
    "print(\"  • clip_range: 0.1-0.3 (smaller = more stable)\")\n",
    "print(\"  • n_steps: 2048 works well for most tasks\")\n",
    "print(\"  • ent_coef: Increase if agent gets stuck\")\n",
    "print(\"  • learning_rate: 3e-4 is a good starting point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training with Vectorized Environments\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              VECTORIZED ENVIRONMENTS                           │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  WHY VECTORIZE?                                               │\n",
    "    │    • Collect data from N envs in parallel                    │\n",
    "    │    • N× faster data collection                               │\n",
    "    │    • Better GPU utilization                                  │\n",
    "    │    • More diverse data per update                            │\n",
    "    │                                                                │\n",
    "    │  SB3 OPTIONS:                                                 │\n",
    "    │                                                                │\n",
    "    │  DummyVecEnv:                                                 │\n",
    "    │    • Single process, sequential                              │\n",
    "    │    • Simple, no overhead                                     │\n",
    "    │    • Good for fast environments                              │\n",
    "    │                                                                │\n",
    "    │  SubprocVecEnv:                                               │\n",
    "    │    • Multiple processes, true parallelism                    │\n",
    "    │    • Good for slow environments                              │\n",
    "    │    • Some communication overhead                             │\n",
    "    │                                                                │\n",
    "    │  make_vec_env(): Easy wrapper for either!                    │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    print(\"TRAINING WITH VECTORIZED ENVIRONMENTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create 4 parallel environments\n",
    "    n_envs = 4\n",
    "    vec_env = make_vec_env('CartPole-v1', n_envs=n_envs)\n",
    "    \n",
    "    print(f\"\\nCreated {n_envs} parallel environments\")\n",
    "    print(f\"  Environment: CartPole-v1\")\n",
    "    print(f\"  Observation space: {vec_env.observation_space}\")\n",
    "    print(f\"  Action space: {vec_env.action_space}\")\n",
    "    \n",
    "    # Create PPO with vectorized env\n",
    "    model = PPO(\n",
    "        'MlpPolicy',\n",
    "        vec_env,\n",
    "        verbose=1,\n",
    "        n_steps=512,      # Steps per env before update\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "    )\n",
    "    \n",
    "    # Total steps per update = n_steps × n_envs = 512 × 4 = 2048\n",
    "    print(f\"\\nSteps per update: {model.n_steps} × {n_envs} = {model.n_steps * n_envs}\")\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nTraining for 20,000 timesteps...\")\n",
    "    model.learn(total_timesteps=20_000)\n",
    "    \n",
    "    # Evaluate\n",
    "    mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=10)\n",
    "    print(f\"\\nEvaluation: {mean_reward:.1f} ± {std_reward:.1f} reward\")\n",
    "    \n",
    "    vec_env.close()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"Install Stable-Baselines3 to run this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monitoring Training with Callbacks\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              CALLBACKS: MONITOR AND CONTROL TRAINING           │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  CALLBACKS LET YOU:                                           │\n",
    "    │    • Log custom metrics                                       │\n",
    "    │    • Save best models automatically                          │\n",
    "    │    • Early stopping                                          │\n",
    "    │    • Visualize during training                               │\n",
    "    │                                                                │\n",
    "    │  BUILT-IN CALLBACKS:                                          │\n",
    "    │    EvalCallback:       Periodic evaluation + save best       │\n",
    "    │    CheckpointCallback: Save model every N steps              │\n",
    "    │    StopTrainingOnReward: Early stopping                      │\n",
    "    │                                                                │\n",
    "    │  CUSTOM CALLBACKS:                                            │\n",
    "    │    Inherit from BaseCallback and override:                   │\n",
    "    │    • _on_step(): Called every step                           │\n",
    "    │    • _on_rollout_start/end(): Rollout boundaries            │\n",
    "    │    • _on_training_start/end(): Training boundaries          │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    # Custom callback example\n",
    "    class RewardLoggerCallback(BaseCallback):\n",
    "        \"\"\"\n",
    "        Custom callback to track rewards during training.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, verbose=0):\n",
    "            super().__init__(verbose)\n",
    "            self.episode_rewards = []\n",
    "            self.current_rewards = None\n",
    "        \n",
    "        def _on_training_start(self):\n",
    "            \"\"\"Called at the start of training.\"\"\"\n",
    "            self.current_rewards = np.zeros(self.training_env.num_envs)\n",
    "        \n",
    "        def _on_step(self) -> bool:\n",
    "            \"\"\"\n",
    "            Called after every step.\n",
    "            \n",
    "            Returns:\n",
    "                bool: If False, stop training.\n",
    "            \"\"\"\n",
    "            # Track rewards\n",
    "            self.current_rewards += self.locals['rewards']\n",
    "            \n",
    "            # Check for episode ends\n",
    "            for i, done in enumerate(self.locals['dones']):\n",
    "                if done:\n",
    "                    self.episode_rewards.append(self.current_rewards[i])\n",
    "                    self.current_rewards[i] = 0\n",
    "            \n",
    "            return True  # Continue training\n",
    "        \n",
    "        def _on_training_end(self):\n",
    "            \"\"\"Called at the end of training.\"\"\"\n",
    "            if self.verbose > 0:\n",
    "                print(f\"\\nTraining complete! {len(self.episode_rewards)} episodes\")\n",
    "    \n",
    "    print(\"TRAINING WITH CUSTOM CALLBACK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create environment and callback\n",
    "    env = make_vec_env('CartPole-v1', n_envs=4)\n",
    "    reward_callback = RewardLoggerCallback(verbose=1)\n",
    "    \n",
    "    # Create and train model with callback\n",
    "    model = PPO('MlpPolicy', env, verbose=0)\n",
    "    model.learn(total_timesteps=20_000, callback=reward_callback)\n",
    "    \n",
    "    # Plot rewards from callback\n",
    "    if len(reward_callback.episode_rewards) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.plot(reward_callback.episode_rewards, alpha=0.3, color='blue')\n",
    "        \n",
    "        # Smoothed\n",
    "        window = min(20, len(reward_callback.episode_rewards) // 3)\n",
    "        if window > 1:\n",
    "            smoothed = np.convolve(reward_callback.episode_rewards, \n",
    "                                   np.ones(window)/window, mode='valid')\n",
    "            ax.plot(range(window-1, len(reward_callback.episode_rewards)), \n",
    "                    smoothed, 'r-', linewidth=2, label='Smoothed')\n",
    "        \n",
    "        ax.set_xlabel('Episode', fontsize=11)\n",
    "        ax.set_ylabel('Reward', fontsize=11)\n",
    "        ax.set_title('Training Progress (via Custom Callback)', fontsize=12, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    env.close()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"Install Stable-Baselines3 to run this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Saving and Loading Models\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              SAVING AND LOADING MODELS                         │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  SAVE MODEL:                                                  │\n",
    "    │    model.save(\"ppo_cartpole\")                                │\n",
    "    │    # Creates: ppo_cartpole.zip                               │\n",
    "    │                                                                │\n",
    "    │  LOAD MODEL:                                                  │\n",
    "    │    model = PPO.load(\"ppo_cartpole\")                          │\n",
    "    │    # Ready to use!                                           │\n",
    "    │                                                                │\n",
    "    │  CONTINUE TRAINING:                                           │\n",
    "    │    model = PPO.load(\"ppo_cartpole\", env=env)                 │\n",
    "    │    model.learn(total_timesteps=more_steps)                   │\n",
    "    │                                                                │\n",
    "    │  WHAT'S SAVED:                                                │\n",
    "    │    • Policy network weights                                  │\n",
    "    │    • Value network weights                                   │\n",
    "    │    • Optimizer state                                         │\n",
    "    │    • Hyperparameters                                         │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    import tempfile\n",
    "    import os\n",
    "    \n",
    "    print(\"SAVING AND LOADING MODELS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create and train a model\n",
    "    env = make_vec_env('CartPole-v1', n_envs=4)\n",
    "    model = PPO('MlpPolicy', env, verbose=0)\n",
    "    \n",
    "    print(\"\\n1. Training original model...\")\n",
    "    model.learn(total_timesteps=10_000)\n",
    "    \n",
    "    # Evaluate\n",
    "    mean_reward1, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    print(f\"   Original model reward: {mean_reward1:.1f}\")\n",
    "    \n",
    "    # Save to temp directory\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        save_path = os.path.join(tmpdir, \"ppo_cartpole\")\n",
    "        \n",
    "        print(f\"\\n2. Saving model to: {save_path}\")\n",
    "        model.save(save_path)\n",
    "        \n",
    "        # Check file exists\n",
    "        print(f\"   File created: {os.path.exists(save_path + '.zip')}\")\n",
    "        \n",
    "        # Load model\n",
    "        print(\"\\n3. Loading model...\")\n",
    "        loaded_model = PPO.load(save_path)\n",
    "        \n",
    "        # Evaluate loaded model\n",
    "        mean_reward2, _ = evaluate_policy(loaded_model, env, n_eval_episodes=10)\n",
    "        print(f\"   Loaded model reward: {mean_reward2:.1f}\")\n",
    "        \n",
    "        # Continue training\n",
    "        print(\"\\n4. Continue training loaded model...\")\n",
    "        loaded_model.set_env(env)\n",
    "        loaded_model.learn(total_timesteps=10_000)\n",
    "        \n",
    "        mean_reward3, _ = evaluate_policy(loaded_model, env, n_eval_episodes=10)\n",
    "        print(f\"   After more training: {mean_reward3:.1f}\")\n",
    "    \n",
    "    env.close()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"Install Stable-Baselines3 to run this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete Training Example\n",
    "\n",
    "Let's put it all together with a proper training setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    print(\"COMPLETE PPO TRAINING EXAMPLE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. Environment Setup\n",
    "    # ========================================\n",
    "    n_envs = 4\n",
    "    env = make_vec_env('CartPole-v1', n_envs=n_envs)\n",
    "    \n",
    "    print(f\"\\n1. Created {n_envs} parallel environments\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. Model Creation with Good Hyperparameters\n",
    "    # ========================================\n",
    "    model = PPO(\n",
    "        policy='MlpPolicy',\n",
    "        env=env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=512,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=0.01,\n",
    "        vf_coef=0.5,\n",
    "        verbose=0,\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n2. Created PPO model with optimized hyperparameters\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. Training with Progress Tracking\n",
    "    # ========================================\n",
    "    reward_callback = RewardLoggerCallback()\n",
    "    \n",
    "    print(\"\\n3. Training for 50,000 timesteps...\")\n",
    "    model.learn(\n",
    "        total_timesteps=50_000,\n",
    "        callback=reward_callback,\n",
    "        progress_bar=True  # Shows nice progress bar\n",
    "    )\n",
    "    \n",
    "    # ========================================\n",
    "    # 4. Evaluation\n",
    "    # ========================================\n",
    "    print(\"\\n4. Evaluating trained model...\")\n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model, env, n_eval_episodes=20, deterministic=True\n",
    "    )\n",
    "    print(f\"   Result: {mean_reward:.1f} ± {std_reward:.1f}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 5. Visualization\n",
    "    # ========================================\n",
    "    if len(reward_callback.episode_rewards) > 10:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Learning curve\n",
    "        ax1 = axes[0]\n",
    "        rewards = reward_callback.episode_rewards\n",
    "        ax1.plot(rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "        \n",
    "        window = min(30, len(rewards) // 3)\n",
    "        if window > 1:\n",
    "            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            ax1.plot(range(window-1, len(rewards)), smoothed, \n",
    "                     'r-', linewidth=2, label='Smoothed')\n",
    "        \n",
    "        ax1.axhline(y=500, color='green', linestyle='--', label='Max Score')\n",
    "        ax1.set_xlabel('Episode', fontsize=11)\n",
    "        ax1.set_ylabel('Reward', fontsize=11)\n",
    "        ax1.set_title('Training Progress', fontsize=12, fontweight='bold')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Reward distribution\n",
    "        ax2 = axes[1]\n",
    "        ax2.hist(rewards[-100:] if len(rewards) > 100 else rewards, \n",
    "                 bins=20, color='#64b5f6', edgecolor='black')\n",
    "        ax2.axvline(x=mean_reward, color='red', linewidth=2, \n",
    "                    label=f'Mean: {mean_reward:.1f}')\n",
    "        ax2.set_xlabel('Reward', fontsize=11)\n",
    "        ax2.set_ylabel('Count', fontsize=11)\n",
    "        ax2.set_title('Final Reward Distribution', fontsize=12, fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(f\"  Episodes completed: {len(reward_callback.episode_rewards)}\")\n",
    "    print(f\"  Final performance: {mean_reward:.1f} ± {std_reward:.1f}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"Install Stable-Baselines3 to run this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### SB3 Quick Reference\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Create model | `model = PPO('MlpPolicy', env)` |\n",
    "| Train | `model.learn(total_timesteps=10000)` |\n",
    "| Evaluate | `evaluate_policy(model, env)` |\n",
    "| Save | `model.save('my_model')` |\n",
    "| Load | `model = PPO.load('my_model')` |\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `learning_rate` | 3e-4 | Adam learning rate |\n",
    "| `n_steps` | 2048 | Steps per rollout |\n",
    "| `batch_size` | 64 | Minibatch size |\n",
    "| `n_epochs` | 10 | Epochs per update |\n",
    "| `clip_range` | 0.2 | PPO clipping |\n",
    "\n",
    "### Vectorized Environments\n",
    "\n",
    "```python\n",
    "env = make_vec_env('CartPole-v1', n_envs=4)\n",
    "```\n",
    "\n",
    "Benefits: 4× faster data collection, more diverse gradients!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why use Stable-Baselines3 instead of implementing from scratch?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "SB3 provides:\n",
    "- Battle-tested, bug-free implementations\n",
    "- Optimized performance\n",
    "- Rich features (logging, callbacks, saving)\n",
    "- Good defaults that work well\n",
    "- Easy to reproduce results\n",
    "\n",
    "Use from-scratch for learning and research; SB3 for production and benchmarks.\n",
    "</details>\n",
    "\n",
    "**2. What does n_envs=4 do?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "It creates 4 parallel copies of the environment. Benefits:\n",
    "- 4× faster data collection\n",
    "- More diverse experiences per update\n",
    "- Better GPU utilization\n",
    "- Total steps per update = n_steps × n_envs\n",
    "</details>\n",
    "\n",
    "**3. What's the purpose of callbacks?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Callbacks let you:\n",
    "- Log custom metrics during training\n",
    "- Save the best model automatically\n",
    "- Implement early stopping\n",
    "- Visualize training progress\n",
    "- Execute custom code at specific training events\n",
    "</details>\n",
    "\n",
    "**4. How do you continue training a saved model?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "```python\n",
    "# Load the model\n",
    "model = PPO.load(\"my_model\")\n",
    "\n",
    "# Set the environment\n",
    "model.set_env(env)\n",
    "\n",
    "# Continue training\n",
    "model.learn(total_timesteps=more_steps)\n",
    "```\n",
    "</details>\n",
    "\n",
    "**5. Which hyperparameter should you adjust first if training is unstable?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Try reducing the learning rate first (e.g., from 3e-4 to 1e-4). Other options:\n",
    "- Reduce clip_range (e.g., 0.2 → 0.1)\n",
    "- Increase n_steps for more stable gradients\n",
    "- Reduce batch_size for more frequent updates\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "You've mastered PPO with Stable-Baselines3! In the next notebook, we'll explore **SAC (Soft Actor-Critic)** for continuous control tasks.\n",
    "\n",
    "**Continue to:** [Notebook 4: SAC for Continuous Control](04_sac_continuous_control.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*SB3: \"Professional tools for professional results!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
