{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO From Scratch: The Algorithm Behind ChatGPT\n",
    "\n",
    "Welcome to PPO - the most popular and practical RL algorithm today! PPO is used to train ChatGPT and many other AI systems.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- Why policy gradient methods need constraints (with a driving analogy!)\n",
    "- What PPO does and why it works\n",
    "- The clipping trick (the secret sauce of PPO)\n",
    "- Actor-Critic architecture\n",
    "- How to implement PPO from scratch\n",
    "- Train an agent on CartPole!\n",
    "\n",
    "**Prerequisites:** `policy-gradient/` notebooks (REINFORCE, Actor-Critic)\n",
    "\n",
    "**Time:** ~45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: Why PPO?\n",
    "\n",
    "### The Problem with Vanilla Policy Gradient\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │            THE POLICY UPDATE PROBLEM                    │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  VANILLA POLICY GRADIENT:                              │\n",
    "    │    \"If an action worked, make it MORE likely\"          │\n",
    "    │                                                         │\n",
    "    │  THE PROBLEM:                                          │\n",
    "    │    Updates can be TOO LARGE!                           │\n",
    "    │                                                         │\n",
    "    │    Before: π(action) = 30%                             │\n",
    "    │    After:  π(action) = 95%   ← Too much change!        │\n",
    "    │                                                         │\n",
    "    │    This can:                                           │\n",
    "    │    • Destabilize training                              │\n",
    "    │    • Cause the policy to \"forget\" good behavior        │\n",
    "    │    • Lead to catastrophic performance drops            │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### The Driving Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │            PPO = CAREFUL DRIVING                        │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  VANILLA POLICY GRADIENT = Aggressive Driver           │\n",
    "    │    \"Turn the wheel as hard as possible!\"               │\n",
    "    │    → Sometimes overshoots and crashes                  │\n",
    "    │                                                         │\n",
    "    │  PPO = Careful Driver                                  │\n",
    "    │    \"Turn the wheel, but not too much at once\"          │\n",
    "    │    → Smooth, stable progress                           │\n",
    "    │                                                         │\n",
    "    │  The key insight:                                      │\n",
    "    │    LIMIT how much the policy can change in one update! │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PPO's Solution: The Clipping Trick\n",
    "\n",
    "PPO limits policy updates using a clever clipping mechanism:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │              PPO CLIPPING EXPLAINED                     │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  First, define the RATIO of new to old policy:         │\n",
    "    │                                                         │\n",
    "    │         π_new(a|s)                                     │\n",
    "    │  r(θ) = ─────────── = \"How much did policy change?\"   │\n",
    "    │         π_old(a|s)                                     │\n",
    "    │                                                         │\n",
    "    │  • r = 1.0: Policy unchanged                           │\n",
    "    │  • r = 2.0: Action is now 2x more likely               │\n",
    "    │  • r = 0.5: Action is now half as likely               │\n",
    "    │                                                         │\n",
    "    │  Then, CLIP the ratio to stay close to 1:              │\n",
    "    │                                                         │\n",
    "    │  clipped_r = clip(r, 1-ε, 1+ε)                         │\n",
    "    │                                                         │\n",
    "    │  With ε = 0.2:                                         │\n",
    "    │    • r can only be between 0.8 and 1.2                 │\n",
    "    │    • Max 20% change per update!                        │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym\n",
    "\n",
    "# Visualize the PPO clipping\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: The ratio and clipping\n",
    "ax1 = axes[0]\n",
    "epsilon = 0.2\n",
    "\n",
    "# Ratio range\n",
    "r = np.linspace(0.5, 2.0, 100)\n",
    "\n",
    "# Clipped ratio\n",
    "clipped_r = np.clip(r, 1 - epsilon, 1 + epsilon)\n",
    "\n",
    "ax1.plot(r, r, 'b-', linewidth=2, label='Original ratio r(θ)')\n",
    "ax1.plot(r, clipped_r, 'r-', linewidth=3, label='Clipped ratio')\n",
    "ax1.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.axhline(y=1+epsilon, color='green', linestyle=':', linewidth=2, label=f'Upper limit (1+ε = {1+epsilon})')\n",
    "ax1.axhline(y=1-epsilon, color='orange', linestyle=':', linewidth=2, label=f'Lower limit (1-ε = {1-epsilon})')\n",
    "\n",
    "ax1.fill_between(r, 1-epsilon, 1+epsilon, alpha=0.2, color='green', label='Allowed range')\n",
    "\n",
    "ax1.set_xlabel('Policy Ratio r(θ) = π_new / π_old', fontsize=12)\n",
    "ax1.set_ylabel('Effective Ratio (after clipping)', fontsize=12)\n",
    "ax1.set_title('PPO Clipping Mechanism\\n(ε = 0.2)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.set_xlim(0.5, 2.0)\n",
    "ax1.set_ylim(0.5, 2.0)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Effect on objective\n",
    "ax2 = axes[1]\n",
    "\n",
    "# For positive advantage (good action)\n",
    "advantage = 1.0\n",
    "objective = r * advantage\n",
    "clipped_objective = clipped_r * advantage\n",
    "ppo_objective = np.minimum(objective, clipped_objective)\n",
    "\n",
    "ax2.plot(r, objective, 'b--', linewidth=2, alpha=0.5, label='Unclipped objective')\n",
    "ax2.plot(r, ppo_objective, 'r-', linewidth=3, label='PPO objective (min of both)')\n",
    "ax2.axvline(x=1, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax2.fill_between(r, 0, ppo_objective, alpha=0.2, color='red')\n",
    "\n",
    "ax2.set_xlabel('Policy Ratio r(θ)', fontsize=12)\n",
    "ax2.set_ylabel('Objective Value', fontsize=12)\n",
    "ax2.set_title('PPO Objective (Positive Advantage)\\n\"Good action - increase probability\"', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.set_xlim(0.5, 2.0)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHT: Why Clipping Works\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n• When ratio > 1.2: No extra reward for going higher\")\n",
    "print(\"  → \"You've increased enough, stop pushing!\"\")\n",
    "print(\"\\n• When ratio < 0.8: No extra penalty for going lower\")\n",
    "print(\"  → \"You've decreased enough, stop pulling!\"\")\n",
    "print(\"\\n• This keeps updates STABLE and SAFE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The PPO-Clip Objective\n",
    "\n",
    "The full PPO-Clip objective is:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │              PPO-CLIP OBJECTIVE                         │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  L^CLIP(θ) = E[ min( r(θ) × A,                         │\n",
    "    │                      clip(r(θ), 1-ε, 1+ε) × A ) ]      │\n",
    "    │                                                         │\n",
    "    │  Where:                                                │\n",
    "    │    r(θ) = π_new(a|s) / π_old(a|s)  (probability ratio) │\n",
    "    │    A    = Advantage (how much better than average?)    │\n",
    "    │    ε    = Clip parameter (usually 0.2)                 │\n",
    "    │                                                         │\n",
    "    │  The MIN takes the MORE PESSIMISTIC option:            │\n",
    "    │    • For good actions (A > 0): caps the benefit        │\n",
    "    │    • For bad actions (A < 0): caps the penalty         │\n",
    "    │                                                         │\n",
    "    │  This prevents \"overshooting\" in either direction!     │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_clip_objective(old_log_probs, new_log_probs, advantages, clip_epsilon=0.2):\n",
    "    \"\"\"\n",
    "    Compute the PPO-Clip objective.\n",
    "    \n",
    "    Args:\n",
    "        old_log_probs: Log probabilities from the OLD policy\n",
    "        new_log_probs: Log probabilities from the NEW policy\n",
    "        advantages: How much better than average each action was\n",
    "        clip_epsilon: How much to clip (default 0.2 = 20%)\n",
    "    \n",
    "    Returns:\n",
    "        The PPO loss (negative because we want to maximize)\n",
    "    \"\"\"\n",
    "    # Step 1: Compute the probability RATIO\n",
    "    # r(θ) = π_new(a|s) / π_old(a|s)\n",
    "    # In log space: log(r) = log(π_new) - log(π_old)\n",
    "    # So: r = exp(log(π_new) - log(π_old))\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    \n",
    "    # Step 2: Compute the CLIPPED ratio\n",
    "    # Keep ratio between [1-ε, 1+ε]\n",
    "    clipped_ratio = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon)\n",
    "    \n",
    "    # Step 3: Compute both objectives\n",
    "    objective1 = ratio * advantages          # Unclipped\n",
    "    objective2 = clipped_ratio * advantages  # Clipped\n",
    "    \n",
    "    # Step 4: Take the MINIMUM (pessimistic bound)\n",
    "    # This prevents too large updates in either direction\n",
    "    ppo_objective = torch.min(objective1, objective2)\n",
    "    \n",
    "    # Return negative because we want to MAXIMIZE the objective\n",
    "    # (but optimizers MINIMIZE loss)\n",
    "    loss = -ppo_objective.mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# Demonstrate the loss function\n",
    "print(\"PPO LOSS FUNCTION DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example data\n",
    "old_log_probs = torch.tensor([-1.0, -0.5, -2.0])\n",
    "new_log_probs = torch.tensor([-0.5, -0.3, -2.5])  # Policy changed\n",
    "advantages = torch.tensor([1.0, -0.5, 2.0])  # Some good, some bad actions\n",
    "\n",
    "# Compute ratio\n",
    "ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "print(f\"\\nProbability ratios (π_new / π_old):\")\n",
    "for i, r in enumerate(ratio):\n",
    "    change = \"MORE likely\" if r > 1 else \"LESS likely\"\n",
    "    print(f\"  Action {i}: ratio = {r:.2f} ({change})\")\n",
    "\n",
    "# Compute loss\n",
    "loss = ppo_clip_objective(old_log_probs, new_log_probs, advantages)\n",
    "print(f\"\\nPPO Loss: {loss:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Actor-Critic Architecture\n",
    "\n",
    "PPO uses an **Actor-Critic** architecture:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │              ACTOR-CRITIC ARCHITECTURE                  │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │                    ┌─────────┐                         │\n",
    "    │                    │  State  │                         │\n",
    "    │                    └────┬────┘                         │\n",
    "    │                         │                              │\n",
    "    │                    ┌────▼────┐                         │\n",
    "    │                    │ Shared  │                         │\n",
    "    │                    │ Layers  │                         │\n",
    "    │                    └────┬────┘                         │\n",
    "    │                   ╱     │     ╲                        │\n",
    "    │                  ╱      │      ╲                       │\n",
    "    │           ┌─────▼───┐       ┌───▼─────┐               │\n",
    "    │           │  ACTOR  │       │ CRITIC  │               │\n",
    "    │           │ (Policy)│       │ (Value) │               │\n",
    "    │           └────┬────┘       └────┬────┘               │\n",
    "    │                │                 │                     │\n",
    "    │           ┌────▼────┐       ┌────▼────┐               │\n",
    "    │           │ Action  │       │  V(s)   │               │\n",
    "    │           │ Probs   │       │(Value)  │               │\n",
    "    │           └─────────┘       └─────────┘               │\n",
    "    │                                                         │\n",
    "    │  ACTOR: \"What action should I take?\"                   │\n",
    "    │  CRITIC: \"How good is this state?\"                     │\n",
    "    │                                                         │\n",
    "    │  They SHARE layers because state understanding is      │\n",
    "    │  useful for both!                                      │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic Network for PPO.\n",
    "    \n",
    "    Has two \"heads\":\n",
    "    - Actor head: outputs action probabilities (policy)\n",
    "    - Critic head: outputs state value V(s)\n",
    "    \n",
    "    They share early layers because understanding the state\n",
    "    is useful for both tasks!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ========================================\n",
    "        # SHARED LAYERS (Feature extraction)\n",
    "        # ========================================\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(),  # Tanh is common in policy networks\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # ACTOR HEAD (Policy: state → action probs)\n",
    "        # ========================================\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        # No softmax here - we'll use it when sampling\n",
    "        \n",
    "        # ========================================\n",
    "        # CRITIC HEAD (Value: state → V(s))\n",
    "        # ========================================\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Returns:\n",
    "            action_logits: Raw scores for each action\n",
    "            value: Estimated state value V(s)\n",
    "        \"\"\"\n",
    "        # Shared feature extraction\n",
    "        features = self.shared(x)\n",
    "        \n",
    "        # Actor head: action logits\n",
    "        action_logits = self.actor(features)\n",
    "        \n",
    "        # Critic head: state value\n",
    "        value = self.critic(features)\n",
    "        \n",
    "        return action_logits, value\n",
    "    \n",
    "    def get_action_and_value(self, state, action=None):\n",
    "        \"\"\"\n",
    "        Get action, log probability, entropy, and value.\n",
    "        \n",
    "        If action is provided, compute its log prob.\n",
    "        If not, sample a new action.\n",
    "        \"\"\"\n",
    "        logits, value = self.forward(state)\n",
    "        \n",
    "        # Create categorical distribution from logits\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        \n",
    "        # Sample action if not provided\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        \n",
    "        # Compute log probability and entropy\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return action, log_prob, entropy, value.squeeze(-1)\n",
    "\n",
    "\n",
    "# Create and demonstrate the network\n",
    "print(\"ACTOR-CRITIC NETWORK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "network = ActorCriticNetwork(state_dim=4, action_dim=2)\n",
    "print(f\"\\nNetwork architecture:\")\n",
    "print(network)\n",
    "\n",
    "# Test with sample state\n",
    "sample_state = torch.FloatTensor([[0.01, 0.02, -0.03, 0.04]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    action, log_prob, entropy, value = network.get_action_and_value(sample_state)\n",
    "\n",
    "print(f\"\\nSample state: {sample_state.numpy()[0]}\")\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  Action sampled: {action.item()} ({'LEFT' if action.item() == 0 else 'RIGHT'})\")\n",
    "print(f\"  Log probability: {log_prob.item():.4f}\")\n",
    "print(f\"  Entropy: {entropy.item():.4f}\")\n",
    "print(f\"  State value V(s): {value.item():.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Generalized Advantage Estimation (GAE)\n",
    "\n",
    "PPO uses **GAE** to compute advantages, which balances bias and variance:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │      GENERALIZED ADVANTAGE ESTIMATION (GAE)             │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  ADVANTAGE = How much BETTER was this action than      │\n",
    "    │              what we expected?                          │\n",
    "    │                                                         │\n",
    "    │  A_t = r_t + γV(s_{t+1}) - V(s_t)    ← TD residual      │\n",
    "    │        ↑     ↑                 ↑                        │\n",
    "    │        │     │                 └── What we expected     │\n",
    "    │        │     └── Future value (discounted)             │\n",
    "    │        └── Immediate reward                            │\n",
    "    │                                                         │\n",
    "    │  GAE combines multiple TD residuals:                   │\n",
    "    │                                                         │\n",
    "    │  A^GAE_t = δ_t + (γλ)δ_{t+1} + (γλ)²δ_{t+2} + ...     │\n",
    "    │                                                         │\n",
    "    │  λ = 0: Use only one-step TD (high bias, low variance) │\n",
    "    │  λ = 1: Use full returns (low bias, high variance)     │\n",
    "    │  λ = 0.95: Good balance (typical choice)               │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation (GAE).\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards [r_0, r_1, ..., r_T]\n",
    "        values: List of value estimates [V(s_0), V(s_1), ..., V(s_T), V(s_{T+1})]\n",
    "        dones: List of done flags [done_0, done_1, ..., done_T]\n",
    "        gamma: Discount factor\n",
    "        lam: GAE lambda (trade-off between bias and variance)\n",
    "    \n",
    "    Returns:\n",
    "        advantages: GAE advantages for each timestep\n",
    "        returns: Discounted returns (advantages + values)\n",
    "    \"\"\"\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    \n",
    "    # Work backwards through time\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        # TD residual: δ_t = r_t + γV(s_{t+1}) - V(s_t)\n",
    "        if dones[t]:\n",
    "            # Terminal state: no future value\n",
    "            delta = rewards[t] - values[t]\n",
    "            gae = delta\n",
    "        else:\n",
    "            delta = rewards[t] + gamma * values[t + 1] - values[t]\n",
    "            gae = delta + gamma * lam * gae\n",
    "        \n",
    "        advantages.insert(0, gae)\n",
    "    \n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    \n",
    "    # Returns = advantages + values\n",
    "    returns = advantages + torch.tensor(values[:-1], dtype=torch.float32)\n",
    "    \n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "# Demonstrate GAE\n",
    "print(\"GAE DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example trajectory\n",
    "rewards = [1, 1, 1, 1, 10]  # Normal steps, then big reward\n",
    "values = [5.0, 5.5, 6.0, 7.0, 9.0, 0.0]  # V(s) estimates (last is terminal)\n",
    "dones = [False, False, False, False, True]\n",
    "\n",
    "advantages, returns = compute_gae(rewards, values, dones)\n",
    "\n",
    "print(f\"\\nRewards:    {rewards}\")\n",
    "print(f\"Values:     {values[:-1]}\")\n",
    "print(f\"Dones:      {dones}\")\n",
    "print(f\"\\nAdvantages: {advantages.tolist()}\")\n",
    "print(f\"Returns:    {returns.tolist()}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Interpretation:\")\n",
    "print(\"  Positive advantage = Action was BETTER than expected\")\n",
    "print(\"  Negative advantage = Action was WORSE than expected\")\n",
    "print(\"  The last action got a big reward, so its advantage is high!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Complete PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    Complete PPO Agent.\n",
    "    \n",
    "    This is the algorithm used to train ChatGPT and many other AI systems!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, \n",
    "                 lam=0.95, clip_epsilon=0.2, epochs=10, batch_size=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_dim: Size of state observations\n",
    "            action_dim: Number of possible actions\n",
    "            lr: Learning rate\n",
    "            gamma: Discount factor\n",
    "            lam: GAE lambda\n",
    "            clip_epsilon: PPO clip parameter\n",
    "            epochs: Number of epochs per update\n",
    "            batch_size: Mini-batch size for updates\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Actor-Critic network\n",
    "        self.network = ActorCriticNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        # Storage for trajectory\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action and store data for later update.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = self.network.get_action_and_value(state_tensor)\n",
    "        \n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "    \n",
    "    def store(self, state, action, log_prob, reward, value, done):\n",
    "        \"\"\"Store one step of interaction.\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def update(self, next_value=0):\n",
    "        \"\"\"\n",
    "        Update the policy using collected trajectories.\n",
    "        \n",
    "        This is where the PPO magic happens!\n",
    "        \"\"\"\n",
    "        # Add next value for GAE computation\n",
    "        values = self.values + [next_value]\n",
    "        \n",
    "        # Compute advantages and returns using GAE\n",
    "        advantages, returns = compute_gae(\n",
    "            self.rewards, values, self.dones, self.gamma, self.lam\n",
    "        )\n",
    "        \n",
    "        # Normalize advantages (important for stable training!)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(self.states))\n",
    "        actions = torch.LongTensor(self.actions)\n",
    "        old_log_probs = torch.FloatTensor(self.log_probs)\n",
    "        \n",
    "        # ========================================\n",
    "        # PPO UPDATE: Multiple epochs over the data\n",
    "        # ========================================\n",
    "        total_loss = 0\n",
    "        n_samples = len(self.states)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            # Random permutation for mini-batches\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            \n",
    "            for start in range(0, n_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                # Get batch data\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                \n",
    "                # Get new log probs and values\n",
    "                _, new_log_probs, entropy, new_values = self.network.get_action_and_value(\n",
    "                    batch_states, batch_actions\n",
    "                )\n",
    "                \n",
    "                # ========================================\n",
    "                # COMPUTE PPO LOSSES\n",
    "                # ========================================\n",
    "                \n",
    "                # 1. Policy loss (PPO-Clip)\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                clipped_ratio = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
    "                policy_loss = -torch.min(\n",
    "                    ratio * batch_advantages,\n",
    "                    clipped_ratio * batch_advantages\n",
    "                ).mean()\n",
    "                \n",
    "                # 2. Value loss (MSE between predicted and actual returns)\n",
    "                value_loss = 0.5 * ((new_values - batch_returns) ** 2).mean()\n",
    "                \n",
    "                # 3. Entropy bonus (encourages exploration)\n",
    "                entropy_loss = -0.01 * entropy.mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = policy_loss + 0.5 * value_loss + entropy_loss\n",
    "                \n",
    "                # Update network\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        # Clear storage\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        \n",
    "        return total_loss / (self.epochs * (n_samples // self.batch_size + 1))\n",
    "\n",
    "\n",
    "print(\"PPO AGENT CREATED\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "PPO combines several key ideas:\n",
    "\n",
    "1. ACTOR-CRITIC: Two outputs from one network\n",
    "   • Actor: Outputs action probabilities\n",
    "   • Critic: Outputs state value V(s)\n",
    "\n",
    "2. GAE: Compute advantages with bias-variance trade-off\n",
    "\n",
    "3. CLIPPING: Limit policy updates to stay stable\n",
    "\n",
    "4. MULTIPLE EPOCHS: Reuse data for efficiency\n",
    "\n",
    "5. ENTROPY BONUS: Encourage exploration\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training PPO on CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env_name='CartPole-v1', n_episodes=500, rollout_length=2048, verbose=True):\n",
    "    \"\"\"\n",
    "    Train a PPO agent.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Gymnasium environment name\n",
    "        n_episodes: Maximum number of episodes\n",
    "        rollout_length: Steps to collect before each update\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        agent: Trained PPO agent\n",
    "        rewards_history: List of episode rewards\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = PPOAgent(state_dim, action_dim)\n",
    "    \n",
    "    rewards_history = []\n",
    "    episode_reward = 0\n",
    "    step_count = 0\n",
    "    episode = 0\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"TRAINING PPO ON CARTPOLE\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    while episode < n_episodes:\n",
    "        # ========================================\n",
    "        # COLLECT ROLLOUT\n",
    "        # ========================================\n",
    "        for _ in range(rollout_length):\n",
    "            # Select action\n",
    "            action, log_prob, value = agent.select_action(state)\n",
    "            \n",
    "            # Take step in environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store experience\n",
    "            agent.store(state, action, log_prob, reward, value, done)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            if done:\n",
    "                rewards_history.append(episode_reward)\n",
    "                episode += 1\n",
    "                \n",
    "                if verbose and episode % 50 == 0:\n",
    "                    avg_reward = np.mean(rewards_history[-50:])\n",
    "                    print(f\"Episode {episode:4d} | Avg Reward (last 50): {avg_reward:.1f}\")\n",
    "                \n",
    "                episode_reward = 0\n",
    "                state, _ = env.reset()\n",
    "                \n",
    "                if episode >= n_episodes:\n",
    "                    break\n",
    "            else:\n",
    "                state = next_state\n",
    "        \n",
    "        # ========================================\n",
    "        # UPDATE POLICY\n",
    "        # ========================================\n",
    "        if len(agent.states) > 0:\n",
    "            # Get value of last state for GAE\n",
    "            with torch.no_grad():\n",
    "                _, _, _, next_value = agent.network.get_action_and_value(\n",
    "                    torch.FloatTensor(state).unsqueeze(0)\n",
    "                )\n",
    "                next_value = next_value.item() if not done else 0\n",
    "            \n",
    "            agent.update(next_value)\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Training complete!\")\n",
    "        final_avg = np.mean(rewards_history[-100:]) if len(rewards_history) >= 100 else np.mean(rewards_history)\n",
    "        print(f\"Final average reward (last 100): {final_avg:.1f}\")\n",
    "        print(f\"Solved threshold: 195.0\")\n",
    "        print(f\"Status: {'SOLVED!' if final_avg >= 195 else 'Keep training...'}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    return agent, rewards_history\n",
    "\n",
    "# Train the agent\n",
    "agent, rewards = train_ppo(n_episodes=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.plot(rewards, alpha=0.3, color='blue', label='Raw')\n",
    "\n",
    "# Smoothed\n",
    "window = 20\n",
    "if len(rewards) >= window:\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(rewards)), smoothed, color='blue', \n",
    "             linewidth=2, label=f'Smoothed (window={window})')\n",
    "\n",
    "plt.axhline(y=195, color='green', linestyle='--', linewidth=2, label='Solved (195)')\n",
    "\n",
    "plt.xlabel('Episode', fontsize=12)\n",
    "plt.ylabel('Total Reward', fontsize=12)\n",
    "plt.title('PPO Training on CartPole', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PPO in the Real World: RLHF for ChatGPT\n",
    "\n",
    "PPO is the algorithm used to train ChatGPT and other language models!\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │           PPO FOR LANGUAGE MODELS (RLHF)                │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  In CartPole:                                          │\n",
    "    │    • State: Cart position, pole angle, etc.            │\n",
    "    │    • Action: Push LEFT or RIGHT                        │\n",
    "    │    • Reward: +1 for each timestep balanced             │\n",
    "    │                                                         │\n",
    "    │  In ChatGPT:                                           │\n",
    "    │    • State: Conversation so far                        │\n",
    "    │    • Action: Next token to generate                    │\n",
    "    │    • Reward: Human preference (helpful? harmless?)     │\n",
    "    │                                                         │\n",
    "    │  The SAME algorithm, just different state/action/reward!│\n",
    "    │                                                         │\n",
    "    │  Why PPO for LLMs?                                     │\n",
    "    │    • Stable: Won't break the language model            │\n",
    "    │    • Sample efficient: Uses each experience well       │\n",
    "    │    • Proven: Works at scale!                           │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### What is PPO?\n",
    "\n",
    "PPO = Policy Gradient + Clipping + Multiple Epochs\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Probability Ratio** | r(θ) = π_new / π_old (how much did policy change?) |\n",
    "| **Clipping** | Keep ratio between [1-ε, 1+ε] |\n",
    "| **PPO Objective** | min(r × A, clip(r) × A) |\n",
    "\n",
    "### Why PPO Works\n",
    "\n",
    "1. **Clipping prevents catastrophic updates** - can't change too much\n",
    "2. **Multiple epochs** - uses data efficiently\n",
    "3. **Simple to implement** - no complex constraints\n",
    "4. **Stable** - works across many environments\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "| Parameter | Typical Value | Meaning |\n",
    "|-----------|---------------|----------|\n",
    "| ε (clip) | 0.2 | Max 20% policy change per update |\n",
    "| λ (GAE) | 0.95 | Advantage estimation smoothing |\n",
    "| epochs | 10 | Reuse each batch 10 times |\n",
    "| γ | 0.99 | Discount factor |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What problem does PPO solve?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "PPO solves the problem of unstable policy updates in policy gradient methods. Without constraints, policy updates can be too large, causing the policy to \"overshoot\" and lose previously learned behavior. PPO clips the objective to prevent this.\n",
    "</details>\n",
    "\n",
    "**2. What is the probability ratio r(θ)?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "r(θ) = π_new(a|s) / π_old(a|s). It measures how much more (or less) likely an action is under the new policy compared to the old policy. If r = 1.0, the policy hasn't changed. If r = 2.0, the action is twice as likely now.\n",
    "</details>\n",
    "\n",
    "**3. What does clipping do in PPO?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Clipping keeps the ratio between [1-ε, 1+ε]. For example, with ε=0.2, the ratio stays between 0.8 and 1.2. This means the policy can change at most 20% per update, preventing large destabilizing changes.\n",
    "</details>\n",
    "\n",
    "**4. Why does PPO use multiple epochs?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "PPO updates the policy multiple times (epochs) on the same batch of data to use it more efficiently. Unlike vanilla policy gradient which uses each sample once, PPO can safely reuse data because the clipping prevents the policy from changing too much.\n",
    "</details>\n",
    "\n",
    "**5. How is PPO used to train ChatGPT?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "In RLHF (Reinforcement Learning from Human Feedback), PPO is used with: State = conversation so far, Action = next token to generate, Reward = score from a reward model trained on human preferences. The clipping is especially important to prevent the language model from changing too much and losing its language abilities.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Excellent work! You've implemented PPO from scratch!\n",
    "\n",
    "In the next notebook, we'll use the **Stable-Baselines3** library which provides highly optimized implementations:\n",
    "- PPO with vectorized environments\n",
    "- Built-in logging and monitoring\n",
    "- Easy hyperparameter tuning\n",
    "\n",
    "**Continue to:** [Notebook 3: PPO with Stable-Baselines](03_ppo_with_stable_baselines.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*You've just implemented the same algorithm used to train ChatGPT! The principles are exactly the same, just with different states (text instead of cart position) and rewards (human preferences instead of balance time).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
