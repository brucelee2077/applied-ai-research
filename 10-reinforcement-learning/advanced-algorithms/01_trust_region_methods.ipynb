{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust Region Methods: Safe Policy Updates\n",
    "\n",
    "Trust region methods solve one of the biggest problems in policy gradients: catastrophic policy updates!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The tightrope walker analogy: why small steps matter\n",
    "- The problem with vanilla policy gradients\n",
    "- KL divergence: measuring policy change\n",
    "- TRPO (Trust Region Policy Optimization)\n",
    "- Why PPO is TRPO's practical cousin\n",
    "\n",
    "**Prerequisites:** Policy Gradient section (A2C/A3C)\n",
    "\n",
    "**Time:** ~25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Tightrope Walker Analogy\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          THE TIGHTROPE WALKER ANALOGY                          â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Imagine learning to walk a tightrope...                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  VANILLA POLICY GRADIENT:                                     â”‚\n",
    "    â”‚    \"The crowd loved my last trick!\"                          â”‚\n",
    "    â”‚    â†’ Takes HUGE leaps based on that feedback                 â”‚\n",
    "    â”‚    â†’ Sometimes leaps right off the rope! ğŸ’€                  â”‚\n",
    "    â”‚    â†’ Performance can COLLAPSE in one bad update              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TRUST REGION METHODS:                                        â”‚\n",
    "    â”‚    \"I'll only take small, careful steps\"                     â”‚\n",
    "    â”‚    â†’ Limits how much the policy can change                   â”‚\n",
    "    â”‚    â†’ Guaranteed to stay on the rope (mostly!)               â”‚\n",
    "    â”‚    â†’ Steady, monotonic improvement                          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE KEY INSIGHT:                                             â”‚\n",
    "    â”‚    In RL, one bad policy update can DESTROY everything!      â”‚\n",
    "    â”‚    Unlike supervised learning, you can't just recover.       â”‚\n",
    "    â”‚    A bad policy collects bad data â†’ worse policy â†’ spiral!   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TRUST REGION = \"Safety zone for policy changes\"             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, Ellipse, FancyArrowPatch\n",
    "from scipy.stats import entropy\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym\n",
    "\n",
    "# Visualize the problem with large policy updates\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Vanilla Policy Gradient - Catastrophic Updates\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(-1, 10)\n",
    "ax1.set_ylim(-1, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Vanilla Policy Gradient\\n\"Big steps can be dangerous!\"', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Draw policy landscape\n",
    "x = np.linspace(0, 9, 100)\n",
    "y = 4 + 2*np.sin(x*0.8) + 0.5*np.sin(x*2)\n",
    "ax1.fill_between(x, 0, y, alpha=0.3, color='#90caf9')\n",
    "ax1.plot(x, y, 'b-', linewidth=2)\n",
    "\n",
    "# Show trajectory with big jumps\n",
    "points = [(1, 5), (3, 6.5), (4.5, 2), (6, 0.5)]  # Catastrophic drop!\n",
    "for i, (px, py) in enumerate(points):\n",
    "    color = '#4caf50' if i < 2 else '#d32f2f'\n",
    "    ax1.scatter([px], [py], s=100, c=color, zorder=5, edgecolors='black')\n",
    "    if i < len(points) - 1:\n",
    "        npx, npy = points[i+1]\n",
    "        ax1.annotate('', xy=(npx, npy), xytext=(px, py),\n",
    "                     arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "ax1.text(4.5, 1.5, 'COLLAPSE!', fontsize=12, color='#d32f2f', fontweight='bold', ha='center')\n",
    "ax1.text(5, 8.5, 'Large gradient â†’ Large step â†’ Disaster', fontsize=10, ha='center')\n",
    "\n",
    "# Right: Trust Region - Safe Updates\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(-1, 10)\n",
    "ax2.set_ylim(-1, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Trust Region Methods\\n\"Small, safe steps only!\"', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Same landscape\n",
    "ax2.fill_between(x, 0, y, alpha=0.3, color='#c8e6c9')\n",
    "ax2.plot(x, y, 'g-', linewidth=2)\n",
    "\n",
    "# Show safe trajectory\n",
    "safe_points = [(1, 5), (1.8, 5.8), (2.6, 6.3), (3.4, 6.5), (4.2, 6.2)]\n",
    "for i, (px, py) in enumerate(safe_points):\n",
    "    ax2.scatter([px], [py], s=100, c='#4caf50', zorder=5, edgecolors='black')\n",
    "    # Draw trust region circle\n",
    "    circle = Circle((px, py), 0.6, fill=False, linestyle='--', color='#81c784', linewidth=2)\n",
    "    ax2.add_patch(circle)\n",
    "    if i < len(safe_points) - 1:\n",
    "        npx, npy = safe_points[i+1]\n",
    "        ax2.annotate('', xy=(npx, npy), xytext=(px, py),\n",
    "                     arrowprops=dict(arrowstyle='->', lw=2, color='#388e3c'))\n",
    "\n",
    "ax2.text(3, 8.5, 'Stay within trust region â†’ Safe progress', fontsize=10, ha='center')\n",
    "ax2.text(3, 3.5, 'Trust Region\\n(max allowed change)', fontsize=9, ha='center', color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWHY TRUST REGIONS?\")\n",
    "print(\"  â€¢ Policy gradients can have HIGH variance\")\n",
    "print(\"  â€¢ One bad update can destroy a good policy\")\n",
    "print(\"  â€¢ Unlike supervised learning, bad policy â†’ bad data â†’ worse policy\")\n",
    "print(\"  â€¢ Trust regions prevent catastrophic updates!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Problem: Policy Collapse\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              THE POLICY COLLAPSE PROBLEM                       â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  SUPERVISED LEARNING:                                         â”‚\n",
    "    â”‚    Bad update? â†’ Training data stays the same                 â”‚\n",
    "    â”‚    â†’ Can recover in the next batch                           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  REINFORCEMENT LEARNING:                                      â”‚\n",
    "    â”‚    Bad update? â†’ Policy becomes bad                          â”‚\n",
    "    â”‚    â†’ Bad policy collects BAD DATA                            â”‚\n",
    "    â”‚    â†’ Train on bad data â†’ WORSE policy                        â”‚\n",
    "    â”‚    â†’ Vicious cycle! ğŸ”„                                       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  EXAMPLE (CartPole):                                          â”‚\n",
    "    â”‚    Good policy: Balances well, sees diverse states           â”‚\n",
    "    â”‚    Bad update: Now always falls left                         â”‚\n",
    "    â”‚    Bad data: Only sees \"falling\" states                     â”‚\n",
    "    â”‚    Learning: Gets even worse at balancing!                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE GOAL:                                                    â”‚\n",
    "    â”‚    Ensure each update IMPROVES (or at least doesn't hurt)    â”‚\n",
    "    â”‚    the policy. MONOTONIC IMPROVEMENT!                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the policy collapse problem\n",
    "\n",
    "def simulate_training(collapse_at=None, n_epochs=20):\n",
    "    \"\"\"\n",
    "    Simulate training with and without policy collapse.\n",
    "    \n",
    "    Args:\n",
    "        collapse_at: Epoch where collapse happens (None = no collapse)\n",
    "        n_epochs: Number of training epochs\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    performance = [10]  # Start at performance 10\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if collapse_at and epoch == collapse_at:\n",
    "            # Catastrophic update!\n",
    "            change = -15 + np.random.randn()\n",
    "        elif collapse_at and epoch > collapse_at:\n",
    "            # Bad policy = bad data = worse policy\n",
    "            change = -0.5 + 0.3 * np.random.randn()  # Slow recovery or continued decline\n",
    "        else:\n",
    "            # Normal improvement\n",
    "            change = 2 + np.random.randn()\n",
    "        \n",
    "        performance.append(max(0, performance[-1] + change))\n",
    "    \n",
    "    return performance\n",
    "\n",
    "# Simulate both scenarios\n",
    "stable_perf = simulate_training(collapse_at=None)\n",
    "collapse_perf = simulate_training(collapse_at=8)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(stable_perf, 'g-', linewidth=2, marker='o', label='Stable Training')\n",
    "ax.plot(collapse_perf, 'r-', linewidth=2, marker='s', label='With Policy Collapse')\n",
    "\n",
    "# Highlight collapse point\n",
    "ax.axvline(x=8, color='red', linestyle='--', alpha=0.5)\n",
    "ax.annotate('Collapse!', xy=(8, collapse_perf[8]), xytext=(10, 35),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'),\n",
    "            fontsize=11, color='red', fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Performance', fontsize=11)\n",
    "ax.set_title('The Danger of Large Policy Updates', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPOLICY COLLAPSE:\")\n",
    "print(f\"  Without collapse: Final performance = {stable_perf[-1]:.1f}\")\n",
    "print(f\"  With collapse: Final performance = {collapse_perf[-1]:.1f}\")\n",
    "print(\"\\n  One bad update can undo all progress!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## KL Divergence: Measuring Policy Change\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              KL DIVERGENCE: HOW DIFFERENT ARE TWO POLICIES?   â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  DEFINITION:                                                  â”‚\n",
    "    â”‚    KL(Ï€_old || Ï€_new) = Î£ Ï€_old(a|s) Ã— log(Ï€_old(a|s)/Ï€_new(a|s))â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  INTUITION:                                                   â”‚\n",
    "    â”‚    â€¢ KL = 0: Policies are identical                          â”‚\n",
    "    â”‚    â€¢ KL small: Policies are similar                          â”‚\n",
    "    â”‚    â€¢ KL large: Policies are very different                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  EXAMPLE:                                                     â”‚\n",
    "    â”‚    Old policy: [0.5, 0.3, 0.2] for actions [A, B, C]         â”‚\n",
    "    â”‚    New policy: [0.6, 0.25, 0.15]                             â”‚\n",
    "    â”‚    KL = 0.03 (small change, probably safe)                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    New policy: [0.1, 0.1, 0.8]                               â”‚\n",
    "    â”‚    KL = 0.58 (big change, might be dangerous!)               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TRUST REGION CONSTRAINT:                                     â”‚\n",
    "    â”‚    \"Only update if KL(Ï€_old || Ï€_new) â‰¤ Î´\"                   â”‚\n",
    "    â”‚    Where Î´ is a small threshold (e.g., 0.01)                 â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Compute KL divergence: KL(P || Q)\n",
    "    \n",
    "    Measures how different Q is from P.\n",
    "    KL(P||Q) = Î£ P(x) * log(P(x)/Q(x))\n",
    "    \"\"\"\n",
    "    p = np.array(p) + 1e-10  # Avoid log(0)\n",
    "    q = np.array(q) + 1e-10\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "\n",
    "# Demonstrate KL divergence\n",
    "print(\"KL DIVERGENCE EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "old_policy = [0.5, 0.3, 0.2]  # Original policy for 3 actions\n",
    "\n",
    "examples = [\n",
    "    ([0.5, 0.3, 0.2], \"Identical policy\"),\n",
    "    ([0.52, 0.28, 0.20], \"Tiny change\"),\n",
    "    ([0.6, 0.25, 0.15], \"Small change\"),\n",
    "    ([0.7, 0.2, 0.1], \"Medium change\"),\n",
    "    ([0.1, 0.1, 0.8], \"Large change\"),\n",
    "    ([0.01, 0.01, 0.98], \"Huge change\"),\n",
    "]\n",
    "\n",
    "print(f\"\\nOld policy: {old_policy}\")\n",
    "print(\"\\nNew policies and their KL divergence:\")\n",
    "\n",
    "kl_values = []\n",
    "for new_policy, desc in examples:\n",
    "    kl = kl_divergence(old_policy, new_policy)\n",
    "    kl_values.append(kl)\n",
    "    safety = \"âœ“ Safe\" if kl < 0.1 else \"âš  Caution\" if kl < 0.3 else \"âœ— Dangerous!\"\n",
    "    print(f\"  {new_policy} ({desc})\")\n",
    "    print(f\"    KL = {kl:.4f} â†’ {safety}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KL divergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Policy distributions\n",
    "ax1 = axes[0]\n",
    "actions = ['Action A', 'Action B', 'Action C']\n",
    "x = np.arange(len(actions))\n",
    "width = 0.25\n",
    "\n",
    "ax1.bar(x - width, old_policy, width, label='Old Policy', color='#64b5f6', edgecolor='black')\n",
    "ax1.bar(x, [0.6, 0.25, 0.15], width, label='New (small Î”)', color='#81c784', edgecolor='black')\n",
    "ax1.bar(x + width, [0.1, 0.1, 0.8], width, label='New (large Î”)', color='#ef5350', edgecolor='black')\n",
    "\n",
    "ax1.set_ylabel('Probability', fontsize=11)\n",
    "ax1.set_title('Policy Distributions', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(actions)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Right: KL divergence values\n",
    "ax2 = axes[1]\n",
    "labels = ['Same', 'Tiny', 'Small', 'Medium', 'Large', 'Huge']\n",
    "colors = ['#4caf50' if kl < 0.1 else '#ffb74d' if kl < 0.3 else '#e53935' for kl in kl_values]\n",
    "\n",
    "bars = ax2.bar(labels, kl_values, color=colors, edgecolor='black')\n",
    "ax2.axhline(y=0.01, color='green', linestyle='--', linewidth=2, label='TRPO constraint (Î´=0.01)')\n",
    "ax2.axhline(y=0.1, color='orange', linestyle='--', linewidth=2, label='Caution zone')\n",
    "\n",
    "ax2.set_ylabel('KL Divergence', fontsize=11)\n",
    "ax2.set_title('KL Divergence: Measuring Policy Change', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTRUST REGION CONSTRAINT:\")\n",
    "print(\"  TRPO constrains: KL(Ï€_old || Ï€_new) â‰¤ Î´\")\n",
    "print(\"  Typical Î´ = 0.01 (very conservative)\")\n",
    "print(\"  This ensures small, safe policy updates!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TRPO: Trust Region Policy Optimization\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              TRPO: THE MATH                                    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  OBJECTIVE (What we want to maximize):                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    L(Î¸) = E[ Ï€_new(a|s) / Ï€_old(a|s) Ã— A(s,a) ]              â”‚\n",
    "    â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚\n",
    "    â”‚               \"Probability ratio\"                             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    This is the \"surrogate\" objective                         â”‚\n",
    "    â”‚    If ratio > 1: New policy takes this action MORE           â”‚\n",
    "    â”‚    If ratio < 1: New policy takes this action LESS           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  CONSTRAINT (Stay in trust region):                           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    KL(Ï€_old || Ï€_new) â‰¤ Î´                                    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TRPO SOLVES:                                                 â”‚\n",
    "    â”‚    Maximize L(Î¸) subject to KL â‰¤ Î´                           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    Uses: Natural gradients + conjugate gradient + line searchâ”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE GUARANTEE:                                               â”‚\n",
    "    â”‚    If KL is bounded, performance is guaranteed to improve!   â”‚\n",
    "    â”‚    (Monotonic improvement theorem)                           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the TRPO optimization\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "\n",
    "# Create contour plot for objective function\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "# Surrogate objective: higher in one direction\n",
    "Z = -0.5*(X-1)**2 - 0.3*(Y-1)**2 + 2\n",
    "\n",
    "contour = ax.contourf(X, Y, Z, levels=20, cmap='RdYlGn', alpha=0.7)\n",
    "plt.colorbar(contour, ax=ax, label='Surrogate Objective L(Î¸)')\n",
    "\n",
    "# Current policy (center)\n",
    "ax.scatter([0], [0], s=200, c='blue', zorder=5, edgecolors='black', label='Current policy Î¸_old')\n",
    "\n",
    "# Trust region\n",
    "trust_circle = Circle((0, 0), 1.0, fill=False, linestyle='--', \n",
    "                       color='blue', linewidth=3, label='Trust region (KL â‰¤ Î´)')\n",
    "ax.add_patch(trust_circle)\n",
    "\n",
    "# Optimal without constraint (outside trust region)\n",
    "ax.scatter([1], [1], s=200, c='red', marker='x', zorder=5, linewidths=3,\n",
    "           label='Unconstrained optimum (risky!)')\n",
    "\n",
    "# TRPO solution (on trust region boundary)\n",
    "# Project (1,1) onto circle of radius 1\n",
    "trpo_x, trpo_y = 1/np.sqrt(2), 1/np.sqrt(2)\n",
    "ax.scatter([trpo_x], [trpo_y], s=200, c='green', zorder=5, edgecolors='black',\n",
    "           label='TRPO solution (safe!)')\n",
    "\n",
    "# Arrow from old to TRPO\n",
    "ax.annotate('', xy=(trpo_x, trpo_y), xytext=(0, 0),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='green'))\n",
    "\n",
    "# Arrow to unconstrained (crossed out)\n",
    "ax.annotate('', xy=(1, 1), xytext=(0, 0),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='red', linestyle='--'))\n",
    "\n",
    "ax.set_xlabel('Policy Parameter Î¸â‚', fontsize=11)\n",
    "ax.set_ylabel('Policy Parameter Î¸â‚‚', fontsize=11)\n",
    "ax.set_title('TRPO: Maximize Objective Within Trust Region', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower left')\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTRPO SUMMARY:\")\n",
    "print(\"  1. Find direction that improves surrogate objective\")\n",
    "print(\"  2. But only step as far as trust region allows\")\n",
    "print(\"  3. Guaranteed improvement (or at least no catastrophe)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surrogate_objective(old_probs, new_probs, advantages):\n",
    "    \"\"\"\n",
    "    Compute the TRPO/PPO surrogate objective.\n",
    "    \n",
    "    L(Î¸) = E[ Ï€_new(a|s) / Ï€_old(a|s) Ã— A(s,a) ]\n",
    "    \n",
    "    This tells us how much better the new policy is.\n",
    "    \"\"\"\n",
    "    # Probability ratio\n",
    "    ratio = new_probs / (old_probs + 1e-10)\n",
    "    \n",
    "    # Surrogate objective\n",
    "    surrogate = ratio * advantages\n",
    "    \n",
    "    return surrogate.mean()\n",
    "\n",
    "\n",
    "# Demonstrate surrogate objective\n",
    "print(\"SURROGATE OBJECTIVE DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example: 5 state-action pairs\n",
    "old_probs = np.array([0.5, 0.3, 0.4, 0.6, 0.2])  # Ï€_old(a|s)\n",
    "advantages = np.array([2.0, -1.0, 1.5, -0.5, 3.0])  # A(s,a)\n",
    "\n",
    "print(f\"\\nOld policy probabilities: {old_probs}\")\n",
    "print(f\"Advantages: {advantages}\")\n",
    "\n",
    "# Try different new policies\n",
    "scenarios = [\n",
    "    (old_probs, \"Same policy (no change)\"),\n",
    "    (old_probs * 1.2, \"Increase all probabilities\"),  # Note: not normalized, just for demo\n",
    "    (np.array([0.7, 0.2, 0.5, 0.5, 0.4]), \"Increase good, decrease bad\"),\n",
    "]\n",
    "\n",
    "print(\"\\nNew policies and their surrogate objectives:\")\n",
    "for new_probs, desc in scenarios:\n",
    "    surr = compute_surrogate_objective(old_probs, new_probs, advantages)\n",
    "    ratio = new_probs / old_probs\n",
    "    kl = kl_divergence(old_probs/old_probs.sum(), new_probs/new_probs.sum())\n",
    "    print(f\"\\n  {desc}\")\n",
    "    print(f\"    Probability ratios: {ratio.round(2)}\")\n",
    "    print(f\"    Surrogate objective L(Î¸): {surr:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSIGHT: Good policy changes increase probability of high-advantage actions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why TRPO is Complex (and Why PPO Exists)\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              TRPO: POWERFUL BUT COMPLEX                        â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TRPO REQUIRES:                                               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  1. Fisher Information Matrix (FIM)                          â”‚\n",
    "    â”‚     - Second-order information about policy                  â”‚\n",
    "    â”‚     - Expensive to compute!                                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  2. Conjugate Gradient                                        â”‚\n",
    "    â”‚     - Iterative solver for natural gradient                  â”‚\n",
    "    â”‚     - Complex to implement                                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  3. Line Search                                               â”‚\n",
    "    â”‚     - Find step size that satisfies constraint               â”‚\n",
    "    â”‚     - Requires multiple forward passes                       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  PPO'S INSIGHT:                                               â”‚\n",
    "    â”‚    \"What if we just CLIP the ratio instead?\"                 â”‚\n",
    "    â”‚    â€¢ No Fisher matrix                                        â”‚\n",
    "    â”‚    â€¢ No conjugate gradient                                   â”‚\n",
    "    â”‚    â€¢ No line search                                          â”‚\n",
    "    â”‚    â€¢ Almost as good in practice!                             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TRPO vs PPO complexity\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Left: TRPO Pipeline\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 12)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('TRPO Pipeline\\n(Complex!)', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "trpo_steps = [\n",
    "    ('Collect Data', '#bbdefb'),\n",
    "    ('Compute Advantages', '#bbdefb'),\n",
    "    ('Compute Policy Gradient', '#fff3e0'),\n",
    "    ('Compute Fisher Matrix', '#ffcdd2'),\n",
    "    ('Conjugate Gradient\\n(natural gradient)', '#ffcdd2'),\n",
    "    ('Line Search\\n(find valid step)', '#ffcdd2'),\n",
    "    ('Update Policy', '#c8e6c9'),\n",
    "]\n",
    "\n",
    "for i, (step, color) in enumerate(trpo_steps):\n",
    "    y = 10.5 - i * 1.4\n",
    "    box = FancyBboxPatch((1, y), 8, 1, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor='#333', linewidth=2)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(5, y + 0.5, step, ha='center', va='center', fontsize=10)\n",
    "    if i < len(trpo_steps) - 1:\n",
    "        ax1.annotate('', xy=(5, y - 0.1), xytext=(5, y),\n",
    "                     arrowprops=dict(arrowstyle='->', lw=1, color='#666'))\n",
    "\n",
    "ax1.text(5, 0.5, 'âŒ Hard to implement\\nâŒ Slow per update', ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "# Right: PPO Pipeline\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 12)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('PPO Pipeline\\n(Simple!)', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "ppo_steps = [\n",
    "    ('Collect Data', '#bbdefb'),\n",
    "    ('Compute Advantages', '#bbdefb'),\n",
    "    ('Compute Clipped Objective', '#c8e6c9'),\n",
    "    ('Standard Gradient Descent', '#c8e6c9'),\n",
    "    ('Update Policy', '#c8e6c9'),\n",
    "]\n",
    "\n",
    "for i, (step, color) in enumerate(ppo_steps):\n",
    "    y = 10.5 - i * 1.8\n",
    "    box = FancyBboxPatch((1, y), 8, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor='#333', linewidth=2)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(5, y + 0.6, step, ha='center', va='center', fontsize=11)\n",
    "    if i < len(ppo_steps) - 1:\n",
    "        ax2.annotate('', xy=(5, y - 0.2), xytext=(5, y),\n",
    "                     arrowprops=dict(arrowstyle='->', lw=1, color='#666'))\n",
    "\n",
    "ax2.text(5, 1.5, 'âœ“ Easy to implement\\nâœ“ Fast updates\\nâœ“ Works just as well!', \n",
    "         ha='center', fontsize=10, color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPPO'S KEY INNOVATION:\")\n",
    "print(\"  Replace KL constraint with clipped objective\")\n",
    "print(\"  Same effect (bounded updates) but much simpler!\")\n",
    "print(\"  See next notebook for PPO details!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preview: The PPO Clip Trick\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              PPO'S CLIPPED OBJECTIVE                           â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TRPO: Constrained optimization (complex)                     â”‚\n",
    "    â”‚    max L(Î¸) such that KL â‰¤ Î´                                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  PPO: Clip the ratio instead (simple!)                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    r(Î¸) = Ï€_new(a|s) / Ï€_old(a|s)                            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    L_CLIP(Î¸) = min(                                          â”‚\n",
    "    â”‚        r(Î¸) Ã— A,                                             â”‚\n",
    "    â”‚        clip(r(Î¸), 1-Îµ, 1+Îµ) Ã— A                              â”‚\n",
    "    â”‚    )                                                          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WHERE Îµ â‰ˆ 0.2                                                â”‚\n",
    "    â”‚    â€¢ Ratio can only be between 0.8 and 1.2                   â”‚\n",
    "    â”‚    â€¢ Prevents large policy changes                           â”‚\n",
    "    â”‚    â€¢ No constraint optimization needed!                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE GENIUS:                                                  â”‚\n",
    "    â”‚    - Positive advantage + ratio > 1.2 â†’ Clipped (no benefit) â”‚\n",
    "    â”‚    - Negative advantage + ratio < 0.8 â†’ Clipped (no benefit) â”‚\n",
    "    â”‚    - No incentive to make BIG changes!                       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PPO clipping\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epsilon = 0.2\n",
    "ratio = np.linspace(0.5, 1.5, 100)\n",
    "\n",
    "# Left: Positive advantage (A > 0)\n",
    "ax1 = axes[0]\n",
    "A = 1.0  # Positive advantage\n",
    "\n",
    "# Unclipped objective\n",
    "unclipped = ratio * A\n",
    "# Clipped objective\n",
    "clipped_ratio = np.clip(ratio, 1 - epsilon, 1 + epsilon)\n",
    "clipped = clipped_ratio * A\n",
    "# PPO objective: min of both\n",
    "ppo_obj = np.minimum(unclipped, clipped)\n",
    "\n",
    "ax1.plot(ratio, unclipped, 'b--', linewidth=2, label='Unclipped: r Ã— A', alpha=0.7)\n",
    "ax1.plot(ratio, clipped, 'r--', linewidth=2, label='Clipped: clip(r) Ã— A', alpha=0.7)\n",
    "ax1.plot(ratio, ppo_obj, 'g-', linewidth=3, label='PPO: min(both)')\n",
    "\n",
    "ax1.axvline(x=1-epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "ax1.axvline(x=1+epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "ax1.axvline(x=1, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "ax1.fill_between(ratio, 0, ppo_obj, where=(ratio > 1 + epsilon), alpha=0.2, color='red')\n",
    "ax1.text(1.35, 1.1, 'No extra\\nbenefit here!', fontsize=9, ha='center', color='red')\n",
    "\n",
    "ax1.set_xlabel('Probability Ratio r(Î¸)', fontsize=11)\n",
    "ax1.set_ylabel('Objective', fontsize=11)\n",
    "ax1.set_title('Positive Advantage (A > 0)\\n\"Good action - want to increase probability\"', fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Negative advantage (A < 0)\n",
    "ax2 = axes[1]\n",
    "A = -1.0  # Negative advantage\n",
    "\n",
    "# Unclipped objective\n",
    "unclipped = ratio * A\n",
    "# Clipped objective\n",
    "clipped = clipped_ratio * A\n",
    "# PPO objective: min of both (but we want to maximize, so it's max of negatives)\n",
    "ppo_obj = np.minimum(unclipped, clipped)\n",
    "\n",
    "ax2.plot(ratio, unclipped, 'b--', linewidth=2, label='Unclipped: r Ã— A', alpha=0.7)\n",
    "ax2.plot(ratio, clipped, 'r--', linewidth=2, label='Clipped: clip(r) Ã— A', alpha=0.7)\n",
    "ax2.plot(ratio, ppo_obj, 'g-', linewidth=3, label='PPO: min(both)')\n",
    "\n",
    "ax2.axvline(x=1-epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "ax2.axvline(x=1+epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "ax2.axvline(x=1, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "ax2.fill_between(ratio, ppo_obj, 0, where=(ratio < 1 - epsilon), alpha=0.2, color='red')\n",
    "ax2.text(0.65, -1.1, 'No extra\\nbenefit here!', fontsize=9, ha='center', color='red')\n",
    "\n",
    "ax2.set_xlabel('Probability Ratio r(Î¸)', fontsize=11)\n",
    "ax2.set_ylabel('Objective', fontsize=11)\n",
    "ax2.set_title('Negative Advantage (A < 0)\\n\"Bad action - want to decrease probability\"', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPPO CLIPPING INTUITION:\")\n",
    "print(\"  â€¢ Positive advantage: Cap how much you can increase probability\")\n",
    "print(\"  â€¢ Negative advantage: Cap how much you can decrease probability\")\n",
    "print(\"  â€¢ Result: Policy changes are bounded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### The Trust Region Concept\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Trust Region** | A boundary around current policy |\n",
    "| **KL Divergence** | Measures how different two policies are |\n",
    "| **Constraint** | KL(Ï€_old \\|\\| Ï€_new) â‰¤ Î´ |\n",
    "\n",
    "### TRPO vs PPO\n",
    "\n",
    "| Aspect | TRPO | PPO |\n",
    "|--------|------|-----|\n",
    "| **Constraint** | KL divergence | Clipped ratio |\n",
    "| **Complexity** | High (2nd order) | Low (1st order) |\n",
    "| **Implementation** | Hard | Easy |\n",
    "| **Performance** | Excellent | Excellent |\n",
    "\n",
    "### Why Trust Regions Matter\n",
    "\n",
    "```\n",
    "Without: Large update â†’ Bad policy â†’ Bad data â†’ Worse policy â†’ Collapse!\n",
    "With:    Small update â†’ Safe change â†’ Good data â†’ Better policy â†’ Progress!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. Why are trust regions important in RL but not in supervised learning?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "In supervised learning, the training data is fixed. A bad update just means lower accuracy on the same data - you can recover next batch.\n",
    "\n",
    "In RL, the policy GENERATES its own data. A bad policy collects bad data, which makes training worse, which makes the policy even worse. This vicious cycle can cause complete collapse. Trust regions prevent this by ensuring small, safe updates.\n",
    "</details>\n",
    "\n",
    "**2. What does KL divergence measure?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "KL divergence KL(P || Q) measures how different probability distribution Q is from P. In the context of RL:\n",
    "- KL = 0 means the policies are identical\n",
    "- Small KL means the policies are similar\n",
    "- Large KL means the policies are very different\n",
    "\n",
    "TRPO constrains KL(Ï€_old || Ï€_new) â‰¤ Î´ to ensure small policy changes.\n",
    "</details>\n",
    "\n",
    "**3. What is the surrogate objective in TRPO?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The surrogate objective is:\n",
    "L(Î¸) = E[Ï€_new(a|s) / Ï€_old(a|s) Ã— A(s,a)]\n",
    "\n",
    "It measures how much better the new policy is by looking at:\n",
    "- The probability ratio (how much more/less likely is this action?)\n",
    "- The advantage (was this action good or bad?)\n",
    "\n",
    "Maximizing this improves the policy.\n",
    "</details>\n",
    "\n",
    "**4. Why is TRPO complex to implement?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "TRPO requires:\n",
    "1. Computing the Fisher Information Matrix (expensive second-order computation)\n",
    "2. Conjugate gradient solver to find the natural gradient direction\n",
    "3. Line search to find a step size that satisfies the KL constraint\n",
    "\n",
    "All of this is mathematically elegant but practically complex.\n",
    "</details>\n",
    "\n",
    "**5. How does PPO simplify TRPO?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "PPO replaces the KL constraint with a clipped objective:\n",
    "- Clip the probability ratio to [1-Îµ, 1+Îµ]\n",
    "- Take the minimum of clipped and unclipped objectives\n",
    "\n",
    "This achieves similar bounded updates without:\n",
    "- Fisher matrix computation\n",
    "- Conjugate gradient\n",
    "- Line search\n",
    "\n",
    "Just standard gradient descent!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "You now understand why trust regions matter. In the next notebook, we'll implement **PPO from scratch** - the most popular RL algorithm today!\n",
    "\n",
    "**Continue to:** [Notebook 2: PPO From Scratch](02_ppo_from_scratch.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Trust Regions: \"Small steps for the policy, giant leaps for performance!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
