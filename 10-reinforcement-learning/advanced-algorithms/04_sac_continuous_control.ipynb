{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic (SAC): Maximum Entropy RL\n",
    "\n",
    "SAC is a state-of-the-art algorithm for continuous control that learns to be optimally random!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The jazz musician analogy: why controlled randomness helps\n",
    "- Maximum entropy RL: reward + entropy\n",
    "- SAC's key innovations (twin critics, automatic temperature)\n",
    "- Off-policy learning: why SAC is sample efficient\n",
    "- Using SAC with Stable-Baselines3\n",
    "- When to choose SAC vs PPO\n",
    "\n",
    "**Prerequisites:** Notebook 2 (PPO From Scratch)\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Jazz Musician Analogy\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          THE JAZZ MUSICIAN ANALOGY                             â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Imagine two musicians learning to improvise...               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  CLASSICAL MUSICIAN (Standard RL):                            â”‚\n",
    "    â”‚    \"I'll learn THE BEST note for each moment\"                â”‚\n",
    "    â”‚    â†’ Always plays the same lick in similar situations        â”‚\n",
    "    â”‚    â†’ Predictable, gets stuck in routines                     â”‚\n",
    "    â”‚    â†’ If one approach fails, struggles to adapt               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  JAZZ MUSICIAN (SAC - Maximum Entropy):                       â”‚\n",
    "    â”‚    \"I'll learn MANY good options and stay creative!\"         â”‚\n",
    "    â”‚    â†’ Keeps multiple approaches ready                         â”‚\n",
    "    â”‚    â†’ Random but controlled improvisation                     â”‚\n",
    "    â”‚    â†’ Adapts easily when things change                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  SAC'S INSIGHT:                                               â”‚\n",
    "    â”‚    Don't just maximize reward - also maximize ENTROPY!       â”‚\n",
    "    â”‚    Entropy = \"controlled randomness\" = keeping options open  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    J(Ï€) = E[Î£ r(s,a)] + Î± Ã— H(Ï€)                             â”‚\n",
    "    â”‚            â””â”€rewardsâ”€â”˜   â””â”€entropy bonusâ”€â”˜                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, Wedge\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym\n",
    "\n",
    "# Check if Stable-Baselines3 is available\n",
    "try:\n",
    "    from stable_baselines3 import SAC, PPO\n",
    "    from stable_baselines3.common.env_util import make_vec_env\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    import torch\n",
    "    SB3_AVAILABLE = True\n",
    "    print(\"âœ“ Stable-Baselines3 is installed!\")\n",
    "except ImportError:\n",
    "    SB3_AVAILABLE = False\n",
    "    print(\"âœ— Stable-Baselines3 not installed.\")\n",
    "    print(\"  Install with: pip install stable-baselines3[extra]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize standard RL vs Maximum Entropy RL\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Standard RL (deterministic policy)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(-3, 3)\n",
    "ax1.set_ylim(-3, 3)\n",
    "\n",
    "# Show deterministic policy\n",
    "ax1.scatter([0], [0], s=200, c='blue', zorder=5, label='Current state')\n",
    "ax1.arrow(0, 0, 1.5, 1, head_width=0.2, head_length=0.1, fc='red', ec='red', linewidth=3)\n",
    "ax1.text(1.8, 1.2, 'Always this\\naction!', fontsize=10, color='red')\n",
    "\n",
    "ax1.set_xlabel('Action dimension 1', fontsize=11)\n",
    "ax1.set_ylabel('Action dimension 2', fontsize=11)\n",
    "ax1.set_title('Standard RL: Deterministic\\n\"Always pick THE BEST action\"', fontsize=12, fontweight='bold', color='#d32f2f')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Right: SAC (stochastic policy with high entropy)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(-3, 3)\n",
    "ax2.set_ylim(-3, 3)\n",
    "\n",
    "# Show stochastic policy as distribution\n",
    "ax2.scatter([0], [0], s=200, c='blue', zorder=5, label='Current state')\n",
    "\n",
    "# Draw distribution cloud\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "for r in [0.5, 1.0, 1.5]:\n",
    "    alpha = 0.3 - r/5\n",
    "    x = 1 + r * np.cos(theta) * 0.8\n",
    "    y = 0.5 + r * np.sin(theta) * 0.8\n",
    "    ax2.fill(x, y, alpha=alpha, color='green')\n",
    "\n",
    "# Sample actions\n",
    "np.random.seed(42)\n",
    "sample_x = 1 + np.random.randn(20) * 0.5\n",
    "sample_y = 0.5 + np.random.randn(20) * 0.5\n",
    "ax2.scatter(sample_x, sample_y, s=30, c='green', alpha=0.7, zorder=4)\n",
    "\n",
    "ax2.arrow(0, 0, 0.8, 0.4, head_width=0.15, head_length=0.1, fc='green', ec='green', linewidth=2, alpha=0.7)\n",
    "ax2.text(2.2, 1.2, 'Distribution of\\ngood actions!', fontsize=10, color='green')\n",
    "\n",
    "ax2.set_xlabel('Action dimension 1', fontsize=11)\n",
    "ax2.set_ylabel('Action dimension 2', fontsize=11)\n",
    "ax2.set_title('SAC: Maximum Entropy\\n\"Many good actions, stay creative!\"', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMAXIMUM ENTROPY BENEFITS:\")\n",
    "print(\"  â€¢ Better exploration (tries more actions)\")\n",
    "print(\"  â€¢ More robust (multiple solutions ready)\")\n",
    "print(\"  â€¢ Easier to fine-tune later\")\n",
    "print(\"  â€¢ Works better in stochastic environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Maximum Entropy RL: The Core Idea\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              MAXIMUM ENTROPY OBJECTIVE                         â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  STANDARD RL OBJECTIVE:                                       â”‚\n",
    "    â”‚    J(Ï€) = E[Î£ Î³áµ— r(sâ‚œ, aâ‚œ)]                                  â”‚\n",
    "    â”‚    \"Maximize expected discounted rewards\"                    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  MAXIMUM ENTROPY OBJECTIVE:                                   â”‚\n",
    "    â”‚    J(Ï€) = E[Î£ Î³áµ— (r(sâ‚œ, aâ‚œ) + Î± H(Ï€(Â·|sâ‚œ)))]                â”‚\n",
    "    â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "    â”‚                               Entropy bonus!                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  WHAT IS ENTROPY?                                             â”‚\n",
    "    â”‚    H(Ï€) = -E[log Ï€(a|s)]                                     â”‚\n",
    "    â”‚    â€¢ High entropy: Actions spread out (random)               â”‚\n",
    "    â”‚    â€¢ Low entropy: Actions concentrated (deterministic)       â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  THE Î± (TEMPERATURE) PARAMETER:                               â”‚\n",
    "    â”‚    â€¢ Î± high: Prioritize entropy (more exploration)           â”‚\n",
    "    â”‚    â€¢ Î± low: Prioritize reward (more exploitation)            â”‚\n",
    "    â”‚    â€¢ SAC learns Î± automatically!                             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(probs):\n",
    "    \"\"\"Compute entropy of a probability distribution.\"\"\"\n",
    "    probs = np.array(probs) + 1e-10\n",
    "    return -np.sum(probs * np.log(probs))\n",
    "\n",
    "# Demonstrate entropy\n",
    "print(\"ENTROPY: MEASURING RANDOMNESS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "distributions = [\n",
    "    ([1.0, 0.0, 0.0, 0.0], \"Deterministic (always action 1)\"),\n",
    "    ([0.7, 0.2, 0.1, 0.0], \"Low entropy (mostly action 1)\"),\n",
    "    ([0.4, 0.3, 0.2, 0.1], \"Medium entropy (preferences)\"),\n",
    "    ([0.25, 0.25, 0.25, 0.25], \"Maximum entropy (uniform)\"),\n",
    "]\n",
    "\n",
    "print(\"\\nAction probability distributions:\")\n",
    "entropies = []\n",
    "for probs, desc in distributions:\n",
    "    H = compute_entropy(probs)\n",
    "    entropies.append(H)\n",
    "    print(f\"  {probs} â†’ H = {H:.3f} ({desc})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Higher entropy = more randomness = more exploration!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entropy and its effect\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Left: Different entropy levels\n",
    "ax1 = axes[0]\n",
    "actions = ['A1', 'A2', 'A3', 'A4']\n",
    "x = np.arange(len(actions))\n",
    "width = 0.2\n",
    "\n",
    "colors = ['#d32f2f', '#ff9800', '#4caf50', '#2196f3']\n",
    "for i, (probs, desc) in enumerate(distributions):\n",
    "    ax1.bar(x + i*width, probs, width, label=f'H={entropies[i]:.2f}', color=colors[i], alpha=0.8)\n",
    "\n",
    "ax1.set_xticks(x + 1.5*width)\n",
    "ax1.set_xticklabels(actions)\n",
    "ax1.set_ylabel('Probability', fontsize=11)\n",
    "ax1.set_title('Action Distributions with Different Entropy', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Middle: Entropy vs spread\n",
    "ax2 = axes[1]\n",
    "# Continuous case: Gaussian with different std\n",
    "x_vals = np.linspace(-4, 4, 100)\n",
    "stds = [0.5, 1.0, 2.0]\n",
    "colors = ['#d32f2f', '#4caf50', '#2196f3']\n",
    "\n",
    "for std, color in zip(stds, colors):\n",
    "    y = np.exp(-x_vals**2 / (2*std**2)) / (std * np.sqrt(2*np.pi))\n",
    "    # Entropy of Gaussian: 0.5 * ln(2Ï€e ÏƒÂ²)\n",
    "    H_gaussian = 0.5 * np.log(2 * np.pi * np.e * std**2)\n",
    "    ax2.plot(x_vals, y, color=color, linewidth=2, label=f'Ïƒ={std}, H={H_gaussian:.2f}')\n",
    "    ax2.fill_between(x_vals, y, alpha=0.2, color=color)\n",
    "\n",
    "ax2.set_xlabel('Action value', fontsize=11)\n",
    "ax2.set_ylabel('Probability density', fontsize=11)\n",
    "ax2.set_title('Continuous Actions: Gaussian Policies', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Effect of temperature Î±\n",
    "ax3 = axes[2]\n",
    "alphas = np.linspace(0, 2, 100)\n",
    "exploration = 1 - np.exp(-alphas*2)\n",
    "exploitation = np.exp(-alphas)\n",
    "\n",
    "ax3.fill_between(alphas, 0, exploration, alpha=0.3, color='#2196f3', label='Exploration')\n",
    "ax3.fill_between(alphas, 0, exploitation, alpha=0.3, color='#4caf50', label='Exploitation')\n",
    "ax3.plot(alphas, exploration, 'b-', linewidth=2)\n",
    "ax3.plot(alphas, exploitation, 'g-', linewidth=2)\n",
    "\n",
    "ax3.axvline(x=0.5, color='red', linestyle='--', label='Typical Î±')\n",
    "ax3.set_xlabel('Temperature Î±', fontsize=11)\n",
    "ax3.set_ylabel('Behavior emphasis', fontsize=11)\n",
    "ax3.set_title('Temperature Controls Exploration', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SAC Architecture: The Key Components\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              SAC ARCHITECTURE                                  â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  1. ACTOR (Policy Network):                                   â”‚\n",
    "    â”‚     â€¢ Outputs: Î¼(s), Ïƒ(s) for Gaussian                       â”‚\n",
    "    â”‚     â€¢ Sample: a = Î¼ + Ïƒ Ã— Îµ, where Îµ ~ N(0,1)                â”‚\n",
    "    â”‚     â€¢ Uses reparameterization trick                          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  2. TWIN CRITICS (Two Q-Networks):                           â”‚\n",
    "    â”‚     â€¢ Qâ‚(s, a) and Qâ‚‚(s, a)                                  â”‚\n",
    "    â”‚     â€¢ Take minimum: min(Qâ‚, Qâ‚‚)                              â”‚\n",
    "    â”‚     â€¢ Prevents overestimation (like TD3)                     â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  3. TARGET NETWORKS:                                          â”‚\n",
    "    â”‚     â€¢ Soft update: Î¸' â† Ï„Î¸ + (1-Ï„)Î¸'                        â”‚\n",
    "    â”‚     â€¢ Stabilizes training                                    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  4. AUTOMATIC TEMPERATURE (Î±):                                â”‚\n",
    "    â”‚     â€¢ Learned to maintain target entropy                     â”‚\n",
    "    â”‚     â€¢ No manual tuning needed!                               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  5. REPLAY BUFFER (Off-Policy):                               â”‚\n",
    "    â”‚     â€¢ Store and reuse past experiences                       â”‚\n",
    "    â”‚     â€¢ Much more sample efficient than PPO                    â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SAC architecture\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('SAC Architecture: Actor-Critic with Maximum Entropy', fontsize=16, fontweight='bold')\n",
    "\n",
    "# State input\n",
    "state_box = FancyBboxPatch((6, 10), 2, 1, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(state_box)\n",
    "ax.text(7, 10.5, 'State s', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Actor (Policy)\n",
    "actor_box = FancyBboxPatch((1, 6.5), 3.5, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(actor_box)\n",
    "ax.text(2.75, 8.3, 'ACTOR', ha='center', fontsize=11, fontweight='bold', color='#388e3c')\n",
    "ax.text(2.75, 7.6, 'Ï€(a|s)', ha='center', fontsize=10)\n",
    "ax.text(2.75, 7, 'Outputs: Î¼, Ïƒ', ha='center', fontsize=9)\n",
    "\n",
    "# Twin Critics\n",
    "critic1_box = FancyBboxPatch((5.5, 6.5), 1.8, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                              facecolor='#e1bee7', edgecolor='#7b1fa2', linewidth=2)\n",
    "ax.add_patch(critic1_box)\n",
    "ax.text(6.4, 8.2, 'Qâ‚', ha='center', fontsize=11, fontweight='bold', color='#7b1fa2')\n",
    "ax.text(6.4, 7.5, 'Qâ‚(s,a)', ha='center', fontsize=9)\n",
    "\n",
    "critic2_box = FancyBboxPatch((7.7, 6.5), 1.8, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                              facecolor='#e1bee7', edgecolor='#7b1fa2', linewidth=2)\n",
    "ax.add_patch(critic2_box)\n",
    "ax.text(8.6, 8.2, 'Qâ‚‚', ha='center', fontsize=11, fontweight='bold', color='#7b1fa2')\n",
    "ax.text(8.6, 7.5, 'Qâ‚‚(s,a)', ha='center', fontsize=9)\n",
    "\n",
    "# Min box\n",
    "min_box = FancyBboxPatch((6.4, 4.5), 2.2, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax.add_patch(min_box)\n",
    "ax.text(7.5, 5.1, 'min(Qâ‚, Qâ‚‚)', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Temperature Î±\n",
    "temp_box = FancyBboxPatch((10.5, 6.5), 2.5, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "ax.add_patch(temp_box)\n",
    "ax.text(11.75, 8.2, 'Temperature', ha='center', fontsize=10, fontweight='bold', color='#d32f2f')\n",
    "ax.text(11.75, 7.5, 'Î± (learned)', ha='center', fontsize=10)\n",
    "ax.text(11.75, 6.9, 'Auto-tuned!', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# Replay Buffer\n",
    "buffer_box = FancyBboxPatch((1, 2.5), 4, 2, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#bbdefb', edgecolor='#1976d2', linewidth=2)\n",
    "ax.add_patch(buffer_box)\n",
    "ax.text(3, 4, 'Replay Buffer', ha='center', fontsize=11, fontweight='bold', color='#1976d2')\n",
    "ax.text(3, 3.2, 'Stores (s, a, r, s\\')', ha='center', fontsize=9)\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(2.75, 9), xytext=(6, 10),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(7.5, 9), xytext=(7, 9.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(6.9, 6.4), xytext=(6.9, 5.8),\n",
    "            arrowprops=dict(arrowstyle='->', lw=1, color='#666'))\n",
    "ax.annotate('', xy=(8.1, 6.4), xytext=(8.1, 5.8),\n",
    "            arrowprops=dict(arrowstyle='->', lw=1, color='#666'))\n",
    "\n",
    "# Labels\n",
    "ax.text(7.5, 1.5, 'OFF-POLICY: Sample from buffer, not current policy!', \n",
    "        ha='center', fontsize=11, fontweight='bold', color='#1976d2')\n",
    "ax.text(7.5, 0.8, 'This makes SAC much more sample efficient than PPO.', \n",
    "        ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSAC'S KEY INNOVATIONS:\")\n",
    "print(\"  1. Twin Critics: Take minimum to prevent overestimation\")\n",
    "print(\"  2. Automatic Temperature: No manual Î± tuning\")\n",
    "print(\"  3. Off-Policy: Reuse past experiences (sample efficient!)\")\n",
    "print(\"  4. Entropy Bonus: Encourages exploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## On-Policy vs Off-Policy: Why SAC is Sample Efficient\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              ON-POLICY vs OFF-POLICY                           â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  ON-POLICY (PPO, A2C):                                        â”‚\n",
    "    â”‚    â€¢ Collect data with current policy                        â”‚\n",
    "    â”‚    â€¢ Use data ONCE for update                                â”‚\n",
    "    â”‚    â€¢ Throw away data after update                            â”‚\n",
    "    â”‚    â€¢ Need lots of new data constantly                        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  OFF-POLICY (SAC, TD3, DQN):                                  â”‚\n",
    "    â”‚    â€¢ Store ALL past experiences in buffer                    â”‚\n",
    "    â”‚    â€¢ Sample random batches for training                      â”‚\n",
    "    â”‚    â€¢ Reuse data many times                                   â”‚\n",
    "    â”‚    â€¢ Much more sample efficient!                             â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  ANALOGY:                                                     â”‚\n",
    "    â”‚    On-policy: Student who only learns from today's class    â”‚\n",
    "    â”‚    Off-policy: Student who reviews all past notes + today   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  SAMPLE EFFICIENCY:                                           â”‚\n",
    "    â”‚    SAC can learn from 100K samples what PPO needs 1M for!   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize on-policy vs off-policy\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: On-policy data flow\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('ON-POLICY (PPO)\\n\"Use once, throw away\"', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Show data flow\n",
    "for i in range(3):\n",
    "    y = 7.5 - i * 2.5\n",
    "    # Collect\n",
    "    collect_box = FancyBboxPatch((1, y), 2, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                                  facecolor='#bbdefb', edgecolor='#1976d2', linewidth=2)\n",
    "    ax1.add_patch(collect_box)\n",
    "    ax1.text(2, y + 0.75, f'Batch {i+1}', ha='center', fontsize=10)\n",
    "    \n",
    "    # Arrow to update\n",
    "    ax1.annotate('', xy=(4, y + 0.75), xytext=(3.2, y + 0.75),\n",
    "                 arrowprops=dict(arrowstyle='->', lw=2, color='#388e3c'))\n",
    "    \n",
    "    # Update\n",
    "    update_box = FancyBboxPatch((4.2, y), 2, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                                 facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "    ax1.add_patch(update_box)\n",
    "    ax1.text(5.2, y + 0.75, 'Update', ha='center', fontsize=10)\n",
    "    \n",
    "    # Arrow to trash\n",
    "    ax1.annotate('', xy=(7.5, y + 0.75), xytext=(6.4, y + 0.75),\n",
    "                 arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f'))\n",
    "    \n",
    "    # Trash\n",
    "    ax1.text(8, y + 0.75, 'ğŸ—‘ï¸', ha='center', fontsize=20)\n",
    "\n",
    "ax1.text(5, 1, 'âŒ Data used once, then discarded', ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "# Right: Off-policy data flow\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('OFF-POLICY (SAC)\\n\"Store and reuse forever\"', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Replay buffer (big)\n",
    "buffer_box = FancyBboxPatch((1, 3), 4, 5, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#bbdefb', edgecolor='#1976d2', linewidth=3)\n",
    "ax2.add_patch(buffer_box)\n",
    "ax2.text(3, 7.5, 'Replay Buffer', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Show accumulated data\n",
    "for i in range(5):\n",
    "    y = 3.5 + i * 0.8\n",
    "    mini_box = FancyBboxPatch((1.5, y), 3, 0.6, boxstyle=\"round,pad=0.05\",\n",
    "                               facecolor='#90caf9', edgecolor='#1976d2', linewidth=1)\n",
    "    ax2.add_patch(mini_box)\n",
    "    ax2.text(3, y + 0.3, f'Data {i+1}', ha='center', fontsize=8)\n",
    "\n",
    "# Arrow: new data in\n",
    "ax2.annotate('', xy=(3, 8.2), xytext=(3, 9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#1976d2'))\n",
    "ax2.text(3, 9.3, 'New data', ha='center', fontsize=9)\n",
    "\n",
    "# Arrow: sample out\n",
    "ax2.annotate('', xy=(6, 5.5), xytext=(5.2, 5.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#388e3c'))\n",
    "\n",
    "# Update box\n",
    "update_box = FancyBboxPatch((6.2, 4.5), 2.5, 2, boxstyle=\"round,pad=0.1\",\n",
    "                             facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax2.add_patch(update_box)\n",
    "ax2.text(7.45, 5.8, 'Update', ha='center', fontsize=11, fontweight='bold')\n",
    "ax2.text(7.45, 5, '(random batch)', ha='center', fontsize=9)\n",
    "\n",
    "ax2.text(5, 1.5, 'âœ“ Data reused many times!', ha='center', fontsize=10, color='#388e3c')\n",
    "ax2.text(5, 0.8, 'âœ“ 10x more sample efficient', ha='center', fontsize=10, color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using SAC with Stable-Baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    print(\"SAC WITH STABLE-BASELINES3\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # SAC works with CONTINUOUS action spaces\n",
    "    # Pendulum-v1 is a classic continuous control task\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    \n",
    "    print(f\"\\nEnvironment: Pendulum-v1\")\n",
    "    print(f\"  Observation space: {env.observation_space}\")\n",
    "    print(f\"  Action space: {env.action_space}\")\n",
    "    print(f\"  â†’ Continuous action! (torque from -2 to +2)\")\n",
    "    \n",
    "    # Create SAC model\n",
    "    model = SAC(\n",
    "        policy='MlpPolicy',\n",
    "        env=env,\n",
    "        learning_rate=3e-4,\n",
    "        buffer_size=50000,        # Replay buffer size\n",
    "        learning_starts=1000,      # Start training after this many steps\n",
    "        batch_size=256,            # Batch size for training\n",
    "        tau=0.005,                 # Soft update coefficient\n",
    "        gamma=0.99,                # Discount factor\n",
    "        train_freq=1,              # Update every step\n",
    "        gradient_steps=1,          # Gradient steps per update\n",
    "        ent_coef='auto',           # Automatic temperature!\n",
    "        verbose=0,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCreated SAC model with:\")\n",
    "    print(f\"  buffer_size: {model.buffer_size}\")\n",
    "    print(f\"  batch_size: {model.batch_size}\")\n",
    "    print(f\"  ent_coef: {model.ent_coef} (auto-tuned!)\")\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nTraining for 10,000 timesteps...\")\n",
    "    model.learn(total_timesteps=10_000, progress_bar=True)\n",
    "    \n",
    "    # Evaluate\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    print(f\"\\nEvaluation: {mean_reward:.1f} Â± {std_reward:.1f}\")\n",
    "    print(\"(Pendulum: higher is better, max around -200)\")\n",
    "    \n",
    "    env.close()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"Install Stable-Baselines3 to run this example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    # Train longer and visualize\n",
    "    print(\"TRAINING SAC FOR LONGER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    env = gym.make('Pendulum-v1')\n",
    "    model = SAC('MlpPolicy', env, verbose=0, ent_coef='auto')\n",
    "    \n",
    "    # Track rewards during training\n",
    "    rewards_during_training = []\n",
    "    \n",
    "    print(\"Training with periodic evaluation...\")\n",
    "    for i in range(10):\n",
    "        model.learn(total_timesteps=5000, reset_num_timesteps=False)\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5, deterministic=True)\n",
    "        rewards_during_training.append(mean_reward)\n",
    "        print(f\"  Step {(i+1)*5000:5d}: Mean reward = {mean_reward:.1f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    steps = [5000 * (i+1) for i in range(len(rewards_during_training))]\n",
    "    ax.plot(steps, rewards_during_training, 'go-', linewidth=2, markersize=8)\n",
    "    ax.axhline(y=-200, color='red', linestyle='--', label='Good performance')\n",
    "    ax.set_xlabel('Training Steps', fontsize=11)\n",
    "    ax.set_ylabel('Mean Reward', fontsize=11)\n",
    "    ax.set_title('SAC Training on Pendulum-v1', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    env.close()\n",
    "else:\n",
    "    print(\"Install Stable-Baselines3 to run this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PPO vs SAC: When to Use Which?\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              PPO vs SAC COMPARISON                             â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  USE PPO WHEN:                                                â”‚\n",
    "    â”‚    âœ“ Discrete actions (games, text)                          â”‚\n",
    "    â”‚    âœ“ Simulations are cheap (can get lots of data)            â”‚\n",
    "    â”‚    âœ“ Need stable, reliable training                          â”‚\n",
    "    â”‚    âœ“ RLHF for language models                                â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  USE SAC WHEN:                                                â”‚\n",
    "    â”‚    âœ“ Continuous actions (robotics, control)                  â”‚\n",
    "    â”‚    âœ“ Real-world where data is expensive                      â”‚\n",
    "    â”‚    âœ“ Need sample efficiency                                  â”‚\n",
    "    â”‚    âœ“ Exploration is important                                â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  KEY DIFFERENCES:                                             â”‚\n",
    "    â”‚    â€¢ PPO: On-policy, stable, less sample efficient           â”‚\n",
    "    â”‚    â€¢ SAC: Off-policy, sample efficient, continuous only      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PPO vs SAC comparison\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('PPO vs SAC: Choose the Right Tool', fontsize=16, fontweight='bold')\n",
    "\n",
    "# PPO column\n",
    "ppo_box = FancyBboxPatch((0.5, 2), 5, 6.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(ppo_box)\n",
    "ax.text(3, 8, 'PPO', ha='center', fontsize=14, fontweight='bold', color='#1976d2')\n",
    "\n",
    "ppo_features = [\n",
    "    'âœ“ Discrete & Continuous',\n",
    "    'âœ“ Very Stable',\n",
    "    'âœ“ Simple to tune',\n",
    "    'âœ“ RLHF standard',\n",
    "    'âœ— Less sample efficient',\n",
    "    'âœ— Needs lots of data',\n",
    "]\n",
    "for i, feat in enumerate(ppo_features):\n",
    "    color = '#388e3c' if feat.startswith('âœ“') else '#d32f2f'\n",
    "    ax.text(1, 7 - i*0.7, feat, fontsize=10, color=color)\n",
    "\n",
    "# SAC column\n",
    "sac_box = FancyBboxPatch((6.5, 2), 5, 6.5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#e8f5e9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(sac_box)\n",
    "ax.text(9, 8, 'SAC', ha='center', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "sac_features = [\n",
    "    'âœ“ Very Sample Efficient',\n",
    "    'âœ“ Great Exploration',\n",
    "    'âœ“ Automatic Tuning (Î±)',\n",
    "    'âœ“ Off-policy (reuse data)',\n",
    "    'âœ— Continuous only',\n",
    "    'âœ— More complex',\n",
    "]\n",
    "for i, feat in enumerate(sac_features):\n",
    "    color = '#388e3c' if feat.startswith('âœ“') else '#d32f2f'\n",
    "    ax.text(7, 7 - i*0.7, feat, fontsize=10, color=color)\n",
    "\n",
    "# Use cases\n",
    "ax.text(3, 1.3, 'Best for:', ha='center', fontsize=10, fontweight='bold')\n",
    "ax.text(3, 0.7, 'Games, LLM alignment', ha='center', fontsize=10)\n",
    "\n",
    "ax.text(9, 1.3, 'Best for:', ha='center', fontsize=10, fontweight='bold')\n",
    "ax.text(9, 0.7, 'Robotics, control', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### Maximum Entropy RL\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Objective** | Maximize reward + entropy |\n",
    "| **Entropy** | H(Ï€) = -E[log Ï€(a\\|s)] |\n",
    "| **Temperature Î±** | Balances reward vs entropy |\n",
    "\n",
    "### SAC Components\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| Twin Critics | Prevent Q overestimation |\n",
    "| Replay Buffer | Sample efficiency |\n",
    "| Auto Temperature | No manual tuning |\n",
    "| Soft Updates | Stable training |\n",
    "\n",
    "### PPO vs SAC\n",
    "\n",
    "| Aspect | PPO | SAC |\n",
    "|--------|-----|-----|\n",
    "| **Policy Type** | On-policy | Off-policy |\n",
    "| **Action Space** | Any | Continuous |\n",
    "| **Sample Efficiency** | Lower | Higher |\n",
    "| **Best For** | Games, RLHF | Robotics |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What does \"maximum entropy\" mean in SAC?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "SAC maximizes both expected reward AND policy entropy:\n",
    "J(Ï€) = E[Î£ r(s,a)] + Î± Ã— H(Ï€)\n",
    "\n",
    "High entropy means the policy maintains randomness/variety in its actions, which:\n",
    "- Improves exploration\n",
    "- Makes the policy more robust\n",
    "- Keeps multiple good solutions ready\n",
    "</details>\n",
    "\n",
    "**2. Why does SAC use twin critics?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Twin critics (Qâ‚ and Qâ‚‚) help prevent overestimation of Q-values:\n",
    "- Each critic may overestimate differently\n",
    "- Taking min(Qâ‚, Qâ‚‚) gives a more conservative estimate\n",
    "- This leads to more stable training\n",
    "\n",
    "This technique comes from TD3 (Twin Delayed DDPG).\n",
    "</details>\n",
    "\n",
    "**3. Why is SAC more sample efficient than PPO?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "SAC is off-policy, meaning it can reuse past experiences:\n",
    "- Stores all transitions in a replay buffer\n",
    "- Samples random batches for training\n",
    "- Uses each experience many times\n",
    "\n",
    "PPO is on-policy: uses data once then discards it.\n",
    "</details>\n",
    "\n",
    "**4. What does automatic temperature (Î±) mean?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "SAC automatically adjusts Î± to maintain a target entropy:\n",
    "- If policy becomes too deterministic â†’ increase Î± (more exploration)\n",
    "- If policy is too random â†’ decrease Î± (more exploitation)\n",
    "\n",
    "This removes the need for manual hyperparameter tuning!\n",
    "</details>\n",
    "\n",
    "**5. When should you choose SAC over PPO?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Choose SAC when:\n",
    "- You have continuous actions (robotics, control)\n",
    "- Data is expensive (real-world, slow simulations)\n",
    "- Sample efficiency is critical\n",
    "- You need good exploration\n",
    "\n",
    "Choose PPO when:\n",
    "- You have discrete actions (games, text)\n",
    "- Simulations are fast/cheap\n",
    "- You need maximum stability\n",
    "- Doing RLHF for LLMs\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "You've learned about SAC, the state-of-the-art for continuous control! In the next notebook, we'll compare all the advanced methods we've learned.\n",
    "\n",
    "**Continue to:** [Notebook 5: Comparing Advanced Methods](05_comparing_advanced_methods.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*SAC: \"Stay random, stay creative, stay efficient!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
