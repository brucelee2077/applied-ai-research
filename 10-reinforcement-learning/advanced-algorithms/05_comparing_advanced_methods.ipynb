{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Advanced Methods: Choosing Your Champion\n",
    "\n",
    "Now that you've learned the major algorithms, how do you choose the right one for your task?\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The tournament analogy: when each algorithm shines\n",
    "- Head-to-head comparisons: TRPO vs PPO vs SAC vs TD3\n",
    "- Benchmark results on different environments\n",
    "- Decision flowchart: picking the right algorithm\n",
    "- Practical considerations: compute, sample efficiency, implementation\n",
    "- Running your own comparisons\n",
    "\n",
    "**Prerequisites:** Notebooks 1-4 (Trust Regions, PPO, SB3, SAC)\n",
    "\n",
    "**Time:** ~25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Tournament Analogy\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          THE RL ALGORITHM TOURNAMENT                           â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  Imagine a sports tournament where athletes compete in        â”‚\n",
    "    â”‚  different events. Each has their specialty!                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  PPO (The All-Rounder):                                       â”‚\n",
    "    â”‚    \"I'm reliable in any event. Maybe not the fastest,        â”‚\n",
    "    â”‚     but I won't let you down!\"                               â”‚\n",
    "    â”‚    â†’ Best for: General purpose, stability matters            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  SAC (The Efficient Learner):                                 â”‚\n",
    "    â”‚    \"I learn the most from every practice session!            â”‚\n",
    "    â”‚     Just give me continuous events.\"                         â”‚\n",
    "    â”‚    â†’ Best for: Robotics, continuous control                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TD3 (The Precision Expert):                                  â”‚\n",
    "    â”‚    \"I'm extremely careful and precise in my training.        â”‚\n",
    "    â”‚     Continuous control is my specialty.\"                     â”‚\n",
    "    â”‚    â†’ Best for: When SAC is too exploratory                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TRPO (The Careful Strategist):                               â”‚\n",
    "    â”‚    \"I guarantee safe progress, but I'm complex to work with.\"â”‚\n",
    "    â”‚    â†’ Best for: Research, theoretical guarantees              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  NO SINGLE CHAMPION! Each excels in their domain.            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, Wedge\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym\n",
    "\n",
    "# Check if Stable-Baselines3 is available\n",
    "try:\n",
    "    from stable_baselines3 import PPO, SAC, TD3, A2C\n",
    "    from stable_baselines3.common.env_util import make_vec_env\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    import torch\n",
    "    SB3_AVAILABLE = True\n",
    "    print(\"âœ“ Stable-Baselines3 is installed!\")\n",
    "except ImportError:\n",
    "    SB3_AVAILABLE = False\n",
    "    print(\"âœ— Stable-Baselines3 not installed.\")\n",
    "    print(\"  Install with: pip install stable-baselines3[extra]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize algorithm specializations\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('The RL Algorithm Tournament: Each Has Their Specialty', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Algorithm cards\n",
    "algorithms = [\n",
    "    ('PPO', 1, 8.5, '#c8e6c9', '#388e3c', 'The All-Rounder', \n",
    "     ['âœ“ Discrete & Continuous', 'âœ“ Very Stable', 'âœ“ Easy to tune', 'âœ— Less sample efficient']),\n",
    "    ('SAC', 7.5, 8.5, '#bbdefb', '#1976d2', 'The Efficient Learner',\n",
    "     ['âœ“ Sample efficient', 'âœ“ Good exploration', 'âœ“ Auto-tuning', 'âœ— Continuous only']),\n",
    "    ('TD3', 1, 3.5, '#fff3e0', '#f57c00', 'The Precision Expert',\n",
    "     ['âœ“ Very stable critic', 'âœ“ Deterministic policy', 'âœ“ Sample efficient', 'âœ— Less exploration']),\n",
    "    ('TRPO', 7.5, 3.5, '#e1bee7', '#7b1fa2', 'The Careful Strategist',\n",
    "     ['âœ“ Monotonic improvement', 'âœ“ Strong theory', 'âœ— Complex to implement', 'âœ— Computationally heavy']),\n",
    "]\n",
    "\n",
    "for name, x, y, fcolor, ecolor, title, features in algorithms:\n",
    "    # Card background\n",
    "    card = FancyBboxPatch((x, y), 5.5, 3.5, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor=fcolor, edgecolor=ecolor, linewidth=3)\n",
    "    ax.add_patch(card)\n",
    "    \n",
    "    # Algorithm name\n",
    "    ax.text(x + 2.75, y + 3, name, ha='center', fontsize=14, fontweight='bold', color=ecolor)\n",
    "    ax.text(x + 2.75, y + 2.5, title, ha='center', fontsize=10, style='italic')\n",
    "    \n",
    "    # Features\n",
    "    for i, feat in enumerate(features):\n",
    "        color = '#388e3c' if feat.startswith('âœ“') else '#d32f2f'\n",
    "        ax.text(x + 0.3, y + 1.8 - i*0.45, feat, fontsize=9, color=color)\n",
    "\n",
    "# Central message\n",
    "ax.text(7, 7, 'CHOOSE BASED ON YOUR TASK!', ha='center', fontsize=12, \n",
    "        fontweight='bold', color='#333')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNO ALGORITHM IS BEST FOR EVERYTHING!\")\n",
    "print(\"  The right choice depends on your specific task.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Reference: Algorithm Overview\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              ALGORITHM OVERVIEW                                â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "    â”‚  â”‚                ON-POLICY METHODS                          â”‚ â”‚\n",
    "    â”‚  â”‚  (Use data once, then discard)                           â”‚ â”‚\n",
    "    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚\n",
    "    â”‚  â”‚                                                          â”‚ â”‚\n",
    "    â”‚  â”‚  TRPO: Trust region constraint (KL divergence)           â”‚ â”‚\n",
    "    â”‚  â”‚        Complex optimization, strong guarantees           â”‚ â”‚\n",
    "    â”‚  â”‚                                                          â”‚ â”‚\n",
    "    â”‚  â”‚  PPO:  Clipped objective (simpler than TRPO)             â”‚ â”‚\n",
    "    â”‚  â”‚        Industry standard, very popular                   â”‚ â”‚\n",
    "    â”‚  â”‚                                                          â”‚ â”‚\n",
    "    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "    â”‚  â”‚               OFF-POLICY METHODS                          â”‚ â”‚\n",
    "    â”‚  â”‚  (Store and reuse data via replay buffer)                â”‚ â”‚\n",
    "    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚\n",
    "    â”‚  â”‚                                                          â”‚ â”‚\n",
    "    â”‚  â”‚  SAC:  Maximum entropy, stochastic policy                â”‚ â”‚\n",
    "    â”‚  â”‚        Best exploration, continuous control              â”‚ â”‚\n",
    "    â”‚  â”‚                                                          â”‚ â”‚\n",
    "    â”‚  â”‚  TD3:  Twin critics, deterministic policy                â”‚ â”‚\n",
    "    â”‚  â”‚        Very stable, less exploration than SAC            â”‚ â”‚\n",
    "    â”‚  â”‚                                                          â”‚ â”‚\n",
    "    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed comparison table\n",
    "print(\"ALGORITHM COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = {\n",
    "    'Feature': ['Type', 'Actions', 'Policy', 'Sample Efficiency', \n",
    "                'Stability', 'Implementation', 'Use Cases'],\n",
    "    'PPO': ['On-policy', 'Discrete/Continuous', 'Stochastic', 'Lower', \n",
    "            'Very High', 'Easy', 'General, RLHF'],\n",
    "    'SAC': ['Off-policy', 'Continuous', 'Stochastic', 'High', \n",
    "            'High', 'Medium', 'Robotics'],\n",
    "    'TD3': ['Off-policy', 'Continuous', 'Deterministic', 'High', \n",
    "            'Very High', 'Medium', 'Precision control'],\n",
    "    'TRPO': ['On-policy', 'Discrete/Continuous', 'Stochastic', 'Lower', \n",
    "             'High', 'Hard', 'Research'],\n",
    "}\n",
    "\n",
    "# Print table\n",
    "header = f\"{'Feature':<20} {'PPO':<15} {'SAC':<15} {'TD3':<15} {'TRPO':<15}\"\n",
    "print(header)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i in range(len(comparison['Feature'])):\n",
    "    row = f\"{comparison['Feature'][i]:<20} \"\n",
    "    row += f\"{comparison['PPO'][i]:<15} \"\n",
    "    row += f\"{comparison['SAC'][i]:<15} \"\n",
    "    row += f\"{comparison['TD3'][i]:<15} \"\n",
    "    row += f\"{comparison['TRPO'][i]:<15}\"\n",
    "    print(row)\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dimension 1: On-Policy vs Off-Policy\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              ON-POLICY vs OFF-POLICY                           â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  ON-POLICY (PPO, TRPO):                                       â”‚\n",
    "    â”‚    â€¢ Data must come from CURRENT policy                       â”‚\n",
    "    â”‚    â€¢ Use once, then throw away                                â”‚\n",
    "    â”‚    â€¢ Need lots of environment interactions                    â”‚\n",
    "    â”‚    â€¢ But: Very stable gradients                               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    When is this OK?                                           â”‚\n",
    "    â”‚      âœ“ Simulations are fast/cheap                             â”‚\n",
    "    â”‚      âœ“ You have lots of compute                               â”‚\n",
    "    â”‚      âœ“ Stability > efficiency                                 â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  OFF-POLICY (SAC, TD3):                                       â”‚\n",
    "    â”‚    â€¢ Can learn from ANY past experience                       â”‚\n",
    "    â”‚    â€¢ Store in replay buffer, reuse many times                 â”‚\n",
    "    â”‚    â€¢ Much more sample efficient                               â”‚\n",
    "    â”‚    â€¢ But: Potential instability from old data                 â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    When is this better?                                       â”‚\n",
    "    â”‚      âœ“ Real-world data (expensive to collect)                 â”‚\n",
    "    â”‚      âœ“ Limited environment interactions                       â”‚\n",
    "    â”‚      âœ“ Continuous control tasks                               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample efficiency difference\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Sample efficiency comparison\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Simulated learning curves (samples needed to reach target performance)\n",
    "samples = np.linspace(0, 1e6, 100)\n",
    "\n",
    "# On-policy (PPO) - needs more samples\n",
    "ppo_perf = 100 * (1 - np.exp(-samples / 3e5))\n",
    "\n",
    "# Off-policy (SAC) - learns faster\n",
    "sac_perf = 100 * (1 - np.exp(-samples / 1e5))\n",
    "\n",
    "ax1.plot(samples/1e6, ppo_perf, 'g-', linewidth=3, label='PPO (on-policy)')\n",
    "ax1.plot(samples/1e6, sac_perf, 'b-', linewidth=3, label='SAC (off-policy)')\n",
    "\n",
    "ax1.axhline(y=90, color='red', linestyle='--', alpha=0.5, label='Target performance')\n",
    "\n",
    "# Mark samples to reach 90%\n",
    "ppo_samples_90 = -3e5 * np.log(1 - 0.9) / 1e6\n",
    "sac_samples_90 = -1e5 * np.log(1 - 0.9) / 1e6\n",
    "\n",
    "ax1.axvline(x=ppo_samples_90, color='green', linestyle=':', alpha=0.7)\n",
    "ax1.axvline(x=sac_samples_90, color='blue', linestyle=':', alpha=0.7)\n",
    "\n",
    "ax1.annotate(f'{ppo_samples_90:.2f}M', xy=(ppo_samples_90, 70), fontsize=10, color='green')\n",
    "ax1.annotate(f'{sac_samples_90:.2f}M', xy=(sac_samples_90, 70), fontsize=10, color='blue')\n",
    "\n",
    "ax1.set_xlabel('Environment Samples (Millions)', fontsize=11)\n",
    "ax1.set_ylabel('Performance (%)', fontsize=11)\n",
    "ax1.set_title('Sample Efficiency: On-Policy vs Off-Policy', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Data reuse visualization\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Data Reuse', fontsize=12, fontweight='bold')\n",
    "\n",
    "# On-policy: use once\n",
    "ax2.text(2.5, 9, 'ON-POLICY', ha='center', fontsize=11, fontweight='bold', color='#388e3c')\n",
    "for i in range(4):\n",
    "    box = FancyBboxPatch((0.5 + i*1.2, 7), 1, 1, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(1 + i*1.2, 7.5, f'D{i+1}', ha='center', fontsize=9)\n",
    "    # Arrow to trash\n",
    "    ax2.annotate('', xy=(1 + i*1.2, 6.9), xytext=(1 + i*1.2, 6.3),\n",
    "                 arrowprops=dict(arrowstyle='->', lw=1, color='#666'))\n",
    "    ax2.text(1 + i*1.2, 5.8, 'ğŸ—‘ï¸', ha='center', fontsize=12)\n",
    "\n",
    "# Off-policy: reuse many times\n",
    "ax2.text(2.5, 4.5, 'OFF-POLICY', ha='center', fontsize=11, fontweight='bold', color='#1976d2')\n",
    "\n",
    "# Replay buffer\n",
    "buffer = FancyBboxPatch((0.5, 1.5), 4, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#bbdefb', edgecolor='#1976d2', linewidth=2)\n",
    "ax2.add_patch(buffer)\n",
    "ax2.text(2.5, 3.7, 'Replay Buffer', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "for i in range(4):\n",
    "    mini = FancyBboxPatch((0.7 + i*0.9, 1.8), 0.7, 0.8, boxstyle=\"round,pad=0.02\",\n",
    "                           facecolor='#90caf9', edgecolor='#1976d2', linewidth=1)\n",
    "    ax2.add_patch(mini)\n",
    "    ax2.text(1.05 + i*0.9, 2.2, f'D{i+1}', ha='center', fontsize=8)\n",
    "\n",
    "# Arrows showing reuse\n",
    "ax2.annotate('', xy=(6, 2.5), xytext=(4.6, 2.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#1976d2'))\n",
    "ax2.annotate('', xy=(4.6, 3), xytext=(6, 3),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#1976d2'))\n",
    "\n",
    "ax2.text(6.5, 2.75, 'Reuse\\nmany\\ntimes!', ha='left', fontsize=10, color='#1976d2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHT:\")\n",
    "print(f\"  Off-policy methods need ~{ppo_samples_90/sac_samples_90:.1f}x FEWER samples!\")\n",
    "print(\"  But on-policy is more stable and works with discrete actions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dimension 2: Stochastic vs Deterministic Policy\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              STOCHASTIC vs DETERMINISTIC                       â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  STOCHASTIC (PPO, SAC, TRPO):                                 â”‚\n",
    "    â”‚    Ï€(a|s) outputs a DISTRIBUTION over actions                 â”‚\n",
    "    â”‚    Sample action from distribution                            â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    âœ“ Natural exploration (randomness built-in)                â”‚\n",
    "    â”‚    âœ“ Works for discrete actions                               â”‚\n",
    "    â”‚    âœ“ Can express uncertainty                                  â”‚\n",
    "    â”‚    âœ— May have suboptimal mean action                          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  DETERMINISTIC (TD3):                                         â”‚\n",
    "    â”‚    Î¼(s) outputs a SINGLE action directly                      â”‚\n",
    "    â”‚    Add exploration noise during training                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚    âœ“ No variance from policy sampling                         â”‚\n",
    "    â”‚    âœ“ Easier to optimize (gradient through action)             â”‚\n",
    "    â”‚    âœ— Need external exploration (noise)                        â”‚\n",
    "    â”‚    âœ— Only continuous actions                                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  SAC's ADVANTAGE:                                             â”‚\n",
    "    â”‚    Stochastic BUT with entropy bonus                          â”‚\n",
    "    â”‚    â†’ Gets benefits of both!                                   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stochastic vs deterministic policies\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Common setup\n",
    "x = np.linspace(-3, 3, 100)\n",
    "\n",
    "# Left: Stochastic (normal distribution)\n",
    "ax1 = axes[0]\n",
    "mu, sigma = 0.5, 0.8\n",
    "y = np.exp(-0.5 * ((x - mu) / sigma) ** 2) / (sigma * np.sqrt(2 * np.pi))\n",
    "ax1.fill_between(x, y, alpha=0.3, color='#4caf50')\n",
    "ax1.plot(x, y, 'g-', linewidth=2)\n",
    "ax1.axvline(x=mu, color='green', linestyle='--', linewidth=2, label=f'Î¼={mu}')\n",
    "\n",
    "# Sample points\n",
    "np.random.seed(42)\n",
    "samples = np.random.normal(mu, sigma, 20)\n",
    "ax1.scatter(samples, np.zeros_like(samples) + 0.02, c='green', s=50, zorder=5, alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Action', fontsize=11)\n",
    "ax1.set_ylabel('Probability Density', fontsize=11)\n",
    "ax1.set_title('Stochastic Policy (PPO, SAC)\\nÏ€(a|s) = N(Î¼, ÏƒÂ²)', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Middle: Deterministic (single point)\n",
    "ax2 = axes[1]\n",
    "ax2.axvline(x=0.5, color='#f57c00', linewidth=4, label='Î¼(s) = 0.5')\n",
    "ax2.scatter([0.5], [0.5], s=200, c='#f57c00', zorder=5, edgecolors='black')\n",
    "\n",
    "# Add noise for exploration (training only)\n",
    "noisy_samples = 0.5 + np.random.normal(0, 0.2, 10)\n",
    "ax2.scatter(noisy_samples, np.zeros_like(noisy_samples) + 0.1, c='orange', s=50, alpha=0.5, label='+ Exploration noise')\n",
    "\n",
    "ax2.set_xlabel('Action', fontsize=11)\n",
    "ax2.set_ylabel('', fontsize=11)\n",
    "ax2.set_title('Deterministic Policy (TD3)\\nÎ¼(s) = single value', fontsize=12, fontweight='bold', color='#f57c00')\n",
    "ax2.set_xlim(-3, 3)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: SAC (stochastic + entropy)\n",
    "ax3 = axes[2]\n",
    "# Show how SAC maintains higher entropy\n",
    "mu, sigma_sac = 0.5, 1.0  # Higher variance due to entropy bonus\n",
    "y_sac = np.exp(-0.5 * ((x - mu) / sigma_sac) ** 2) / (sigma_sac * np.sqrt(2 * np.pi))\n",
    "ax3.fill_between(x, y_sac, alpha=0.3, color='#2196f3')\n",
    "ax3.plot(x, y_sac, 'b-', linewidth=2, label='SAC (high entropy)')\n",
    "\n",
    "# Compare with PPO (lower entropy as training progresses)\n",
    "sigma_ppo = 0.5\n",
    "y_ppo = np.exp(-0.5 * ((x - mu) / sigma_ppo) ** 2) / (sigma_ppo * np.sqrt(2 * np.pi))\n",
    "ax3.plot(x, y_ppo, 'g--', linewidth=2, label='PPO (no entropy bonus)')\n",
    "\n",
    "ax3.set_xlabel('Action', fontsize=11)\n",
    "ax3.set_ylabel('Probability Density', fontsize=11)\n",
    "ax3.set_title('SAC: Entropy Bonus\\nKeeps exploration high!', fontsize=12, fontweight='bold', color='#1976d2')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHT:\")\n",
    "print(\"  â€¢ Stochastic: Exploration built into policy\")\n",
    "print(\"  â€¢ Deterministic: Need to add noise for exploration\")\n",
    "print(\"  â€¢ SAC: Best of both worlds with entropy bonus!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dimension 3: Key Innovations\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              KEY INNOVATIONS BY ALGORITHM                      â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TRPO:                                                        â”‚\n",
    "    â”‚    â€¢ KL divergence constraint for safe updates                â”‚\n",
    "    â”‚    â€¢ Natural gradient + conjugate gradient                    â”‚\n",
    "    â”‚    â€¢ Theoretical monotonic improvement guarantee              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  PPO:                                                         â”‚\n",
    "    â”‚    â€¢ Clipped surrogate objective                              â”‚\n",
    "    â”‚    â€¢ ratio âˆˆ [1-Îµ, 1+Îµ] bounds policy change                 â”‚\n",
    "    â”‚    â€¢ Simple to implement, works as well as TRPO              â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  SAC:                                                         â”‚\n",
    "    â”‚    â€¢ Maximum entropy objective (reward + entropy)             â”‚\n",
    "    â”‚    â€¢ Twin critics to prevent Q overestimation                 â”‚\n",
    "    â”‚    â€¢ Automatic temperature (Î±) tuning                         â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  TD3:                                                         â”‚\n",
    "    â”‚    â€¢ Twin critics (from SAC paper, but used first here)      â”‚\n",
    "    â”‚    â€¢ Delayed policy updates                                   â”‚\n",
    "    â”‚    â€¢ Target policy smoothing (noise on target actions)        â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key innovations\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top-left: TRPO - KL constraint\n",
    "ax1 = axes[0, 0]\n",
    "ax1.set_xlim(-3, 3)\n",
    "ax1.set_ylim(-3, 3)\n",
    "\n",
    "# Objective contours\n",
    "x = np.linspace(-3, 3, 50)\n",
    "y = np.linspace(-3, 3, 50)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = -0.5*(X-1)**2 - 0.3*(Y-1)**2 + 2\n",
    "ax1.contourf(X, Y, Z, levels=15, cmap='RdYlGn', alpha=0.6)\n",
    "\n",
    "# Current policy and trust region\n",
    "ax1.scatter([0], [0], s=150, c='blue', zorder=5, edgecolors='black', label='Î¸_old')\n",
    "circle = plt.Circle((0, 0), 1.0, fill=False, linestyle='--', color='blue', linewidth=3)\n",
    "ax1.add_patch(circle)\n",
    "\n",
    "ax1.set_xlabel('Î¸â‚', fontsize=11)\n",
    "ax1.set_ylabel('Î¸â‚‚', fontsize=11)\n",
    "ax1.set_title('TRPO: KL Constraint\\nKL(Ï€_old || Ï€_new) â‰¤ Î´', fontsize=12, fontweight='bold', color='#7b1fa2')\n",
    "ax1.text(0, -2, 'Trust Region', fontsize=10, ha='center', color='blue')\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Top-right: PPO - Clipped objective\n",
    "ax2 = axes[0, 1]\n",
    "epsilon = 0.2\n",
    "ratio = np.linspace(0.5, 1.5, 100)\n",
    "\n",
    "# Positive advantage\n",
    "A = 1\n",
    "unclipped = ratio * A\n",
    "clipped = np.clip(ratio, 1-epsilon, 1+epsilon) * A\n",
    "ppo_obj = np.minimum(unclipped, clipped)\n",
    "\n",
    "ax2.plot(ratio, ppo_obj, 'g-', linewidth=3, label='PPO objective')\n",
    "ax2.axvline(x=1-epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "ax2.axvline(x=1+epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "ax2.axvline(x=1, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "ax2.fill_between(ratio, 0, ppo_obj, where=(ratio > 1 + epsilon), alpha=0.2, color='red')\n",
    "ax2.text(1.35, 1.15, 'Clipped!', fontsize=10, color='red')\n",
    "\n",
    "ax2.set_xlabel('Probability Ratio r(Î¸)', fontsize=11)\n",
    "ax2.set_ylabel('Objective', fontsize=11)\n",
    "ax2.set_title('PPO: Clipped Objective\\nclip(r, 1-Îµ, 1+Îµ)', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom-left: SAC - Twin critics\n",
    "ax3 = axes[1, 0]\n",
    "steps = np.arange(100)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Q values with overestimation\n",
    "true_q = 50 + 30 * (1 - np.exp(-steps/30))\n",
    "q1 = true_q + np.random.randn(100) * 5 + 5  # Overestimates\n",
    "q2 = true_q + np.random.randn(100) * 5 + 3  # Also overestimates\n",
    "min_q = np.minimum(q1, q2)  # Taking min reduces overestimation\n",
    "\n",
    "ax3.plot(steps, true_q, 'k-', linewidth=2, label='True Q')\n",
    "ax3.plot(steps, q1, 'r-', alpha=0.5, linewidth=1, label='Qâ‚')\n",
    "ax3.plot(steps, q2, 'b-', alpha=0.5, linewidth=1, label='Qâ‚‚')\n",
    "ax3.plot(steps, min_q, 'g-', linewidth=2, label='min(Qâ‚, Qâ‚‚)')\n",
    "\n",
    "ax3.set_xlabel('Training Steps', fontsize=11)\n",
    "ax3.set_ylabel('Q Value', fontsize=11)\n",
    "ax3.set_title('SAC/TD3: Twin Critics\\nmin(Qâ‚, Qâ‚‚) prevents overestimation', fontsize=12, fontweight='bold', color='#1976d2')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom-right: TD3 - Delayed updates\n",
    "ax4 = axes[1, 1]\n",
    "ax4.set_xlim(0, 10)\n",
    "ax4.set_ylim(0, 10)\n",
    "ax4.axis('off')\n",
    "\n",
    "ax4.set_title('TD3: Delayed Policy Updates\\nUpdate policy every d critic updates', fontsize=12, fontweight='bold', color='#f57c00')\n",
    "\n",
    "# Critic updates (every step)\n",
    "ax4.text(1, 8, 'Critic:', fontsize=11, fontweight='bold')\n",
    "for i in range(6):\n",
    "    box = FancyBboxPatch((1.5 + i*1.2, 7), 0.8, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#e1bee7', edgecolor='#7b1fa2', linewidth=2)\n",
    "    ax4.add_patch(box)\n",
    "    ax4.text(1.9 + i*1.2, 7.4, 'âœ“', ha='center', fontsize=12, color='#7b1fa2')\n",
    "\n",
    "# Policy updates (every 2 steps)\n",
    "ax4.text(1, 5, 'Policy:', fontsize=11, fontweight='bold')\n",
    "for i in range(6):\n",
    "    if i % 2 == 1:  # Only every 2nd\n",
    "        box = FancyBboxPatch((1.5 + i*1.2, 4), 0.8, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                              facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "        ax4.add_patch(box)\n",
    "        ax4.text(1.9 + i*1.2, 4.4, 'âœ“', ha='center', fontsize=12, color='#f57c00')\n",
    "    else:\n",
    "        ax4.text(1.9 + i*1.2, 4.4, 'âœ—', ha='center', fontsize=12, color='#999')\n",
    "\n",
    "ax4.text(5, 2, 'Why? Critic needs to stabilize\\nbefore guiding the policy!', \n",
    "         ha='center', fontsize=10, color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Decision Flowchart: Choosing Your Algorithm\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              ALGORITHM DECISION FLOWCHART                      â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚                    START                                       â”‚\n",
    "    â”‚                      â”‚                                         â”‚\n",
    "    â”‚                      â–¼                                         â”‚\n",
    "    â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚\n",
    "    â”‚              â”‚  Discrete   â”‚                                   â”‚\n",
    "    â”‚              â”‚  Actions?   â”‚                                   â”‚\n",
    "    â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                   â”‚\n",
    "    â”‚                     â”‚                                          â”‚\n",
    "    â”‚           YES â—„â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â–º NO                                 â”‚\n",
    "    â”‚            â”‚                â”‚                                  â”‚\n",
    "    â”‚            â–¼                â–¼                                  â”‚\n",
    "    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
    "    â”‚     â”‚    PPO    â”‚    â”‚   Sample    â”‚                          â”‚\n",
    "    â”‚     â”‚ (Default) â”‚    â”‚ Efficiency  â”‚                          â”‚\n",
    "    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  Matters?   â”‚                          â”‚\n",
    "    â”‚                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                           â”‚\n",
    "    â”‚                             â”‚                                  â”‚\n",
    "    â”‚                   NO â—„â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â–º YES                       â”‚\n",
    "    â”‚                    â”‚                  â”‚                        â”‚\n",
    "    â”‚                    â–¼                  â–¼                        â”‚\n",
    "    â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "    â”‚             â”‚    PPO    â”‚      â”‚ Exploration â”‚                 â”‚\n",
    "    â”‚             â”‚ (Stable)  â”‚      â”‚ Important?  â”‚                 â”‚\n",
    "    â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "    â”‚                                       â”‚                        â”‚\n",
    "    â”‚                             YES â—„â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â–º NO               â”‚\n",
    "    â”‚                              â”‚                â”‚                â”‚\n",
    "    â”‚                              â–¼                â–¼                â”‚\n",
    "    â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "    â”‚                       â”‚    SAC    â”‚    â”‚    TD3    â”‚          â”‚\n",
    "    â”‚                       â”‚ (Entropy) â”‚    â”‚ (Stable)  â”‚          â”‚\n",
    "    â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive decision helper\n",
    "\n",
    "def recommend_algorithm(discrete_actions=False, sample_efficiency_important=False, \n",
    "                        exploration_important=True, rlhf=False):\n",
    "    \"\"\"\n",
    "    Recommend an RL algorithm based on your task requirements.\n",
    "    \n",
    "    Args:\n",
    "        discrete_actions: True if action space is discrete (games, text)\n",
    "        sample_efficiency_important: True if environment samples are expensive\n",
    "        exploration_important: True if task requires good exploration\n",
    "        rlhf: True if doing RL from human feedback for LLMs\n",
    "    \n",
    "    Returns:\n",
    "        str: Recommended algorithm with reasoning\n",
    "    \"\"\"\n",
    "    \n",
    "    if rlhf:\n",
    "        return \"PPO\", \"Standard choice for RLHF. Stable, works with discrete tokens.\"\n",
    "    \n",
    "    if discrete_actions:\n",
    "        return \"PPO\", \"Best general choice for discrete actions.\"\n",
    "    \n",
    "    # Continuous actions\n",
    "    if not sample_efficiency_important:\n",
    "        return \"PPO\", \"Stable and reliable for continuous control when samples are cheap.\"\n",
    "    \n",
    "    # Sample efficiency matters\n",
    "    if exploration_important:\n",
    "        return \"SAC\", \"Sample efficient + entropy bonus for exploration.\"\n",
    "    else:\n",
    "        return \"TD3\", \"Sample efficient, very stable, deterministic policy.\"\n",
    "\n",
    "\n",
    "print(\"ALGORITHM RECOMMENDATION EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scenarios = [\n",
    "    (\"Atari Game\", dict(discrete_actions=True)),\n",
    "    (\"LLM Fine-tuning (RLHF)\", dict(rlhf=True)),\n",
    "    (\"Robot Arm (simulation)\", dict(sample_efficiency_important=False)),\n",
    "    (\"Real Robot (expensive)\", dict(sample_efficiency_important=True, exploration_important=True)),\n",
    "    (\"Drone Control (precise)\", dict(sample_efficiency_important=True, exploration_important=False)),\n",
    "]\n",
    "\n",
    "for name, kwargs in scenarios:\n",
    "    algo, reason = recommend_algorithm(**kwargs)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  â†’ {algo}\")\n",
    "    print(f\"  Reason: {reason}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision flowchart\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 14)\n",
    "ax.axis('off')\n",
    "ax.set_title('Algorithm Decision Flowchart', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Start\n",
    "start = Circle((7, 13), 0.5, facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(start)\n",
    "ax.text(7, 13, 'START', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Decision 1: Discrete actions?\n",
    "d1 = FancyBboxPatch((4.5, 10.5), 5, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                     facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax.add_patch(d1)\n",
    "ax.text(7, 11.25, 'Discrete Actions?', ha='center', va='center', fontsize=11)\n",
    "ax.annotate('', xy=(7, 12), xytext=(7, 12.4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# PPO for discrete\n",
    "ppo1 = FancyBboxPatch((1, 8.5), 2.5, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                       facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(ppo1)\n",
    "ax.text(2.25, 9.25, 'PPO', ha='center', va='center', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax.annotate('', xy=(2.25, 10), xytext=(4.4, 11),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#388e3c'))\n",
    "ax.text(3, 10.7, 'YES', fontsize=10, color='#388e3c')\n",
    "\n",
    "# Decision 2: Sample efficiency?\n",
    "d2 = FancyBboxPatch((8, 8), 5, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                     facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax.add_patch(d2)\n",
    "ax.text(10.5, 8.75, 'Sample Efficiency\\nMatters?', ha='center', va='center', fontsize=10)\n",
    "ax.annotate('', xy=(10.5, 9.6), xytext=(9.6, 10.4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.text(10.5, 10.3, 'NO', fontsize=10, color='#666')\n",
    "\n",
    "# PPO for continuous (samples cheap)\n",
    "ppo2 = FancyBboxPatch((5.5, 5.5), 2.5, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                       facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(ppo2)\n",
    "ax.text(6.75, 6.25, 'PPO', ha='center', va='center', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax.annotate('', xy=(6.75, 7.1), xytext=(7.9, 7.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#388e3c'))\n",
    "ax.text(7, 7.7, 'NO', fontsize=10, color='#388e3c')\n",
    "\n",
    "# Decision 3: Exploration important?\n",
    "d3 = FancyBboxPatch((9.5, 5), 4, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                     facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax.add_patch(d3)\n",
    "ax.text(11.5, 5.75, 'Exploration\\nImportant?', ha='center', va='center', fontsize=10)\n",
    "ax.annotate('', xy=(11.5, 6.6), xytext=(11.5, 7.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.text(11.7, 7.5, 'YES', fontsize=10, color='#666')\n",
    "\n",
    "# SAC\n",
    "sac = FancyBboxPatch((8.5, 2.5), 2.5, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='#bbdefb', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(sac)\n",
    "ax.text(9.75, 3.25, 'SAC', ha='center', va='center', fontsize=12, fontweight='bold', color='#1976d2')\n",
    "ax.annotate('', xy=(9.75, 4.1), xytext=(10.5, 4.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#1976d2'))\n",
    "ax.text(9.8, 4.6, 'YES', fontsize=10, color='#1976d2')\n",
    "\n",
    "# TD3\n",
    "td3 = FancyBboxPatch((11.5, 2.5), 2.5, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='#fff3e0', edgecolor='#f57c00', linewidth=3)\n",
    "ax.add_patch(td3)\n",
    "ax.text(12.75, 3.25, 'TD3', ha='center', va='center', fontsize=12, fontweight='bold', color='#f57c00')\n",
    "ax.annotate('', xy=(12.75, 4.1), xytext=(12.5, 4.9),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#f57c00'))\n",
    "ax.text(12.8, 4.6, 'NO', fontsize=10, color='#f57c00')\n",
    "\n",
    "# Legend\n",
    "ax.text(2, 3, 'QUICK GUIDE:', fontsize=11, fontweight='bold')\n",
    "ax.text(2, 2.3, 'â€¢ Discrete â†’ PPO', fontsize=10, color='#388e3c')\n",
    "ax.text(2, 1.7, 'â€¢ Continuous + cheap samples â†’ PPO', fontsize=10, color='#388e3c')\n",
    "ax.text(2, 1.1, 'â€¢ Continuous + need efficiency + exploration â†’ SAC', fontsize=10, color='#1976d2')\n",
    "ax.text(2, 0.5, 'â€¢ Continuous + need efficiency + stability â†’ TD3', fontsize=10, color='#f57c00')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Running Side-by-Side Comparisons with SB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    print(\"RUNNING ALGORITHM COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nComparing PPO, SAC, and TD3 on Pendulum-v1 (continuous control)\")\n",
    "    print(\"Note: This is a quick demo. Real benchmarks need more timesteps!\\n\")\n",
    "    \n",
    "    # Environment setup\n",
    "    env_name = 'Pendulum-v1'\n",
    "    n_envs = 4\n",
    "    total_timesteps = 10_000  # Short for demo; use 100k+ for real comparison\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # PPO\n",
    "    print(\"Training PPO...\")\n",
    "    env = make_vec_env(env_name, n_envs=n_envs)\n",
    "    ppo_model = PPO('MlpPolicy', env, verbose=0)\n",
    "    ppo_model.learn(total_timesteps=total_timesteps)\n",
    "    ppo_reward, ppo_std = evaluate_policy(ppo_model, env, n_eval_episodes=10)\n",
    "    results['PPO'] = (ppo_reward, ppo_std)\n",
    "    print(f\"  PPO: {ppo_reward:.1f} Â± {ppo_std:.1f}\")\n",
    "    env.close()\n",
    "    \n",
    "    # SAC\n",
    "    print(\"Training SAC...\")\n",
    "    env = gym.make(env_name)\n",
    "    sac_model = SAC('MlpPolicy', env, verbose=0)\n",
    "    sac_model.learn(total_timesteps=total_timesteps)\n",
    "    sac_reward, sac_std = evaluate_policy(sac_model, env, n_eval_episodes=10)\n",
    "    results['SAC'] = (sac_reward, sac_std)\n",
    "    print(f\"  SAC: {sac_reward:.1f} Â± {sac_std:.1f}\")\n",
    "    env.close()\n",
    "    \n",
    "    # TD3\n",
    "    print(\"Training TD3...\")\n",
    "    env = gym.make(env_name)\n",
    "    td3_model = TD3('MlpPolicy', env, verbose=0)\n",
    "    td3_model.learn(total_timesteps=total_timesteps)\n",
    "    td3_reward, td3_std = evaluate_policy(td3_model, env, n_eval_episodes=10)\n",
    "    results['TD3'] = (td3_reward, td3_std)\n",
    "    print(f\"  TD3: {td3_reward:.1f} Â± {td3_std:.1f}\")\n",
    "    env.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"Install Stable-Baselines3 to run this comparison.\")\n",
    "    results = {\n",
    "        'PPO': (-800, 100),  # Simulated results\n",
    "        'SAC': (-400, 80),\n",
    "        'TD3': (-450, 90),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison results\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "algorithms = list(results.keys())\n",
    "rewards = [results[a][0] for a in algorithms]\n",
    "stds = [results[a][1] for a in algorithms]\n",
    "\n",
    "colors = ['#4caf50', '#2196f3', '#ff9800']\n",
    "bars = ax.bar(algorithms, rewards, yerr=stds, capsize=10, \n",
    "              color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.axhline(y=-200, color='red', linestyle='--', linewidth=2, label='Good performance')\n",
    "\n",
    "ax.set_ylabel('Mean Reward', fontsize=12)\n",
    "ax.set_title(f'Algorithm Comparison on Pendulum-v1\\n({total_timesteps if SB3_AVAILABLE else \"10k\"} timesteps)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, reward, std in zip(bars, rewards, stds):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 20,\n",
    "            f'{reward:.0f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nINTERPRETATION:\")\n",
    "print(\"  â€¢ Pendulum is continuous control - SAC/TD3 should do well\")\n",
    "print(\"  â€¢ Off-policy methods (SAC, TD3) are more sample efficient\")\n",
    "print(\"  â€¢ For fair comparison, train longer (100k+ timesteps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practical Considerations\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              PRACTICAL CONSIDERATIONS                          â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  COMPUTE REQUIREMENTS:                                        â”‚\n",
    "    â”‚    PPO:  Moderate (on-policy, needs many env interactions)    â”‚\n",
    "    â”‚    SAC:  Higher memory (replay buffer)                        â”‚\n",
    "    â”‚    TD3:  Similar to SAC                                       â”‚\n",
    "    â”‚    TRPO: Highest (second-order optimization)                  â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  HYPERPARAMETER SENSITIVITY:                                  â”‚\n",
    "    â”‚    PPO:  Very robust (defaults work well)                     â”‚\n",
    "    â”‚    SAC:  Robust (auto temperature helps)                      â”‚\n",
    "    â”‚    TD3:  Moderate                                             â”‚\n",
    "    â”‚    TRPO: Complex to tune                                      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  IMPLEMENTATION COMPLEXITY:                                   â”‚\n",
    "    â”‚    PPO:  Simple (clipping is easy)                            â”‚\n",
    "    â”‚    SAC:  Medium (entropy, twin critics)                       â”‚\n",
    "    â”‚    TD3:  Medium (delayed updates, noise)                      â”‚\n",
    "    â”‚    TRPO: Hard (natural gradient, line search)                 â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  REPRODUCIBILITY:                                             â”‚\n",
    "    â”‚    All algorithms can be sensitive to random seeds!           â”‚\n",
    "    â”‚    Always run multiple seeds and report mean Â± std.           â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize practical considerations as radar chart\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Categories\n",
    "categories = ['Stability', 'Sample\\nEfficiency', 'Implementation\\nEase', \n",
    "              'Discrete\\nSupport', 'Exploration', 'Robustness']\n",
    "N = len(categories)\n",
    "\n",
    "# Scores (1-5 scale)\n",
    "scores = {\n",
    "    'PPO':  [5, 2, 5, 5, 3, 5],\n",
    "    'SAC':  [4, 5, 3, 1, 5, 4],\n",
    "    'TD3':  [5, 5, 3, 1, 2, 4],\n",
    "    'TRPO': [4, 2, 1, 5, 3, 3],\n",
    "}\n",
    "\n",
    "# Angles for each category\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]  # Complete the loop\n",
    "\n",
    "# Plot each algorithm\n",
    "colors = ['#4caf50', '#2196f3', '#ff9800', '#9c27b0']\n",
    "for (algo, score), color in zip(scores.items(), colors):\n",
    "    values = score + score[:1]  # Complete the loop\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=algo, color=color)\n",
    "    ax.fill(angles, values, alpha=0.1, color=color)\n",
    "\n",
    "# Set category labels\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=10)\n",
    "ax.set_ylim(0, 5)\n",
    "\n",
    "ax.set_title('Algorithm Comparison Radar', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRADAR CHART INTERPRETATION:\")\n",
    "print(\"  â€¢ Larger area = better overall\")\n",
    "print(\"  â€¢ PPO: Great all-rounder (large, balanced area)\")\n",
    "print(\"  â€¢ SAC: Strong on efficiency and exploration\")\n",
    "print(\"  â€¢ TD3: Strong on efficiency and stability\")\n",
    "print(\"  â€¢ TRPO: Theoretical strengths, practical challenges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### Algorithm Quick Reference\n",
    "\n",
    "| Algorithm | Best For | Key Innovation |\n",
    "|-----------|----------|----------------|\n",
    "| **PPO** | General purpose, RLHF | Clipped objective |\n",
    "| **SAC** | Robotics, exploration | Maximum entropy |\n",
    "| **TD3** | Precision control | Delayed updates |\n",
    "| **TRPO** | Research | KL constraint |\n",
    "\n",
    "### Decision Summary\n",
    "\n",
    "```\n",
    "Discrete actions?     â†’ PPO\n",
    "RLHF for LLMs?        â†’ PPO  \n",
    "Continuous + cheap?   â†’ PPO\n",
    "Continuous + explore? â†’ SAC\n",
    "Continuous + stable?  â†’ TD3\n",
    "```\n",
    "\n",
    "### Key Tradeoffs\n",
    "\n",
    "| Tradeoff | On-Policy (PPO) | Off-Policy (SAC/TD3) |\n",
    "|----------|-----------------|----------------------|\n",
    "| Sample efficiency | Lower | Higher |\n",
    "| Stability | Higher | Moderate |\n",
    "| Memory | Lower | Higher (buffer) |\n",
    "| Action types | Any | Continuous |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. When should you choose PPO over SAC?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Choose PPO when:\n",
    "- You have discrete actions (SAC only works with continuous)\n",
    "- Doing RLHF for language models\n",
    "- Environment samples are cheap (simulation is fast)\n",
    "- Stability is more important than sample efficiency\n",
    "- You want the simplest implementation\n",
    "</details>\n",
    "\n",
    "**2. What's the main advantage of off-policy methods?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Off-policy methods (SAC, TD3) can reuse past experiences stored in a replay buffer. This makes them much more sample efficient - they can learn from the same data multiple times rather than discarding it after one use. This is crucial when environment interactions are expensive (real robots, complex simulations).\n",
    "</details>\n",
    "\n",
    "**3. Why does SAC often explore better than TD3?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "SAC uses maximum entropy reinforcement learning, which adds an entropy bonus to the reward. This explicitly encourages the policy to maintain randomness in its actions, leading to better exploration. TD3 uses a deterministic policy and relies on adding external noise, which is less principled and can be harder to tune.\n",
    "</details>\n",
    "\n",
    "**4. What makes TRPO impractical compared to PPO?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "TRPO requires:\n",
    "- Computing the Fisher Information Matrix (expensive)\n",
    "- Conjugate gradient optimization\n",
    "- Line search to satisfy KL constraint\n",
    "\n",
    "PPO achieves similar results with just a clipped objective - much simpler to implement and faster to run, while maintaining the benefits of bounded policy updates.\n",
    "</details>\n",
    "\n",
    "**5. For a new RL project, which algorithm should you try first?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Start with PPO! Reasons:\n",
    "- Works with both discrete and continuous actions\n",
    "- Very stable and robust to hyperparameters\n",
    "- Easy to implement and debug\n",
    "- Defaults work well out of the box\n",
    "\n",
    "Then switch to SAC/TD3 if you need sample efficiency with continuous actions, or stay with PPO if stability is paramount.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the **Advanced Algorithms** section! You now understand:\n",
    "\n",
    "- âœ… Trust region methods and why they matter (TRPO)\n",
    "- âœ… PPO: The practical, industry-standard algorithm\n",
    "- âœ… Using Stable-Baselines3 for production RL\n",
    "- âœ… SAC: Maximum entropy for continuous control\n",
    "- âœ… How to choose the right algorithm for your task\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Move on to **[RLHF](../rlhf/)** to learn how these algorithms (especially PPO!) are used to align Large Language Models with human preferences!\n",
    "\n",
    "---\n",
    "\n",
    "*\"The best algorithm is the one that works for YOUR problem!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
