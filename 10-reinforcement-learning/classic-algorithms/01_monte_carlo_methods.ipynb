{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods: Learning from Experience\n",
    "\n",
    "Welcome to Monte Carlo methods - the intuitive approach to learning by trial and error!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- What Monte Carlo means (with a casino analogy!)\n",
    "- How to learn from complete episodes\n",
    "- First-visit vs every-visit MC\n",
    "- Monte Carlo prediction (evaluating policies)\n",
    "- Monte Carlo control (finding optimal policies)\n",
    "- Epsilon-greedy exploration\n",
    "- Why exploration is crucial!\n",
    "\n",
    "**Prerequisites:** Fundamentals section (especially Bellman equations)\n",
    "\n",
    "**Time:** ~35 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Casino Analogy\n",
    "\n",
    "Monte Carlo methods are named after the famous casino in Monaco. Here's why:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │            THE MONTE CARLO ANALOGY: THE GAMBLER                │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Imagine you're a gambler trying to figure out:                │\n",
    "    │  \"What's my expected return at this casino?\"                   │\n",
    "    │                                                                │\n",
    "    │  METHOD 1: Mathematics (Dynamic Programming)                   │\n",
    "    │    - Study all the rules and probabilities                     │\n",
    "    │    - Calculate expected values mathematically                  │\n",
    "    │    - Requires knowing all the rules!                          │\n",
    "    │                                                                │\n",
    "    │  METHOD 2: Experience (Monte Carlo)                            │\n",
    "    │    - Just PLAY many games                                     │\n",
    "    │    - Track your winnings/losses                               │\n",
    "    │    - Average your results                                     │\n",
    "    │    - No need to know the rules!                               │\n",
    "    │                                                                │\n",
    "    │  MONTE CARLO = \"Learn by trying many times and averaging\"     │\n",
    "    │                                                                │\n",
    "    │  Key insight: If you play enough games, your average          │\n",
    "    │  will converge to the TRUE expected value!                    │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Monte Carlo RL: Learn value functions by averaging returns from many episodes!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, Circle, FancyArrowPatch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from collections import defaultdict\n",
    "\n",
    "# Visualize the Monte Carlo idea\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Simulating many trials\n",
    "ax1 = axes[0]\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate dice rolls to estimate expected value (should be 3.5)\n",
    "n_trials = [10, 100, 500, 1000, 5000]\n",
    "estimates = []\n",
    "for n in n_trials:\n",
    "    rolls = np.random.randint(1, 7, n)\n",
    "    estimates.append(np.mean(rolls))\n",
    "\n",
    "ax1.bar(range(len(n_trials)), estimates, color=['#f44336', '#ff9800', '#ffeb3b', '#8bc34a', '#4caf50'],\n",
    "        edgecolor='black', linewidth=2)\n",
    "ax1.axhline(y=3.5, color='blue', linestyle='--', linewidth=2, label='True Expected Value (3.5)')\n",
    "ax1.set_xticks(range(len(n_trials)))\n",
    "ax1.set_xticklabels([f'{n} trials' for n in n_trials])\n",
    "ax1.set_ylabel('Estimated Expected Value', fontsize=12)\n",
    "ax1.set_title('Monte Carlo: More Trials = Better Estimate\\n(Estimating E[dice roll])', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.set_ylim(2.5, 4.5)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, est in enumerate(estimates):\n",
    "    ax1.text(i, est + 0.1, f'{est:.2f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Right: Convergence visualization\n",
    "ax2 = axes[1]\n",
    "n_samples = 2000\n",
    "rolls = np.random.randint(1, 7, n_samples)\n",
    "running_avg = np.cumsum(rolls) / np.arange(1, n_samples + 1)\n",
    "\n",
    "ax2.plot(running_avg, color='#2196f3', linewidth=2, label='Running Average')\n",
    "ax2.axhline(y=3.5, color='red', linestyle='--', linewidth=2, label='True Value (3.5)')\n",
    "ax2.fill_between(range(n_samples), running_avg, 3.5, alpha=0.2, color='blue')\n",
    "ax2.set_xlabel('Number of Trials', fontsize=12)\n",
    "ax2.set_ylabel('Running Average', fontsize=12)\n",
    "ax2.set_title('Monte Carlo Convergence\\n(More samples → Closer to truth)', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.set_xlim(0, n_samples)\n",
    "ax2.set_ylim(2.5, 4.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THE MONTE CARLO PRINCIPLE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "To estimate the EXPECTED VALUE of something:\n",
    "  1. Sample many times (run many trials/episodes)\n",
    "  2. Compute the average\n",
    "  3. As samples → ∞, average → true expected value!\n",
    "\n",
    "For RL:\n",
    "  V(s) = E[Return | starting from s]\n",
    "       ≈ Average of returns observed from state s across many episodes\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monte Carlo for RL: The Core Idea\n",
    "\n",
    "In RL, Monte Carlo estimates value functions by averaging **actual returns** from **complete episodes**:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              MONTE CARLO FOR REINFORCEMENT LEARNING            │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  STEP 1: Generate a complete episode                          │\n",
    "    │                                                                │\n",
    "    │    s₀ → a₀ → r₁ → s₁ → a₁ → r₂ → ... → sT (terminal)         │\n",
    "    │                                                                │\n",
    "    │  STEP 2: Calculate returns for each state                     │\n",
    "    │                                                                │\n",
    "    │    G(s₀) = r₁ + γr₂ + γ²r₃ + ...                              │\n",
    "    │    G(s₁) = r₂ + γr₃ + γ²r₄ + ...                              │\n",
    "    │    ...                                                        │\n",
    "    │                                                                │\n",
    "    │  STEP 3: Update value estimates                               │\n",
    "    │                                                                │\n",
    "    │    V(s) ← average of all returns observed from s              │\n",
    "    │                                                                │\n",
    "    │  REPEAT many episodes!                                        │\n",
    "    │                                                                │\n",
    "    │  KEY REQUIREMENT: Need COMPLETE episodes (must reach end)     │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    4x4 Grid World for demonstrating Monte Carlo methods.\n",
    "    \n",
    "    Layout:\n",
    "        ┌───┬───┬───┬───┐\n",
    "        │ S │   │   │   │   S = Start (0,0)\n",
    "        ├───┼───┼───┼───┤\n",
    "        │   │   │   │   │\n",
    "        ├───┼───┼───┼───┤\n",
    "        │   │   │   │   │\n",
    "        ├───┼───┼───┼───┤\n",
    "        │   │   │   │ G │   G = Goal (3,3)\n",
    "        └───┴───┴───┴───┘\n",
    "    \n",
    "    Actions: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "    Rewards: -1 per step, +10 at goal\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.goal = (3, 3)\n",
    "        self.action_names = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
    "        self.action_symbols = ['↑', '→', '↓', '←']\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to start position.\"\"\"\n",
    "        self.pos = (0, 0)\n",
    "        return self.pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return (next_state, reward, done).\"\"\"\n",
    "        row, col = self.pos\n",
    "        \n",
    "        if action == 0:    # UP\n",
    "            row = max(0, row - 1)\n",
    "        elif action == 1:  # RIGHT\n",
    "            col = min(3, col + 1)\n",
    "        elif action == 2:  # DOWN\n",
    "            row = min(3, row + 1)\n",
    "        elif action == 3:  # LEFT\n",
    "            col = max(0, col - 1)\n",
    "        \n",
    "        self.pos = (row, col)\n",
    "        done = self.pos == self.goal\n",
    "        reward = 10 if done else -1\n",
    "        \n",
    "        return self.pos, reward, done\n",
    "\n",
    "\n",
    "def random_policy(state):\n",
    "    \"\"\"Random policy: pick any action with equal probability.\"\"\"\n",
    "    return np.random.randint(0, 4)\n",
    "\n",
    "\n",
    "def generate_episode(env, policy, max_steps=100):\n",
    "    \"\"\"\n",
    "    Generate a complete episode by following the policy.\n",
    "    \n",
    "    Returns:\n",
    "        episode: List of (state, action, reward) tuples\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = policy(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return episode\n",
    "\n",
    "\n",
    "# Generate and visualize an example episode\n",
    "env = GridWorld()\n",
    "np.random.seed(42)\n",
    "episode = generate_episode(env, random_policy)\n",
    "\n",
    "print(\"EXAMPLE EPISODE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFollowing a random policy from Start (0,0) to Goal (3,3):\\n\")\n",
    "print(f\"{'Step':<6} {'State':<12} {'Action':<10} {'Reward':<8}\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "for i, (s, a, r) in enumerate(episode[:15]):\n",
    "    print(f\"{i:<6} {str(s):<12} {env.action_names[a]:<10} {r:<8}\")\n",
    "\n",
    "if len(episode) > 15:\n",
    "    print(f\"  ... ({len(episode) - 15} more steps)\")\n",
    "\n",
    "print(f\"\\nTotal steps: {len(episode)}\")\n",
    "print(f\"Total reward: {sum(r for _, _, r in episode)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the episode as a path on the grid\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Draw grid\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        if (row, col) == env.goal:\n",
    "            color = '#c8e6c9'\n",
    "        elif (row, col) == (0, 0):\n",
    "            color = '#bbdefb'\n",
    "        else:\n",
    "            color = 'white'\n",
    "        \n",
    "        y = 3 - row\n",
    "        rect = Rectangle((col, y), 1, 1, facecolor=color, \n",
    "                           edgecolor='black', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "# Labels\n",
    "ax.text(0.5, 3.5, 'START', ha='center', va='center', fontsize=11, \n",
    "        fontweight='bold', color='#1976d2')\n",
    "ax.text(3.5, 0.5, 'GOAL', ha='center', va='center', fontsize=11, \n",
    "        fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Draw path from episode\n",
    "path_states = [(0, 0)] + [s for s, _, _ in episode[1:]] + [env.goal]\n",
    "\n",
    "# Draw arrows for path (only first 20 steps to avoid clutter)\n",
    "max_draw = min(20, len(episode))\n",
    "for i in range(max_draw):\n",
    "    s, a, _ = episode[i]\n",
    "    row, col = s\n",
    "    x1, y1 = col + 0.5, 3 - row + 0.5\n",
    "    \n",
    "    # Get direction\n",
    "    dx, dy = [(0, 0.25), (0.25, 0), (0, -0.25), (-0.25, 0)][a]\n",
    "    \n",
    "    # Draw arrow\n",
    "    alpha = 1.0 - (i / max_draw) * 0.6  # Fade older arrows\n",
    "    ax.arrow(x1, y1, dx, dy, head_width=0.1, head_length=0.05,\n",
    "            fc='#f44336', ec='#f44336', alpha=alpha, linewidth=2)\n",
    "\n",
    "ax.set_xlim(0, 4)\n",
    "ax.set_ylim(0, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title(f'Episode Path (showing first {max_draw} steps)\\n'\n",
    "             f'Random policy took {len(episode)} steps to reach goal',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The random policy wanders around before finding the goal!\")\n",
    "print(\"MC will learn that states closer to the goal have higher values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## First-Visit vs Every-Visit Monte Carlo\n",
    "\n",
    "When a state appears multiple times in an episode, how do we count it?\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          FIRST-VISIT vs EVERY-VISIT MONTE CARLO                │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Example episode: s₁ → s₂ → s₃ → s₂ → s₃ → s₄ (goal)          │\n",
    "    │                    ↑         ↑                                 │\n",
    "    │                 1st visit  2nd visit                          │\n",
    "    │                                                                │\n",
    "    │  FIRST-VISIT MC:                                              │\n",
    "    │    Only count the FIRST time we visit each state              │\n",
    "    │    V(s₂) ← average of returns from first visits only          │\n",
    "    │                                                                │\n",
    "    │    ✓ Simpler to understand                                    │\n",
    "    │    ✓ Unbiased estimate                                        │\n",
    "    │    ✓ Most commonly used                                       │\n",
    "    │                                                                │\n",
    "    │  EVERY-VISIT MC:                                              │\n",
    "    │    Count EVERY time we visit each state                       │\n",
    "    │    V(s₂) ← average of returns from ALL visits                 │\n",
    "    │                                                                │\n",
    "    │    ✓ More data per episode                                    │\n",
    "    │    ✓ Also converges to correct value                          │\n",
    "    │    ✓ Better for states visited rarely                         │\n",
    "    │                                                                │\n",
    "    │  BOTH converge to true V(s) as episodes → ∞                   │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize First-Visit vs Every-Visit\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 7.5, 'First-Visit vs Every-Visit Monte Carlo', \n",
    "        ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Example episode\n",
    "states = ['A', 'B', 'C', 'B', 'C', 'D']\n",
    "rewards = [-1, -1, -1, -1, -1, 10]\n",
    "gamma = 0.9\n",
    "\n",
    "# Draw episode as boxes\n",
    "ax.text(7, 6.5, 'Episode: A → B → C → B → C → D(goal)', ha='center', fontsize=12)\n",
    "\n",
    "for i, (s, r) in enumerate(zip(states, rewards)):\n",
    "    x = 2 + i * 2\n",
    "    color = '#c8e6c9' if s == 'D' else '#bbdefb' if s in ['B', 'C'] else '#fff3e0'\n",
    "    \n",
    "    box = FancyBboxPatch((x - 0.5, 5.2), 1, 1, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, 5.7, s, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    ax.text(x, 5.0, f'r={r}', ha='center', fontsize=9, color='#666')\n",
    "    \n",
    "    if i < len(states) - 1:\n",
    "        ax.annotate('', xy=(x + 0.7, 5.7), xytext=(x + 0.5, 5.7),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Mark first vs second visits\n",
    "ax.text(4, 4.4, '1st visit', ha='center', fontsize=9, color='#388e3c', fontweight='bold')\n",
    "ax.text(6, 4.4, '1st visit', ha='center', fontsize=9, color='#388e3c', fontweight='bold')\n",
    "ax.text(8, 4.4, '2nd visit', ha='center', fontsize=9, color='#d32f2f', fontweight='bold')\n",
    "ax.text(10, 4.4, '2nd visit', ha='center', fontsize=9, color='#d32f2f', fontweight='bold')\n",
    "\n",
    "# First-visit explanation\n",
    "fv_box = FancyBboxPatch((0.5, 1.5), 6, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#e8f5e9', edgecolor='#388e3c', linewidth=2)\n",
    "ax.add_patch(fv_box)\n",
    "ax.text(3.5, 3.5, 'FIRST-VISIT MC', ha='center', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax.text(3.5, 2.9, 'For state B:', ha='center', fontsize=10)\n",
    "ax.text(3.5, 2.4, 'Only use return from step 1', ha='center', fontsize=10)\n",
    "ax.text(3.5, 1.9, 'G = -1 + 0.9×(-1) + 0.9²×(-1) + ...', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "# Every-visit explanation\n",
    "ev_box = FancyBboxPatch((7.5, 1.5), 6, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax.add_patch(ev_box)\n",
    "ax.text(10.5, 3.5, 'EVERY-VISIT MC', ha='center', fontsize=12, fontweight='bold', color='#f57c00')\n",
    "ax.text(10.5, 2.9, 'For state B:', ha='center', fontsize=10)\n",
    "ax.text(10.5, 2.4, 'Use returns from BOTH visits', ha='center', fontsize=10)\n",
    "ax.text(10.5, 1.9, 'Average both returns together', ha='center', fontsize=9, color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monte Carlo Prediction: Evaluating a Policy\n",
    "\n",
    "**Goal:** Given a policy π, estimate V^π(s) for all states.\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                 MONTE CARLO PREDICTION                         │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  ALGORITHM (First-Visit MC Prediction):                        │\n",
    "    │                                                                │\n",
    "    │  Initialize:                                                   │\n",
    "    │    V(s) = 0 for all states                                    │\n",
    "    │    Returns(s) = [] for all states (to store observed returns) │\n",
    "    │                                                                │\n",
    "    │  For each episode:                                            │\n",
    "    │    1. Generate episode: s₀,a₀,r₁, s₁,a₁,r₂, ... sT           │\n",
    "    │    2. G ← 0 (return accumulator)                              │\n",
    "    │    3. For t = T-1, T-2, ... 0 (backwards!):                   │\n",
    "    │         G ← γ×G + r_{t+1}                                     │\n",
    "    │         If s_t is FIRST visit in episode:                     │\n",
    "    │           Append G to Returns(s_t)                            │\n",
    "    │           V(s_t) ← average(Returns(s_t))                      │\n",
    "    │                                                                │\n",
    "    │  Return V                                                      │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_prediction(env, policy, n_episodes=1000, gamma=0.9, verbose=False):\n",
    "    \"\"\"\n",
    "    First-visit Monte Carlo prediction.\n",
    "    \n",
    "    Estimates V(s) by averaging returns from first visits to each state.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment\n",
    "        policy: Function mapping state -> action\n",
    "        n_episodes: Number of episodes to run\n",
    "        gamma: Discount factor\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        V: Dictionary mapping state -> value estimate\n",
    "        history: List of V[(0,0)] at each episode (for visualization)\n",
    "    \"\"\"\n",
    "    V = defaultdict(float)\n",
    "    returns = defaultdict(list)  # Store all returns for each state\n",
    "    history = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        # ========================================\n",
    "        # STEP 1: Generate a complete episode\n",
    "        # ========================================\n",
    "        episode = generate_episode(env, policy)\n",
    "        \n",
    "        # ========================================\n",
    "        # STEP 2: Calculate returns backwards\n",
    "        # ========================================\n",
    "        G = 0  # Return accumulator\n",
    "        visited = set()  # Track first visits\n",
    "        \n",
    "        # Go backwards through the episode\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward  # Discounted return\n",
    "            \n",
    "            # ========================================\n",
    "            # STEP 3: First-visit check\n",
    "            # ========================================\n",
    "            if state not in visited:\n",
    "                visited.add(state)\n",
    "                returns[state].append(G)\n",
    "                V[state] = np.mean(returns[state])\n",
    "        \n",
    "        # Track progress\n",
    "        history.append(V[(0, 0)])\n",
    "        \n",
    "        if verbose and (ep + 1) % 500 == 0:\n",
    "            print(f\"Episode {ep+1:5d} | V(start) = {V[(0,0)]:.2f}\")\n",
    "    \n",
    "    return dict(V), history\n",
    "\n",
    "\n",
    "# Run Monte Carlo prediction\n",
    "env = GridWorld()\n",
    "\n",
    "print(\"FIRST-VISIT MONTE CARLO PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nEvaluating the random policy...\\n\")\n",
    "\n",
    "V_random, history = first_visit_mc_prediction(\n",
    "    env, random_policy, n_episodes=5000, verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Estimated Value Function V^π(s):\")\n",
    "print(\"-\"*40)\n",
    "for row in range(4):\n",
    "    values = [V_random.get((row, col), 0.0) for col in range(4)]\n",
    "    print(\" \".join([f\"{v:8.2f}\" for v in values]))\n",
    "print(\"-\"*40)\n",
    "print(\"(Higher values = closer to goal = better!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learning process and value function\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Learning curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(history, alpha=0.4, color='blue', label='Raw')\n",
    "\n",
    "# Smoothed version\n",
    "window = 100\n",
    "smoothed = np.convolve(history, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(history)), smoothed, color='blue', linewidth=2, label=f'Smoothed (window={window})')\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('V(start state)', fontsize=12)\n",
    "ax1.set_title('Monte Carlo Learning Curve\\n(Value of starting state over time)', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Value function heatmap\n",
    "ax2 = axes[1]\n",
    "V_grid = np.array([[V_random.get((r, c), 0.0) for c in range(4)] for r in range(4)])\n",
    "\n",
    "im = ax2.imshow(V_grid, cmap='RdYlGn')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        value = V_grid[i, j]\n",
    "        color = 'white' if value < np.mean(V_grid) else 'black'\n",
    "        if (i, j) == env.goal:\n",
    "            ax2.text(j, i, 'GOAL\\n0', ha='center', va='center', \n",
    "                    fontsize=10, fontweight='bold', color='white')\n",
    "        else:\n",
    "            ax2.text(j, i, f'{value:.1f}', ha='center', va='center', \n",
    "                    fontsize=11, fontweight='bold', color=color)\n",
    "\n",
    "ax2.set_title('Estimated V(s) for Random Policy\\n(Green = Higher Value)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(range(4))\n",
    "ax2.set_yticks(range(4))\n",
    "plt.colorbar(im, ax=ax2, label='Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"• States closer to the goal have higher values\")\n",
    "print(\"• The random policy results in relatively low values\")\n",
    "print(\"• MC converges but has high variance (noisy learning curve)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monte Carlo Control: Finding the Optimal Policy\n",
    "\n",
    "Now let's use MC to **find** the best policy, not just evaluate a given one!\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                    MONTE CARLO CONTROL                         │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  THE EXPLORATION PROBLEM:                                      │\n",
    "    │                                                                │\n",
    "    │    If we always take the \"best\" action, we might miss         │\n",
    "    │    better actions we haven't tried!                           │\n",
    "    │                                                                │\n",
    "    │    ANALOGY: Always ordering the same dish at a restaurant    │\n",
    "    │    You might miss your new favorite dish!                     │\n",
    "    │                                                                │\n",
    "    │  SOLUTION: EPSILON-GREEDY EXPLORATION                          │\n",
    "    │                                                                │\n",
    "    │    With probability (1-ε): Take the BEST action (exploit)     │\n",
    "    │    With probability ε:     Take a RANDOM action (explore)     │\n",
    "    │                                                                │\n",
    "    │    Example with ε = 0.1:                                      │\n",
    "    │    • 90% of the time: Best action                             │\n",
    "    │    • 10% of the time: Random action                           │\n",
    "    │                                                                │\n",
    "    │  MC CONTROL LEARNS Q(s,a) instead of V(s)!                    │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize epsilon-greedy exploration\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Epsilon-greedy distribution\n",
    "ax1 = axes[0]\n",
    "epsilon = 0.1\n",
    "n_actions = 4\n",
    "\n",
    "# Assume action 1 (RIGHT) is best\n",
    "probs = np.ones(n_actions) * (epsilon / n_actions)\n",
    "probs[1] += (1 - epsilon)  # Best action gets extra probability\n",
    "\n",
    "colors = ['#90caf9', '#4caf50', '#90caf9', '#90caf9']\n",
    "bars = ax1.bar(['UP', 'RIGHT\\n(best)', 'DOWN', 'LEFT'], probs * 100, \n",
    "              color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax1.set_ylabel('Probability (%)', fontsize=12)\n",
    "ax1.set_title(f'ε-Greedy Action Selection (ε={epsilon})\\n\"Mostly exploit, sometimes explore\"', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "for i, p in enumerate(probs):\n",
    "    ax1.text(i, p*100 + 2, f'{p*100:.1f}%', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add annotations\n",
    "ax1.annotate('EXPLOIT\\n(best action)', xy=(1, 92), xytext=(2.5, 85),\n",
    "            fontsize=10, ha='center', arrowprops=dict(arrowstyle='->', color='#388e3c'))\n",
    "ax1.annotate('EXPLORE\\n(try others)', xy=(0, 2.5), xytext=(-0.5, 30),\n",
    "            fontsize=10, ha='center', arrowprops=dict(arrowstyle='->', color='#1976d2'))\n",
    "\n",
    "# Right: Effect of epsilon\n",
    "ax2 = axes[1]\n",
    "epsilons = np.linspace(0, 1, 100)\n",
    "exploit_probs = 1 - epsilons + epsilons/4  # Prob of taking best action\n",
    "explore_probs = epsilons * 3/4  # Prob of taking non-best action\n",
    "\n",
    "ax2.plot(epsilons, exploit_probs * 100, label='Best action probability', \n",
    "         color='#4caf50', linewidth=3)\n",
    "ax2.plot(epsilons, explore_probs * 100, label='Other actions probability',\n",
    "         color='#f44336', linewidth=3)\n",
    "\n",
    "ax2.axvline(x=0.1, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.text(0.12, 80, 'ε=0.1\\n(typical)', fontsize=10)\n",
    "\n",
    "ax2.set_xlabel('Epsilon (ε)', fontsize=12)\n",
    "ax2.set_ylabel('Probability (%)', fontsize=12)\n",
    "ax2.set_title('Effect of Epsilon Value\\n(Trade-off: Exploration vs Exploitation)', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEpsilon controls the exploration-exploitation trade-off:\")\n",
    "print(\"• ε = 0: Pure exploitation (greedy, might get stuck)\")\n",
    "print(\"• ε = 1: Pure exploration (random, never learns)\")\n",
    "print(\"• ε = 0.1: Good balance (mostly exploit, occasionally explore)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, n_episodes=10000, gamma=0.9, epsilon=0.1, verbose=False):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control with epsilon-greedy exploration.\n",
    "    \n",
    "    Learns the optimal Q(s,a) and extracts the optimal policy.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment\n",
    "        n_episodes: Number of episodes\n",
    "        gamma: Discount factor\n",
    "        epsilon: Exploration rate\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        Q: Action-value function (dictionary)\n",
    "        policy: Optimal policy (dictionary)\n",
    "        history: Episode rewards over time\n",
    "    \"\"\"\n",
    "    # Initialize Q(s,a) = 0 for all state-action pairs\n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    returns = defaultdict(list)  # Store returns for each (s,a)\n",
    "    episode_rewards = []  # Track performance\n",
    "    \n",
    "    def epsilon_greedy_policy(state):\n",
    "        \"\"\"Choose action using epsilon-greedy strategy.\"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(0, 4)  # Explore\n",
    "        else:\n",
    "            return np.argmax(Q[state])  # Exploit\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        # ========================================\n",
    "        # STEP 1: Generate episode with epsilon-greedy\n",
    "        # ========================================\n",
    "        episode = generate_episode(env, epsilon_greedy_policy)\n",
    "        episode_rewards.append(sum(r for _, _, r in episode))\n",
    "        \n",
    "        # ========================================\n",
    "        # STEP 2: Calculate returns and update Q\n",
    "        # ========================================\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        \n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            \n",
    "            sa_pair = (state, action)\n",
    "            if sa_pair not in visited:\n",
    "                visited.add(sa_pair)\n",
    "                returns[sa_pair].append(G)\n",
    "                Q[state][action] = np.mean(returns[sa_pair])\n",
    "        \n",
    "        if verbose and (ep + 1) % 2000 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-500:])\n",
    "            print(f\"Episode {ep+1:5d} | Avg Reward (last 500): {avg_reward:.1f}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 3: Extract greedy policy from Q\n",
    "    # ========================================\n",
    "    policy = {s: np.argmax(Q[s]) for s in Q}\n",
    "    \n",
    "    return dict(Q), policy, episode_rewards\n",
    "\n",
    "\n",
    "# Run MC Control\n",
    "print(\"MONTE CARLO CONTROL\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nLearning the optimal policy with ε-greedy exploration...\\n\")\n",
    "\n",
    "Q, policy, rewards = mc_control_epsilon_greedy(\n",
    "    env, n_episodes=10000, epsilon=0.1, verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LEARNED OPTIMAL POLICY:\")\n",
    "print(\"-\"*30)\n",
    "for row in range(4):\n",
    "    actions = [env.action_symbols[policy.get((row, col), 0)] for col in range(4)]\n",
    "    print(\"     \".join(actions))\n",
    "print(\"-\"*30)\n",
    "print(\"(Arrows show the best action in each state)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning progress and learned policy\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Learning curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rewards, alpha=0.3, color='blue', label='Raw')\n",
    "\n",
    "# Smoothed\n",
    "window = 200\n",
    "smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(rewards)), smoothed, color='blue', linewidth=2, \n",
    "         label=f'Smoothed (window={window})')\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Episode Reward', fontsize=12)\n",
    "ax1.set_title('MC Control Learning Curve\\n(Reward improves as policy improves)', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Policy visualization\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Draw grid\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        color = '#c8e6c9' if (row, col) == env.goal else '#e3f2fd' if (row, col) == (0, 0) else 'white'\n",
    "        rect = Rectangle((col, 3 - row), 1, 1, facecolor=color, edgecolor='black', linewidth=2)\n",
    "        ax2.add_patch(rect)\n",
    "\n",
    "# Draw arrows for policy\n",
    "arrow_dx = [0, 0.35, 0, -0.35]  # up, right, down, left\n",
    "arrow_dy = [0.35, 0, -0.35, 0]\n",
    "\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        if (row, col) == env.goal:\n",
    "            ax2.text(col + 0.5, 3 - row + 0.5, 'GOAL', ha='center', va='center', \n",
    "                     fontsize=10, fontweight='bold', color='#388e3c')\n",
    "        else:\n",
    "            a = policy.get((row, col), 0)\n",
    "            cx, cy = col + 0.5, 3 - row + 0.5\n",
    "            ax2.arrow(cx - arrow_dx[a]/2, cy - arrow_dy[a]/2, \n",
    "                      arrow_dx[a], arrow_dy[a],\n",
    "                      head_width=0.15, head_length=0.1, \n",
    "                      fc='#f44336', ec='#f44336', linewidth=2)\n",
    "\n",
    "ax2.set_xlim(0, 4)\n",
    "ax2.set_ylim(0, 4)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Learned Optimal Policy\\n(Arrows = Best Actions)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe learned policy moves directly toward the goal!\")\n",
    "print(\"Much better than the random policy we evaluated earlier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparing Random vs Optimal Policy\n",
    "\n",
    "Let's see the difference in performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy_func, n_episodes=1000):\n",
    "    \"\"\"Evaluate a policy by running episodes and averaging returns.\"\"\"\n",
    "    returns = []\n",
    "    steps = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        episode = generate_episode(env, policy_func)\n",
    "        returns.append(sum(r for _, _, r in episode))\n",
    "        steps.append(len(episode))\n",
    "    \n",
    "    return np.mean(returns), np.std(returns), np.mean(steps)\n",
    "\n",
    "\n",
    "# Create a greedy policy from learned Q\n",
    "def learned_policy(state):\n",
    "    return policy.get(state, 0)\n",
    "\n",
    "\n",
    "print(\"POLICY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate random policy\n",
    "rand_return, rand_std, rand_steps = evaluate_policy(env, random_policy)\n",
    "print(f\"\\nRandom Policy:\")\n",
    "print(f\"  Average Return: {rand_return:.1f} ± {rand_std:.1f}\")\n",
    "print(f\"  Average Steps:  {rand_steps:.1f}\")\n",
    "\n",
    "# Evaluate learned policy\n",
    "learn_return, learn_std, learn_steps = evaluate_policy(env, learned_policy)\n",
    "print(f\"\\nLearned (Optimal) Policy:\")\n",
    "print(f\"  Average Return: {learn_return:.1f} ± {learn_std:.1f}\")\n",
    "print(f\"  Average Steps:  {learn_steps:.1f}\")\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Return: {learn_return - rand_return:+.1f} ({(learn_return - rand_return)/abs(rand_return)*100:+.0f}%)\")\n",
    "print(f\"  Steps:  {rand_steps - learn_steps:.1f} fewer steps on average\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why Exploration Matters: A Demonstration\n",
    "\n",
    "Without exploration, the agent might get stuck with a suboptimal policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different epsilon values\n",
    "\n",
    "epsilons = [0.0, 0.05, 0.1, 0.2, 0.5]\n",
    "results = []\n",
    "\n",
    "print(\"EFFECT OF EXPLORATION (ε)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for eps in epsilons:\n",
    "    _, pol, rewards = mc_control_epsilon_greedy(env, n_episodes=5000, epsilon=eps)\n",
    "    final_reward = np.mean(rewards[-500:])\n",
    "    results.append(final_reward)\n",
    "    print(f\"ε = {eps:.2f}: Final avg reward = {final_reward:.1f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['#f44336' if e == 0 else '#4caf50' if e == 0.1 else '#2196f3' for e in epsilons]\n",
    "bars = ax.bar([f'ε={e}' for e in epsilons], results, color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.set_ylabel('Final Average Reward', fontsize=12)\n",
    "ax.set_title('Effect of Exploration Rate on Learning\\n(Higher is better)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(results):\n",
    "    ax.text(i, v + 0.2, f'{v:.1f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Annotate\n",
    "if results[0] < max(results):\n",
    "    ax.annotate('No exploration!\\n(might get stuck)', xy=(0, results[0]), xytext=(0.5, results[0] - 2),\n",
    "                fontsize=10, ha='center', arrowprops=dict(arrowstyle='->', color='#d32f2f'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LESSON: Some exploration is essential!\")\n",
    "print(\"• ε = 0: May never discover better actions\")\n",
    "print(\"• ε = 0.1: Good balance for this problem\")\n",
    "print(\"• ε too high: Wastes time on random actions\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monte Carlo: Pros and Cons\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │           MONTE CARLO: ADVANTAGES & DISADVANTAGES              │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  ADVANTAGES:                                                   │\n",
    "    │  ✓ No need for environment model (model-free)                 │\n",
    "    │  ✓ Unbiased estimates (averages actual returns)               │\n",
    "    │  ✓ Simple to understand and implement                         │\n",
    "    │  ✓ Works well for episodic tasks with clear endings           │\n",
    "    │                                                                │\n",
    "    │  DISADVANTAGES:                                                │\n",
    "    │  ✗ Needs COMPLETE episodes (can't learn mid-episode)          │\n",
    "    │  ✗ High variance (noisy returns)                              │\n",
    "    │  ✗ Slow learning (many episodes needed)                       │\n",
    "    │  ✗ Can't be used for continuing (non-episodic) tasks          │\n",
    "    │                                                                │\n",
    "    │  BEST FOR:                                                     │\n",
    "    │  • Card games (Blackjack, Poker)                              │\n",
    "    │  • Board games (episodes end with win/loss)                   │\n",
    "    │  • Tasks with short episodes                                  │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### The Monte Carlo Principle\n",
    "\n",
    "**Estimate by averaging**: V(s) ≈ average of returns observed from state s\n",
    "\n",
    "### Two Variants\n",
    "\n",
    "| Method | What it counts | Properties |\n",
    "|--------|---------------|------------|\n",
    "| **First-Visit MC** | Only first visit to each state | Simpler, most common |\n",
    "| **Every-Visit MC** | All visits to each state | More data per episode |\n",
    "\n",
    "### Prediction vs Control\n",
    "\n",
    "| Task | Goal | Learns |\n",
    "|------|------|--------|\n",
    "| **MC Prediction** | Evaluate a policy | V(s) |\n",
    "| **MC Control** | Find optimal policy | Q(s,a) |\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **ε-Greedy** | Explore with probability ε, exploit otherwise |\n",
    "| **Returns** | Sum of discounted rewards from a state |\n",
    "| **Model-Free** | No need to know transition probabilities |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What does \"Monte Carlo\" mean in the context of RL?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Monte Carlo means estimating values by averaging many random samples. In RL, we estimate V(s) or Q(s,a) by running many episodes and averaging the actual returns observed. The name comes from the Monte Carlo casino, emphasizing the use of randomness and repetition.\n",
    "</details>\n",
    "\n",
    "**2. Why does MC require complete episodes?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "MC computes returns by summing rewards from a state to the end of the episode: G = r₁ + γr₂ + γ²r₃ + ... We need the episode to end to know all these future rewards. This is different from TD learning, which uses estimates instead.\n",
    "</details>\n",
    "\n",
    "**3. What's the difference between First-Visit and Every-Visit MC?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "First-Visit MC only counts the return from the FIRST time we visit a state in each episode. Every-Visit MC counts returns from EVERY visit. Both converge to the true value, but First-Visit is simpler and more commonly used.\n",
    "</details>\n",
    "\n",
    "**4. Why is epsilon-greedy exploration important?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Without exploration, the agent might never try actions that seem worse initially but are actually better. ε-greedy ensures we occasionally try random actions, discovering potentially better strategies. With ε=0.1, we exploit 90% of the time and explore 10%.\n",
    "</details>\n",
    "\n",
    "**5. Why does MC Control learn Q(s,a) instead of V(s)?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "With Q(s,a), we can directly pick the best action: π*(s) = argmax_a Q(s,a). With V(s) alone, we'd need to know the transition probabilities to figure out which action leads to high-value states. Q makes the policy extraction model-free.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Great work! You've learned Monte Carlo methods - the foundation of learning from experience!\n",
    "\n",
    "**The main limitation of MC:** We must wait until the episode ends to learn.\n",
    "\n",
    "In the next notebook, we'll learn **Temporal Difference (TD) Learning** - which can learn from **every step**, not just at the end!\n",
    "\n",
    "**Continue to:** [Notebook 2: Temporal Difference Learning](02_temporal_difference_learning.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Monte Carlo: \"Try many times, average the results.\" Simple but powerful!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
