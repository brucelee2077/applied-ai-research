{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning: The Best of Both Worlds\n",
    "\n",
    "Welcome to TD learning - the algorithm that combines the best of Monte Carlo and Dynamic Programming!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- Why waiting until episode end is inefficient (with a weather analogy!)\n",
    "- The TD update rule: bootstrapping from estimates\n",
    "- TD(0) prediction: learning every step\n",
    "- TD error: the surprise signal\n",
    "- Comparing TD vs Monte Carlo\n",
    "- Why TD is the foundation for Q-learning!\n",
    "\n",
    "**Prerequisites:** Notebook 1 (Monte Carlo Methods)\n",
    "\n",
    "**Time:** ~35 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Weather Forecast Analogy\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          TD LEARNING: THE WEATHER FORECAST ANALOGY             │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  You want to predict tomorrow's temperature.                   │\n",
    "    │                                                                │\n",
    "    │  MONTE CARLO APPROACH:                                        │\n",
    "    │    Wait until tomorrow ends                                   │\n",
    "    │    → \"Tomorrow was 75°F\"                                      │\n",
    "    │    → Update your model                                        │\n",
    "    │    PROBLEM: You can only update ONCE per day!                 │\n",
    "    │                                                                │\n",
    "    │  TD APPROACH:                                                 │\n",
    "    │    At noon, check the current temperature (72°F)              │\n",
    "    │    → \"If it's 72°F now, tomorrow will probably be ~74°F\"     │\n",
    "    │    → Update your model based on this ESTIMATE                 │\n",
    "    │    ADVANTAGE: You can update multiple times per day!          │\n",
    "    │                                                                │\n",
    "    │  KEY INSIGHT:                                                 │\n",
    "    │    TD uses ESTIMATES to update ESTIMATES                      │\n",
    "    │    This is called BOOTSTRAPPING                               │\n",
    "    │                                                                │\n",
    "    │    \"I don't know the final outcome, but I can guess          │\n",
    "    │     based on what I know so far!\"                            │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, Circle, FancyArrowPatch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from collections import defaultdict\n",
    "\n",
    "# Visualize MC vs TD\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Monte Carlo\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Monte Carlo\\n\"Wait until the end\"', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Episode timeline\n",
    "for i, (label, color) in enumerate([('s₀', '#bbdefb'), ('s₁', '#bbdefb'), ('s₂', '#bbdefb'), \n",
    "                                     ('s₃', '#bbdefb'), ('END', '#c8e6c9')]):\n",
    "    x = 1 + i * 1.8\n",
    "    box = FancyBboxPatch((x - 0.4, 6), 0.8, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(x, 6.4, label, ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "    if i < 4:\n",
    "        ax1.annotate('', xy=(x + 0.6, 6.4), xytext=(x + 0.4, 6.4),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Arrow showing update only at end\n",
    "ax1.annotate('', xy=(5, 4.5), xytext=(8.6, 5.9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#d32f2f',\n",
    "                           connectionstyle='arc3,rad=0.2'))\n",
    "ax1.text(5, 4, 'Update V(s₀), V(s₁),\\nV(s₂), V(s₃) at END', \n",
    "         ha='center', fontsize=11, color='#d32f2f')\n",
    "\n",
    "ax1.text(5, 2, '❌ Must wait for episode to finish\\n❌ High variance (noisy returns)\\n❌ Can\\'t use for continuing tasks',\n",
    "         ha='center', fontsize=10, color='#666')\n",
    "\n",
    "# Right: TD\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Temporal Difference\\n\"Learn every step\"', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Episode timeline\n",
    "for i, (label, color) in enumerate([('s₀', '#bbdefb'), ('s₁', '#bbdefb'), ('s₂', '#bbdefb'), \n",
    "                                     ('s₃', '#bbdefb'), ('END', '#c8e6c9')]):\n",
    "    x = 1 + i * 1.8\n",
    "    box = FancyBboxPatch((x - 0.4, 6), 0.8, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax2.add_patch(box)\n",
    "    ax2.text(x, 6.4, label, ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "    if i < 4:\n",
    "        ax2.annotate('', xy=(x + 0.6, 6.4), xytext=(x + 0.4, 6.4),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Arrows showing updates at each step\n",
    "for i in range(4):\n",
    "    x = 1 + i * 1.8\n",
    "    ax2.annotate('', xy=(x, 5.3), xytext=(x, 5.9),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='#388e3c'))\n",
    "    ax2.text(x, 4.8, 'Update\\nnow!', ha='center', fontsize=8, color='#388e3c')\n",
    "\n",
    "ax2.text(5, 2, '✓ Update after EVERY step\\n✓ Lower variance\\n✓ Works for continuing tasks',\n",
    "         ha='center', fontsize=10, color='#388e3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THE KEY DIFFERENCE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "MONTE CARLO: Uses actual complete return G\n",
    "    V(s) ← V(s) + α × (G - V(s))\n",
    "    ↑ Must wait for G (episode must end)\n",
    "\n",
    "TEMPORAL DIFFERENCE: Uses estimated return (r + γV(s'))\n",
    "    V(s) ← V(s) + α × (r + γV(s') - V(s))\n",
    "    ↑ Can update immediately (bootstrapping!)\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The TD Update Rule\n",
    "\n",
    "The heart of TD learning is this update rule:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                   THE TD(0) UPDATE RULE                        │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  V(s) ← V(s) + α × ( r + γ×V(s') - V(s) )                     │\n",
    "    │                     └─────┬─────┘  └──┬──┘                     │\n",
    "    │                      TD target    current estimate             │\n",
    "    │                                                                │\n",
    "    │  Where:                                                       │\n",
    "    │    α = learning rate (how much to update)                     │\n",
    "    │    r = immediate reward                                       │\n",
    "    │    γ = discount factor                                        │\n",
    "    │    V(s') = estimated value of NEXT state (bootstrapping!)     │\n",
    "    │                                                                │\n",
    "    │  The TD ERROR (δ):                                            │\n",
    "    │                                                                │\n",
    "    │    δ = r + γ×V(s') - V(s)                                     │\n",
    "    │        └────┬────┘   └──┬──┘                                  │\n",
    "    │         better      what we                                   │\n",
    "    │        estimate    predicted                                  │\n",
    "    │                                                                │\n",
    "    │    δ > 0: \"Things are better than expected!\" (surprise!)     │\n",
    "    │    δ < 0: \"Things are worse than expected\" (disappointment)  │\n",
    "    │    δ = 0: \"Exactly as expected\" (no update needed)           │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Analogy:** The TD error is like a \"surprise\" signal. Dopamine neurons in the brain produce a similar signal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the TD update components\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 9.5, 'The TD(0) Update: Breaking It Down', ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Current state\n",
    "s_box = FancyBboxPatch((1, 5.5), 2, 2, boxstyle=\"round,pad=0.1\",\n",
    "                        facecolor='#bbdefb', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(s_box)\n",
    "ax.text(2, 6.8, 'State s', ha='center', fontsize=12, fontweight='bold', color='#1976d2')\n",
    "ax.text(2, 6.0, 'V(s) = 5.0', ha='center', fontsize=11)\n",
    "\n",
    "# Action and reward\n",
    "ax.annotate('', xy=(5, 6.5), xytext=(3.2, 6.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#666'))\n",
    "ax.text(4.1, 7.2, 'Take action a', ha='center', fontsize=10)\n",
    "ax.text(4.1, 5.8, 'Get reward r = 2', ha='center', fontsize=10, color='#388e3c', fontweight='bold')\n",
    "\n",
    "# Next state\n",
    "sp_box = FancyBboxPatch((5.5, 5.5), 2, 2, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(sp_box)\n",
    "ax.text(6.5, 6.8, \"State s'\", ha='center', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "ax.text(6.5, 6.0, \"V(s') = 8.0\", ha='center', fontsize=11)\n",
    "\n",
    "# TD Target calculation\n",
    "calc_box = FancyBboxPatch((9, 5.5), 4.5, 2, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax.add_patch(calc_box)\n",
    "ax.text(11.25, 7.0, 'TD Target (γ=0.9):', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(11.25, 6.2, 'r + γ×V(s\\') = 2 + 0.9×8', ha='center', fontsize=10)\n",
    "ax.text(11.25, 5.6, '= 9.2', ha='center', fontsize=12, fontweight='bold', color='#f57c00')\n",
    "\n",
    "ax.annotate('', xy=(8.9, 6.5), xytext=(7.6, 6.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#f57c00'))\n",
    "\n",
    "# TD Error\n",
    "error_box = FancyBboxPatch((4, 2), 6, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "ax.add_patch(error_box)\n",
    "ax.text(7, 4.0, 'TD Error δ = TD Target - V(s)', ha='center', fontsize=12, fontweight='bold')\n",
    "ax.text(7, 3.2, 'δ = 9.2 - 5.0 = 4.2', ha='center', fontsize=11)\n",
    "ax.text(7, 2.5, '\"Things are BETTER than expected!\"', ha='center', fontsize=10, \n",
    "        style='italic', color='#388e3c')\n",
    "\n",
    "# Update\n",
    "ax.text(7, 0.8, 'Update (α=0.1): V(s) ← 5.0 + 0.1 × 4.2 = 5.42', \n",
    "        ha='center', fontsize=12, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='#c8e6c9', edgecolor='#388e3c'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe value increased because we got a better-than-expected outcome!\")\n",
    "print(\"TD error > 0 → Increase value (positive surprise)\")\n",
    "print(\"TD error < 0 → Decrease value (disappointment)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    4x4 Grid World for TD learning.\n",
    "    \n",
    "    Same as Monte Carlo notebook for comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.goal = (3, 3)\n",
    "        self.action_names = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
    "        self.action_symbols = ['↑', '→', '↓', '←']\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pos = (0, 0)\n",
    "        return self.pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        row, col = self.pos\n",
    "        \n",
    "        if action == 0: row = max(0, row - 1)\n",
    "        elif action == 1: col = min(3, col + 1)\n",
    "        elif action == 2: row = min(3, row + 1)\n",
    "        elif action == 3: col = max(0, col - 1)\n",
    "        \n",
    "        self.pos = (row, col)\n",
    "        done = self.pos == self.goal\n",
    "        reward = 10 if done else -1\n",
    "        \n",
    "        return self.pos, reward, done\n",
    "\n",
    "\n",
    "def random_policy(state):\n",
    "    return np.random.randint(0, 4)\n",
    "\n",
    "\n",
    "env = GridWorld()\n",
    "print(\"Grid World Environment Created!\")\n",
    "print(f\"Goal: {env.goal}\")\n",
    "print(f\"Rewards: -1 per step, +10 at goal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TD(0) Prediction: The Algorithm\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                   TD(0) PREDICTION ALGORITHM                   │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Initialize V(s) = 0 for all states                           │\n",
    "    │                                                                │\n",
    "    │  For each episode:                                            │\n",
    "    │      s ← initial state                                        │\n",
    "    │                                                                │\n",
    "    │      For each step in episode:                                │\n",
    "    │          a ← action from policy                               │\n",
    "    │          r, s' ← take action a                                │\n",
    "    │                                                                │\n",
    "    │          # THE KEY UPDATE (happens EVERY step!)               │\n",
    "    │          V(s) ← V(s) + α × (r + γ×V(s') - V(s))               │\n",
    "    │                                                                │\n",
    "    │          s ← s'                                               │\n",
    "    │                                                                │\n",
    "    │  Repeat until convergence                                     │\n",
    "    │                                                                │\n",
    "    │  KEY DIFFERENCE FROM MC:                                      │\n",
    "    │    • MC: Update after ENTIRE episode                          │\n",
    "    │    • TD: Update after EACH step                               │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_0_prediction(env, policy, n_episodes=1000, alpha=0.1, gamma=0.9, verbose=False):\n",
    "    \"\"\"\n",
    "    TD(0) Prediction: Estimate V(s) for a given policy.\n",
    "    \n",
    "    Update rule: V(s) ← V(s) + α × (r + γ×V(s') - V(s))\n",
    "    \n",
    "    Args:\n",
    "        env: The environment\n",
    "        policy: Function mapping state -> action\n",
    "        n_episodes: Number of episodes\n",
    "        alpha: Learning rate\n",
    "        gamma: Discount factor\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        V: Dictionary mapping state -> value\n",
    "        history: V[(0,0)] at each episode\n",
    "        td_errors: List of TD errors for analysis\n",
    "    \"\"\"\n",
    "    V = defaultdict(float)\n",
    "    history = []\n",
    "    td_errors = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_errors = []\n",
    "        \n",
    "        for step in range(100):  # Max steps per episode\n",
    "            # Take action from policy\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # ========================================\n",
    "            # THE TD(0) UPDATE - happens EVERY step!\n",
    "            # ========================================\n",
    "            \n",
    "            # TD Target: r + γV(s')\n",
    "            td_target = reward + gamma * V[next_state]\n",
    "            \n",
    "            # TD Error: δ = target - prediction\n",
    "            td_error = td_target - V[state]\n",
    "            episode_errors.append(abs(td_error))\n",
    "            \n",
    "            # Update value\n",
    "            V[state] = V[state] + alpha * td_error\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        history.append(V[(0, 0)])\n",
    "        td_errors.append(np.mean(episode_errors))\n",
    "        \n",
    "        if verbose and (episode + 1) % 500 == 0:\n",
    "            print(f\"Episode {episode+1:5d} | V(start) = {V[(0,0)]:.2f} | Avg TD Error: {td_errors[-1]:.3f}\")\n",
    "    \n",
    "    return dict(V), history, td_errors\n",
    "\n",
    "\n",
    "# Run TD(0) prediction\n",
    "print(\"TD(0) PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nEvaluating the random policy with TD(0)...\\n\")\n",
    "\n",
    "V_td, history_td, errors_td = td_0_prediction(\n",
    "    env, random_policy, n_episodes=5000, alpha=0.1, verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Estimated Value Function V(s):\")\n",
    "print(\"-\"*40)\n",
    "for row in range(4):\n",
    "    values = [V_td.get((row, col), 0.0) for col in range(4)]\n",
    "    print(\" \".join([f\"{v:8.2f}\" for v in values]))\n",
    "print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TD learning\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Left: Learning curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(history_td, alpha=0.4, color='#2196f3', label='Raw')\n",
    "\n",
    "window = 100\n",
    "smoothed = np.convolve(history_td, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(history_td)), smoothed, color='#2196f3', \n",
    "         linewidth=2, label=f'Smoothed')\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('V(start state)', fontsize=12)\n",
    "ax1.set_title('TD(0) Learning Curve', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Middle: TD Errors over time\n",
    "ax2 = axes[1]\n",
    "ax2.plot(errors_td, alpha=0.4, color='#f57c00')\n",
    "smoothed_errors = np.convolve(errors_td, np.ones(window)/window, mode='valid')\n",
    "ax2.plot(range(window-1, len(errors_td)), smoothed_errors, color='#f57c00', \n",
    "         linewidth=2, label='Smoothed')\n",
    "\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Average |TD Error|', fontsize=12)\n",
    "ax2.set_title('TD Errors Decrease Over Time\\n(Learning is working!)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Value heatmap\n",
    "ax3 = axes[2]\n",
    "V_grid = np.array([[V_td.get((r, c), 0.0) for c in range(4)] for r in range(4)])\n",
    "\n",
    "im = ax3.imshow(V_grid, cmap='RdYlGn')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        value = V_grid[i, j]\n",
    "        color = 'white' if value < np.mean(V_grid) else 'black'\n",
    "        if (i, j) == env.goal:\n",
    "            ax3.text(j, i, 'GOAL\\n0', ha='center', va='center', \n",
    "                    fontsize=10, fontweight='bold', color='white')\n",
    "        else:\n",
    "            ax3.text(j, i, f'{value:.1f}', ha='center', va='center', \n",
    "                    fontsize=11, fontweight='bold', color=color)\n",
    "\n",
    "ax3.set_title('Estimated V(s)', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(range(4))\n",
    "ax3.set_yticks(range(4))\n",
    "plt.colorbar(im, ax=ax3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTD errors decrease as the value function converges!\")\n",
    "print(\"When TD error ≈ 0, our predictions match our observations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparing TD vs Monte Carlo\n",
    "\n",
    "Let's see which learns faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo for comparison\n",
    "\n",
    "def generate_episode(env, policy, max_steps=100):\n",
    "    \"\"\"Generate a complete episode.\"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = policy(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return episode\n",
    "\n",
    "\n",
    "def mc_prediction(env, policy, n_episodes=1000, gamma=0.9):\n",
    "    \"\"\"\n",
    "    First-visit Monte Carlo prediction.\n",
    "    \"\"\"\n",
    "    V = defaultdict(float)\n",
    "    returns = defaultdict(list)\n",
    "    history = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        episode = generate_episode(env, policy)\n",
    "        \n",
    "        G = 0\n",
    "        visited = set()\n",
    "        \n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            \n",
    "            if state not in visited:\n",
    "                visited.add(state)\n",
    "                returns[state].append(G)\n",
    "                V[state] = np.mean(returns[state])\n",
    "        \n",
    "        history.append(V[(0, 0)])\n",
    "    \n",
    "    return dict(V), history\n",
    "\n",
    "\n",
    "# Compare learning curves\n",
    "print(\"COMPARING TD(0) vs MONTE CARLO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_episodes = 3000\n",
    "\n",
    "# Run both algorithms\n",
    "print(\"Running TD(0)...\")\n",
    "V_td, history_td, _ = td_0_prediction(env, random_policy, n_episodes)\n",
    "\n",
    "print(\"Running Monte Carlo...\")\n",
    "V_mc, history_mc = mc_prediction(env, random_policy, n_episodes)\n",
    "\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Learning curves\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Raw\n",
    "ax1.plot(history_td, alpha=0.2, color='#2196f3')\n",
    "ax1.plot(history_mc, alpha=0.2, color='#f44336')\n",
    "\n",
    "# Smoothed\n",
    "window = 100\n",
    "td_smooth = np.convolve(history_td, np.ones(window)/window, mode='valid')\n",
    "mc_smooth = np.convolve(history_mc, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax1.plot(range(window-1, len(history_td)), td_smooth, color='#2196f3', \n",
    "         linewidth=3, label='TD(0)')\n",
    "ax1.plot(range(window-1, len(history_mc)), mc_smooth, color='#f44336', \n",
    "         linewidth=3, label='Monte Carlo')\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('V(start state)', fontsize=12)\n",
    "ax1.set_title('Learning Speed: TD(0) vs Monte Carlo', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Variance comparison\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Calculate rolling variance\n",
    "var_window = 100\n",
    "td_vars = [np.var(history_td[max(0,i-var_window):i+1]) for i in range(len(history_td))]\n",
    "mc_vars = [np.var(history_mc[max(0,i-var_window):i+1]) for i in range(len(history_mc))]\n",
    "\n",
    "ax2.plot(td_vars, alpha=0.6, color='#2196f3', linewidth=2, label='TD(0) variance')\n",
    "ax2.plot(mc_vars, alpha=0.6, color='#f44336', linewidth=2, label='MC variance')\n",
    "\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Variance (rolling window)', fontsize=12)\n",
    "ax2.set_title('Variance: TD(0) vs Monte Carlo\\n(Lower is better)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOBSERVATIONS:\")\n",
    "print(\"-\"*60)\n",
    "print(\"1. TD(0) often converges faster (bootstrapping helps!)\")\n",
    "print(\"2. TD(0) has lower variance (uses estimates, not noisy returns)\")\n",
    "print(\"3. MC has no bias but high variance\")\n",
    "print(\"4. TD has some bias but lower variance\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why TD Works: Bootstrapping Explained\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              WHY BOOTSTRAPPING WORKS                           │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  The BELLMAN EQUATION says:                                   │\n",
    "    │    V(s) = E[r + γV(s')]                                       │\n",
    "    │                                                                │\n",
    "    │  TD uses this relationship!                                   │\n",
    "    │    TD Target = r + γV(s') ← an ESTIMATE of V(s)              │\n",
    "    │                                                                │\n",
    "    │  Even though V(s') is itself an estimate, it works because:   │\n",
    "    │                                                                │\n",
    "    │  1. Nearby states have similar values                         │\n",
    "    │     → V(s') is often a good estimate                          │\n",
    "    │                                                                │\n",
    "    │  2. Errors cancel out over many updates                       │\n",
    "    │     → Overestimates and underestimates average out            │\n",
    "    │                                                                │\n",
    "    │  3. Information propagates backward                           │\n",
    "    │     → Terminal state values → nearby states → all states     │\n",
    "    │                                                                │\n",
    "    │  ANALOGY: Grading by Comparison                               │\n",
    "    │    Instead of waiting for the \"true\" answer,                 │\n",
    "    │    compare your answer to a classmate's                      │\n",
    "    │    → You both improve over time!                             │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize value propagation\n",
    "\n",
    "def td_with_snapshots(env, policy, n_episodes=500, alpha=0.1, gamma=0.9):\n",
    "    \"\"\"TD(0) that saves value function snapshots.\"\"\"\n",
    "    V = defaultdict(float)\n",
    "    snapshots = []\n",
    "    \n",
    "    snapshot_episodes = [0, 10, 50, 100, 200, 500]\n",
    "    \n",
    "    for episode in range(n_episodes + 1):\n",
    "        if episode in snapshot_episodes:\n",
    "            # Save snapshot\n",
    "            V_grid = np.array([[V.get((r, c), 0.0) for c in range(4)] for r in range(4)])\n",
    "            snapshots.append((episode, V_grid.copy()))\n",
    "        \n",
    "        if episode == n_episodes:\n",
    "            break\n",
    "            \n",
    "        state = env.reset()\n",
    "        for _ in range(100):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            td_target = reward + gamma * V[next_state]\n",
    "            V[state] += alpha * (td_target - V[state])\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    return snapshots\n",
    "\n",
    "\n",
    "# Get snapshots\n",
    "snapshots = td_with_snapshots(env, random_policy)\n",
    "\n",
    "# Plot snapshots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
    "\n",
    "for idx, (ax, (ep, V_grid)) in enumerate(zip(axes.flat, snapshots)):\n",
    "    im = ax.imshow(V_grid, cmap='RdYlGn', vmin=-10, vmax=5)\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            color = 'white' if V_grid[i, j] < 0 else 'black'\n",
    "            ax.text(j, i, f'{V_grid[i, j]:.1f}', ha='center', va='center', \n",
    "                    fontsize=10, fontweight='bold', color=color)\n",
    "    \n",
    "    ax.set_title(f'Episode {ep}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('TD(0): Values Propagate Backward from Goal\\n(Watch how values spread over episodes!)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice how:\")\n",
    "print(\"• Goal-adjacent states get values first\")\n",
    "print(\"• Values propagate backward through the grid\")\n",
    "print(\"• Eventually all states have reasonable values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The TD Error as a Learning Signal\n",
    "\n",
    "The TD error δ is remarkably similar to dopamine signals in the brain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TD error patterns\n",
    "\n",
    "def run_episodes_with_errors(env, policy, n_episodes=5, alpha=0.1, gamma=0.9):\n",
    "    \"\"\"Run episodes and collect TD errors.\"\"\"\n",
    "    V = defaultdict(float)\n",
    "    \n",
    "    # Pre-train for reasonable values\n",
    "    for _ in range(500):\n",
    "        state = env.reset()\n",
    "        for _ in range(100):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            td_target = reward + gamma * V[next_state]\n",
    "            V[state] += alpha * (td_target - V[state])\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    # Collect errors from a few episodes\n",
    "    episode_data = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        errors = []\n",
    "        rewards = []\n",
    "        states = []\n",
    "        \n",
    "        state = env.reset()\n",
    "        for _ in range(50):  # Limit for visualization\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            td_target = reward + gamma * V[next_state]\n",
    "            td_error = td_target - V[state]\n",
    "            \n",
    "            errors.append(td_error)\n",
    "            rewards.append(reward)\n",
    "            states.append(state)\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_data.append({'errors': errors, 'rewards': rewards, 'states': states})\n",
    "    \n",
    "    return episode_data, V\n",
    "\n",
    "\n",
    "episode_data, V_learned = run_episodes_with_errors(env, random_policy, n_episodes=3)\n",
    "\n",
    "# Plot TD errors for episodes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (ax, data) in enumerate(zip(axes, episode_data)):\n",
    "    errors = data['errors']\n",
    "    rewards = data['rewards']\n",
    "    \n",
    "    x = range(len(errors))\n",
    "    colors = ['#4caf50' if e > 0 else '#f44336' for e in errors]\n",
    "    \n",
    "    ax.bar(x, errors, color=colors, edgecolor='black', alpha=0.7)\n",
    "    ax.axhline(y=0, color='black', linewidth=1)\n",
    "    \n",
    "    # Mark when we hit the goal\n",
    "    if rewards[-1] == 10:\n",
    "        ax.axvline(x=len(errors)-1, color='gold', linewidth=2, linestyle='--', label='Reached Goal!')\n",
    "        ax.scatter([len(errors)-1], [errors[-1]], s=200, color='gold', zorder=5, marker='*')\n",
    "    \n",
    "    ax.set_xlabel('Step', fontsize=11)\n",
    "    ax.set_ylabel('TD Error δ', fontsize=11)\n",
    "    ax.set_title(f'Episode {idx+1}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    if rewards[-1] == 10:\n",
    "        ax.legend()\n",
    "\n",
    "plt.suptitle('TD Errors: Green = Positive (better than expected), Red = Negative (worse than expected)',\n",
    "             fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTD Error Interpretation:\")\n",
    "print(\"-\"*50)\n",
    "print(\"• δ > 0 (green): \\\"This is BETTER than I expected!\\\"\")\n",
    "print(\"• δ < 0 (red): \\\"This is WORSE than I expected\\\"\")\n",
    "print(\"• δ ≈ 0: \\\"Exactly as expected\\\"\")\n",
    "print(\"\\nNotice the big positive spike when reaching the goal!\")\n",
    "print(\"The brain's dopamine works similarly - reward prediction error!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TD vs MC: A Detailed Comparison\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              TD vs MONTE CARLO: DETAILED COMPARISON            │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  PROPERTY              │  TD(0)           │  MONTE CARLO       │\n",
    "    │  ─────────────────────────────────────────────────────────────│\n",
    "    │  When to update        │  Every step      │  Episode end      │\n",
    "    │  What it uses          │  Estimates       │  Actual returns   │\n",
    "    │  Bootstraps            │  Yes             │  No               │\n",
    "    │  Bias                  │  Some (initial)  │  None             │\n",
    "    │  Variance              │  Lower           │  Higher           │\n",
    "    │  Continuing tasks      │  Yes             │  No               │\n",
    "    │  Sample efficiency     │  Better          │  Worse            │\n",
    "    │  Sensitivity to α      │  More sensitive  │  Less sensitive   │\n",
    "    │                                                                │\n",
    "    │  WHEN TO USE EACH:                                            │\n",
    "    │                                                                │\n",
    "    │  TD(0):                                                       │\n",
    "    │    • Continuing tasks (no clear episode end)                  │\n",
    "    │    • Need fast learning                                       │\n",
    "    │    • Can tolerate some bias                                   │\n",
    "    │                                                                │\n",
    "    │  Monte Carlo:                                                 │\n",
    "    │    • Episodic tasks with clear endings                        │\n",
    "    │    • Need unbiased estimates                                  │\n",
    "    │    • Short episodes (otherwise too slow)                      │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison table\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(6, 9.5, 'TD(0) vs Monte Carlo', ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Headers\n",
    "ax.text(1.5, 8.5, 'Property', ha='center', fontsize=12, fontweight='bold')\n",
    "ax.text(5, 8.5, 'TD(0)', ha='center', fontsize=12, fontweight='bold', color='#2196f3')\n",
    "ax.text(9, 8.5, 'Monte Carlo', ha='center', fontsize=12, fontweight='bold', color='#f44336')\n",
    "\n",
    "# Draw header line\n",
    "ax.axhline(y=8.2, xmin=0.05, xmax=0.95, color='black', linewidth=2)\n",
    "\n",
    "# Properties\n",
    "properties = [\n",
    "    ('Updates', 'Every step ✓', 'Episode end'),\n",
    "    ('Uses', 'Estimates', 'Actual returns'),\n",
    "    ('Bias', 'Some bias', 'Unbiased ✓'),\n",
    "    ('Variance', 'Lower ✓', 'Higher'),\n",
    "    ('Continuing tasks', 'Yes ✓', 'No'),\n",
    "    ('Sample efficiency', 'Better ✓', 'Worse'),\n",
    "]\n",
    "\n",
    "for i, (prop, td_val, mc_val) in enumerate(properties):\n",
    "    y = 7.5 - i * 1.0\n",
    "    ax.text(1.5, y, prop, ha='center', fontsize=11)\n",
    "    \n",
    "    td_color = '#388e3c' if '✓' in td_val else 'black'\n",
    "    mc_color = '#388e3c' if '✓' in mc_val else 'black'\n",
    "    \n",
    "    ax.text(5, y, td_val, ha='center', fontsize=11, color=td_color)\n",
    "    ax.text(9, y, mc_val, ha='center', fontsize=11, color=mc_color)\n",
    "\n",
    "# Summary boxes\n",
    "td_box = FancyBboxPatch((3, 0.3), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#e3f2fd', edgecolor='#2196f3', linewidth=2)\n",
    "ax.add_patch(td_box)\n",
    "ax.text(4.5, 1.3, 'TD(0)', ha='center', fontsize=11, fontweight='bold', color='#2196f3')\n",
    "ax.text(4.5, 0.7, 'Fast, flexible', ha='center', fontsize=10)\n",
    "\n",
    "mc_box = FancyBboxPatch((7, 0.3), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#ffebee', edgecolor='#f44336', linewidth=2)\n",
    "ax.add_patch(mc_box)\n",
    "ax.text(8.5, 1.3, 'Monte Carlo', ha='center', fontsize=11, fontweight='bold', color='#f44336')\n",
    "ax.text(8.5, 0.7, 'Simple, unbiased', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TD Learning is the Foundation for Q-Learning!\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              TD → Q-LEARNING → DQN → ChatGPT                   │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  TD(0) for V(s):                                              │\n",
    "    │    V(s) ← V(s) + α × (r + γV(s') - V(s))                      │\n",
    "    │                                                                │\n",
    "    │  Q-LEARNING for Q(s,a):                                       │\n",
    "    │    Q(s,a) ← Q(s,a) + α × (r + γ×max Q(s',a') - Q(s,a))       │\n",
    "    │                                                                │\n",
    "    │  The key insight: Same update structure!                      │\n",
    "    │    TD uses V(s') for next state value                         │\n",
    "    │    Q-learning uses max Q(s',a') for next state value         │\n",
    "    │                                                                │\n",
    "    │  Evolution:                                                   │\n",
    "    │    TD(0) → Q-Learning → DQN → PPO → RLHF → ChatGPT           │\n",
    "    │                                                                │\n",
    "    │  Everything builds on the TD idea!                            │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### The TD Update\n",
    "\n",
    "```\n",
    "V(s) ← V(s) + α × (r + γV(s') - V(s))\n",
    "              └────────┬────────┘\n",
    "                  TD error δ\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Bootstrapping** | Using estimates to update estimates |\n",
    "| **TD Target** | r + γV(s') - what we expect |\n",
    "| **TD Error** | δ = target - prediction (surprise signal) |\n",
    "\n",
    "### TD vs Monte Carlo\n",
    "\n",
    "| Property | TD | MC |\n",
    "|----------|-------|--------|\n",
    "| Updates | Every step | Episode end |\n",
    "| Variance | Lower | Higher |\n",
    "| Bias | Some | None |\n",
    "| Continuing tasks | Yes | No |\n",
    "\n",
    "### Why TD Matters\n",
    "\n",
    "TD is the foundation for:\n",
    "- Q-learning\n",
    "- SARSA\n",
    "- DQN\n",
    "- All of modern deep RL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What is bootstrapping in TD learning?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Bootstrapping means using estimates to update estimates. In TD, we use V(s') (an estimate) to update V(s) (another estimate). We don't wait for the true return - we use our current best guess of future values.\n",
    "</details>\n",
    "\n",
    "**2. What is the TD error and what does it represent?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "TD error δ = r + γV(s') - V(s). It represents the \"surprise\" - how different reality is from our prediction. If δ > 0, things were better than expected. If δ < 0, things were worse. It's similar to dopamine signals in the brain!\n",
    "</details>\n",
    "\n",
    "**3. Why can TD learn from continuing (non-episodic) tasks but MC cannot?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "MC needs complete returns (sum of all rewards until episode end). In continuing tasks, there's no episode end, so you can never compute the complete return. TD only needs one step of experience (r + γV(s')), so it works for any task.\n",
    "</details>\n",
    "\n",
    "**4. What's the trade-off between bias and variance in TD vs MC?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "MC has zero bias (uses actual returns) but high variance (returns are noisy). TD has some bias (uses estimated V(s')) but lower variance (estimates are smoother than actual returns). TD's bias decreases as learning progresses.\n",
    "</details>\n",
    "\n",
    "**5. How does TD relate to Q-learning?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Q-learning is TD applied to action-values instead of state-values:\n",
    "- TD: V(s) ← V(s) + α(r + γV(s') - V(s))\n",
    "- Q-learning: Q(s,a) ← Q(s,a) + α(r + γ×max Q(s',a') - Q(s,a))\n",
    "\n",
    "Same TD update structure, just for Q instead of V!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Excellent work! You now understand TD learning - the foundation of modern RL!\n",
    "\n",
    "In the next notebook, we'll learn **Q-Learning** - applying TD to learn optimal action-values directly!\n",
    "\n",
    "**Continue to:** [Notebook 3: Q-Learning](03_q_learning.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*TD learning: \"Don't wait for the end - learn from every step!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
