{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA: The Cautious Learner\n",
    "\n",
    "Welcome to SARSA - the algorithm that learns to be safe by considering its own mistakes!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The SARSA update rule (with a cautious driver analogy!)\n",
    "- On-policy vs off-policy: the crucial difference\n",
    "- Why SARSA is \"safer\" than Q-learning\n",
    "- The famous cliff walking example\n",
    "- When to use SARSA vs Q-learning\n",
    "\n",
    "**Prerequisites:** Notebook 3 (Q-Learning)\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Two Drivers\n",
    "\n",
    "Imagine two drivers learning a mountain road with cliffs:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          SARSA vs Q-LEARNING: THE TWO DRIVERS                  │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Q-LEARNING = The Aggressive Driver                           │\n",
    "    │    \"The shortest path is right along the cliff edge.\"         │\n",
    "    │    \"Sure, I might slip sometimes, but the optimal             │\n",
    "    │     route is the fastest one!\"                                │\n",
    "    │                                                                │\n",
    "    │    → Learns the OPTIMAL path (ignoring exploration risk)      │\n",
    "    │    → Assumes future actions are PERFECT (greedy)              │\n",
    "    │    → Falls off cliff occasionally during learning             │\n",
    "    │                                                                │\n",
    "    │  SARSA = The Cautious Driver                                  │\n",
    "    │    \"I know I sometimes make mistakes.\"                        │\n",
    "    │    \"I'll stay away from the cliff edge because                │\n",
    "    │     I might accidentally steer wrong!\"                        │\n",
    "    │                                                                │\n",
    "    │    → Learns a SAFE path (accounting for own mistakes)         │\n",
    "    │    → Assumes future actions include EXPLORATION               │\n",
    "    │    → Avoids dangerous situations altogether                   │\n",
    "    │                                                                │\n",
    "    │  KEY INSIGHT:                                                 │\n",
    "    │    Q-learning asks: \"What's best if I'm PERFECT?\"            │\n",
    "    │    SARSA asks: \"What's best given how I ACTUALLY behave?\"    │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, Circle, FancyArrowPatch, Arrow\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from collections import defaultdict\n",
    "\n",
    "# Visualize the two drivers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Q-Learning (aggressive)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Q-Learning\\n\"The Aggressive Driver\"', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Draw road and cliff\n",
    "road = Rectangle((0, 3), 10, 4, facecolor='#e0e0e0', edgecolor='black', linewidth=2)\n",
    "ax1.add_patch(road)\n",
    "cliff = Rectangle((0, 0), 10, 3, facecolor='#795548', edgecolor='black', linewidth=2)\n",
    "ax1.add_patch(cliff)\n",
    "ax1.text(5, 1.5, 'CLIFF', ha='center', va='center', fontsize=14, color='white', fontweight='bold')\n",
    "\n",
    "# Q-learning path (along edge)\n",
    "path_x = [1, 3, 5, 7, 9]\n",
    "path_y = [5, 3.5, 3.5, 3.5, 5]\n",
    "ax1.plot(path_x, path_y, 'r-', linewidth=3, marker='o', markersize=10, label='Q-learning path')\n",
    "ax1.text(5, 4.5, 'Optimal but RISKY!', ha='center', fontsize=11, color='#d32f2f', fontweight='bold')\n",
    "\n",
    "# Occasional fall\n",
    "ax1.annotate('', xy=(5, 2), xytext=(5, 3.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f', linestyle='--'))\n",
    "ax1.text(5.3, 2.5, 'Sometimes\\nfalls!', fontsize=9, color='#d32f2f')\n",
    "\n",
    "ax1.text(5, 9, 'Learns: \"Edge is fastest\"', ha='center', fontsize=11, style='italic')\n",
    "ax1.text(5, 8.2, '(Ignores exploration mistakes)', ha='center', fontsize=10, color='#666')\n",
    "\n",
    "# Right: SARSA (cautious)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('SARSA\\n\"The Cautious Driver\"', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Draw road and cliff\n",
    "road2 = Rectangle((0, 3), 10, 4, facecolor='#e0e0e0', edgecolor='black', linewidth=2)\n",
    "ax2.add_patch(road2)\n",
    "cliff2 = Rectangle((0, 0), 10, 3, facecolor='#795548', edgecolor='black', linewidth=2)\n",
    "ax2.add_patch(cliff2)\n",
    "ax2.text(5, 1.5, 'CLIFF', ha='center', va='center', fontsize=14, color='white', fontweight='bold')\n",
    "\n",
    "# SARSA path (away from edge)\n",
    "path_x2 = [1, 3, 5, 7, 9]\n",
    "path_y2 = [5, 5.5, 6, 5.5, 5]\n",
    "ax2.plot(path_x2, path_y2, 'g-', linewidth=3, marker='o', markersize=10, label='SARSA path')\n",
    "ax2.text(5, 4.5, 'Longer but SAFE!', ha='center', fontsize=11, color='#388e3c', fontweight='bold')\n",
    "\n",
    "ax2.text(5, 9, 'Learns: \"Stay away from edge\"', ha='center', fontsize=11, style='italic')\n",
    "ax2.text(5, 8.2, '(Accounts for own mistakes)', ha='center', fontsize=10, color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THE KEY DIFFERENCE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Q-LEARNING (Off-Policy):\n",
    "  Updates using: max Q(s', a')\n",
    "  → \"What if I take the BEST action next?\"\n",
    "  → Learns optimal policy, but risky during learning\n",
    "\n",
    "SARSA (On-Policy):\n",
    "  Updates using: Q(s', a') where a' is the ACTUAL next action\n",
    "  → \"What if I take the action I WOULD actually take next?\"\n",
    "  → Learns a policy that accounts for exploration\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The SARSA Update Rule\n",
    "\n",
    "SARSA gets its name from the quintuple: **(S, A, R, S', A')**\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                   THE SARSA UPDATE RULE                        │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Q(s, a) ← Q(s, a) + α × ( r + γ×Q(s', a') - Q(s, a) )        │\n",
    "    │                              └────┬─────┘                      │\n",
    "    │                            ACTUAL next action                  │\n",
    "    │                          (from exploration policy!)            │\n",
    "    │                                                                │\n",
    "    │  Compare to Q-Learning:                                       │\n",
    "    │  Q(s, a) ← Q(s, a) + α × ( r + γ×max_a'Q(s', a') - Q(s, a) )  │\n",
    "    │                              └─────────┬───────┘               │\n",
    "    │                                 BEST action                    │\n",
    "    │                             (ignores exploration!)             │\n",
    "    │                                                                │\n",
    "    │  THE NAME: S-A-R-S'-A'                                        │\n",
    "    │    State → Action → Reward → next State → next Action         │\n",
    "    │                                                                │\n",
    "    │  We need ALL FIVE elements to do the update!                  │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the SARSA update components\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 9.5, 'The SARSA Quintuple: S, A, R, S\\', A\\'', ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Timeline boxes\n",
    "elements = [\n",
    "    {'name': 'S', 'desc': 'State', 'x': 1.5, 'color': '#bbdefb', 'edge': '#1976d2'},\n",
    "    {'name': 'A', 'desc': 'Action', 'x': 4, 'color': '#fff3e0', 'edge': '#f57c00'},\n",
    "    {'name': 'R', 'desc': 'Reward', 'x': 6.5, 'color': '#c8e6c9', 'edge': '#388e3c'},\n",
    "    {'name': \"S'\", 'desc': 'Next State', 'x': 9, 'color': '#bbdefb', 'edge': '#1976d2'},\n",
    "    {'name': \"A'\", 'desc': 'Next Action', 'x': 11.5, 'color': '#ffcdd2', 'edge': '#d32f2f'},\n",
    "]\n",
    "\n",
    "for i, elem in enumerate(elements):\n",
    "    # Box\n",
    "    box = FancyBboxPatch((elem['x']-0.8, 6), 1.6, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=elem['color'], edgecolor=elem['edge'], linewidth=3)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(elem['x'], 7.3, elem['name'], ha='center', va='center', fontsize=18, fontweight='bold')\n",
    "    ax.text(elem['x'], 6.5, elem['desc'], ha='center', fontsize=10)\n",
    "    \n",
    "    # Arrow to next\n",
    "    if i < len(elements) - 1:\n",
    "        next_x = elements[i+1]['x']\n",
    "        ax.annotate('', xy=(next_x-0.9, 7), xytext=(elem['x']+0.9, 7),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "# Highlight the key difference\n",
    "highlight = FancyBboxPatch((10.5, 5.8), 2.2, 2.5, boxstyle=\"round,pad=0.05\",\n",
    "                            facecolor='none', edgecolor='#d32f2f', linewidth=3, linestyle='--')\n",
    "ax.add_patch(highlight)\n",
    "ax.text(11.6, 5.4, 'KEY: Uses ACTUAL\\nnext action, not max!', ha='center', fontsize=10, color='#d32f2f', fontweight='bold')\n",
    "\n",
    "# Update equation at bottom\n",
    "eq_box = FancyBboxPatch((2, 1.5), 10, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#f5f5f5', edgecolor='#333', linewidth=2)\n",
    "ax.add_patch(eq_box)\n",
    "ax.text(7, 3.3, 'SARSA Update:', ha='center', fontsize=12, fontweight='bold')\n",
    "ax.text(7, 2.5, r'Q(S, A) ← Q(S, A) + α × ( R + γ×Q(S\\', A\\') - Q(S, A) )', \n",
    "        ha='center', fontsize=13, family='monospace')\n",
    "ax.text(7, 1.8, 'Uses all five elements!', ha='center', fontsize=10, color='#666', style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## On-Policy vs Off-Policy: The Core Distinction\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              ON-POLICY vs OFF-POLICY LEARNING                  │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  ON-POLICY (SARSA):                                           │\n",
    "    │    \"Learn about the policy I'm actually following\"            │\n",
    "    │                                                                │\n",
    "    │    • Target policy = Behavior policy (same!)                  │\n",
    "    │    • Evaluates: Q^π (value of current policy)                 │\n",
    "    │    • If I explore, my Q-values account for exploration        │\n",
    "    │                                                                │\n",
    "    │  OFF-POLICY (Q-Learning):                                     │\n",
    "    │    \"Learn about the optimal policy while following another\"   │\n",
    "    │                                                                │\n",
    "    │    • Target policy ≠ Behavior policy (different!)             │\n",
    "    │    • Evaluates: Q* (value of optimal policy)                  │\n",
    "    │    • Q-values assume greedy actions, even if we explore       │\n",
    "    │                                                                │\n",
    "    │  ANALOGY:                                                     │\n",
    "    │    ON-POLICY: \"How good am I at basketball right now?\"        │\n",
    "    │    OFF-POLICY: \"How good would I be if I were perfect?\"       │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize on-policy vs off-policy\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: On-policy (SARSA)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('ON-POLICY (SARSA)\\n\"Same policy for acting and learning\"', \n",
    "              fontsize=14, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Behavior policy box\n",
    "bp1 = FancyBboxPatch((3, 5), 4, 2, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax1.add_patch(bp1)\n",
    "ax1.text(5, 6.3, 'ε-Greedy Policy', ha='center', fontsize=12, fontweight='bold')\n",
    "ax1.text(5, 5.5, '(Used for BOTH)', ha='center', fontsize=10)\n",
    "\n",
    "# Arrows\n",
    "ax1.annotate('', xy=(3.5, 4.9), xytext=(3.5, 4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#388e3c'))\n",
    "ax1.text(3.5, 3.5, 'Acting', ha='center', fontsize=10, color='#388e3c', fontweight='bold')\n",
    "\n",
    "ax1.annotate('', xy=(6.5, 4.9), xytext=(6.5, 4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#388e3c'))\n",
    "ax1.text(6.5, 3.5, 'Learning', ha='center', fontsize=10, color='#388e3c', fontweight='bold')\n",
    "\n",
    "# Result\n",
    "ax1.text(5, 2, 'Learns Q^π (value of THIS policy)', ha='center', fontsize=11, \n",
    "         style='italic', color='#388e3c')\n",
    "ax1.text(5, 1.2, 'Accounts for exploration mistakes!', ha='center', fontsize=10, color='#666')\n",
    "\n",
    "ax1.text(5, 8.5, '✓ Safer learning\\n✓ Accounts for own behavior', \n",
    "         ha='center', fontsize=10, color='#388e3c')\n",
    "\n",
    "# Right: Off-policy (Q-Learning)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('OFF-POLICY (Q-Learning)\\n\"Different policies for acting and learning\"', \n",
    "              fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "# Behavior policy\n",
    "bp2 = FancyBboxPatch((1.5, 5), 3, 2, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='#bbdefb', edgecolor='#1976d2', linewidth=2)\n",
    "ax2.add_patch(bp2)\n",
    "ax2.text(3, 6.2, 'ε-Greedy', ha='center', fontsize=11, fontweight='bold')\n",
    "ax2.text(3, 5.5, 'Behavior Policy', ha='center', fontsize=9)\n",
    "\n",
    "# Target policy\n",
    "tp2 = FancyBboxPatch((5.5, 5), 3, 2, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "ax2.add_patch(tp2)\n",
    "ax2.text(7, 6.2, 'Greedy', ha='center', fontsize=11, fontweight='bold')\n",
    "ax2.text(7, 5.5, 'Target Policy', ha='center', fontsize=9)\n",
    "\n",
    "# Arrows\n",
    "ax2.annotate('', xy=(3, 4.9), xytext=(3, 4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#1976d2'))\n",
    "ax2.text(3, 3.5, 'Acting', ha='center', fontsize=10, color='#1976d2', fontweight='bold')\n",
    "\n",
    "ax2.annotate('', xy=(7, 4.9), xytext=(7, 4),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f'))\n",
    "ax2.text(7, 3.5, 'Learning', ha='center', fontsize=10, color='#d32f2f', fontweight='bold')\n",
    "\n",
    "# Result\n",
    "ax2.text(5, 2, 'Learns Q* (value of OPTIMAL policy)', ha='center', fontsize=11, \n",
    "         style='italic', color='#d32f2f')\n",
    "ax2.text(5, 1.2, 'Ignores exploration in value estimates!', ha='center', fontsize=10, color='#666')\n",
    "\n",
    "ax2.text(5, 8.5, '✓ Finds optimal policy\\n✗ Risky during learning', \n",
    "         ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Cliff Walking Environment\n",
    "\n",
    "This classic example perfectly illustrates the difference between SARSA and Q-learning:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                    CLIFF WALKING                               │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  ┌───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┐           │\n",
    "    │  │   │   │   │   │   │   │   │   │   │   │   │   │           │\n",
    "    │  ├───┼───┼───┼───┼───┼───┼───┼───┼───┼───┼───┼───┤           │\n",
    "    │  │   │   │   │   │   │   │   │   │   │   │   │   │           │\n",
    "    │  ├───┼───┼───┼───┼───┼───┼───┼───┼───┼───┼───┼───┤           │\n",
    "    │  │   │   │   │   │   │   │   │   │   │   │   │   │           │\n",
    "    │  ├───┼───┼───┼───┼───┼───┼───┼───┼───┼───┼───┼───┤           │\n",
    "    │  │ S │ C │ C │ C │ C │ C │ C │ C │ C │ C │ C │ G │           │\n",
    "    │  └───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┘           │\n",
    "    │                                                                │\n",
    "    │  S = Start (3, 0)                                             │\n",
    "    │  G = Goal (3, 11)                                             │\n",
    "    │  C = Cliff (-100 reward, sent back to start!)                 │\n",
    "    │  Regular step = -1 reward                                     │\n",
    "    │                                                                │\n",
    "    │  THE DILEMMA:                                                 │\n",
    "    │    • Shortest path: Right along the bottom (near cliff)       │\n",
    "    │    • Safest path: Up, across the top, then down               │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalking:\n",
    "    \"\"\"\n",
    "    The Cliff Walking environment.\n",
    "    \n",
    "    A 4x12 grid where the agent must go from Start to Goal.\n",
    "    The bottom row (except Start and Goal) is a cliff.\n",
    "    Falling off the cliff gives -100 reward and returns to start.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.height = 4\n",
    "        self.width = 12\n",
    "        self.start = (3, 0)    # Bottom left\n",
    "        self.goal = (3, 11)    # Bottom right\n",
    "        self.cliff = [(3, i) for i in range(1, 11)]  # Bottom row except start/goal\n",
    "        \n",
    "        self.action_names = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
    "        self.action_symbols = ['↑', '→', '↓', '←']\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to start position.\"\"\"\n",
    "        self.pos = self.start\n",
    "        return self.pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return (next_state, reward, done).\"\"\"\n",
    "        row, col = self.pos\n",
    "        \n",
    "        # Move\n",
    "        if action == 0: row = max(0, row - 1)              # UP\n",
    "        elif action == 1: col = min(self.width - 1, col + 1)  # RIGHT\n",
    "        elif action == 2: row = min(self.height - 1, row + 1) # DOWN\n",
    "        elif action == 3: col = max(0, col - 1)            # LEFT\n",
    "        \n",
    "        self.pos = (row, col)\n",
    "        \n",
    "        # Check if fell off cliff\n",
    "        if self.pos in self.cliff:\n",
    "            self.pos = self.start  # Sent back to start!\n",
    "            return self.pos, -100, False\n",
    "        \n",
    "        # Check if reached goal\n",
    "        if self.pos == self.goal:\n",
    "            return self.pos, -1, True\n",
    "        \n",
    "        return self.pos, -1, False\n",
    "\n",
    "\n",
    "# Visualize the cliff walking environment\n",
    "env = CliffWalking()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "# Draw grid\n",
    "for row in range(env.height):\n",
    "    for col in range(env.width):\n",
    "        y = env.height - 1 - row  # Flip y for visualization\n",
    "        \n",
    "        if (row, col) == env.start:\n",
    "            color = '#bbdefb'\n",
    "            label = 'S'\n",
    "        elif (row, col) == env.goal:\n",
    "            color = '#c8e6c9'\n",
    "            label = 'G'\n",
    "        elif (row, col) in env.cliff:\n",
    "            color = '#795548'\n",
    "            label = 'C'\n",
    "        else:\n",
    "            color = 'white'\n",
    "            label = ''\n",
    "        \n",
    "        rect = Rectangle((col, y), 1, 1, facecolor=color, edgecolor='black', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        text_color = 'white' if (row, col) in env.cliff else 'black'\n",
    "        ax.text(col + 0.5, y + 0.5, label, ha='center', va='center', \n",
    "                fontsize=12, fontweight='bold', color=text_color)\n",
    "\n",
    "ax.set_xlim(0, env.width)\n",
    "ax.set_ylim(0, env.height)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Cliff Walking Environment\\n(S = Start, G = Goal, C = Cliff)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "ax.text(6, -0.5, 'Cliff: -100 reward, back to start!   |   Regular step: -1 reward', \n",
    "        ha='center', fontsize=11, color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe agent must navigate from S to G without falling off the cliff!\")\n",
    "print(\"Two possible strategies:\")\n",
    "print(\"  1. Short path: Along the edge (risky with exploration)\")\n",
    "print(\"  2. Safe path: Up, across, down (longer but safer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implementing SARSA\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │                    SARSA ALGORITHM                             │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Initialize Q(s, a) = 0 for all s, a                          │\n",
    "    │                                                                │\n",
    "    │  For each episode:                                            │\n",
    "    │      s ← initial state                                        │\n",
    "    │      a ← ε-greedy action for s                                │\n",
    "    │                                                                │\n",
    "    │      For each step in episode:                                │\n",
    "    │          Take action a, observe r, s'                         │\n",
    "    │          a' ← ε-greedy action for s'  ← KEY DIFFERENCE!       │\n",
    "    │                                                                │\n",
    "    │          # SARSA update (uses actual next action a')          │\n",
    "    │          Q(s, a) ← Q(s, a) + α × (r + γ×Q(s', a') - Q(s, a))  │\n",
    "    │                                                                │\n",
    "    │          s ← s'                                               │\n",
    "    │          a ← a'                                               │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, n_episodes=500, alpha=0.5, gamma=1.0, epsilon=0.1, verbose=False):\n",
    "    \"\"\"\n",
    "    SARSA: On-policy TD control.\n",
    "    \n",
    "    The key difference from Q-learning:\n",
    "    We use the ACTUAL next action a' (from ε-greedy policy),\n",
    "    not the max action.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment\n",
    "        n_episodes: Number of episodes to train\n",
    "        alpha: Learning rate\n",
    "        gamma: Discount factor\n",
    "        epsilon: Exploration rate\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        Q: Learned action-value function\n",
    "        rewards_history: Total reward per episode\n",
    "        paths: Saved paths for visualization\n",
    "    \"\"\"\n",
    "    # Initialize Q-values\n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    rewards_history = []\n",
    "    paths = []\n",
    "    \n",
    "    def epsilon_greedy(state):\n",
    "        \"\"\"Choose action using ε-greedy policy.\"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(0, 4)  # Explore\n",
    "        return np.argmax(Q[state])  # Exploit\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # Initialize\n",
    "        state = env.reset()\n",
    "        action = epsilon_greedy(state)  # Choose FIRST action\n",
    "        total_reward = 0\n",
    "        path = [state]\n",
    "        \n",
    "        for step in range(200):  # Max steps per episode\n",
    "            # Take action, observe reward and next state\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            path.append(next_state)\n",
    "            \n",
    "            # ========================================\n",
    "            # KEY: Choose next action BEFORE update!\n",
    "            # This is what makes SARSA \"on-policy\"\n",
    "            # ========================================\n",
    "            next_action = epsilon_greedy(next_state)\n",
    "            \n",
    "            # ========================================\n",
    "            # SARSA UPDATE: uses Q(s', a') not max!\n",
    "            # ========================================\n",
    "            td_target = reward + gamma * Q[next_state][next_action]\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_error\n",
    "            \n",
    "            # Move to next state-action pair\n",
    "            state = next_state\n",
    "            action = next_action  # Use the SAME action we chose!\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        # Save some paths for visualization\n",
    "        if episode in [0, n_episodes//4, n_episodes//2, n_episodes-1]:\n",
    "            paths.append((episode, path.copy()))\n",
    "        \n",
    "        if verbose and (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-50:])\n",
    "            print(f\"Episode {episode+1:4d} | Avg Reward (last 50): {avg_reward:.1f}\")\n",
    "    \n",
    "    return dict(Q), rewards_history, paths\n",
    "\n",
    "\n",
    "def q_learning(env, n_episodes=500, alpha=0.5, gamma=1.0, epsilon=0.1, verbose=False):\n",
    "    \"\"\"\n",
    "    Q-Learning: Off-policy TD control.\n",
    "    \n",
    "    Uses max Q(s', a') instead of actual next action.\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    rewards_history = []\n",
    "    paths = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        path = [state]\n",
    "        \n",
    "        for step in range(200):\n",
    "            # ε-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(0, 4)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            path.append(next_state)\n",
    "            \n",
    "            # ========================================\n",
    "            # Q-LEARNING UPDATE: uses MAX, not actual!\n",
    "            # ========================================\n",
    "            td_target = reward + gamma * np.max(Q[next_state])\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_error\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if episode in [0, n_episodes//4, n_episodes//2, n_episodes-1]:\n",
    "            paths.append((episode, path.copy()))\n",
    "        \n",
    "        if verbose and (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-50:])\n",
    "            print(f\"Episode {episode+1:4d} | Avg Reward (last 50): {avg_reward:.1f}\")\n",
    "    \n",
    "    return dict(Q), rewards_history, paths\n",
    "\n",
    "\n",
    "# Train both algorithms\n",
    "print(\"TRAINING ON CLIFF WALKING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = CliffWalking()\n",
    "\n",
    "print(\"\\nTraining SARSA...\")\n",
    "Q_sarsa, rewards_sarsa, paths_sarsa = sarsa(env, n_episodes=500, verbose=True)\n",
    "\n",
    "print(\"\\nTraining Q-Learning...\")\n",
    "Q_qlearn, rewards_qlearn, paths_qlearn = q_learning(env, n_episodes=500, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learning curves\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Raw rewards\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rewards_sarsa, alpha=0.3, color='#388e3c', label='SARSA (raw)')\n",
    "ax1.plot(rewards_qlearn, alpha=0.3, color='#d32f2f', label='Q-Learning (raw)')\n",
    "\n",
    "# Smoothed\n",
    "window = 20\n",
    "sarsa_smooth = np.convolve(rewards_sarsa, np.ones(window)/window, mode='valid')\n",
    "qlearn_smooth = np.convolve(rewards_qlearn, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax1.plot(range(window-1, len(rewards_sarsa)), sarsa_smooth, \n",
    "         color='#388e3c', linewidth=3, label='SARSA (smoothed)')\n",
    "ax1.plot(range(window-1, len(rewards_qlearn)), qlearn_smooth, \n",
    "         color='#d32f2f', linewidth=3, label='Q-Learning (smoothed)')\n",
    "\n",
    "ax1.axhline(y=-13, color='gray', linestyle='--', alpha=0.5, label='Optimal (no cliff)')\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward per Episode', fontsize=12)\n",
    "ax1.set_title('Learning Curves: SARSA vs Q-Learning', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Focus on later episodes (after learning)\n",
    "ax2 = axes[1]\n",
    "late_sarsa = rewards_sarsa[-100:]\n",
    "late_qlearn = rewards_qlearn[-100:]\n",
    "\n",
    "ax2.boxplot([late_sarsa, late_qlearn], labels=['SARSA', 'Q-Learning'])\n",
    "ax2.axhline(y=-13, color='gray', linestyle='--', alpha=0.5, label='Optimal')\n",
    "\n",
    "ax2.set_ylabel('Total Reward (last 100 episodes)', fontsize=12)\n",
    "ax2.set_title('Reward Distribution After Learning', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add mean annotations\n",
    "ax2.text(1, np.mean(late_sarsa) + 3, f'Mean: {np.mean(late_sarsa):.1f}', \n",
    "         ha='center', fontsize=10, color='#388e3c')\n",
    "ax2.text(2, np.mean(late_qlearn) + 3, f'Mean: {np.mean(late_qlearn):.1f}', \n",
    "         ha='center', fontsize=10, color='#d32f2f')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOBSERVATIONS:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"SARSA average reward (last 100): {np.mean(late_sarsa):.1f}\")\n",
    "print(f\"Q-Learning average reward (last 100): {np.mean(late_qlearn):.1f}\")\n",
    "print(\"\\nQ-Learning has MORE VARIANCE (occasional cliff falls!)\")\n",
    "print(\"SARSA is MORE CONSISTENT (stays away from cliff)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned paths!\n",
    "\n",
    "def extract_greedy_path(Q, env):\n",
    "    \"\"\"Extract the greedy path from Q-values.\"\"\"\n",
    "    path = []\n",
    "    state = env.reset()\n",
    "    path.append(state)\n",
    "    \n",
    "    for _ in range(50):  # Max steps\n",
    "        action = np.argmax(Q.get(state, np.zeros(4)))\n",
    "        next_state, _, done = env.step(action)\n",
    "        path.append(next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return path\n",
    "\n",
    "\n",
    "# Get greedy paths\n",
    "sarsa_path = extract_greedy_path(Q_sarsa, CliffWalking())\n",
    "qlearn_path = extract_greedy_path(Q_qlearn, CliffWalking())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, path, title, color in [(axes[0], sarsa_path, 'SARSA: Safe Path', '#388e3c'),\n",
    "                                 (axes[1], qlearn_path, 'Q-Learning: Optimal Path', '#d32f2f')]:\n",
    "    # Draw grid\n",
    "    for row in range(env.height):\n",
    "        for col in range(env.width):\n",
    "            y = env.height - 1 - row\n",
    "            \n",
    "            if (row, col) == env.start:\n",
    "                cell_color = '#bbdefb'\n",
    "            elif (row, col) == env.goal:\n",
    "                cell_color = '#c8e6c9'\n",
    "            elif (row, col) in env.cliff:\n",
    "                cell_color = '#795548'\n",
    "            else:\n",
    "                cell_color = 'white'\n",
    "            \n",
    "            rect = Rectangle((col, y), 1, 1, facecolor=cell_color, \n",
    "                              edgecolor='black', linewidth=1)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    # Draw path\n",
    "    path_x = [p[1] + 0.5 for p in path]\n",
    "    path_y = [env.height - 1 - p[0] + 0.5 for p in path]\n",
    "    \n",
    "    ax.plot(path_x, path_y, color=color, linewidth=4, marker='o', markersize=8, \n",
    "            alpha=0.8, label='Greedy path')\n",
    "    \n",
    "    # Start and end markers\n",
    "    ax.scatter([path_x[0]], [path_y[0]], s=200, c='blue', marker='s', zorder=5, label='Start')\n",
    "    ax.scatter([path_x[-1]], [path_y[-1]], s=200, c='green', marker='*', zorder=5, label='Goal')\n",
    "    \n",
    "    ax.set_xlim(0, env.width)\n",
    "    ax.set_ylim(0, env.height)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'{title}\\n(Path length: {len(path)-1} steps)', fontsize=14, fontweight='bold', color=color)\n",
    "\n",
    "plt.suptitle('Learned Paths: SARSA vs Q-Learning', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPATH ANALYSIS:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"SARSA path length: {len(sarsa_path)-1} steps (takes the safe route!)\")\n",
    "print(f\"Q-Learning path length: {len(qlearn_path)-1} steps (takes the optimal route!)\")\n",
    "print(\"\\nBut Q-Learning's 'optimal' path is only optimal if you NEVER make mistakes!\")\n",
    "print(\"With ε-greedy exploration, Q-Learning occasionally falls off the cliff.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Q-values to understand the learned policies\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, Q, title, cmap_name in [(axes[0], Q_sarsa, 'SARSA: Avoids cliff edge', 'Greens'),\n",
    "                                  (axes[1], Q_qlearn, 'Q-Learning: Edge is optimal', 'Reds')]:\n",
    "    \n",
    "    # Draw grid with values and arrows\n",
    "    for row in range(env.height):\n",
    "        for col in range(env.width):\n",
    "            y = env.height - 1 - row\n",
    "            \n",
    "            if (row, col) == env.start:\n",
    "                cell_color = '#bbdefb'\n",
    "            elif (row, col) == env.goal:\n",
    "                cell_color = '#c8e6c9'\n",
    "            elif (row, col) in env.cliff:\n",
    "                cell_color = '#795548'\n",
    "            else:\n",
    "                # Color based on max Q-value\n",
    "                max_q = np.max(Q.get((row, col), np.zeros(4)))\n",
    "                cell_color = 'white'\n",
    "            \n",
    "            rect = Rectangle((col, y), 1, 1, facecolor=cell_color, \n",
    "                              edgecolor='black', linewidth=1)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Draw arrow for best action (if not cliff or goal)\n",
    "            if (row, col) not in env.cliff and (row, col) != env.goal:\n",
    "                q_vals = Q.get((row, col), np.zeros(4))\n",
    "                best_action = np.argmax(q_vals)\n",
    "                \n",
    "                # Arrow directions\n",
    "                dx, dy = [(0, 0.35), (0.35, 0), (0, -0.35), (-0.35, 0)][best_action]\n",
    "                \n",
    "                ax.arrow(col + 0.5 - dx/2, y + 0.5 - dy/2, dx, dy,\n",
    "                         head_width=0.15, head_length=0.1, \n",
    "                         fc='black', ec='black', linewidth=1)\n",
    "    \n",
    "    ax.set_xlim(0, env.width)\n",
    "    ax.set_ylim(0, env.height)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Learned Policies (Arrows = Best Action)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNOTICE:\")\n",
    "print(\"-\"*60)\n",
    "print(\"SARSA: Arrows point UP early (away from cliff edge)\")\n",
    "print(\"Q-Learning: Arrows point RIGHT immediately (along cliff edge)\")\n",
    "print(\"\\nThis is the core difference in learned behavior!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why SARSA is Safer: A Deeper Look\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │              WHY SARSA AVOIDS THE CLIFF                        │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  Consider being one step from the cliff edge:                 │\n",
    "    │                                                                │\n",
    "    │      ┌───┬───┐                                                │\n",
    "    │      │ A │   │  A = Agent                                     │\n",
    "    │      ├───┼───┤                                                │\n",
    "    │      │ C │ C │  C = Cliff                                     │\n",
    "    │      └───┴───┘                                                │\n",
    "    │                                                                │\n",
    "    │  Q-LEARNING thinks:                                           │\n",
    "    │    \"If I move right (best action), Q(right) is high\"          │\n",
    "    │    \"I'll learn that being here is GOOD\"                       │\n",
    "    │    → Ignores the 10% chance of random DOWN action!            │\n",
    "    │                                                                │\n",
    "    │  SARSA thinks:                                                │\n",
    "    │    \"My next action will be ε-greedy\"                          │\n",
    "    │    \"10% of the time, I'll randomly go DOWN (into cliff!)\"     │\n",
    "    │    → Q(s, a) includes the ACTUAL risk of exploration          │\n",
    "    │    → Being near the cliff edge has LOWER value                │\n",
    "    │    → I learn to stay away from dangerous positions!           │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the difference in Q-values near the cliff\n",
    "\n",
    "print(\"Q-VALUES NEAR THE CLIFF\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Position (2, 1) is one row above the cliff at (3, 1)\n",
    "critical_state = (2, 1)  # One step above cliff\n",
    "\n",
    "print(f\"\\nState {critical_state} (one step above cliff):\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "print(\"\\nSARSA Q-values:\")\n",
    "sarsa_q = Q_sarsa.get(critical_state, np.zeros(4))\n",
    "for i, name in enumerate(['UP', 'RIGHT', 'DOWN', 'LEFT']):\n",
    "    danger = \" ← DANGER!\" if name == 'DOWN' else \"\"\n",
    "    print(f\"  {name}: {sarsa_q[i]:.2f}{danger}\")\n",
    "\n",
    "print(\"\\nQ-Learning Q-values:\")\n",
    "qlearn_q = Q_qlearn.get(critical_state, np.zeros(4))\n",
    "for i, name in enumerate(['UP', 'RIGHT', 'DOWN', 'LEFT']):\n",
    "    danger = \" ← DANGER!\" if name == 'DOWN' else \"\"\n",
    "    print(f\"  {name}: {qlearn_q[i]:.2f}{danger}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"SARSA Q(DOWN): {sarsa_q[2]:.2f} (very negative - cliff death!)\")\n",
    "print(f\"Q-Learning Q(DOWN): {qlearn_q[2]:.2f}\")\n",
    "print(\"\\nSARSA learns that DOWN is terrible because with ε-greedy,\")\n",
    "print(\"there's always a chance of accidentally going DOWN!\")\n",
    "print(\"\\nQ-Learning ignores this risk because it assumes max action.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of epsilon on the difference\n",
    "\n",
    "print(\"EFFECT OF EXPLORATION RATE (ε)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nWith higher ε, SARSA becomes MORE cautious!\")\n",
    "print(\"(More exploration = more chance of accidents)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "epsilons = [0.0, 0.1, 0.2, 0.3]\n",
    "sarsa_results = []\n",
    "qlearn_results = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    env = CliffWalking()\n",
    "    \n",
    "    _, rewards_s, _ = sarsa(env, n_episodes=300, epsilon=eps)\n",
    "    _, rewards_q, _ = q_learning(env, n_episodes=300, epsilon=eps)\n",
    "    \n",
    "    sarsa_results.append(np.mean(rewards_s[-50:]))\n",
    "    qlearn_results.append(np.mean(rewards_q[-50:]))\n",
    "    \n",
    "    print(f\"ε = {eps:.1f}: SARSA avg = {sarsa_results[-1]:.1f}, Q-Learning avg = {qlearn_results[-1]:.1f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(epsilons))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, sarsa_results, width, label='SARSA', color='#388e3c')\n",
    "bars2 = ax.bar(x + width/2, qlearn_results, width, label='Q-Learning', color='#d32f2f')\n",
    "\n",
    "ax.axhline(y=-13, color='gray', linestyle='--', alpha=0.5, label='Optimal (no exploration)')\n",
    "\n",
    "ax.set_xlabel('Epsilon (ε)', fontsize=12)\n",
    "ax.set_ylabel('Average Reward', fontsize=12)\n",
    "ax.set_title('Effect of Exploration Rate on Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'ε={e}' for e in epsilons])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSIGHT:\")\n",
    "print(\"-\"*60)\n",
    "print(\"• With ε=0 (no exploration), both are similar\")\n",
    "print(\"• As ε increases, Q-Learning suffers more cliff falls!\")\n",
    "print(\"• SARSA adapts by taking safer paths as ε increases\")\n",
    "print(\"• Q-Learning keeps learning the 'optimal' edge path\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## When to Use SARSA vs Q-Learning\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────────────┐\n",
    "    │          SARSA vs Q-LEARNING: WHEN TO USE EACH                │\n",
    "    ├────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                │\n",
    "    │  USE SARSA WHEN:                                              │\n",
    "    │    ✓ Exploration mistakes are COSTLY                          │\n",
    "    │      (robotics, real-world systems, cliff-like scenarios)     │\n",
    "    │    ✓ You care about the policy you're ACTUALLY following      │\n",
    "    │    ✓ Safety during learning is important                      │\n",
    "    │    ✓ You'll continue using ε-greedy after learning            │\n",
    "    │                                                                │\n",
    "    │  USE Q-LEARNING WHEN:                                         │\n",
    "    │    ✓ You want the truly OPTIMAL policy                        │\n",
    "    │    ✓ Exploration mistakes are cheap (simulation)              │\n",
    "    │    ✓ You'll use greedy policy after learning (ε → 0)          │\n",
    "    │    ✓ You have a separate exploration strategy                 │\n",
    "    │                                                                │\n",
    "    │  EXAMPLES:                                                    │\n",
    "    │                                                                │\n",
    "    │    SARSA: Robot navigation (falling is expensive!)           │\n",
    "    │           Real-world trading (losses are real!)              │\n",
    "    │           Medical treatment (mistakes can harm patients)      │\n",
    "    │                                                                │\n",
    "    │    Q-Learning: Video game AI (can reset)                     │\n",
    "    │                Simulation environments (no real cost)         │\n",
    "    │                When you'll stop exploring after training      │\n",
    "    │                                                                │\n",
    "    └────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(6, 9.5, 'SARSA vs Q-Learning: Complete Comparison', ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Table headers\n",
    "headers = ['Property', 'SARSA', 'Q-Learning']\n",
    "header_x = [1.5, 5, 9]\n",
    "for x, h in zip(header_x, headers):\n",
    "    ax.text(x, 8.5, h, ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.axhline(y=8.2, xmin=0.05, xmax=0.95, color='black', linewidth=2)\n",
    "\n",
    "# Table rows\n",
    "rows = [\n",
    "    ('Type', 'On-policy', 'Off-policy'),\n",
    "    ('Update uses', 'Q(s\\', a\\')', 'max Q(s\\', a\\')'),\n",
    "    ('Learns', 'Q^π (current policy)', 'Q* (optimal policy)'),\n",
    "    ('Safety', 'Safer ✓', 'Riskier'),\n",
    "    ('Optimal policy?', 'Not always', 'Yes ✓'),\n",
    "    ('Best for', 'Costly mistakes', 'Cheap simulation'),\n",
    "]\n",
    "\n",
    "for i, (prop, sarsa_val, ql_val) in enumerate(rows):\n",
    "    y = 7.5 - i * 1.0\n",
    "    ax.text(1.5, y, prop, ha='center', fontsize=11)\n",
    "    \n",
    "    sarsa_color = '#388e3c' if '✓' in sarsa_val else 'black'\n",
    "    ql_color = '#388e3c' if '✓' in ql_val else 'black'\n",
    "    \n",
    "    ax.text(5, y, sarsa_val, ha='center', fontsize=11, color=sarsa_color)\n",
    "    ax.text(9, y, ql_val, ha='center', fontsize=11, color=ql_color)\n",
    "\n",
    "# Summary boxes\n",
    "sarsa_box = FancyBboxPatch((3, 0.3), 3, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=2)\n",
    "ax.add_patch(sarsa_box)\n",
    "ax.text(4.5, 1.1, 'SARSA', ha='center', fontsize=11, fontweight='bold', color='#388e3c')\n",
    "ax.text(4.5, 0.6, 'Safe & Practical', ha='center', fontsize=10)\n",
    "\n",
    "ql_box = FancyBboxPatch((7, 0.3), 3, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#ffcdd2', edgecolor='#d32f2f', linewidth=2)\n",
    "ax.add_patch(ql_box)\n",
    "ax.text(8.5, 1.1, 'Q-Learning', ha='center', fontsize=11, fontweight='bold', color='#d32f2f')\n",
    "ax.text(8.5, 0.6, 'Optimal & Bold', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### The SARSA Update\n",
    "\n",
    "```\n",
    "Q(S, A) ← Q(S, A) + α × (R + γ×Q(S', A') - Q(S, A))\n",
    "                              └── actual next action!\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **On-Policy** | Learns about the policy it's following |\n",
    "| **S-A-R-S'-A'** | Uses all 5 elements: State, Action, Reward, next State, next Action |\n",
    "| **Safety** | Accounts for exploration in value estimates |\n",
    "\n",
    "### SARSA vs Q-Learning\n",
    "\n",
    "| | SARSA | Q-Learning |\n",
    "|-|-------|------------|\n",
    "| Type | On-policy | Off-policy |\n",
    "| Update | Q(s', a') | max Q(s', a') |\n",
    "| Learns | Q^π | Q* |\n",
    "| Safety | Higher | Lower |\n",
    "| Use when | Mistakes costly | Simulation OK |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What does SARSA stand for and why?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "SARSA stands for State-Action-Reward-State'-Action'. It's named after the quintuple (S, A, R, S', A') that it uses for each update. The key is that it needs the next action A' before making the update, unlike Q-learning which uses max.\n",
    "</details>\n",
    "\n",
    "**2. What's the difference between on-policy and off-policy learning?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "On-policy (SARSA): Learns about the SAME policy it uses for acting. The behavior policy = target policy.\n",
    "\n",
    "Off-policy (Q-Learning): Learns about a DIFFERENT policy than it follows. Behavior policy (ε-greedy) ≠ Target policy (greedy).\n",
    "</details>\n",
    "\n",
    "**3. Why does SARSA take the safer path in cliff walking?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "SARSA's Q-values include the actual risk of exploration. When near the cliff, SARSA knows that ε% of the time it will randomly take a bad action and fall. So being near the cliff has lower value. SARSA learns to stay away!\n",
    "\n",
    "Q-learning assumes perfect (greedy) future behavior, so it thinks the edge is fine.\n",
    "</details>\n",
    "\n",
    "**4. When should you use SARSA instead of Q-learning?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Use SARSA when:\n",
    "- Mistakes are costly (real robots, trading, medical systems)\n",
    "- You'll continue exploring after learning\n",
    "- Safety during learning matters more than finding the absolute optimal policy\n",
    "\n",
    "Use Q-learning when:\n",
    "- You're in simulation where mistakes are free\n",
    "- You'll use a greedy policy after training\n",
    "- You want the truly optimal policy\n",
    "</details>\n",
    "\n",
    "**5. What happens to SARSA's path as ε increases?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "As ε increases, SARSA becomes MORE cautious. Higher ε means more random actions, which means higher risk of accidentally falling off the cliff. SARSA's Q-values reflect this higher risk, so it learns to stay even further from dangerous areas.\n",
    "\n",
    "Q-learning's learned policy doesn't change much with ε because it always assumes greedy future behavior.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Excellent work! You now understand SARSA and the crucial distinction between on-policy and off-policy learning.\n",
    "\n",
    "In the final notebook of this section, we'll compare ALL the algorithms we've learned!\n",
    "\n",
    "**Continue to:** [Notebook 5: Comparing Algorithms](05_comparing_algorithms.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*SARSA: \"I know I'm not perfect, so I'll plan accordingly.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
