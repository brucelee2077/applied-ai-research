{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning: Learning the Best Actions\n",
    "\n",
    "Welcome to Q-Learning - one of the most famous and influential RL algorithms! This is the foundation for Deep Q-Networks (DQN) which famously learned to play Atari games.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- What Q-values are and why they're useful (with a restaurant analogy!)\n",
    "- The Q-Learning update rule, step by step\n",
    "- Off-policy learning (what makes Q-learning special)\n",
    "- How to implement Q-Learning from scratch\n",
    "- Watch the agent learn in real-time with visualizations!\n",
    "\n",
    "**Prerequisites:** Notebook 2 (TD Learning)\n",
    "\n",
    "**Time:** ~35 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The Restaurant Guide Analogy\n",
    "\n",
    "Imagine you're new to a city and want to find the best restaurants:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │         Q-LEARNING: THE RESTAURANT GUIDE               │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  You have a NOTEBOOK (Q-table) with ratings:           │\n",
    "    │                                                         │\n",
    "    │  ┌─────────────────────────────────────────────┐       │\n",
    "    │  │ Location       │ Pizza │ Sushi │ Tacos │   │       │\n",
    "    │  ├─────────────────────────────────────────────┤       │\n",
    "    │  │ Downtown       │  8.5  │  7.2  │  9.1  │   │       │\n",
    "    │  │ Uptown         │  6.3  │  9.0  │  7.8  │   │       │\n",
    "    │  │ Suburb         │  7.1  │  5.5  │  8.2  │   │       │\n",
    "    │  └─────────────────────────────────────────────┘       │\n",
    "    │                                                         │\n",
    "    │  Q(state, action) = \"Expected enjoyment if I eat       │\n",
    "    │                      this food in this location\"       │\n",
    "    │                                                         │\n",
    "    │  HOW YOU LEARN:                                        │\n",
    "    │    1. Try a restaurant                                 │\n",
    "    │    2. Rate your experience                             │\n",
    "    │    3. Update your notebook                             │\n",
    "    │    4. Repeat!                                          │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "This is exactly what Q-learning does:\n",
    "- **Q-table:** A lookup table of (state, action) → expected value\n",
    "- **Learning:** Update estimates based on actual experiences\n",
    "- **Goal:** Find the best action for every state!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What is a Q-Value?\n",
    "\n",
    "**Q(s, a)** = \"How good is it to take action **a** in state **s**?\"\n",
    "\n",
    "More precisely:\n",
    "> **Q(s, a) = Expected total future reward if I take action a in state s, and then act optimally forever after.**\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │                   Q-VALUE INTUITION                     │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  State: You're at position (1, 1) in a grid            │\n",
    "    │                                                         │\n",
    "    │           Q(state, UP)    = 5.2                        │\n",
    "    │           Q(state, RIGHT) = 8.7  ← BEST!               │\n",
    "    │           Q(state, DOWN)  = 3.1                        │\n",
    "    │           Q(state, LEFT)  = 4.5                        │\n",
    "    │                                                         │\n",
    "    │  Interpretation:                                       │\n",
    "    │    \"If I go RIGHT, I expect to eventually get 8.7      │\n",
    "    │     total reward. That's the best option!\"             │\n",
    "    │                                                         │\n",
    "    │  Optimal Policy:                                       │\n",
    "    │    π*(s) = argmax_a Q(s, a) = RIGHT                    │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Visualize Q-values for a single state\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Central state\n",
    "center = (5, 5)\n",
    "state_circle = plt.Circle(center, 1.5, facecolor='#bbdefb', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(state_circle)\n",
    "ax.text(5, 5, 'State\\n(1,1)', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Q-values for each action\n",
    "actions = [\n",
    "    {'name': 'UP', 'pos': (5, 8), 'q': 5.2, 'color': '#4caf50'},\n",
    "    {'name': 'RIGHT', 'pos': (8, 5), 'q': 8.7, 'color': '#ff9800'},  # Best!\n",
    "    {'name': 'DOWN', 'pos': (5, 2), 'q': 3.1, 'color': '#f44336'},\n",
    "    {'name': 'LEFT', 'pos': (2, 5), 'q': 4.5, 'color': '#9c27b0'},\n",
    "]\n",
    "\n",
    "for act in actions:\n",
    "    x, y = act['pos']\n",
    "    # Draw action box\n",
    "    size = 0.3 + act['q'] / 10  # Size proportional to Q-value\n",
    "    color = '#ff9800' if act['q'] == 8.7 else '#e0e0e0'\n",
    "    box = FancyBboxPatch((x-0.8, y-0.5), 1.6, 1, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y, f'{act[\"name\"]}\\nQ={act[\"q\"]}', ha='center', va='center', \n",
    "            fontsize=11, fontweight='bold' if act['q'] == 8.7 else 'normal')\n",
    "    \n",
    "    # Draw arrow from state to action\n",
    "    dx, dy = (x - 5) * 0.4, (y - 5) * 0.4\n",
    "    ax.annotate('', xy=(5 + dx * 2.5, 5 + dy * 2.5), xytext=(5 + dx, 5 + dy),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color=act['color']))\n",
    "\n",
    "# Highlight best action\n",
    "ax.text(8, 6.5, '← BEST ACTION!', fontsize=12, color='#ff9800', fontweight='bold')\n",
    "\n",
    "ax.set_title('Q-Values: Expected Reward for Each Action', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.text(5, 0.5, 'Policy: π*(s) = argmax Q(s, a) = RIGHT', ha='center', fontsize=12, \n",
    "        style='italic', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nQ-values tell us the expected TOTAL FUTURE reward.\")\n",
    "print(\"The best policy simply picks the action with highest Q-value!\")\n",
    "print(\"\\n  π*(s) = argmax_a Q(s, a)\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Q-Learning Update Rule\n",
    "\n",
    "Q-learning updates Q-values using this rule:\n",
    "\n",
    "```\n",
    "Q(s, a) ← Q(s, a) + α × [r + γ × max Q(s', a') - Q(s, a)]\n",
    "           ───────       ─   ─   ────────────   ─────────\n",
    "           old value     │   │   best future   current estimate\n",
    "                         │   │   value\n",
    "                         │   └── discount\n",
    "                         └── immediate reward\n",
    "```\n",
    "\n",
    "Let's break this down:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │              THE Q-LEARNING UPDATE                      │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  TD Target = r + γ × max_a' Q(s', a')                  │\n",
    "    │              ─   ─────────────────────                  │\n",
    "    │              │   \"best possible future\"                 │\n",
    "    │              └── \"what I got now\"                       │\n",
    "    │                                                         │\n",
    "    │  TD Error = TD Target - Q(s, a)                        │\n",
    "    │           = \"how wrong was my prediction?\"             │\n",
    "    │                                                         │\n",
    "    │  New Q = Old Q + α × TD Error                          │\n",
    "    │        = \"nudge my estimate toward the truth\"          │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step demonstration of ONE Q-learning update\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP-BY-STEP Q-LEARNING UPDATE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Set up the scenario\n",
    "state = (1, 1)\n",
    "action = 1  # RIGHT\n",
    "next_state = (1, 2)\n",
    "reward = -1  # Step cost\n",
    "\n",
    "# Current Q-values (before update)\n",
    "Q_current = 3.0\n",
    "Q_next_state = [2.5, 5.0, 1.5, 2.0]  # Q-values for next state (UP, RIGHT, DOWN, LEFT)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "print(f\"\\nScenario:\")\n",
    "print(f\"  State:       {state}\")\n",
    "print(f\"  Action:      RIGHT\")\n",
    "print(f\"  Next State:  {next_state}\")\n",
    "print(f\"  Reward:      {reward}\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  α (learning rate): {alpha}\")\n",
    "print(f\"  γ (discount):      {gamma}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"THE UPDATE CALCULATION:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Step 1: Find max Q for next state\n",
    "max_Q_next = max(Q_next_state)\n",
    "print(f\"\\n1. Find BEST action in next state:\")\n",
    "print(f\"   Q-values at {next_state}: {Q_next_state}\")\n",
    "print(f\"   max Q(s', a') = {max_Q_next}\")\n",
    "\n",
    "# Step 2: Calculate TD Target\n",
    "td_target = reward + gamma * max_Q_next\n",
    "print(f\"\\n2. Calculate TD TARGET:\")\n",
    "print(f\"   TD Target = r + γ × max Q(s', a')\")\n",
    "print(f\"             = {reward} + {gamma} × {max_Q_next}\")\n",
    "print(f\"             = {td_target}\")\n",
    "\n",
    "# Step 3: Calculate TD Error\n",
    "td_error = td_target - Q_current\n",
    "print(f\"\\n3. Calculate TD ERROR:\")\n",
    "print(f\"   TD Error = TD Target - Q(s, a)\")\n",
    "print(f\"            = {td_target} - {Q_current}\")\n",
    "print(f\"            = {td_error}\")\n",
    "if td_error > 0:\n",
    "    print(f\"   Interpretation: We UNDERESTIMATED! Should increase Q.\")\n",
    "else:\n",
    "    print(f\"   Interpretation: We OVERESTIMATED! Should decrease Q.\")\n",
    "\n",
    "# Step 4: Update Q-value\n",
    "Q_new = Q_current + alpha * td_error\n",
    "print(f\"\\n4. UPDATE Q-value:\")\n",
    "print(f\"   Q_new = Q_old + α × TD Error\")\n",
    "print(f\"         = {Q_current} + {alpha} × {td_error}\")\n",
    "print(f\"         = {Q_new}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"RESULT: Q({state}, RIGHT) updated from {Q_current} → {Q_new}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Off-Policy Learning: The Secret Sauce\n",
    "\n",
    "What makes Q-learning special is that it's **off-policy**:\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │            OFF-POLICY vs ON-POLICY                      │\n",
    "    ├────────────────────────────────────────────────────────┤\n",
    "    │                                                         │\n",
    "    │  ON-POLICY (e.g., SARSA):                              │\n",
    "    │    \"Learn about the policy I'm currently following\"    │\n",
    "    │    Update uses: Q(s', action_I_actually_took)          │\n",
    "    │                                                         │\n",
    "    │  OFF-POLICY (Q-Learning):                              │\n",
    "    │    \"Learn the OPTIMAL policy, regardless of what I do\" │\n",
    "    │    Update uses: max Q(s', a')  (best possible action)  │\n",
    "    │                                                         │\n",
    "    │  Why this matters:                                     │\n",
    "    │    • I can EXPLORE (try random actions)                │\n",
    "    │    • But still LEARN the optimal policy                │\n",
    "    │    • Separates exploration from optimization!          │\n",
    "    │                                                         │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Analogy:** You're learning the best routes through a city. You can take random detours and explore (behavior policy), but your map still records the actual shortest paths (target policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Let's Build a Grid World Environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    A simple 4x4 Grid World for demonstrating Q-Learning.\n",
    "    \n",
    "    Layout:\n",
    "        ┌───┬───┬───┬───┐\n",
    "        │ S │   │   │   │   S = Start (0,0)\n",
    "        ├───┼───┼───┼───┤\n",
    "        │   │   │   │   │\n",
    "        ├───┼───┼───┼───┤\n",
    "        │   │   │   │   │\n",
    "        ├───┼───┼───┼───┤\n",
    "        │   │   │   │ G │   G = Goal (3,3)\n",
    "        └───┴───┴───┴───┘\n",
    "    \n",
    "    Actions: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "    Rewards: -1 per step, +10 at goal\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.goal = (size-1, size-1)\n",
    "        self.action_names = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
    "        self.action_symbols = ['↑', '→', '↓', '←']\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset agent to start position.\"\"\"\n",
    "        self.pos = (0, 0)\n",
    "        return self.pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action.\n",
    "        \n",
    "        Returns: (next_state, reward, done)\n",
    "        \"\"\"\n",
    "        row, col = self.pos\n",
    "        \n",
    "        if action == 0:    # UP\n",
    "            row = max(0, row - 1)\n",
    "        elif action == 1:  # RIGHT\n",
    "            col = min(self.size - 1, col + 1)\n",
    "        elif action == 2:  # DOWN\n",
    "            row = min(self.size - 1, row + 1)\n",
    "        elif action == 3:  # LEFT\n",
    "            col = max(0, col - 1)\n",
    "        \n",
    "        self.pos = (row, col)\n",
    "        done = self.pos == self.goal\n",
    "        reward = 10 if done else -1\n",
    "        \n",
    "        return self.pos, reward, done\n",
    "    \n",
    "    def render(self, Q=None, path=None):\n",
    "        \"\"\"Visualize the grid with optional Q-values and path.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "        # Draw grid\n",
    "        for row in range(self.size):\n",
    "            for col in range(self.size):\n",
    "                # Color cells\n",
    "                if (row, col) == self.goal:\n",
    "                    color = '#c8e6c9'\n",
    "                elif (row, col) == (0, 0):\n",
    "                    color = '#bbdefb'\n",
    "                else:\n",
    "                    color = 'white'\n",
    "                \n",
    "                rect = Rectangle((col, self.size - 1 - row), 1, 1, \n",
    "                                   facecolor=color, edgecolor='black', linewidth=2)\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add Q-value arrows if provided\n",
    "                if Q is not None and (row, col) in Q:\n",
    "                    q_vals = Q[(row, col)]\n",
    "                    best_action = np.argmax(q_vals)\n",
    "                    cx, cy = col + 0.5, self.size - 1 - row + 0.5\n",
    "                    \n",
    "                    # Draw best action arrow\n",
    "                    dx, dy = [(0, 0.3), (0.3, 0), (0, -0.3), (-0.3, 0)][best_action]\n",
    "                    ax.arrow(cx - dx/2, cy - dy/2, dx, dy, head_width=0.15, \n",
    "                            head_length=0.1, fc='#f44336', ec='#f44336', linewidth=2)\n",
    "        \n",
    "        # Draw path if provided\n",
    "        if path is not None:\n",
    "            path_x = [col + 0.5 for (row, col) in path]\n",
    "            path_y = [self.size - 1 - row + 0.5 for (row, col) in path]\n",
    "            ax.plot(path_x, path_y, 'b-o', linewidth=3, markersize=10, alpha=0.5)\n",
    "        \n",
    "        # Labels\n",
    "        ax.text(0.5, self.size - 0.5, 'START', ha='center', va='center', \n",
    "                fontsize=10, fontweight='bold', color='#1976d2')\n",
    "        ax.text(self.size - 0.5, 0.5, 'GOAL', ha='center', va='center', \n",
    "                fontsize=10, fontweight='bold', color='#388e3c')\n",
    "        \n",
    "        ax.set_xlim(0, self.size)\n",
    "        ax.set_ylim(0, self.size)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "        ax.set_title('Grid World', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and display environment\n",
    "env = GridWorld(size=4)\n",
    "print(\"Grid World Environment Created!\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Grid size: {env.size}x{env.size}\")\n",
    "print(f\"Start: (0, 0)\")\n",
    "print(f\"Goal: {env.goal}\")\n",
    "print(f\"Actions: {env.action_names}\")\n",
    "print(f\"Rewards: -1 per step, +10 at goal\")\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Complete Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, n_episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1, verbose=False):\n",
    "    \"\"\"\n",
    "    Q-Learning Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment\n",
    "        n_episodes: Number of episodes to train\n",
    "        alpha: Learning rate (how much to update Q-values)\n",
    "        gamma: Discount factor (how much to value future rewards)\n",
    "        epsilon: Exploration rate (probability of random action)\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        Q: Learned Q-values\n",
    "        rewards_history: Total reward per episode\n",
    "    \"\"\"\n",
    "    # Initialize Q-table with zeros\n",
    "    # Q[state] = [Q(s,UP), Q(s,RIGHT), Q(s,DOWN), Q(s,LEFT)]\n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    \n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(100):  # Max steps per episode\n",
    "            # ==========================================\n",
    "            # STEP 1: Choose action (epsilon-greedy)\n",
    "            # ==========================================\n",
    "            if np.random.random() < epsilon:\n",
    "                # EXPLORE: Random action\n",
    "                action = np.random.randint(0, 4)\n",
    "            else:\n",
    "                # EXPLOIT: Best known action\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            # ==========================================\n",
    "            # STEP 2: Take action, observe result\n",
    "            # ==========================================\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # ==========================================\n",
    "            # STEP 3: Q-Learning update (OFF-POLICY!)\n",
    "            # ==========================================\n",
    "            # Uses max Q(s', a') regardless of what action we'll actually take\n",
    "            best_next_q = np.max(Q[next_state])\n",
    "            td_target = reward + gamma * best_next_q\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_error\n",
    "            \n",
    "            # ==========================================\n",
    "            # STEP 4: Move to next state\n",
    "            # ==========================================\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        # Progress printing\n",
    "        if verbose and (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-100:])\n",
    "            print(f\"Episode {episode+1:4d} | Avg Reward (last 100): {avg_reward:.2f}\")\n",
    "    \n",
    "    return dict(Q), rewards_history\n",
    "\n",
    "# Display the algorithm structure\n",
    "print(\"Q-LEARNING ALGORITHM\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "For each episode:\n",
    "    state = start\n",
    "    \n",
    "    While not done:\n",
    "        1. CHOOSE ACTION (epsilon-greedy)\n",
    "           - With prob epsilon: random action (explore)\n",
    "           - With prob 1-epsilon: best action (exploit)\n",
    "        \n",
    "        2. TAKE ACTION\n",
    "           - Get reward and next_state\n",
    "        \n",
    "        3. UPDATE Q-VALUE (the key step!)\n",
    "           - TD Target = r + γ × max Q(s', a')\n",
    "           - TD Error = TD Target - Q(s, a)\n",
    "           - Q(s, a) += α × TD Error\n",
    "        \n",
    "        4. MOVE TO NEXT STATE\n",
    "           - state = next_state\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training the Agent - Watch It Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Q-learning agent\n",
    "env = GridWorld(size=4)\n",
    "\n",
    "print(\"Training Q-Learning Agent...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "Q, rewards_history = q_learning(\n",
    "    env,\n",
    "    n_episodes=2000,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    epsilon=0.1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Raw rewards\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rewards_history, alpha=0.3, color='blue')\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward', fontsize=12)\n",
    "ax1.set_title('Learning Curve (Raw)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Smoothed rewards\n",
    "ax2 = axes[1]\n",
    "window = 50\n",
    "smoothed = np.convolve(rewards_history, np.ones(window)/window, mode='valid')\n",
    "ax2.plot(smoothed, color='blue', linewidth=2)\n",
    "ax2.axhline(y=4, color='green', linestyle='--', linewidth=2, label='Optimal (≈4)')\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Total Reward (Smoothed)', fontsize=12)\n",
    "ax2.set_title(f'Learning Curve (Smoothed, window={window})', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal average reward (last 100 episodes): {np.mean(rewards_history[-100:]):.2f}\")\n",
    "print(f\"Optimal reward: 4 (6 steps × -1 + 10 for goal = 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualizing the Learned Q-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_table(Q, env):\n",
    "    \"\"\"Create a detailed visualization of the Q-table.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Left: Grid with best actions (arrows)\n",
    "    ax1 = axes[0]\n",
    "    ax1.set_xlim(0, env.size)\n",
    "    ax1.set_ylim(0, env.size)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Learned Policy (Best Actions)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for row in range(env.size):\n",
    "        for col in range(env.size):\n",
    "            # Draw cell\n",
    "            if (row, col) == env.goal:\n",
    "                color = '#c8e6c9'\n",
    "            elif (row, col) == (0, 0):\n",
    "                color = '#bbdefb'\n",
    "            else:\n",
    "                color = 'white'\n",
    "            \n",
    "            y = env.size - 1 - row\n",
    "            rect = Rectangle((col, y), 1, 1, facecolor=color, \n",
    "                               edgecolor='black', linewidth=2)\n",
    "            ax1.add_patch(rect)\n",
    "            \n",
    "            # Draw arrow for best action\n",
    "            if (row, col) in Q and (row, col) != env.goal:\n",
    "                q_vals = Q[(row, col)]\n",
    "                best_action = np.argmax(q_vals)\n",
    "                cx, cy = col + 0.5, y + 0.5\n",
    "                \n",
    "                # Arrow directions\n",
    "                arrows = [(0, 0.3), (0.3, 0), (0, -0.3), (-0.3, 0)]\n",
    "                dx, dy = arrows[best_action]\n",
    "                \n",
    "                ax1.arrow(cx - dx/2, cy - dy/2, dx, dy, \n",
    "                         head_width=0.15, head_length=0.1, \n",
    "                         fc='#f44336', ec='#f44336', linewidth=2)\n",
    "    \n",
    "    ax1.text(0.5, env.size - 0.2, 'START', ha='center', fontsize=9, color='#1976d2')\n",
    "    ax1.text(env.size - 0.5, 0.2, 'GOAL', ha='center', fontsize=9, color='#388e3c')\n",
    "    \n",
    "    # Right: Q-value heatmap for best Q-value in each cell\n",
    "    ax2 = axes[1]\n",
    "    q_max_grid = np.zeros((env.size, env.size))\n",
    "    \n",
    "    for row in range(env.size):\n",
    "        for col in range(env.size):\n",
    "            if (row, col) in Q:\n",
    "                q_max_grid[row, col] = np.max(Q[(row, col)])\n",
    "    \n",
    "    im = ax2.imshow(q_max_grid, cmap='RdYlGn', origin='upper')\n",
    "    ax2.set_title('Maximum Q-Value per State', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for row in range(env.size):\n",
    "        for col in range(env.size):\n",
    "            value = q_max_grid[row, col]\n",
    "            color = 'white' if value < np.mean(q_max_grid) else 'black'\n",
    "            ax2.text(col, row, f'{value:.1f}', ha='center', va='center', \n",
    "                    fontsize=12, color=color, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax2, label='Q-value')\n",
    "    ax2.set_xticks(range(env.size))\n",
    "    ax2.set_yticks(range(env.size))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the learned Q-values\n",
    "visualize_q_table(Q, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Q-table in detail\n",
    "print(\"DETAILED Q-TABLE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'State':<10} {'UP':>8} {'RIGHT':>8} {'DOWN':>8} {'LEFT':>8} {'Best':>8}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "action_symbols = ['↑', '→', '↓', '←']\n",
    "\n",
    "for row in range(env.size):\n",
    "    for col in range(env.size):\n",
    "        state = (row, col)\n",
    "        if state in Q:\n",
    "            q_vals = Q[state]\n",
    "            best = np.argmax(q_vals)\n",
    "            print(f\"{str(state):<10} {q_vals[0]:>8.2f} {q_vals[1]:>8.2f} \"\n",
    "                  f\"{q_vals[2]:>8.2f} {q_vals[3]:>8.2f} {action_symbols[best]:>8}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Watch the Trained Agent Play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, Q, render_steps=True):\n",
    "    \"\"\"Run a single episode using the learned Q-values (greedy policy).\"\"\"\n",
    "    state = env.reset()\n",
    "    path = [state]\n",
    "    total_reward = 0\n",
    "    actions_taken = []\n",
    "    \n",
    "    for step in range(20):  # Max 20 steps\n",
    "        # Choose best action (greedy)\n",
    "        if state in Q:\n",
    "            action = np.argmax(Q[state])\n",
    "        else:\n",
    "            action = np.random.randint(0, 4)\n",
    "        \n",
    "        actions_taken.append(env.action_names[action])\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        path.append(next_state)\n",
    "        \n",
    "        if render_steps:\n",
    "            print(f\"Step {step+1}: {state} → {env.action_names[action]} → {next_state} \"\n",
    "                  f\"(reward: {reward:+d})\")\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return path, total_reward, actions_taken\n",
    "\n",
    "# Run and visualize an episode\n",
    "print(\"WATCHING THE TRAINED AGENT\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nStep-by-step execution:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "env = GridWorld(size=4)\n",
    "path, total_reward, actions = run_episode(env, Q, render_steps=True)\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"\\nTotal steps: {len(path) - 1}\")\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "print(f\"Actions: {' → '.join(actions)}\")\n",
    "print(f\"\\nOptimal path length: 6 steps\")\n",
    "print(f\"Optimal reward: 4 (6 × -1 + 10 = 4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the path taken\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Draw grid\n",
    "for row in range(env.size):\n",
    "    for col in range(env.size):\n",
    "        if (row, col) == env.goal:\n",
    "            color = '#c8e6c9'\n",
    "        elif (row, col) == (0, 0):\n",
    "            color = '#bbdefb'\n",
    "        else:\n",
    "            color = 'white'\n",
    "        \n",
    "        y = env.size - 1 - row\n",
    "        rect = Rectangle((col, y), 1, 1, facecolor=color, \n",
    "                           edgecolor='black', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "# Draw path\n",
    "path_x = [col + 0.5 for (row, col) in path]\n",
    "path_y = [env.size - 1 - row + 0.5 for (row, col) in path]\n",
    "\n",
    "ax.plot(path_x, path_y, 'b-', linewidth=4, alpha=0.5)\n",
    "\n",
    "for i, ((row, col), action) in enumerate(zip(path[:-1], actions)):\n",
    "    x, y = col + 0.5, env.size - 1 - row + 0.5\n",
    "    ax.scatter(x, y, s=200, c='blue', zorder=5)\n",
    "    ax.text(x, y, str(i+1), ha='center', va='center', fontsize=10, \n",
    "            color='white', fontweight='bold')\n",
    "\n",
    "# Mark start and end\n",
    "ax.scatter(path_x[0], path_y[0], s=300, c='green', marker='s', zorder=6, label='Start')\n",
    "ax.scatter(path_x[-1], path_y[-1], s=300, c='red', marker='*', zorder=6, label='Goal')\n",
    "\n",
    "ax.set_xlim(0, env.size)\n",
    "ax.set_ylim(0, env.size)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title(f'Agent\\'s Path ({len(path)-1} steps, reward={total_reward})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploring Hyperparameters\n",
    "\n",
    "Let's see how different hyperparameters affect learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Learning rate comparison\n",
    "alphas = [0.01, 0.1, 0.5]\n",
    "ax = axes[0]\n",
    "\n",
    "for alpha in alphas:\n",
    "    env = GridWorld()\n",
    "    _, rewards = q_learning(env, n_episodes=1000, alpha=alpha, gamma=0.99, epsilon=0.1)\n",
    "    smoothed = np.convolve(rewards, np.ones(50)/50, mode='valid')\n",
    "    ax.plot(smoothed, label=f'α={alpha}')\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Learning Rate (α) Comparison', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Discount factor comparison\n",
    "gammas = [0.5, 0.9, 0.99]\n",
    "ax = axes[1]\n",
    "\n",
    "for gamma in gammas:\n",
    "    env = GridWorld()\n",
    "    _, rewards = q_learning(env, n_episodes=1000, alpha=0.1, gamma=gamma, epsilon=0.1)\n",
    "    smoothed = np.convolve(rewards, np.ones(50)/50, mode='valid')\n",
    "    ax.plot(smoothed, label=f'γ={gamma}')\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Discount Factor (γ) Comparison', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Exploration rate comparison\n",
    "epsilons = [0.01, 0.1, 0.3]\n",
    "ax = axes[2]\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    env = GridWorld()\n",
    "    _, rewards = q_learning(env, n_episodes=1000, alpha=0.1, gamma=0.99, epsilon=epsilon)\n",
    "    smoothed = np.convolve(rewards, np.ones(50)/50, mode='valid')\n",
    "    ax.plot(smoothed, label=f'ε={epsilon}')\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Exploration Rate (ε) Comparison', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHYPERPARAMETER INSIGHTS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nα (Learning Rate):\")\n",
    "print(\"  - Too small (0.01): Slow learning\")\n",
    "print(\"  - Too large (0.5): Unstable, may oscillate\")\n",
    "print(\"  - Just right (0.1): Good balance\")\n",
    "print(\"\\nγ (Discount Factor):\")\n",
    "print(\"  - Low (0.5): Short-sighted, may not plan well\")\n",
    "print(\"  - High (0.99): Considers future, better planning\")\n",
    "print(\"\\nε (Exploration Rate):\")\n",
    "print(\"  - Too low (0.01): May get stuck in suboptimal policy\")\n",
    "print(\"  - Too high (0.3): Too random, poor performance\")\n",
    "print(\"  - Just right (0.1): Good exploration-exploitation balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### What is Q-Learning?\n",
    "\n",
    "Q-learning learns **action values Q(s, a)** - how good each action is in each state.\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "```\n",
    "Q(s, a) ← Q(s, a) + α × [r + γ × max Q(s', a') - Q(s, a)]\n",
    "```\n",
    "\n",
    "| Component | Meaning |\n",
    "|-----------|----------|\n",
    "| α | Learning rate: how fast to update |\n",
    "| r | Immediate reward received |\n",
    "| γ | Discount: how much to value future |\n",
    "| max Q(s', a') | Best possible future value |\n",
    "\n",
    "### Off-Policy Learning\n",
    "\n",
    "- The agent can **explore** (random actions)\n",
    "- But still **learns the optimal policy** (uses max Q)\n",
    "- Separates behavior from learning!\n",
    "\n",
    "### Epsilon-Greedy\n",
    "\n",
    "- With probability ε: take random action (explore)\n",
    "- With probability 1-ε: take best action (exploit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What does Q(s, a) represent?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Q(s, a) represents the expected total future reward if we take action a in state s, and then follow the optimal policy thereafter. It tells us how \"good\" an action is in a given state.\n",
    "</details>\n",
    "\n",
    "**2. Why does Q-learning use `max Q(s', a')` in its update?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "It uses max because Q-learning is off-policy - it always estimates the value assuming we'll take the BEST action in the future, regardless of what action we actually take for exploration. This allows us to learn the optimal policy while following an exploratory policy.\n",
    "</details>\n",
    "\n",
    "**3. What is the TD error in Q-learning?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "TD Error = (r + γ × max Q(s', a')) - Q(s, a)\n",
    "\n",
    "It's the difference between what we predicted [Q(s,a)] and what we actually observed [r + γ × max Q(s',a')]. If positive, we underestimated; if negative, we overestimated.\n",
    "</details>\n",
    "\n",
    "**4. Why do we need epsilon-greedy exploration?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Without exploration, the agent might get stuck in a suboptimal policy because it never tries actions that might be better. Epsilon-greedy ensures the agent sometimes takes random actions to discover potentially better strategies, while mostly exploiting what it has learned.\n",
    "</details>\n",
    "\n",
    "**5. What happens if γ = 0?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "If γ = 0, the agent becomes completely myopic (short-sighted). It only cares about immediate rewards and ignores all future rewards. The update becomes Q(s,a) ← Q(s,a) + α(r - Q(s,a)), which only considers the immediate reward r.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "Excellent work! You've learned one of the most important algorithms in RL!\n",
    "\n",
    "In the next notebook, we'll learn **SARSA** - the on-policy cousin of Q-learning:\n",
    "- How SARSA differs from Q-learning\n",
    "- When to use each algorithm\n",
    "- The cliff walking problem\n",
    "\n",
    "**Continue to:** [Notebook 4: SARSA](04_sarsa.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Q-learning is the foundation for Deep Q-Networks (DQN), which achieved superhuman performance on Atari games. You're now ready to understand how that works!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
