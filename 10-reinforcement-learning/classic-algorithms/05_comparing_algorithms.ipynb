{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing RL Algorithms: Choosing the Right Tool\n",
    "\n",
    "Welcome to the final notebook of the Classic Algorithms section! Let's put everything together and understand when to use each algorithm.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How Monte Carlo, TD, Q-Learning, and SARSA compare\n",
    "- Learning speed, variance, and convergence differences\n",
    "- When each algorithm shines (with a toolbox analogy!)\n",
    "- How to choose the right algorithm for your problem\n",
    "- Empirical analysis on multiple environments\n",
    "\n",
    "**Prerequisites:** Notebooks 1-4 (MC, TD, Q-Learning, SARSA)\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Big Picture: The RL Toolbox\n",
    "\n",
    "Think of these algorithms as tools in a toolbox - each is best for certain situations:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                 THE RL ALGORITHM TOOLBOX                       â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  ğŸ”¨ MONTE CARLO = The Careful Craftsman                        â”‚\n",
    "    â”‚     \"I wait until the job is COMPLETELY done before I judge.\"  â”‚\n",
    "    â”‚     â€¢ Waits for full episode                                  â”‚\n",
    "    â”‚     â€¢ Unbiased but slow                                       â”‚\n",
    "    â”‚     â€¢ High variance                                           â”‚\n",
    "    â”‚     Best for: Card games, episodic tasks with clear endings   â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  âš¡ TD LEARNING = The Quick Learner                            â”‚\n",
    "    â”‚     \"I update my estimates as I go - no need to wait!\"        â”‚\n",
    "    â”‚     â€¢ Updates every step                                      â”‚\n",
    "    â”‚     â€¢ Some bias but faster                                    â”‚\n",
    "    â”‚     â€¢ Lower variance                                          â”‚\n",
    "    â”‚     Best for: Continuous tasks, faster learning               â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  ğŸ¯ Q-LEARNING = The Optimist                                 â”‚\n",
    "    â”‚     \"I assume I'll always take the BEST action in the future.\"â”‚\n",
    "    â”‚     â€¢ Off-policy: learns optimal regardless of behavior       â”‚\n",
    "    â”‚     â€¢ Can be risky during exploration                         â”‚\n",
    "    â”‚     Best for: Games, simulations where mistakes are cheap     â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â”‚  ğŸ›¡ï¸ SARSA = The Realist                                       â”‚\n",
    "    â”‚     \"I account for my OWN imperfect behavior.\"                â”‚\n",
    "    â”‚     â€¢ On-policy: learns value of actual behavior              â”‚\n",
    "    â”‚     â€¢ Safer during exploration                                â”‚\n",
    "    â”‚     Best for: Robotics, trading, safety-critical systems      â”‚\n",
    "    â”‚                                                                â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, Circle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Visualize the algorithm characteristics\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 9.5, 'The Four Classic RL Algorithms', ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "algorithms = [\n",
    "    {\n",
    "        'name': 'Monte Carlo',\n",
    "        'icon': 'ğŸ”¨',\n",
    "        'color': '#bbdefb',\n",
    "        'edge': '#1976d2',\n",
    "        'traits': ['Unbiased', 'High variance', 'Episode-based'],\n",
    "        'x': 1.5\n",
    "    },\n",
    "    {\n",
    "        'name': 'TD Learning',\n",
    "        'icon': 'âš¡',\n",
    "        'color': '#fff3e0',\n",
    "        'edge': '#f57c00',\n",
    "        'traits': ['Bootstraps', 'Low variance', 'Step-based'],\n",
    "        'x': 4.5\n",
    "    },\n",
    "    {\n",
    "        'name': 'Q-Learning',\n",
    "        'icon': 'ğŸ¯',\n",
    "        'color': '#c8e6c9',\n",
    "        'edge': '#388e3c',\n",
    "        'traits': ['Off-policy', 'Finds optimal', 'Bold'],\n",
    "        'x': 7.5\n",
    "    },\n",
    "    {\n",
    "        'name': 'SARSA',\n",
    "        'icon': 'ğŸ›¡ï¸',\n",
    "        'color': '#f3e5f5',\n",
    "        'edge': '#7b1fa2',\n",
    "        'traits': ['On-policy', 'Safe', 'Cautious'],\n",
    "        'x': 10.5\n",
    "    }\n",
    "]\n",
    "\n",
    "for algo in algorithms:\n",
    "    # Main box\n",
    "    box = FancyBboxPatch((algo['x'], 3), 2.5, 5, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=algo['color'], edgecolor=algo['edge'], linewidth=3)\n",
    "    ax.add_patch(box)\n",
    "    \n",
    "    # Icon and name\n",
    "    ax.text(algo['x'] + 1.25, 7.2, algo['icon'], ha='center', fontsize=20)\n",
    "    ax.text(algo['x'] + 1.25, 6.5, algo['name'], ha='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Traits\n",
    "    for i, trait in enumerate(algo['traits']):\n",
    "        ax.text(algo['x'] + 1.25, 5.5 - i*0.7, f'â€¢ {trait}', ha='center', fontsize=9)\n",
    "\n",
    "# Arrows showing relationships\n",
    "ax.annotate('', xy=(4.4, 5.5), xytext=(3.6, 5.5),\n",
    "            arrowprops=dict(arrowstyle='<->', lw=2, color='#666'))\n",
    "ax.text(4, 5.9, 'bias-variance\\ntradeoff', ha='center', fontsize=8, color='#666')\n",
    "\n",
    "ax.annotate('', xy=(10.4, 5.5), xytext=(9.6, 5.5),\n",
    "            arrowprops=dict(arrowstyle='<->', lw=2, color='#666'))\n",
    "ax.text(10, 5.9, 'safety-optimality\\ntradeoff', ha='center', fontsize=8, color='#666')\n",
    "\n",
    "# Summary at bottom\n",
    "ax.text(7, 1.5, 'All are model-free, tabular methods for learning value functions', \n",
    "        ha='center', fontsize=12, style='italic', color='#666')\n",
    "ax.text(7, 0.8, 'The right choice depends on your specific problem!', \n",
    "        ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setting Up the Environments\n",
    "\n",
    "We'll compare algorithms on two environments:\n",
    "1. **GridWorld** - Standard navigation task\n",
    "2. **CliffWalking** - Highlights safety differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Standard 4x4 Grid World.\n",
    "    Goal: Navigate from (0,0) to (3,3)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.goal = (3, 3)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pos = (0, 0)\n",
    "        return self.pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        row, col = self.pos\n",
    "        if action == 0: row = max(0, row - 1)      # UP\n",
    "        elif action == 1: col = min(3, col + 1)   # RIGHT\n",
    "        elif action == 2: row = min(3, row + 1)   # DOWN\n",
    "        elif action == 3: col = max(0, col - 1)   # LEFT\n",
    "        \n",
    "        self.pos = (row, col)\n",
    "        done = self.pos == self.goal\n",
    "        reward = 10 if done else -1\n",
    "        return self.pos, reward, done\n",
    "\n",
    "\n",
    "class CliffWalking:\n",
    "    \"\"\"\n",
    "    Cliff Walking environment.\n",
    "    Falling off the cliff gives -100 and resets to start.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.height = 4\n",
    "        self.width = 12\n",
    "        self.start = (3, 0)\n",
    "        self.goal = (3, 11)\n",
    "        self.cliff = [(3, i) for i in range(1, 11)]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pos = self.start\n",
    "        return self.pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        row, col = self.pos\n",
    "        if action == 0: row = max(0, row - 1)\n",
    "        elif action == 1: col = min(self.width - 1, col + 1)\n",
    "        elif action == 2: row = min(self.height - 1, row + 1)\n",
    "        elif action == 3: col = max(0, col - 1)\n",
    "        \n",
    "        self.pos = (row, col)\n",
    "        \n",
    "        if self.pos in self.cliff:\n",
    "            self.pos = self.start\n",
    "            return self.pos, -100, False\n",
    "        \n",
    "        if self.pos == self.goal:\n",
    "            return self.pos, -1, True\n",
    "        \n",
    "        return self.pos, -1, False\n",
    "\n",
    "\n",
    "print(\"Environments created!\")\n",
    "print(\"  â€¢ GridWorld: 4x4 grid, -1 per step, +10 at goal\")\n",
    "print(\"  â€¢ CliffWalking: 4x12 grid with cliff, -100 if fall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implementing All Algorithms\n",
    "\n",
    "Let's implement all four algorithms with consistent interfaces for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_control(env, n_episodes, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control with Îµ-greedy exploration.\n",
    "    \n",
    "    KEY: Updates only at END of each episode.\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    returns = defaultdict(list)\n",
    "    rewards_history = []\n",
    "    \n",
    "    def policy(state):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(0, 4)\n",
    "        return np.argmax(Q[state])\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        # Generate complete episode\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(200):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update Q-values from complete episode\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "            if (s, a) not in visited:\n",
    "                visited.add((s, a))\n",
    "                returns[(s, a)].append(G)\n",
    "                Q[s][a] = np.mean(returns[(s, a)])\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "    \n",
    "    return Q, rewards_history\n",
    "\n",
    "\n",
    "def td_prediction(env, n_episodes, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    TD(0) with Q-values (essentially same as Q-learning for control).\n",
    "    \n",
    "    KEY: Updates EVERY step using bootstrap.\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    rewards_history = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(200):\n",
    "            # Îµ-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(0, 4)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # TD update (same as Q-learning)\n",
    "            td_target = reward + gamma * np.max(Q[next_state])\n",
    "            Q[state][action] += alpha * (td_target - Q[state][action])\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "    \n",
    "    return Q, rewards_history\n",
    "\n",
    "\n",
    "def q_learning(env, n_episodes, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Q-Learning: Off-policy TD control.\n",
    "    \n",
    "    KEY: Uses max Q(s', a') - learns optimal policy.\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    rewards_history = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(200):\n",
    "            # Îµ-greedy behavior policy\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(0, 4)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Q-LEARNING UPDATE: max over next actions\n",
    "            td_target = reward + gamma * np.max(Q[next_state])\n",
    "            Q[state][action] += alpha * (td_target - Q[state][action])\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "    \n",
    "    return Q, rewards_history\n",
    "\n",
    "\n",
    "def sarsa(env, n_episodes, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    SARSA: On-policy TD control.\n",
    "    \n",
    "    KEY: Uses Q(s', a') where a' is actual next action.\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    rewards_history = []\n",
    "    \n",
    "    def policy(state):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(0, 4)\n",
    "        return np.argmax(Q[state])\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        action = policy(state)\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(200):\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_action = policy(next_state)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # SARSA UPDATE: uses actual next action\n",
    "            td_target = reward + gamma * Q[next_state][next_action]\n",
    "            Q[state][action] += alpha * (td_target - Q[state][action])\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "    \n",
    "    return Q, rewards_history\n",
    "\n",
    "\n",
    "print(\"All algorithms implemented!\")\n",
    "print(\"  â€¢ monte_carlo_control() - Episode-based learning\")\n",
    "print(\"  â€¢ td_prediction() - Step-based with bootstrapping\")\n",
    "print(\"  â€¢ q_learning() - Off-policy, finds optimal\")\n",
    "print(\"  â€¢ sarsa() - On-policy, safer exploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparison 1: GridWorld Performance\n",
    "\n",
    "Let's compare all algorithms on the standard GridWorld task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison on GridWorld\n",
    "\n",
    "n_episodes = 2000\n",
    "n_runs = 10  # Average over multiple runs for reliable results\n",
    "\n",
    "algorithms = {\n",
    "    'Monte Carlo': monte_carlo_control,\n",
    "    'Q-Learning': q_learning,\n",
    "    'SARSA': sarsa\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'Monte Carlo': '#1976d2',\n",
    "    'Q-Learning': '#388e3c',\n",
    "    'SARSA': '#7b1fa2'\n",
    "}\n",
    "\n",
    "results_grid = {name: [] for name in algorithms}\n",
    "\n",
    "print(\"GRIDWORLD COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Running {n_runs} trials of {n_episodes} episodes each...\\n\")\n",
    "\n",
    "for name, algo in algorithms.items():\n",
    "    print(f\"  {name}...\", end=\" \", flush=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        env = GridWorld()\n",
    "        _, rewards = algo(env, n_episodes)\n",
    "        results_grid[name].append(rewards)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"done ({elapsed:.1f}s)\")\n",
    "\n",
    "# Calculate means and standard deviations\n",
    "means_grid = {name: np.mean(runs, axis=0) for name, runs in results_grid.items()}\n",
    "stds_grid = {name: np.std(runs, axis=0) for name, runs in results_grid.items()}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GridWorld results\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Learning curves\n",
    "ax1 = axes[0]\n",
    "window = 50\n",
    "\n",
    "for name, mean_rewards in means_grid.items():\n",
    "    smoothed = np.convolve(mean_rewards, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(mean_rewards)), smoothed, \n",
    "             label=name, color=colors[name], linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward (smoothed)', fontsize=12)\n",
    "ax1.set_title('Learning Curves on GridWorld', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Final performance boxplot\n",
    "ax2 = axes[1]\n",
    "final_results = [np.array(results_grid[name])[:, -100:].mean(axis=1) for name in algorithms]\n",
    "bp = ax2.boxplot(final_results, labels=list(algorithms.keys()), patch_artist=True)\n",
    "\n",
    "for patch, name in zip(bp['boxes'], algorithms.keys()):\n",
    "    patch.set_facecolor(colors[name])\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax2.set_ylabel('Average Reward (last 100 episodes)', fontsize=12)\n",
    "ax2.set_title('Final Performance Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nFINAL PERFORMANCE (last 100 episodes):\")\n",
    "print(\"-\"*50)\n",
    "for name in algorithms:\n",
    "    final_avg = np.mean(means_grid[name][-100:])\n",
    "    final_std = np.std([np.mean(r[-100:]) for r in results_grid[name]])\n",
    "    print(f\"{name:15s}: {final_avg:.2f} Â± {final_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparison 2: CliffWalking (Safety Test)\n",
    "\n",
    "This environment highlights the SARSA vs Q-Learning difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison on CliffWalking\n",
    "\n",
    "n_episodes = 500\n",
    "n_runs = 10\n",
    "\n",
    "# Only Q-Learning and SARSA for cliff (MC is slow)\n",
    "cliff_algorithms = {\n",
    "    'Q-Learning': q_learning,\n",
    "    'SARSA': sarsa\n",
    "}\n",
    "\n",
    "results_cliff = {name: [] for name in cliff_algorithms}\n",
    "\n",
    "print(\"CLIFF WALKING COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(\"This environment tests SAFETY during exploration!\\n\")\n",
    "\n",
    "for name, algo in cliff_algorithms.items():\n",
    "    print(f\"  {name}...\", end=\" \", flush=True)\n",
    "    for run in range(n_runs):\n",
    "        env = CliffWalking()\n",
    "        _, rewards = algo(env, n_episodes, alpha=0.5, gamma=1.0)\n",
    "        results_cliff[name].append(rewards)\n",
    "    print(\"done\")\n",
    "\n",
    "means_cliff = {name: np.mean(runs, axis=0) for name, runs in results_cliff.items()}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CliffWalking results\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Learning curves\n",
    "ax1 = axes[0]\n",
    "window = 20\n",
    "\n",
    "for name, mean_rewards in means_cliff.items():\n",
    "    ax1.plot(mean_rewards, alpha=0.3, color=colors[name])\n",
    "    smoothed = np.convolve(mean_rewards, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(mean_rewards)), smoothed, \n",
    "             label=name, color=colors[name], linewidth=3)\n",
    "\n",
    "ax1.axhline(y=-13, color='gray', linestyle='--', alpha=0.5, label='Optimal (no cliff)')\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward', fontsize=12)\n",
    "ax1.set_title('Cliff Walking: Learning Curves\\n(Lower variance = safer)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Variance analysis\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Calculate variance over episodes for each algorithm\n",
    "variances = {}\n",
    "for name, runs in results_cliff.items():\n",
    "    # Variance across runs at each episode\n",
    "    var_per_ep = np.var(runs, axis=0)\n",
    "    # Smooth it\n",
    "    variances[name] = np.convolve(var_per_ep, np.ones(window)/window, mode='valid')\n",
    "\n",
    "for name, var in variances.items():\n",
    "    ax2.plot(range(window-1, len(results_cliff[name][0])), var, \n",
    "             label=name, color=colors[name], linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Variance Across Runs', fontsize=12)\n",
    "ax2.set_title('Reward Variance\\n(Lower = More Consistent)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print analysis\n",
    "print(\"\\nCLIFF WALKING ANALYSIS:\")\n",
    "print(\"-\"*50)\n",
    "for name in cliff_algorithms:\n",
    "    final_mean = np.mean(means_cliff[name][-50:])\n",
    "    final_var = np.mean([np.var(r[-50:]) for r in results_cliff[name]])\n",
    "    print(f\"{name:15s}: Mean = {final_mean:.1f}, Variance = {final_var:.1f}\")\n",
    "\n",
    "print(\"\\nSARSA has LOWER variance (fewer cliff falls!)\")\n",
    "print(\"Q-Learning has HIGHER variance (occasional disasters!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparison 3: Learning Speed\n",
    "\n",
    "How quickly does each algorithm reach good performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze learning speed\n",
    "\n",
    "def episodes_to_threshold(rewards, threshold=-5, window=50):\n",
    "    \"\"\"Find how many episodes to reach threshold performance.\"\"\"\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    above_threshold = np.where(smoothed > threshold)[0]\n",
    "    if len(above_threshold) > 0:\n",
    "        return above_threshold[0] + window\n",
    "    return len(rewards)  # Never reached\n",
    "\n",
    "\n",
    "print(\"LEARNING SPEED ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Episodes needed to reach average reward > -5 on GridWorld:\\n\")\n",
    "\n",
    "speed_results = {name: [] for name in algorithms}\n",
    "\n",
    "for name in algorithms:\n",
    "    for rewards in results_grid[name]:\n",
    "        eps = episodes_to_threshold(rewards)\n",
    "        speed_results[name].append(eps)\n",
    "    \n",
    "    mean_eps = np.mean(speed_results[name])\n",
    "    std_eps = np.std(speed_results[name])\n",
    "    print(f\"{name:15s}: {mean_eps:.0f} Â± {std_eps:.0f} episodes\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "positions = range(len(algorithms))\n",
    "means = [np.mean(speed_results[name]) for name in algorithms]\n",
    "stds = [np.std(speed_results[name]) for name in algorithms]\n",
    "\n",
    "bars = ax.bar(positions, means, yerr=stds, capsize=5, \n",
    "              color=[colors[name] for name in algorithms], \n",
    "              edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(list(algorithms.keys()))\n",
    "ax.set_ylabel('Episodes to Reach Threshold', fontsize=12)\n",
    "ax.set_title('Learning Speed Comparison\\n(Lower = Faster Learning)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, mean) in enumerate(zip(bars, means)):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 30, \n",
    "            f'{mean:.0f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Complete Comparison Table\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                    COMPLETE ALGORITHM COMPARISON                           â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                            â”‚\n",
    "    â”‚  PROPERTY           â”‚ MONTE CARLO  â”‚ TD/Q-LEARNING â”‚ SARSA                â”‚\n",
    "    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n",
    "    â”‚  Update timing      â”‚ End of ep.   â”‚ Every step    â”‚ Every step           â”‚\n",
    "    â”‚  Bootstraps?        â”‚ No           â”‚ Yes           â”‚ Yes                  â”‚\n",
    "    â”‚  Bias               â”‚ None         â”‚ Some (init)   â”‚ Some (init)          â”‚\n",
    "    â”‚  Variance           â”‚ High         â”‚ Lower         â”‚ Lower                â”‚\n",
    "    â”‚  Policy type        â”‚ On-policy    â”‚ Off-policy    â”‚ On-policy            â”‚\n",
    "    â”‚  Learns             â”‚ Q^Ï€          â”‚ Q*            â”‚ Q^Ï€                  â”‚\n",
    "    â”‚  Continuing tasks?  â”‚ No           â”‚ Yes           â”‚ Yes                  â”‚\n",
    "    â”‚  Safety             â”‚ N/A          â”‚ Risky         â”‚ Safe                 â”‚\n",
    "    â”‚  Sample efficiency  â”‚ Lower        â”‚ Higher        â”‚ Higher               â”‚\n",
    "    â”‚                                                                            â”‚\n",
    "    â”‚  BEST USE CASES:                                                          â”‚\n",
    "    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n",
    "    â”‚  Monte Carlo        â”‚ Blackjack, card games, episodic with clear endings  â”‚\n",
    "    â”‚  Q-Learning         â”‚ Video games, simulations, finding optimal policy    â”‚\n",
    "    â”‚  SARSA              â”‚ Robotics, trading, safety-critical applications     â”‚\n",
    "    â”‚                                                                            â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual decision tree\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 11.5, 'How to Choose Your RL Algorithm', ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Decision node 1\n",
    "box1 = FancyBboxPatch((4.5, 9), 5, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                       facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "ax.add_patch(box1)\n",
    "ax.text(7, 9.6, 'Does your task have clear episodes?', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Branches from node 1\n",
    "ax.annotate('', xy=(3.5, 8), xytext=(5.5, 9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.text(4, 8.7, 'No', fontsize=10, color='#d32f2f')\n",
    "\n",
    "ax.annotate('', xy=(10.5, 8), xytext=(8.5, 9),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.text(10, 8.7, 'Yes', fontsize=10, color='#388e3c')\n",
    "\n",
    "# Left branch: No episodes\n",
    "box2a = FancyBboxPatch((1, 6.5), 5, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                        facecolor='#fff3e0', edgecolor='#f57c00', linewidth=2)\n",
    "ax.add_patch(box2a)\n",
    "ax.text(3.5, 7.1, 'Use TD-based methods!', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Right branch: Has episodes\n",
    "box2b = FancyBboxPatch((8, 6.5), 5, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                        facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "ax.add_patch(box2b)\n",
    "ax.text(10.5, 7.1, 'Are episodes short?', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Decision 2a: Safety question\n",
    "ax.annotate('', xy=(2, 5.5), xytext=(2.5, 6.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "ax.annotate('', xy=(5, 5.5), xytext=(4.5, 6.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#666'))\n",
    "\n",
    "box3a = FancyBboxPatch((0.5, 4), 3, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                        facecolor='#e3f2fd', edgecolor='#1976d2', linewidth=2)\n",
    "ax.add_patch(box3a)\n",
    "ax.text(2, 4.6, 'Safety critical?', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Final answers from left branch\n",
    "ax.annotate('', xy=(1, 3), xytext=(1.5, 4),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#388e3c'))\n",
    "ax.text(0.8, 3.5, 'Yes', fontsize=9, color='#388e3c')\n",
    "\n",
    "sarsa_box = FancyBboxPatch((0, 1.5), 2, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor='#f3e5f5', edgecolor='#7b1fa2', linewidth=3)\n",
    "ax.add_patch(sarsa_box)\n",
    "ax.text(1, 2.1, 'ğŸ›¡ï¸ SARSA', ha='center', fontsize=12, fontweight='bold', color='#7b1fa2')\n",
    "\n",
    "ax.annotate('', xy=(3, 3), xytext=(2.5, 4),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f'))\n",
    "ax.text(3.2, 3.5, 'No', fontsize=9, color='#d32f2f')\n",
    "\n",
    "ql_box = FancyBboxPatch((2.5, 1.5), 2.5, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#c8e6c9', edgecolor='#388e3c', linewidth=3)\n",
    "ax.add_patch(ql_box)\n",
    "ax.text(3.75, 2.1, 'ğŸ¯ Q-Learning', ha='center', fontsize=12, fontweight='bold', color='#388e3c')\n",
    "\n",
    "# Right branch: Episode length\n",
    "ax.annotate('', xy=(9, 5.5), xytext=(9.5, 6.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#388e3c'))\n",
    "ax.text(8.8, 6, 'Yes', fontsize=9, color='#388e3c')\n",
    "\n",
    "ax.annotate('', xy=(12, 5.5), xytext=(11.5, 6.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#d32f2f'))\n",
    "ax.text(12.2, 6, 'No', fontsize=9, color='#d32f2f')\n",
    "\n",
    "# Monte Carlo and TD from right branch\n",
    "mc_box = FancyBboxPatch((7.5, 4), 3, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#bbdefb', edgecolor='#1976d2', linewidth=3)\n",
    "ax.add_patch(mc_box)\n",
    "ax.text(9, 4.6, 'ğŸ”¨ Monte Carlo', ha='center', fontsize=12, fontweight='bold', color='#1976d2')\n",
    "\n",
    "td_box = FancyBboxPatch((11, 4), 2.5, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='#fff3e0', edgecolor='#f57c00', linewidth=3)\n",
    "ax.add_patch(td_box)\n",
    "ax.text(12.25, 4.6, 'âš¡ TD', ha='center', fontsize=12, fontweight='bold', color='#f57c00')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### The Four Algorithms\n",
    "\n",
    "| Algorithm | Key Insight | Best For |\n",
    "|-----------|------------|----------|\n",
    "| **Monte Carlo** | Uses actual complete returns | Short episodic tasks, unbiased estimates |\n",
    "| **TD Learning** | Bootstraps from estimates | Continuous tasks, faster learning |\n",
    "| **Q-Learning** | Off-policy, learns optimal Q* | Simulations, games, finding best policy |\n",
    "| **SARSA** | On-policy, accounts for exploration | Safety-critical, real-world systems |\n",
    "\n",
    "### Key Trade-offs\n",
    "\n",
    "| Trade-off | Left Side | Right Side |\n",
    "|-----------|-----------|------------|\n",
    "| **Bias vs Variance** | MC (unbiased, high var) | TD (biased, low var) |\n",
    "| **Safety vs Optimality** | SARSA (safe) | Q-Learning (optimal) |\n",
    "| **Episode vs Step** | MC (end of episode) | TD (every step) |\n",
    "\n",
    "### Quick Decision Guide\n",
    "\n",
    "1. **No clear episodes?** â†’ Use TD/Q-Learning/SARSA\n",
    "2. **Short episodes?** â†’ Monte Carlo is fine\n",
    "3. **Safety matters?** â†’ Use SARSA\n",
    "4. **Want optimal policy?** â†’ Use Q-Learning\n",
    "5. **Need fast learning?** â†’ Use TD methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your Understanding\n",
    "\n",
    "**1. What's the main difference between MC and TD methods?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "MC waits until the end of an episode to update using actual returns (G). TD updates every step using bootstrapped estimates (r + Î³V(s')). MC is unbiased but high variance; TD has some bias but lower variance and can learn faster.\n",
    "</details>\n",
    "\n",
    "**2. Why does Q-Learning find the optimal policy but SARSA doesn't always?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Q-Learning uses max Q(s', a') which assumes optimal future behavior. SARSA uses the actual next action which includes exploration. So Q-Learning learns Q* (optimal value) while SARSA learns Q^Ï€ (value of current Îµ-greedy policy).\n",
    "</details>\n",
    "\n",
    "**3. When would you choose SARSA over Q-Learning?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Choose SARSA when:\n",
    "- Mistakes during learning are costly (robotics, trading)\n",
    "- Safety is more important than finding the absolute optimal policy\n",
    "- You'll continue using Îµ-greedy behavior after training\n",
    "- The environment has \"cliffs\" or dangerous states\n",
    "</details>\n",
    "\n",
    "**4. Why can't Monte Carlo be used for continuing (non-episodic) tasks?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "MC requires complete returns: G = râ‚ + Î³râ‚‚ + Î³Â²râ‚ƒ + ... until episode end. In continuing tasks, there's no end, so you can never compute G. TD methods can update every step using bootstrapping, so they work for any task.\n",
    "</details>\n",
    "\n",
    "**5. If you had to pick ONE algorithm for a new RL problem, which would be the safest choice?**\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Q-Learning is often the best default choice because:\n",
    "- Works for episodic and continuing tasks\n",
    "- Learns the optimal policy\n",
    "- Simple to implement\n",
    "- Fast learning due to bootstrapping\n",
    "\n",
    "But if safety matters, switch to SARSA. If episodes are short and you want unbiased estimates, consider MC.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the **Classic Algorithms** section! You now understand:\n",
    "\n",
    "- âœ… **Monte Carlo** - Learning from complete episodes\n",
    "- âœ… **TD Learning** - Bootstrapping from estimates  \n",
    "- âœ… **Q-Learning** - Off-policy optimal learning\n",
    "- âœ… **SARSA** - On-policy safe learning\n",
    "- âœ… **How to choose** the right algorithm for your problem!\n",
    "\n",
    "These are the foundations that ALL modern RL builds upon!\n",
    "\n",
    "**Next Steps:**\n",
    "- **[Deep RL](../deep-rl/)** - Use neural networks to scale to complex problems\n",
    "- **[Policy Gradient](../policy-gradient/)** - Learn policies directly\n",
    "\n",
    "---\n",
    "\n",
    "*\"Now you have the complete classic RL toolkit. Choose wisely!\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
